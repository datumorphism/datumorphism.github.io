<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Datumorphism</title><link>https://datumorphism.leima.is/</link><description>Recent content on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Sat, 13 Aug 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://datumorphism.leima.is/index.xml" rel="self" type="application/rss+xml"/><item><title>Neural ODE Basics</title><link>https://datumorphism.leima.is/wiki/machine-learning/neural-ode/neural-ode-basics/</link><pubDate>Sat, 13 Aug 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/neural-ode/neural-ode-basics/</guid><description>In [[neural networks]] Artificial Neural Networks Simple artificial neural networks using multilayer perceptron , residual connections is a popular architecture to build very deep neural networks.1 Apart from residual networks, there are many other designs for deep neural networks.23456 These methods share similar ideas that the layered structure in deep neural networks can be treated as a dynamical system and these different architectures are different numerical approaches of solving the dynamical system.
Figure taken from He K, Zhang X, Ren S, Sun J. Deep Residual Learning for Image Recognition. arXiv [cs.CV]. 2015. Available: http://arxiv.org/abs/1512.03385
The degradation problem For deep neural networks, not all deep architectures are able to produce results that are significantly better than shallow architectures.</description></item><item><title>The Time Series Forecasting Problem</title><link>https://datumorphism.leima.is/wiki/forecasting/forecasting-problem/</link><pubDate>Sun, 24 Apr 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/forecasting/forecasting-problem/</guid><description>There are many different types of tasks on time series data:
classification, anomaly detection, forecasting. Forecasting Problem A time series forecasting problem can be formulated as the following.
Given a dataset $\mathcal D$, with
$y^{(i)}_t$, the sequential variable to be forecasted, $x^{(i)}_t$, exogenous data for the time series data, $u^{(i)}_t$, some features that can be obtained or planned in advance, where ${}^{(i)}$ indicates the $i$th variable, ${}_ t$ denotes time. In a forecasting task, we use $y^{(i)} _ {t-K:t}$, $x^{(i) _ {t-K:t}}$, and $u^{(i)} _ {t-K:t+H}$, to forecast the future $y^{(i)} _ {t+1:t+H}$.
A model $f$ will use $x^{(i)} _ {t-K:t}$ and $u^{(i)} _ {t-K:t+H}$ to forecast $y^{(i)} _ {t+1:t+H}$.</description></item><item><title>Prediction Space in Forecasting</title><link>https://datumorphism.leima.is/cards/forecasting/prediction-space/</link><pubDate>Fri, 08 Apr 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/forecasting/prediction-space/</guid><description>In a forecasting problem, we have
$\mathcal P$, the priors, e.g., price and demand is negatively correlated, $\mathcal D$, available dataset, $Y$, the observations, and $F$, the forecasts. Information Set $\mathcal A$
The priors $\mathcal D$ and the available data $\mathcal P$ can be summarized together as the information set $\mathcal A$. Under a probabilistic view, a forecaster will find out or approximate a CDF $\mathcal F$ such that1
$$ \mathcal F(Y\vert \mathcal D, \mathcal P) \to F. $$
Naively speaking, once the density $\rho(F, Y)$ is determined or estimated, a probabilistic forecaster can be formed.</description></item><item><title>Feasibility of Learning</title><link>https://datumorphism.leima.is/wiki/learning-theory/feasibility-of-learning/</link><pubDate>Sun, 17 Oct 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/learning-theory/feasibility-of-learning/</guid><description>Why is learning from data even possible? To discuss this problem, we need a framework for learning. Operationally, we can think of learning as the following framework1.
Abu-Mostafa2012
Naive View Naively speaking, a model should have two key properties,
enough capacity to hold the necessary information embedded in the data, and a method to find the combination of parameters so that the model can generate/complete new data. Most neural networks have enough capacity to hold the necessary information in the data2. The problem is, the capacity is so large. Why does backprop even work? How did backprop find a suitable set of parameters that can generalize?</description></item><item><title>What is Graph</title><link>https://datumorphism.leima.is/wiki/graph/basics/what-is-graph/</link><pubDate>Sat, 25 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/graph/basics/what-is-graph/</guid><description>Graph A graph $\mathcal G$ has nodes $\mathcal V$ and edges $\mathcal E$,
$$ \mathcal G = ( \mathcal V, \mathcal E). $$
Edges
Edges are relations between nodes. For $u\in \mathcal V$ and $v\in \mathcal V$, if there is an edge between them, then $(u, v)\in \mathcal E$. Representations of Graph There are different representations of a graph.
Adjacency Matrix A adjacency matrix of a graph represents the nodes using row and column indices and edges using elements of the matrix.
For simple graph, the adjacency matrix is rank two and dimension $\lvert \mathcal V \rvert \times \lvert \mathcal V \rvert$.</description></item><item><title>An Introduction to Generative Models</title><link>https://datumorphism.leima.is/wiki/machine-learning/generative-models/generative/</link><pubDate>Fri, 13 Aug 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/generative-models/generative/</guid><description>Discriminative model:
The conditional probability of class label on data (posterior) $p(C_k\mid x)$ Generative models:
Likelihood $p(x\mid C_k)$ Sample from the likelihood to generate data With latent variables $z$ and some neural network parameters $\theta$: $P(x,z\mid \theta) = p(x\mid z, \theta)p(z)$</description></item><item><title>Contrastive Model</title><link>https://datumorphism.leima.is/wiki/machine-learning/contrastive-models/contrastive/</link><pubDate>Fri, 13 Aug 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/contrastive-models/contrastive/</guid><description>Contrastive models learn to compare1. Contrastive use special objective functions such as [[NCE]] Noise Contrastive Estimation: NCE Noise contrastive estimation (NCE) objective function is1 $$ \mathcal L = \mathbb E_{x, x^{+}, x^{-}} \left[ - \ln \frac{ C(x, x^{+})}{ C(x,x^{+}) + C(x,x^{-}) } \right], $$ where $x^{+}$ represents data similar to $x$, $x^{-}$ represents data dissimilar to $x$, $C(\cdot, \cdot)$ is a function to compute the similarities. For example, we can use $$ C(x, x^{+}) = e^{ f(x)^T f(x^{+}) }, $$ so that the objective function becomes $$ \mathcal L = \mathbb E_{x, x^{+}, x^{-}} \left[ - \ln \frac{ e^{ … and [[Mutual Information]] Mutual Information Mutual information is defined as $$ I(X;Y) = \mathbb E_{p_{XY}} \ln \frac{P_{XY}}{P_X P_Y}.</description></item><item><title>GAN</title><link>https://datumorphism.leima.is/wiki/machine-learning/adversarial-models/gan/</link><pubDate>Fri, 13 Aug 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/adversarial-models/gan/</guid><description>The task of GAN is to generate features $X$ from some noise $\xi$ and class labels $Y$,
$$\xi, Y \to X.$$
Many different GANs are proposed. Vanilla GAN has a simple structure with a single discriminator and a single generator. It uses the minmax game setup. However, it is not stable to use minmax game to train a GAN model. WassersteinGAN was proposed to solve the stability problem during training1. More advanced GANs like BiGAN and ALI have more complex structures.
Vanilla GAN Minmax Game Suppose we have two players $G$ and $D$, and a utility $v(D, G)$, a minmax game is maximizing the utility $v(D, G)$ for the worst case of $G=\hat G$ that minimizes $v$ then we have to find $D=\hat D$ that maximizes $v$, i.</description></item><item><title>MaxEnt Model</title><link>https://datumorphism.leima.is/wiki/machine-learning/energy-based-model/maxent-energy-based-model/</link><pubDate>Mon, 31 May 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/energy-based-model/maxent-energy-based-model/</guid><description>The Maximum Entropy model, aka MaxEnt model, is a fascinating generative model as it is based on a very intuitive idea from statistical physics - the Principle of Maximum Entropy.
The Idea The essence of the MaxEnt model is that the underlying probability distribution $p(x)$ of the random variables $x$ should
gives the whole system the largest uncertainty, while producing reasonable observables. Uncertainty The uncertainty of the whole system is described by the Shannon entropy based on the probability distributions $p(x)$,
$$ S[p] = -\operatorname{Tr} p(x) \log p(x). $$
The [[Shannon entropy]] Coding Theory Concepts The code function produces code words.</description></item><item><title>Data Engineering for Data Scientists: Checklist</title><link>https://datumorphism.leima.is/wiki/data-engeering-for-data-scientist/checklist/</link><pubDate>Wed, 05 May 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/data-engeering-for-data-scientist/checklist/</guid><description>It is always good for a data scientist to understand more about data engineering, especially the blueprint of a fully productionized data platform.
There are several things to get into:
Connection to Data Sources Connect to DB Connect to Streaming Data Message Queues Connect to Website Scraping [[Node Crawler]] Node Crawler Write a crawler using nodejs API Other Data Services [[Data Storage]] Data Storage Storing big data Data Lake [[Data Warehouse]] Data Warehouse Take care of your data and your data will show you its power.</description></item><item><title>Gibbs Sampling</title><link>https://datumorphism.leima.is/wiki/monte-carlo/gibbs-sampling/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/monte-carlo/gibbs-sampling/</guid><description/></item><item><title>Principles of Design</title><link>https://datumorphism.leima.is/wiki/data-visualization/design/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/data-visualization/design/</guid><description>There are many principles of designing a visual representation of data. However, before we understand how data is represented visually, it would benefit us a lot if we understand the basic principles of designing on 2D surface.
Robin&amp;rsquo;s CRAP Robin Williams proposed the four elements of design:
Contrast Repetition Alignment Proximity Contrast Use some contrast to distinguish the elements of different contents.
Repetition Repeat the design of similar elements on the same page and across pages to make sure the readers learn the meaning of the design quickly.
Alignment Find a strong line and stick to it.</description></item><item><title>Model Selection</title><link>https://datumorphism.leima.is/wiki/model-selection/model-selection/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/model-selection/model-selection/</guid><description>Suppose we have a generating process that generates some numbers based on a distribution. Based on a data sample, we could reconstruct some sort of theoretical models to represent the actual generating process.
Which is a Good Model? (1)The black curve represent the generating process. The red rectangle is a very simple model that captures some major samples. The blue step-wise model is capturing more sample data but with more parameters.
In the above example, the red model on the left is not that good in most cases while the blue model seems to be better. In reality, the choice depends on the usage of the model.</description></item><item><title>Receiver Operating Characteristics: ROC</title><link>https://datumorphism.leima.is/wiki/machine-learning/performance/roc/</link><pubDate>Wed, 13 May 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/performance/roc/</guid><description>ROC space is the two-dimensional space spanned by True Positive Rate and False Positive Rate.
ROC Space. The color boxes are indicating the confusion matrices. Green is the fraction of true positive. Orange is the fraction of false positive. Refer to Confusion Matrix for more details.
AUC: Area under Curve TPR = TP Rate FPR = FP Rate The ROC curve is defined by the relation $f(TPR, FPR)$. Area under the ROC curve is
$$ \int TPR(FPR) d(FPR) \sim \sum_i TPR_i *\Delta FPR. $$
If AUC = 1, we have TP Rate = 1 for all FP Rate.</description></item><item><title>Tree-based Learning</title><link>https://datumorphism.leima.is/wiki/machine-learning/tree-based/overview/</link><pubDate>Wed, 25 Dec 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/tree-based/overview/</guid><description>Decision tree is an easy-to-interpret method in supervised learning. Though simple, it is being used in some widely used algorithms such as random forest method.</description></item><item><title>Embedding</title><link>https://datumorphism.leima.is/wiki/machine-learning/embedding/overview/</link><pubDate>Sun, 13 Oct 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/embedding/overview/</guid><description/></item><item><title>Factorization</title><link>https://datumorphism.leima.is/wiki/machine-learning/factorization/overview/</link><pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/factorization/overview/</guid><description/></item><item><title>Feature Engineering</title><link>https://datumorphism.leima.is/wiki/machine-learning/feature-engineering/overview/</link><pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/feature-engineering/overview/</guid><description/></item><item><title>Naive Bayes</title><link>https://datumorphism.leima.is/wiki/machine-learning/bayesian/naive-bayes/</link><pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/bayesian/naive-bayes/</guid><description>Naive Bayesian is a classifier using [[Bayes&amp;#39; Theorem]] Bayes&amp;#39; Theorem Bayes' Theorem is stated as $$ P(A\mid B) = \frac{P(B \mid A) P(A)}{P(B)} $$ $P(A\mid B)$: likelihood of A given B $P(A)$: marginal probability of A There is a nice tree diagram for the Bayes' theorem on Wikipedia. Tree diagram of Bayes' theorem with &amp;lsquo;naive&amp;rsquo; assumptions.
Problems with Conditional Probability Calculation By definition, the conditional probability of event $\mathbf Y$ given features $\mathbf X$ is $$ \begin{equation} P(\mathbf Y\mid \mathbf X) = \frac{P(\mathbf Y, \mathbf X)}{ P(\mathbf X) }, \label{def-cp-y-given-x} \end{equation} $$
where
$P(\mathbf X)$ is probability of an event having the features $\mathbf X$, $P(\mathbf Y, \mathbf X)$ is the probability of the event $Y$ with features $\mathbf X$.</description></item><item><title>Confusion Matrix (Contingency Table)</title><link>https://datumorphism.leima.is/wiki/machine-learning/basics/confusion-matrix/</link><pubDate>Fri, 31 May 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/basics/confusion-matrix/</guid><description>Confusion Matrix It is much easier to understand the confusion matrix if we use a binary classification problem as an example. For example, we have a bunch of cat photos and the user labeled &amp;ldquo;cute or not&amp;rdquo; data. Now we are using the labeled data to train a cute-or-not binary classifier.
Then we apply the classifier on the test dataset and we would only find four different kinds of results.
Labeled as Cute Labeled as Not Cute Classifier Predicted to be Cute True Positive (TP) False Positive (FP) Classifier Predicted to be Not Cute False Negative (FN) True Negative (TN) This table is easy enough to comprehend.</description></item><item><title>Normal Distribution</title><link>https://datumorphism.leima.is/wiki/distributions/normal-distribution/</link><pubDate>Tue, 22 Jan 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/distributions/normal-distribution/</guid><description>Visualization Math The formula of normal distribution is
$$ \begin{equation} e^{ ( (x - \mu) / \sqrt{2} \sigma )^2 } \end{equation} $$
where $\mu$ controls the &amp;ldquo;center&amp;rdquo; or &amp;ldquo;peak&amp;rdquo; of the distribution and $\sigma$ tells us how &amp;ldquo;wide&amp;rdquo; or &amp;ldquo;disperse&amp;rdquo; the distribution is.
To understand the distribution, we take some limits.
$x = \mu$ First of all, when $x = \mu$ we have
$$ e^0 = 1. $$
Notice the argument of the exponential is some squared value and can not be negative. This condition gives us the peak value.
$x=\mu-a$ and $x=\mu + a$ For $x=\mu-a$, we have</description></item><item><title>Statistical Hypothesis Testing</title><link>https://datumorphism.leima.is/wiki/statistical-hypothesis-testing/hypothesis-testing/</link><pubDate>Sun, 20 Jan 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/statistical-hypothesis-testing/hypothesis-testing/</guid><description>When we have a sample of the population, we immediately calculate the mean using the sample, say the result is $\mu_0$. Of course, the population mean $\mu_p$ is unknown and probably can never be known.
This specific sample mean $\mu_0$ is nothing but like an advanced educated guess. Then again, how do we know if our this specific sample mean $\mu_0$ is a faithful representation of the population mean? In fact, this question is not limited to mean. It applies to any statistical measurement.
The Statistical Estimation Theory Solution to This Problem
In statistical estimation theory, we would tell the sample value with some indication of the degrees of faithfulness.</description></item><item><title>Why Estimation Theory</title><link>https://datumorphism.leima.is/wiki/statistical-estimation/why-estimation-theory/</link><pubDate>Sun, 20 Jan 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/statistical-estimation/why-estimation-theory/</guid><description>In statistics, we work with samples. For example, the sample mean is easily calculated. However, it is the population mean that is more valuable.
Suppose we have one sample $S_i$, which is used to calculate the mean of the sample $\mu_i$. We have two key problems to solve at this moment.
Can we use this sample mean $\mu_i$ to represent the population mean $\mu_p$? How good is our estimations? To answer these questions, we need to work out the properties of the samples themselves and work out a theory to instruct us to infer population statistics from sample statistics.</description></item><item><title>What is Statistics</title><link>https://datumorphism.leima.is/wiki/statistics/what-is-statistics/</link><pubDate>Fri, 18 Jan 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/statistics/what-is-statistics/</guid><description>A Case Study We have a problem.
In our lab, we found a huge amount of similar robots on a planet (physical population). To know more about the weight of these robots (statistical population), we first need to choose some of them (physical sample), then obtain the weight of them (statistical sample).
To describe the data, we could calculate the mean of the weight. We found that the mean weight is 93kg (descriptive statistics).
We could simply give a number to standard for the mean weight of all the robots. (point estimate) We could tell a number as the mean weight of all the robots together with a range that tells us how disperse our measurement is.</description></item><item><title>Association Rules</title><link>https://datumorphism.leima.is/wiki/pattern-mining/association-rules/</link><pubDate>Sun, 06 Jan 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/pattern-mining/association-rules/</guid><description>Association rule is a method for pattern mining. In this article, we perform an association rule analysis of some demo data.
The Problem Defined Suppose we own a store called KIOSK. Here at KIOSK, we sell 4 different things.
Milk Croissant Coffee Fries We need to know what items are associated with each other when the customers are buying.
We have collected the following data. Beware that this small amount of data might not be enough for a real-world problem.
INDEX Items 1 croissant, milk 2 coffee, croissant 3 coffee, croissant 4 coffee, croissant, milk 5 coffee, milk 6 fries, milk 7 coffee, croissant, fries 8 croissant, fries 9 croissant, milk 10 croissant, fries, milk 11 coffee, croissant, milk The Rule, Support, and Confidence The Association rule has three components: the rule, the support, and the confidence.</description></item><item><title>Some Concepts about Data Warehouse</title><link>https://datumorphism.leima.is/wiki/data-warehouse/data-warehouse-concepts/</link><pubDate>Fri, 23 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/data-warehouse/data-warehouse-concepts/</guid><description>The Three Key Ideas about Warehouse The purpose of the data warehouse should be clear. In most cases, it is for the analysis of data, not for data production.1
Subject-oriented: since data warehouses are for decision-makers, arrange them into subjects makes it much easier to access. Integrated: many sources are integrated for easy analysis Time-variant: observation time should be recorded since the data is also used to analyze the time evolution Nonvolatile: simply for analysis OLTP and OLAP OLTP: online transaction processing OLAP: online analytical processing OLTP OLAP user customer data scientist, managers purpose production analysis content everything cleaner data database entity relation model, application-oriented star/snowflake model, subject-oriented history usually no need to record the history history is crucial query short and frequent read and write read-only and but complicated analysis Scope of Data Warehouse Enterprise warehouse: targeting the whole organization Data mart: for a specific group of people Virtual warehouse: views not tables Fact and Dimension Fact is the value of something specified by the dimension.</description></item><item><title>Artificial Neural Networks</title><link>https://datumorphism.leima.is/wiki/machine-learning/neural-networks/artificial-neural-networks/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/neural-networks/artificial-neural-networks/</guid><description>Artificial neural networks works pretty well for solving some differential equations.
Universal Approximators Maxwell Stinchcombe and Halber White proved that no theoretical constraints for the feedforward networks to approximate any measurable function. In principle, one can use feedforward networks to approximate measurable functions to any accuracy.
However, the convergence slows down if we have a lot of hidden units. There is a balance between accuracy and convergence rate. More hidden units lead to slow convergence but more accuracy.
Here is a quick review of the history of this topic.
Kolmogorov’s Theorem
Kolmogorov&amp;rsquo;s theorem shows that one can use a finite number of carefully chosen continuous functions to mix up by sums and multiplication with weights to a continuous multivariable function on a compact set.</description></item><item><title>Ordinary Differential Equations</title><link>https://datumorphism.leima.is/wiki/dynamical-system/ordinary-differential-method/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/dynamical-system/ordinary-differential-method/</guid><description>For a first order differentiation $\frac{\partial f}{\partial t}$, we might have many finite differencing methods.
Euler Method For linear first ODE,
$$ \frac{dy}{dx} = f(x, y), $$
we can discretize the equation using a step size $\delta x \cdot$ so that the differential equation becomes
$$ \frac{y_{n+1} - y_n }{ \delta x } = f(x_n, y_n), $$
which is also written as
$$ y_{n+1} = y_n + \delta x \cdot f(x_n, y_n). \label{euler-method-discretized-form-y-n-plus-1} $$
This is also called forward Euler differencing. It is first order accurate in $\Delta t$.
Generally speaking, a simple iteraction will do the work.
Adams' Method Taylor Expansion of Functions</description></item><item><title>Basics of Computation</title><link>https://datumorphism.leima.is/wiki/computation/basics-of-computation/</link><pubDate>Thu, 13 Sep 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/computation/basics-of-computation/</guid><description>Storage, Precision, Error, etc To have some understanding of how the numbers are processed in computers, we have to understand how the numbers are stored first.
Computers stores everything in binary form 1. Suppose we randomly get some segments in the memory, we have no idea what that stands for since we do not know the type of data it represents.
Some of the most used data types in data science are
integer, float, string, time. Integers Integers can occupy $2^0$, $2^1$, $2^2$, $2^3$ bytes in memory.
1 byte : | | | | | | | | | 2 bytes: | | | | | | | | | | | | | | | | | 4 bytes: | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | .</description></item><item><title>Introduction to Node Crawler Series</title><link>https://datumorphism.leima.is/wiki/nodecrawler/node-crawler-introduction/</link><pubDate>Sun, 15 Jul 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/nodecrawler/node-crawler-introduction/</guid><description>This is a set of tutorials that will help you with your very first crawler with node.js.
The plan of this tutorial is as follows. First of all, we will write a functional crawler using node.js and dump the data into files or simply print it on screen. In the following article, we will use MongoDB as our data management system and organize our data. Then we will optimize and attack some of the pitfalls.
As mentioned, Node.js and MongoDB are required for the full tutorial. Here we link to the articles that talks about the installation and configuration of them.</description></item><item><title>Jupyter Notebook</title><link>https://datumorphism.leima.is/wiki/tools/jupyter/</link><pubDate>Wed, 20 Jun 2018 15:58:49 -0400</pubDate><guid>https://datumorphism.leima.is/wiki/tools/jupyter/</guid><description>Magics %lsmagic will show all the magics, including line magics and cell magics.
Line magics are magics start with one %; Cell magics are magics that can be used in the whole cell even with line breaks, where the cell should start with %%. %env can be used when setting environment variables inside the notebook.
%env MONGO_URI=localhost:27072 %%bash is a cell magic that allows bash commands in the cell.
%%bash ls pip install datahubxyz %%time enables timing of functions.
%%time for i in range(1000): i*i %timeit is the corresponding line magic which times the function of the corresponding line.</description></item><item><title>Regular Expression Basics</title><link>https://datumorphism.leima.is/wiki/sugar/regular-experssions/</link><pubDate>Wed, 20 Jun 2018 15:58:49 -0400</pubDate><guid>https://datumorphism.leima.is/wiki/sugar/regular-experssions/</guid><description>List of Keys Anchors at the beginning of line ^ import re p = re.compile('^T', re.I) line = &amp;quot;The email address is this this the do you see&amp;quot; result = p.findall(line) print(result) # ['T'] at the end of the line $ import re p = re.compile('e$', re.I) line = &amp;quot;The email address is this this the do you see&amp;quot; result = p.findall(line) print(result) # ['e'] Character Classes Printable Characters any character . import re p = re.compile('^T.', re.I) line = &amp;quot;The email address is this this the do you see&amp;quot; result = p.findall(line) print(result) # ['Th'] single character of digit \d word character \w (including alphanumeric character and underscore): import re line = &amp;quot;The email address is this this the do you see&amp;quot; result = re.</description></item><item><title>Short-Time-Fourier-Transform</title><link>https://datumorphism.leima.is/wiki/time-series/short-time-fourier-transform/</link><pubDate>Wed, 20 Jun 2018 15:58:49 -0400</pubDate><guid>https://datumorphism.leima.is/wiki/time-series/short-time-fourier-transform/</guid><description>Short-Time-Fourier-Transform We Fourier transform the time series data using a Fourier transform, with some window function
\begin{equation} \tilde Y[n,k] = \sum_m Y[n+m] W[m] e^{-i \lambda_k m}, \end{equation}
where $\lambda_k=2\pi k/N$ and $W[m]$ is the window function at $m$.
References and Notes Cousera</description></item><item><title>Linear Methods</title><link>https://datumorphism.leima.is/wiki/machine-learning/linear/linear-methods/</link><pubDate>Fri, 25 May 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/linear/linear-methods/</guid><description>Solving Classification Problems with Linear Models One simple idea behind classification is to calculate the posterior probability of each class given the variables.
Suppose a dataset have features $F_\alpha$ where $\alpha = 1, 2, \cdots, K$, with corresponding class labels $G_\alpha$. The dataset that provides $N$ datapoints with each deoted as $X_i$. The posterior of the classification is $P(G = G_\alpha \vert X = X_i)$.
A naive idea is to classify the data into two classes $m$ and $n$ using the boundary of a linear model
$$ P(G = G_\alpha \vert X = X_i) = P(G = G_\beta \vert X = X_i).</description></item><item><title>Machine Learning Overview</title><link>https://datumorphism.leima.is/wiki/machine-learning/overview/</link><pubDate>Fri, 25 May 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/overview/</guid><description>What is Machine Learning Abu-Mostafa, Magdon-Ismail, and Lin summarized machine learning problem using the following chart 1 2. Utilimately, we need to find an approximation $g$ of the true map $f$ from features $\mathcal X$ to targets $\mathcal Y$ on a specific probability distribution of features $P$. This process is done by using an algorithm to select some hypothesis that works.
From the book Learning From Data by Abu-Mostafa, Magdon-Ismail, and Lin. I am using a version by Deckert.
In the core of machine learning models, we have three components3:
Representation: encode data and problem representation, i.</description></item><item><title>Unsupervised Learning</title><link>https://datumorphism.leima.is/wiki/machine-learning/unsupervised/overview/</link><pubDate>Fri, 25 May 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/unsupervised/overview/</guid><description>Unsupervised Learning!
Principle components analysis Clustering K-means Clustering Algorithm:
Assign data points to a group Iterate through until no change: Find centroid Find the point that is closest to the centroids. Assign that data point to the corresponding group of the centroids. How Many Groups
The art of chosing K. Hierarchical Clustering Bottom-up hierarchical groups can be read out from the dendrogram.</description></item><item><title>Some Basic Ideas of Algorithms</title><link>https://datumorphism.leima.is/wiki/algorithms/algorithms-basics/</link><pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/algorithms/algorithms-basics/</guid><description>This set of notes on algorithms is not meant to be comprehensive or complete. These notes are being used as a skeleton framework. There are many useful books to learn about algorithms from a utilitarian point of view. I have listed a few in the references section.
Numerical recipes1 is a very comprehensive book that I used during my PhD. It covers almost all the algorithms you need for scientific computing.
Grokking Algorithms2 is another good book to learn the basics of algorithms. It is barely entry level but is fun to read.
An Outline Data Structure mind the data structure Basics of MapReduce mapreduce numrec Press, W.</description></item><item><title>The C++ Language</title><link>https://datumorphism.leima.is/wiki/programming-languages/cpp/references/</link><pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/programming-languages/cpp/references/</guid><description>C++!
Books The C++ Programming Language Programming Principles and Practice Using C++ The C++ Primer Lectures C++ Beginners Tutorial 1 (For Absolute Beginners) C++ Programming Introduction to C++ Coursear Course: C++ For C Programmers, Part A Top C++ Courses and Tutorials On SoloLearn: C++ Tutorial Practice SoloLearn provides this code playground that we can use to test c++ codes. There is also repl.it
Libraries For solving differential equations:
http://headmyshoulder.github.io/odeint-v2/ http://www.mcs.anl.gov/petsc/ https://github.com/trilinos/Trilinos http://homepage.math.uiowa.edu/~dstewart/meschach/meschach.html http://www.boost.org/ http://www.feynarts.de/cuba/ https://www.gnu.org/software/gsl/ Linear algebra
http://www.simunova.com/mtl4 Comparison of libs
Boost is faster than GSL in terms of random numbers and odeint.</description></item><item><title>The Python Language: Basics</title><link>https://datumorphism.leima.is/wiki/programming-languages/python/basics/</link><pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/programming-languages/python/basics/</guid><description>Numbers, Arithmetics Two types of numbers exist,
int float, 15 digits, other digits are float error It is worth noting that in Python 2, we have
print(1.0/3) # will give us float numbers # 0.333333333333 while
print(1/3) # will only give us int # 0 However, this was changed in Python 3.
Variables, Functions, Conditions A variable name should start with either a letter or an underscore.
Variables defined inside a function is local and there is no way to find it or use it outside the function. It is even possible to reuse an already used global variable inside a function.</description></item><item><title>Curriculum</title><link>https://datumorphism.leima.is/awesome/curriculum/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/awesome/curriculum/</guid><description>Prerequisites Programming Bash: [[all posts with the bash tag]] Bash Python: [[The Python Language]] The Python Language Python as a programming language [[all posts with the python tag]] Python C++: [[C/C&amp;#43;&amp;#43;]] C/C++ [[all posts with the C&amp;#43;&amp;#43; tag]] C++ alternatives:
R Matlab Python Some essential libraries:
Data numpy [[Articles with numpy tag]] numpy scipy [[Articles with scipy tag]] scipy pandas [[Articles with pandas tag]] pandas dask [[PySpark]] Data Processing - (Py)Spark Processing Data using (Py)Spark Visualization matplotlib [[Articles with matplotlib tag]] Matplotlib seaborn plotly and your machine learning libraries Use virtual environments:</description></item><item><title>Time Series Forecasting with Deep Learning</title><link>https://datumorphism.leima.is/wiki/forecasting/forecasting-with-deep-learning/</link><pubDate>Sun, 24 Apr 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/forecasting/forecasting-with-deep-learning/</guid><description>The Encoder-Decoder Framework Many of the models for [[time series forecasting]] The Time Series Forecasting Problem Forecasting time series using deep learning are following some sort of encoder-decoder architecture.
Encoder: $g_{\text{enc}}(x^{(i)} _ {t-K:t}, u^{(i)} _ {t-K:t}) \to z_t$, Decoder: $g_{\text{dec}}(z_t, u^{(i)} _ {t+1: t+H}) \to y_{t+1:t+H}$.</description></item><item><title>Statistics of Graphs</title><link>https://datumorphism.leima.is/wiki/graph/basics/statistics-of-graphs/</link><pubDate>Sat, 25 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/graph/basics/statistics-of-graphs/</guid><description>Local Statistics Node Degree Node Degree Node degree of a node $u$ $$ d_u = \sum_{v\in \mathcal V} A[u,v], $$ where $A$ is the adjacency matrix. Node Centrality Importance of a node on a graph:
Eigenvector Centrality of a Graph Given a graph with adjacency matrix $\mathbf A$, the eigenvector centrality is $$ \mathbf e_u = \frac{1}{\lambda} \sum_{v\in\mathcal V} \mathbf A[u,v] \mathbf e_v, \qquad \forall u \in \mathcal V. $$ Why is it called Eigenvector Centrality The definition is equivalent to $$ \lambda \mathbf e = \mathbf A\mathbf e. $$ Power Iteration The solution to $\mathbf e$ is the eigenvector that corresponds to the largest eigenvalue $\lambda_1$.</description></item><item><title>Contrastive Model: Context-Instance</title><link>https://datumorphism.leima.is/wiki/machine-learning/contrastive-models/context-instance/</link><pubDate>Fri, 13 Aug 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/contrastive-models/context-instance/</guid><description>In contrastive methods, we can manipulate the data to create data entries and infer the changes using a model. These methods are models that &amp;ldquo;predict relative position&amp;rdquo;1. Common tricks are
shuffling image sections like jigsaw, and rotate the image. We can also adjust the model to discriminate the similarities and differences. For example, to generate contrast, we can also use [[Mutual Information]] Mutual Information Mutual information is defined as $$ I(X;Y) = \mathbb E_{p_{XY}} \ln \frac{P_{XY}}{P_X P_Y}. $$ In the case that $X$ and $Y$ are independent variables, we have $P_{XY} = P_X P_Y$, thus $I(X;Y) = 0$.</description></item><item><title>f-GAN</title><link>https://datumorphism.leima.is/wiki/machine-learning/adversarial-models/f-gan/</link><pubDate>Fri, 13 Aug 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/adversarial-models/f-gan/</guid><description>The essence of [[GAN]] GAN The task of GAN is to generate features $X$ from some noise $\xi$ and class labels $Y$, $$\xi, Y \to X.$$ Many different GANs are proposed. Vanilla GAN has a simple structure with a single discriminator and a single generator. It uses the minmax game setup. However, it is not stable to use minmax game to train a GAN model. WassersteinGAN was proposed to solve the stability problem during training1. More advanced GANs like BiGAN and ALI have more complex structures. Vanilla GAN Minmax Game … is comparing the generated distribution $p_G$ and the data distribution $p_\text{data}$.</description></item><item><title>Generative Model: Autoregressive Model</title><link>https://datumorphism.leima.is/wiki/machine-learning/generative-models/autoregressive-model/</link><pubDate>Fri, 13 Aug 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/generative-models/autoregressive-model/</guid><description>An autoregressive (AR) model is autoregressive,
$$ \begin{equation} \log p_\theta (x) = \sum_{t=1}^T \log p_\theta ( x_{t} \mid {x_{&amp;lt;t}} ). \end{equation} $$
In the above example, the likelihood is modeled as
$$ \begin{align} p_\theta (x) &amp;= \Pi_{t=1}^T p_\theta (x_t \mid x_{1:t-1}) \\ &amp;= p_\theta(x_2 \mid x_{1:1}) p_\theta(x_3 \mid x_{1:2}) \cdots p_\theta(x_T \mid x_{1:T-1}) \end{align} $$
Taking the log of it
$$ \ln p_\theta (x) = \sum_{t=1}^T \ln p_\theta (x_t \mid x_{1:t-1}) $$
Notations and Conventions
In AR models, we have to mention the preceding nodes (${x_{&amp;lt;t}}$) of a specific node ($x_{t}$). For $t=5$, the relations between ${x_{&amp;lt;5}}$ and $x_5$ is shown in the following illustration.</description></item><item><title>Poisson Regression</title><link>https://datumorphism.leima.is/wiki/machine-learning/linear/poisson-regression/</link><pubDate>Fri, 07 May 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/linear/poisson-regression/</guid><description>Poisson regression is a generalized linear model for count data.
To model a dataset that is generated from a [[Poisson distribution]] Poisson Process , we only need to model the mean $\mu$ as it is the only parameters. The simplest model we can have for some given features $X$ is a linear model. However, for count data, the effects of the predictors are often multiplicative. The next simplest model we can have is
$$ \mu = \exp\left(\beta X\right). $$
The $\exp$ makes sure that the mean is positive as this is required for count data.</description></item><item><title>Principles of Colors</title><link>https://datumorphism.leima.is/wiki/data-visualization/colors/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/data-visualization/colors/</guid><description>ColorTeller
I wrote a python package called colorteller to help us manage and benchmark colors. Basic Concepts of Colors Color Wheel and Color Sphere There are two dimensions in the color wheel:
Hue Saturation When we add another dimension, lightness, to the wheel, we have a color sphere (1, 2).
Many color systems have been invented. Color wheel and color sphere are two examples of them.</description></item><item><title>Goodness-of-fit</title><link>https://datumorphism.leima.is/wiki/model-selection/goodness-of-fit/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/model-selection/goodness-of-fit/</guid><description>Does the data agree with the model?
Calculate the distance between data and model predictions. Apply Bayesian methods such as likelihood estimation: likelihood of observing the data if we assume the model; the results will be a set of fitting parameters. &amp;hellip; Why don&amp;rsquo;t we always use goodness-of-fit as a measure of the goodness of a model?
We may experience overfitting. The model may not be intuitive. This is why we would like to balance it with parsimony using some measures of generalizability.
K-means and overfitting
The overfitting problem is easily demonstrated using the K-means model.</description></item><item><title>Data Types and Level of Measurement in Machine Learning</title><link>https://datumorphism.leima.is/wiki/machine-learning/feature-engineering/data-types/</link><pubDate>Wed, 15 Jan 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/feature-engineering/data-types/</guid><description>Types of Data There are several debatable categorization methods of data.
The first widely spread theory, or level of measurement, is by S. Stevens. The theory categorizes data into four types, nominal, ordinal, interval, and ratio.
Other methods are proposed for other fields of research. For example, N. R. Chrisman proposed a different method for cartography. However, these are not generic enough for data science. They are more general than a specific field of research.
For machine learning, many statistical data types have been proposed. Some examples of data types and their relations with the level of measurement are shown in the following chart.</description></item><item><title>Decision Tree</title><link>https://datumorphism.leima.is/wiki/machine-learning/tree-based/decision-tree/</link><pubDate>Wed, 25 Dec 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/tree-based/decision-tree/</guid><description>In this article, we will explain how decision trees work and build a tree by hand.
The code used in this article can be found in this repo. Definition of the problem We will decide whether one should go to work today. In this demo project, we consider the following features.
feature possible values health 0: feeling bad, 1: feeling good weather 0: bad weather, 1: good weather holiday 1: holiday, 0: not holiday For more compact notations, we use the abstract notation $\{0,1\}^3$ to describe a set of three features each with 0 and 1 as possible values.</description></item><item><title>Bayesian Linear Regression</title><link>https://datumorphism.leima.is/wiki/machine-learning/bayesian/bayesian-linear-regression/</link><pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/bayesian/bayesian-linear-regression/</guid><description>Linear Regression and Likelihood The linear estimator $y$ is
$$ \begin{equation} y^n = \beta^m X_m^{\phantom{m}n}. \label{eq-linear-model} \end{equation} $$
As usual, we have redefined our data to get rid of the intercept $\beta^0$.
In ordinary linear models, we find the error being the difference between the target $\hat y$ and the estimator $y$
$$ \epsilon = \hat y - y, $$
which is required to have a minimum absolute value.
In linear regressions, we use least squares to solve the problem. In Bayesian linear regression, instead of using a deterministic estimator $\beta^m X_m^{\phantom{m}n}$, we assume a Gaussian random estimator
$$ \begin{equation} \mathcal{N}(\mu, \sigma^2) = \mathcal{N}(\beta^m X_m^{\phantom{m}n}, \sigma^2), \end{equation} $$</description></item><item><title>NMF: Nonnegative Matrix Factorizatioin</title><link>https://datumorphism.leima.is/wiki/machine-learning/factorization/nmf/</link><pubDate>Thu, 13 Jun 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/factorization/nmf/</guid><description>Decomposition For simplicity, we start with a data point $\mathbf P$ in a $k$-dimensional space spanned by $k$ basis vectors $\mathbf V^k$. Naturally, we could write down the component decomposition of the point using the basis vectors $\mathbf V^k$,
$$ \mathbf P = P_k \mathbf V^k. $$
This is immediately obvious to us since we have been dealing with rank 2 $(k, 1)$ basis vectors and we are talking about the $k$ coordinates for a point.
This point is represented by a matrix of rank 2 $(k, 1)$ given this basis.
$$ \mathbf P \to \begin{pmatrix} P_1, P_2, \cdots, P_k \end{pmatrix} $$</description></item><item><title>Word2vec</title><link>https://datumorphism.leima.is/wiki/machine-learning/embedding/word2vec/</link><pubDate>Thu, 13 Jun 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/embedding/word2vec/</guid><description>Word2vec is a word embedding model that learns the probability of some words being neighbours in a sentence $p_{neighbours}(w_i, w_o)$.
Build a dataset of adjacent words. CBOW; skipgram; negative sampling; Encode the words using vectors. Build a model $f(\{\theta_i\})$ to calculate the probability of the words being neighours and improve the parameters $\{\theta_i\}$ using the dataset.</description></item><item><title>Bias-Variance</title><link>https://datumorphism.leima.is/wiki/machine-learning/basics/bias-variance/</link><pubDate>Fri, 07 Jun 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/basics/bias-variance/</guid><description>Bias and Variance Suppose $f(X)$ is a perfect model that represents a &amp;ldquo;tight&amp;rdquo; model of the dataset $(X,Y)$ but some irredicible error $\epsilon$,
$$ \begin{equation} Y = f(X) + \epsilon. \label{dataset-using-true-model} \end{equation} $$
On the other hand, we build another model using a specific method such as k-nearest neighbors, which is denoted as $k(X)$.
Why the two models?
Why are we talking about the perfect model and a model using a specific method?
The perfect model $f(X)$ is our ultimate goal, while the model using a specific method $k(X)$ is our effort of approaching the ultimate model.
The bias measures the deficit between $k(X)$ and the perfect model $f(X)$,</description></item><item><title>Types of Errors in Statistical Hypothesis Testing</title><link>https://datumorphism.leima.is/wiki/statistical-hypothesis-testing/type-1-error-and-type-2-error/</link><pubDate>Fri, 31 May 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/statistical-hypothesis-testing/type-1-error-and-type-2-error/</guid><description>Type I and Type II Errors In statistical hypothesis testing, we always have a null hypothesis $H_0$ which refers to the statement to be tested. We have two possible conclusions from a hypothesis testing,
to accept the hypothesis, that is concluding that $H_0$ is true, to reject the hypothesis, that is concluding that $H_0$ is false. However, it is possible that our conclusion is not correct. There are four possible results.
$H_0$ is True (Ground Truth) $H_0$ is False (Ground Truth) Accept $H_0$ (after hypothesis testing) Correct Type II Error Reject $H_0$ (after hypothesis testing) Type I Error Correct We could tell that there are two types of errors:</description></item><item><title>Amazon CloudWatch Logs</title><link>https://datumorphism.leima.is/wiki/tools/awslogs/</link><pubDate>Mon, 11 Mar 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/tools/awslogs/</guid><description>Why Suppose we have all kinds of pipelines written in different languages, using different tools, and located in different places. It would be frustrating to pull out the logs.
This is why we need a centralized log service, for example cloudwatch.
Sending logs to CloudWatch First of all, send your logs to awslogs. The easies way is to use boto.
Retrieving and Analyzing Logs First of all, we need this: awslogs. With the logs sent to cloudwatch, we then could read out the logs using the following command:
awslogs get etl-tools --start='1d ago' --timestamp --output text | grep error This will print out the logs 1 day ago.</description></item><item><title>Confidence Interval</title><link>https://datumorphism.leima.is/wiki/statistical-estimation/confidence-interval/</link><pubDate>Sun, 20 Jan 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/statistical-estimation/confidence-interval/</guid><description>We will use upper cases for the abstract variable and lower cases for the actual numbers.
Why is Confidence Interval Needed? Suppose I sample the population multiple times, the mean value $\mu_i$ of the sample is calculated for each sample. It is a good question to ask how different these $\mu_i$ are compared to the true mean $\mu_p$ of the population.
In this article, we would need to specify several notations.
$X$ is the quantity we are measuring. $\bar X$ is the mean of the quantity $X$. Confidence Interval This theorem states that the probability for the true mean $\mu_p$ to fall into a specific range can be calculated using</description></item><item><title>Jargons</title><link>https://datumorphism.leima.is/wiki/statistics/jargons/</link><pubDate>Sat, 24 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/statistics/jargons/</guid><description>Accuracy and Precision Accuracy: the measurement compared to the truth Precision: variability of repeated measurements; the more precise, the less variations during each measurement. Accurate Inaccurate Precise Close to true value, small variations in each measurement Far from true value, small variations in each measurement Imprecise Close to true value, large variations in each measurement Far from true value, large variations in each measurement Here is an example. Suppose we have a huge population (with true mean $M_0$) and we draw samples from it. For the first time, we have sample $S_1$.</description></item><item><title>Extract, Transform and Load</title><link>https://datumorphism.leima.is/wiki/data-warehouse/extract-transform-load/</link><pubDate>Fri, 23 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/data-warehouse/extract-transform-load/</guid><description>ETL Process ETL
ETL
Extract: extract data from sources Transform: transform it to proper format Load: load it to data storage infrastructure E for Extract Should not affect the source system. T for Transform Cleaning Filtering Enriching Splitting Joining L for Load Deal with sync and waiting</description></item><item><title>Partial Differential Equations</title><link>https://datumorphism.leima.is/wiki/dynamical-system/partial-difference-method/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/dynamical-system/partial-difference-method/</guid><description>Forward Time Centered Space For $\frac{d f}{d t} = - v \frac{ d f }{ dx }$, we write down the finite difference form 1
$$ \frac{f(t_{n+1}, x_i ) - f(t_n, x_i)}{ \Delta t } = - v \frac{ f(t_n, x_{i+1}) - f(t_n, x_{i-1}) }{ 2\Delta x }. $$
FTCS is an explicit method and is not stable.
Lax Method Change the term $f(t_n, x_i)$ in FTCS to $( f(t_n, x_{i+1}) + f(t_n, x_{i-1}) )/2$ 1.
Stability condition is
$$ \frac{ \lvert v \rvert \Delta t }{ \Delta x } \leq 1, $$
which is the Courant-Fridriches-Lewy stability criterion.
Staggered Leapfrog $$ \frac{f(t_{n+1}, x_i) - f(t_{n-1}, x_i)}{2 \Delta t} = -v \frac{ f(t_n, x_{i+1} ) - f(t_n, x_{i-1} ) }{ 2\Delta x} $$</description></item><item><title>Basics of Programming</title><link>https://datumorphism.leima.is/wiki/computation/basics-of-programming/</link><pubDate>Sun, 23 Sep 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/computation/basics-of-programming/</guid><description>Recursive and Iterative Solving problems with iterative and recursive methods are two quite different approaches, somehow, to the same kind of problems.
Here we will calculate the factorial of $n$. We define two functions using the iterative method and the recursive method.
Run the program on Repl.it.
def recursiveFactorial(n): if n == 0: return 1 else: return n * recursiveFactorial(n - 1) def iterativeFactorial(n): ans = 1 i=1 while i &amp;lt;= n: ans = ans * i i=i+1 return ans print(recursiveFactorial(0)) print(iterativeFactorial(0))</description></item><item><title>Basic Node Crawler</title><link>https://datumorphism.leima.is/wiki/nodecrawler/basic-crawler/</link><pubDate>Sun, 15 Jul 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/nodecrawler/basic-crawler/</guid><description>Prerequisites Nodejs &amp;gt;= 8.9 Overview A model for a crawler is as follows.
A crawler requests data from the server, while the server responds with some data. Here is a graphic illustration
+----------+ +-----------+ | | HTTP Request | | | +----------------&amp;gt; | | Nodejs | | Servers | | &amp;lt;----------------+ | | | HTTP Response | | +----------+ +-----------+ HTTP Requests For a good introduction of HTTP requests, please refer to this video on youtube: Explained HTTP, HTTPS, SSL/TLS API As for the first step, we need to find which url to request.</description></item><item><title>Autoregressive Model</title><link>https://datumorphism.leima.is/wiki/time-series/autoregressive-model/</link><pubDate>Wed, 20 Jun 2018 15:58:49 -0400</pubDate><guid>https://datumorphism.leima.is/wiki/time-series/autoregressive-model/</guid><description>Autoregressive Given a time series ${T^i}$, a simple predictive model can be constructed using an autoregressive model.
$$ \begin{equation} T^t = \sum_{i=1}^p \beta_i T^{t - i} + \beta^t + \beta^0. \end{equation} $$
Such a model is usually called an AR(p) model due to the fact that we are using data back in $p$ steps.
Differential Equation For simplicity we will look at a AR(1) model. Assume the time series has a step size of $dt$, our model can be rewritten as
$$ T^t = \beta_1 T^{t - 1} + \beta^t + \beta^0 $$
which can be rewritten in the following way</description></item><item><title>Unsupervised Learning: PCA</title><link>https://datumorphism.leima.is/wiki/machine-learning/unsupervised/pca/</link><pubDate>Fri, 25 May 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/unsupervised/pca/</guid><description>We use the Einstein summation notation in this article. Principal Component Analysis (PCA) is a commonly used trick for dimensionality reduction so that the new features represents most of the variances of the data.
Representations of Dataset In theory, a dataset can be represented by a matrix if we specify the basis. However, the initial given basis is not always the most convinient one. Suppose we find a new set of basis for the dataset, the matrix representation may be simpler and easier to use.
For convenience, we do not distinguish the representation and the abstract dataset in this article.</description></item><item><title>Data Structure</title><link>https://datumorphism.leima.is/wiki/algorithms/data-structure/</link><pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/algorithms/data-structure/</guid><description>Dealing with data structure is like dealing with your clothes. Some people randomly drop their clothes somewhere without thinking. But it takes time to retrieve a specific T-shirt. Some people spend more time folding and arranging their clothes. This process makes it easy to find a specific T-shirt. Similar to retrieving clothes, there is always a balance between the computation time (retrieving clothes) and the coding time (folding clothes).
Some Useful Data Structures This section serves as some kind of flashcard keywords. I am using this section to remind myself of the important concepts.
Binary Tree [[Tree]] Data Structure: Tree mind the data structure: here comes the tree ; Binary tree Traverse a tree: Pre-order traversal: parent-&amp;gt;left-&amp;gt;right In-orer traversal: left-&amp;gt;parent-&amp;gt;right Post-order traversal: left-&amp;gt;right-&amp;gt;parent Level-order traversal: top-&amp;gt;bottom, by each level from left to right of the whole tree Array Suppose I bought 5 movie tickets for the movie Tenet.</description></item><item><title>The C++ Language: Basics</title><link>https://datumorphism.leima.is/wiki/programming-languages/cpp/basics/</link><pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/programming-languages/cpp/basics/</guid><description>Make it Work Apart from the traditional way of running C++ code, Jupyter notebook has a clingkernel that make it possibel to run C++ in a Jupyter notebook. Here is the post: Interactive C++ for HPC.
Concepts Namespace Operators: assignment operators (=,+=,-=,*=,/=,%=), increment/decrement operator (++x,x++,--x,x--), relational operators (&amp;gt;,&amp;lt;,&amp;gt;=,&amp;lt;=,==,!=), logicl operators (&amp;amp;&amp;amp;,||,!), left shift (&amp;lt;&amp;lt;), extration operator (&amp;gt;&amp;gt;, or right shift), understand the operator precedence Variables: variable name starts with underscore or latin letters, Pascal case (PascalCase), Camel case (pascalCase) if/else and Loops: if (condition is true ){ then something } Data Types: string (double quote), character (char, 1 byte ASCII character, using single quote), float (4 bytes, always signed), double (8 bytes, always signed), long double (8 or 16 bytes, always signed), singed or unsigned short or long int (signed long int, unsigned int) Pointers: ampersand (&amp;amp;) accesses the address, pointer is variable thus needs to be declared using asterisk (*), can be declared to be int or double or float or char (int \*pt; int\* pt; int * pt;) Functions: overload, recursion Class: identity, atrributes, method/behavior, access specifiers (private or public or protected, by default it is set to private), instantiation of object (creating object), constructor, destructor, encapsulation, scope resolution operator (TheClassYouNeed::somefunction()), selection operator (dot member selection .</description></item><item><title>The Python Language: Decorators</title><link>https://datumorphism.leima.is/wiki/programming-languages/python/decorators/</link><pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/programming-languages/python/decorators/</guid><description>Functions: first-class objects; can be passed around as arguments.
What that tells us about is that functions can be pass into a function or even returned by a function. For example,
def a_decoration_function( yet_another_function ): def wrapper(): print(&amp;#39;Before yet_another_function&amp;#39;) yet_another_function() print(&amp;#39;After yet_another_function&amp;#39;) return wraper def yet_another_function(): print(&amp;#39;This is yet_another_function&amp;#39;) When we execute a_decoration_function, we will have
Before yet_another_function This is yet_another_function After yet_another_function So a decorator is simply a function that takes a function as an argument, adds some salt to it.
To use the decorator, we simply use @
@a_decoration_function def my_function(): print(&amp;#39;This is my function&amp;#39;) my_function() This piece of code will return the decorated function.</description></item><item><title>A Physicist's Crash Course on Artificial Neural Network</title><link>https://datumorphism.leima.is/wiki/machine-learning/neural-networks/physicists-crash-course-neural-network/</link><pubDate>Sat, 02 May 2015 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/neural-networks/physicists-crash-course-neural-network/</guid><description>What is a Neuron What a neuron does is to response when a stimulation is given. This response could be strong or weak or even null. If I would draw a figure, of this behavior, it looks like this.
Neuron response Using simple single neuron responses, we could compose complicated responses. To achieve that, we study the transformations of the response first.
transformations Artificial Neural Network A simple network is a collection of neurons that response to stimulations, which could be the responses of other neurons.
neural network A given input signal is spreaded onto three different neurons.</description></item><item><title>Workflows</title><link>https://datumorphism.leima.is/awesome/workflows/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/awesome/workflows/</guid><description>The scope of exploratory data analysis is not universally defined. Some of the contents discussed here may have crossed the line. The whole modeling process is never decoupled anyway. Data wrangling is mostly guided by the exploratory data analysis (EDA). In other words, the data cleaning process should be mostly guided by questions from business and stakeholder or out of curiosity.
There are three key components in EDA.
Clearly state the purpose of this EDA. Are we asking the right question? Does the dataset fit in memory or shall I use distributed preprocessing? Is the dataset good enough to solve the problem?</description></item><item><title>Evaluating Time Series Models</title><link>https://datumorphism.leima.is/wiki/forecasting/evalutate-time-series-models/</link><pubDate>Fri, 08 Apr 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/forecasting/evalutate-time-series-models/</guid><description>Evaluating time series models is usually different from most other machine learning tasks as we usually don&amp;rsquo;t have i.i.d. data.
Out-of-sample Out-of-Sample with Sliding Window
If the sliding window size is 1, then we have the simplest out-of-sample holdout scenario.
Prequential Prequential with Gap
Prequential with Growing Train
Prequential with Sliding Blocks
Cross-validation Cross-validation
Cross-validation with Neighbor removed</description></item><item><title>Conformal Prediction</title><link>https://datumorphism.leima.is/wiki/statistical-estimation/conformal-prediction/</link><pubDate>Fri, 01 Apr 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/statistical-estimation/conformal-prediction/</guid><description>Conformal prediction is a method to predict a consistent confidence interval in an on-line setting. The algorithms is following the [[Neyman-Pearson hypothesis testing]] Neyman-Pearson Theory The Neyman-Pearson hypothesis testing tests two hypothesis, hypothesis $H$, and an alternative hypothesis $H_A$. Neyman-Pearson Lemma The Neyman-Pearson Lemma is an very intuitive lemma to understand how to choose a hypothesis. The lecture notes from PennState is a very good read on this topic1. An example For simplicity, we assume that there exists a test statistic $T$ and $T$ can be used to measure how likely the hypothesis $H$ is true, e.g., the hypothesis $H$ is false, corresponds to $T$ … framework, thus providing solid theoretical support for the predicted region.</description></item><item><title>Graphs Spectral Methods</title><link>https://datumorphism.leima.is/wiki/graph/basics/graph-spectral-methods/</link><pubDate>Sat, 25 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/graph/basics/graph-spectral-methods/</guid><description>The [[Ratio Cut]] Graph Cuts Cut For a subset of nodes $\mathcal A\subset \mathcal V$, the rest of nodes can be denoted as $\bar {\mathcal A} = \mathcal V \setminus \mathcal A$. In other words, $\mathcal A \cup \bar {\mathcal A} = \mathcal V$ and $\mathcal A \cap \bar {\mathcal A} = \emptyset$. That being said, the nodes can be partitioned into two subsets, $\mathcal A$ and $\bar {\mathcal A}$. The cut of this partition is defined as the total number of edges between them, $$ \operatorname{Cut} \left( \mathcal A, … is closely related to the [[Graph Laplacians]] Graph Laplacians Laplacian is a useful representation of graphs.</description></item><item><title>Contrastive Model: Instance-Instance</title><link>https://datumorphism.leima.is/wiki/machine-learning/contrastive-models/instance-instance/</link><pubDate>Fri, 13 Aug 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/contrastive-models/instance-instance/</guid><description>It was discovered that the success of [[mutual information based contrastive learning]] Contrastive Model: Context-Instance In contrastive methods, we can manipulate the data to create data entries and infer the changes using a model. These methods are models that &amp;ldquo;predict relative position&amp;rdquo;1. Common tricks are shuffling image sections like jigsaw, and rotate the image. We can also adjust the model to discriminate the similarities and differences. For example, to generate contrast, we can also use [[Mutual Information]] Mutual Information Mutual information is defined as $$ I(X;Y) = \mathbb E_{p_{XY}} … is more related to the encoder architecture and the negative sampling strategy1.</description></item><item><title>Generative Model: Flow</title><link>https://datumorphism.leima.is/wiki/machine-learning/generative-models/flow/</link><pubDate>Fri, 13 Aug 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/generative-models/flow/</guid><description>For a probability density $p(x)$ and a transformation of coordinate $x=g(z)$ or $z=f(x)$, the density can be expressed using the coordinate transformations, i.e.,
$$ \begin{align} p(x) &amp;= \tilde p (f(x)) \lvert \operatorname{det} \operatorname{D} g(f(x)) \rvert^{-1} \\ &amp;= \tilde p(f(x)) \lvert \operatorname{det}\operatorname{D} f(x) \rvert \end{align} $$
where the Jacobian is
$$ \operatorname{D} g(z) \to \frac{\partial }{\partial z} g. $$
The operation $g _ { * }\circ \tilde p(z)$ is the push forward of $\tilde p(z)$. The operation $g _ { * }$ will pushforward simple distribution $\tilde p(z)$ to a more complex distribution $p(x)$.
The generative direction: sample $z$ from distribution $\tilde p(z)$, apply transformation $g(z)$; The normalizing direction: &amp;ldquo;simplify&amp;rdquo; $p(x)$ to some simple distribution $\tilde p(z)$.</description></item><item><title>infoGAN</title><link>https://datumorphism.leima.is/wiki/machine-learning/adversarial-models/infogan/</link><pubDate>Fri, 13 Aug 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/adversarial-models/infogan/</guid><description>In GAN, the latent space input is usually random noise, e.g., Gaussian noise. The objective of [[GAN]] GAN The task of GAN is to generate features $X$ from some noise $\xi$ and class labels $Y$, $$\xi, Y \to X.$$ Many different GANs are proposed. Vanilla GAN has a simple structure with a single discriminator and a single generator. It uses the minmax game setup. However, it is not stable to use minmax game to train a GAN model. WassersteinGAN was proposed to solve the stability problem during training1. More advanced GANs like BiGAN and ALI have more complex structures. Vanilla GAN Minmax Game … is a very generic one.</description></item><item><title>Logistic Regression</title><link>https://datumorphism.leima.is/wiki/machine-learning/linear/logistic-regression/</link><pubDate>Thu, 27 May 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/linear/logistic-regression/</guid><description>In a classification problem, given a list of features values $x$ and their corresponding classes $\{c_i\}$, the posterior for of the classes, aka conditional probability of the classes, is
$$ p(C=c_i\mid X=x). $$
Likelihood
The likelihood of the data is
$$ p(X=x\mid C=c_i). $$
Logistic Regression for Two Classes For two classes, the simplest model for the posterior is a linear model,
$$ \log \frac{p(C=c_1\mid X=x) }{p(C=c_2\mid X=x)} = \beta_0 + \beta_1 \cdot x, $$
which is equivalent to
$$ p(C=c_1\mid X=x) = \exp\left(\beta_0 + \beta_1 \cdot x\right) p(C=c_2\mid X=x) . $$
Why
The reason that we proposing a linear model for the quantity</description></item><item><title>VC Dimension</title><link>https://datumorphism.leima.is/wiki/learning-theory/vc-dimension/</link><pubDate>Sun, 21 Feb 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/learning-theory/vc-dimension/</guid><description>Two of the key elements in a learning problem are:
a set of hypothesis $\mathcal H$, and a set of data samples $\mathcal S$. $\mathcal H$
Inside $\mathcal H$, we have a lot of hypotheses, for example, $\mathcal h$. Given some input, e.g., $x_1$ and $x_2$, we can produce some outputs, e.g., $h(x_1)$ and $h(x_2)$. $\mathcal S$
A sample $\mathcal S$ is a fair sample drawn from all the possible inputs $\mathcal X$, where $\mathcal X$ is called the input space. A dataset $\mathcal S$ can be used as a probe of the hypothesis set $\mathcal H$.</description></item><item><title>Measures of Generalizability</title><link>https://datumorphism.leima.is/wiki/model-selection/measures-of-generalizability/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/model-selection/measures-of-generalizability/</guid><description>To measure the generalization, we define a generalization error,
$$ \begin{align} \mathcal G = \mathcal L_{P}(\hat f) - \mathcal L_E(\hat f), \end{align} $$
where $\mathcal L_{P}$ is the population loss, $\mathcal L_E$ is the empirical loss, and $\hat f$ is our model by minimizing the empirical loss.
However, we do not know the actual joint probability $p(x, y)$ of our dataset $\{x_i, y_i\}$. Thus the population loss is not known. In machine learning, we usually use [[cross validation]] Cross Validation Cross validation is a method to estimate the [[risk]] The Learning Problem The learning problem posed by Vapnik:1 Given a sample: $\{z_i\}$ in the probability space $Z$; Assuming a probability measure on the probability space $Z$; Assuming a set of functions $Q(z, \alpha)$ (e.</description></item><item><title>Random Forest</title><link>https://datumorphism.leima.is/wiki/machine-learning/tree-based/random-forest/</link><pubDate>Wed, 25 Dec 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/tree-based/random-forest/</guid><description>Random forest is an ensemble method based on decision trees. Instead of using one decision tree and model on all the features, the decision tree method can model on a random set of features (feature subspace) using many decision trees and make decisions by democratizing the trees.
Given a proper dataset $\mathscr D(\mathbf X, \mathbf y)$, the ensemble of trees is denoted as ${f_i(\mathbf X)}$, will predict an ensemble of results. There are several key ideas in random forests.
Are the predicted results representative? How to democratize the ensemble of results? What determines the quality of the predictions? Margin, Strength, and Correlations The margin of the model, the strength of the trees, and the correlation between the trees are crucial to answer the questions.</description></item><item><title>Predictions Using Time Series Data</title><link>https://datumorphism.leima.is/wiki/time-series/predictions-time-series-data/</link><pubDate>Fri, 21 Jun 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/time-series/predictions-time-series-data/</guid><description>General Phenological Model for Seasonality In business, time series data $f(t)$ usually carries information about trend $g(t)$ ($g$ is used since trend is usually growth), seasonalities (periodical effects) $p(t)$, holiday effects (structural effects) $s(t)$, etc. We will decompose a time series $f(t)$ into four components
$$ \begin{equation} f(t) = g(t) + p(t) + s(t) + \epsilon(t). \end{equation} $$
To train a model for the predictions, we need to write down the exact models of these three predictable components.</description></item><item><title>Tensor Factorization</title><link>https://datumorphism.leima.is/wiki/machine-learning/factorization/tensor-factorization/</link><pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/factorization/tensor-factorization/</guid><description>Tensors We will be talking about tensors but we will skip the introduction to tensor for now.
In this article, we follow a commonly used convention for tensors in physics, the abstract index notation. We will denote tensors as $T^{ab\cdots}_ {\phantom{ab\cdots}cd\cdots}$, where the latin indices such as $^{a}$ are simply a placebo for the slot for this &amp;ldquo;tensor machine&amp;rdquo;. For a given basis (coordinate system), we can write down the components of this tensor $T^{\alpha\beta\cdots} _ {\phantom{\alpha\beta\cdots}\gamma\delta\cdots}$.
Okay, But Why
What is usually seen in blog posts is the use of component forms of tensors, $T^{\alpha\beta\cdots}_{\phantom{\alpha\beta\cdots}\gamma\delta\cdots}$. Those are the numbers for a given basis.</description></item><item><title>Anscombe's quartet</title><link>https://datumorphism.leima.is/wiki/data-visualization/anscombes-quartet/</link><pubDate>Mon, 18 Mar 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/data-visualization/anscombes-quartet/</guid><description>Anscombe&amp;rsquo;s Quartet Anscombe&amp;rsquo;s quartet is a brilliant idea that shows the importance and convenience of visual representation of data.
Anscombe&amp;rsquo;s quartet has four datasets. The values of each dataset are shown below.
x1 = [10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5] y1 = [8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68] x2 = [10.0, 8.0, 13.0, 9.0, 11.0, 14.0, 6.0, 4.0, 12.0, 7.0, 5.0] y2 = [9.14, 8.14, 8.74, 8.77, 9.26, 8.1, 6.13, 3.1, 9.13, 7.26, 4.74] x3 = [10.0, 8.0, 13.0, 9.0, 11.0, 14.0, 6.0, 4.0, 12.0, 7.0, 5.0], y3 = [7.46, 6.</description></item><item><title>OLAP Operations</title><link>https://datumorphism.leima.is/wiki/data-warehouse/olap-operations/</link><pubDate>Fri, 23 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/data-warehouse/olap-operations/</guid><description>Roll-up or Drill-up The word &amp;lsquo;up&amp;rsquo; in the names refers to going up in concept hierarchies.
For example, we would like to know the revenue of the whole year. However, the record of data is
Date Revenue 2018-01-01 1023 2018-01-02 934 &amp;hellip; &amp;hellip; 2018-12-30 1244 2018-12-31 1302 Roll-up is performed by summing up everything of the column revenue. It gives us the revenue of the whole year. Monthly and quarterly roll-up is also straightforward.</description></item><item><title>Finite Element Method</title><link>https://datumorphism.leima.is/wiki/dynamical-system/finite-element-method/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/dynamical-system/finite-element-method/</guid><description>Differential Equations and Boundary Conditions Two Types of Boundary Conditions As an example, we have a partial differential equation
$$ \frac{d^2u}{dx^2} + f = 0, $$
which describes a 1D problem.
Dirichlet boundary condition: specify values for $u$, such as $u(0)=u_0$ and $u(L)=u_L$; Neumann boundary condition: specifiy values for $u_{,x}$. If we have only Neumann boundary condition, the solution is not unique. One example for it is tossing a bar, which can have both Neumann BC at both ends but it is moving.
Example Problems Elasticity Problem We consider the displacement $u(x)$ at each space coordinate $x$ of a elastic bar under some external force.</description></item><item><title>Chi-square Correlation Test for Nominal Data</title><link>https://datumorphism.leima.is/wiki/statistics/correlation-analysis-chi-square/</link><pubDate>Sun, 18 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/statistics/correlation-analysis-chi-square/</guid><description>In this article, we will discuss the chi-square correlation test for detecting correlations between two series.
Steps Find out all the possible values of the two nominal series A and B; Count the co-occurrences of the combinations (A, B); Calculate the expected co-occurrences of the combinations (A, B); Calculate chi-square; Determine whether the hypothesis can be rejected. Define the Series Suppose we are analyzing two series A and B. Series A can take values $a_1$ and $a_2$, while series B can take values $b_1$ , $b_2$ and $b_3$.
$$ \begin{align} A &amp;:= \{a1, a2\} \\ B &amp;:= \{b1,b2,b3\} \end{align} $$</description></item><item><title>Basics of Network</title><link>https://datumorphism.leima.is/wiki/computation/basics-of-network/</link><pubDate>Sun, 23 Sep 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/computation/basics-of-network/</guid><description>HTTP Keywords Hyper Text Transfer Protocal: deliver hyper text from server to local browser etc. Based on TCP/IP Current version: HTTP/2 Server - Client Client can request through GET, HEAD, POST, PUT, DELETE, TRACE, OPTIONS, CONNECT, PATCH. Transfer anything defined by Content-Type Connectionless Protocol: doesn&amp;rsquo;t maintain the connection all the time Stateless protocal: A very nice explanation URL Keywords Uniform Resource Locator Interpret each part of this URL: http://abc.com:8000/folder/file.html#title1location?param1=123&amp;amp;param2=234 No limits on length of URL by HTTP itself. However, some servers or clients do set limits. Difference between URI and URL and URN: The Difference Between URLs and URIs checkout the Venn diagram.</description></item><item><title>Unsupervised Learning: SVM</title><link>https://datumorphism.leima.is/wiki/machine-learning/unsupervised/svm/</link><pubDate>Fri, 17 Aug 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/unsupervised/svm/</guid><description>SVM is calculating a hyperplane to separate the data points into groups according to the label.
Hyperplane A hyperplane is defined to be of the following form
$$ \begin{equation} \boldsymbol{\beta} \cdot \mathbf x = \beta_0. \end{equation} $$
where $\boldsymbol\beta$ is the normal vector to the plane and is required to be constant.
It is straight forward to show that the distance $d$ from an arbitrary point $\mathbf x'$ to the hyperplane is
$$ \begin{equation} d = \boldsymbol\beta \cdot \mathbf x' - \beta_0. \end{equation} $$
A Few Key Concepts in SVM Though the concept of SVM is simple, one might find the algorithm to be quite complicated at first glance.</description></item><item><title>Manage Data Using MongoDB</title><link>https://datumorphism.leima.is/wiki/nodecrawler/manage-data-using-mongodb/</link><pubDate>Wed, 18 Jul 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/nodecrawler/manage-data-using-mongodb/</guid><description>In most cases, databases makes the management of data quite convenient. In this article, we would scrape data using the code we discussed before but write data into MongoDB.
For installation of MongoDB, please refer to the official documentation.
The Code To write data to MongoDB using Node.js, we choose the package mongojs, which provides almost exactly the standard MongoDB syntax.
To install mongojs,
npm i mongojs --save Here is a module that can write data to MongoDB. We create a file named dao.js and copy/paste the following code into it.
// use mongojs const mongojs = require(&amp;#39;mongojs&amp;#39;) // connect to the database &amp;#39;simple_spider&amp;#39; in MongoDB and use collection &amp;#39;test&amp;#39; const localdb = mongojs(&amp;#39;simple_spider&amp;#39;, [&amp;#39;test&amp;#39;]) // a function that saves data to MongoDB const saveData = (data,cb) =&amp;gt; { localdb.</description></item><item><title>The Python Language: Multi-Processing</title><link>https://datumorphism.leima.is/wiki/programming-languages/python/multiprocessing/</link><pubDate>Thu, 10 May 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/programming-languages/python/multiprocessing/</guid><description>Python has built-in multiprocessing module in its standard library.
One simple example of using the Pool class is the following.
def myfunc(myfuncargs): &amp;#39;some thing here&amp;#39; with Pool(10) as p: records = p.map(myfunc, myfuncargs) However, there are limitations on this, especially on pickles. Another approach.
from multiprocessing import Pool from multiprocessing.dummy import Pool as ThreadPool with ThreadPool(1) as p: records = p.map(myfunc, myfuncargs) Beware that map function will feed in a list of args to the function. So I have to use p.map(myfunc, [arg]) for one arg.</description></item><item><title>Data Structure: Tree</title><link>https://datumorphism.leima.is/wiki/algorithms/data-structure-tree/</link><pubDate>Tue, 27 Mar 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/algorithms/data-structure-tree/</guid><description>mind the data structure: here comes the tree</description></item><item><title>The C++ Language: Numerical Methods</title><link>https://datumorphism.leima.is/wiki/programming-languages/cpp/numerical/</link><pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/programming-languages/cpp/numerical/</guid><description>Modularize The code should be designed to separate physics or model from numerical methods. Speed vectors are convenient but slow. 1 Do not copy arrays if not necessary. The example would be for a function return. Most of the time, we can pass the pointer of an array to the function and update the array itself without copying anything and no return is needed at all. inline function. Use namespace instead of class if no data structure is stored in it. Refs http://en.cppreference.com/w/cpp/container/vector&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>GNUPlot</title><link>https://datumorphism.leima.is/wiki/tools/gnuplot/</link><pubDate>Mon, 04 Sep 2017 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/tools/gnuplot/</guid><description>Examples Plot .csv data. Suppose we have data of such.
-0.00999983, 0.99995 -0.0199987, 0.9998 -0.0299955, 0.99955 -0.0399893, 0.9992 -0.0499792, 0.99875 -0.059964, 0.998201 To plot the second column against the first column, we use the using parameter in gnuplot.
gnuplot -e &amp;#34;set terminal png; set datafile separator &amp;#39;,&amp;#39; ; plot &amp;#39;complex.txt&amp;#39; using 1:2&amp;#34; | imgcat # datafile seperator is not always necessary # imgcat is a script in iterm2 on mac</description></item><item><title>Boltzmann Machine</title><link>https://datumorphism.leima.is/wiki/machine-learning/energy-based-model/boltzmann-machine/</link><pubDate>Sun, 27 Aug 2017 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/energy-based-model/boltzmann-machine/</guid><description>Boltzmann machine is much like a spin glass model in physics. In short words, Boltzmann machine is a machine that has nodes that can take values, and the nodes are connected through some weight. It is just like any other neural nets but with complications and theoretical implications.
Boltzmann machine is usually used as a generative model.
Boltzmann Machine and Physics To obtain a good understanding of Boltzmann machine for a physicist, we begin with Ising model. We construct a system of neurons ${ s_i}$ which can take values of 1 or -1, where each pair of them $s_i$ and $s_j$ is connected by weight $J_{ij}$.</description></item><item><title>Neyman-Pearson Theory</title><link>https://datumorphism.leima.is/wiki/statistical-hypothesis-testing/neyman-pearson-theory/</link><pubDate>Sat, 02 Apr 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/statistical-hypothesis-testing/neyman-pearson-theory/</guid><description>The Neyman-Pearson hypothesis testing tests two hypothesis, hypothesis $H$, and an alternative hypothesis $H_A$.
Neyman-Pearson Lemma The Neyman-Pearson Lemma is an very intuitive lemma to understand how to choose a hypothesis. The lecture notes from PennState is a very good read on this topic1.
An example For simplicity, we assume that there exists a test statistic $T$ and $T$ can be used to measure how likely the hypothesis $H$ is true, e.g., the hypothesis $H$ is false, corresponds to $T$ being small.
The reference from Shafer2007 assumes a random variable $T$ to be large if the hypothesis $H$ is false[^Shafer2007]. One example is the ratio of likelihood2,</description></item><item><title>Solving Problems on Graph</title><link>https://datumorphism.leima.is/wiki/graph/basics/ml-problems-on-graph/</link><pubDate>Sat, 25 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/graph/basics/ml-problems-on-graph/</guid><description>Graphs can be used in many problem and there are many possible problems on graphs. We will mention a few popular problems on graphs12.
Node Classification Is the user in black a bot or a normal user?Created based on the text in Hamilton2020
Given graph that has incomplete attribute labeling of the nodes, predict the attributes on the nodes.
The following concepts can be used to classify nodes.
[[Homophily]] Homophily on Graph Homophily is the principle that a contact between similar people occurs at ahigher rate than among dissimilar people &amp;ndash; McPherson20011 McPherson2001 McPherson M, Smith-Lovin L, Cook JM.</description></item><item><title>Contrastive Predictive Coding</title><link>https://datumorphism.leima.is/wiki/machine-learning/contrastive-models/contrastive-predictive-codeing/</link><pubDate>Wed, 08 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/contrastive-models/contrastive-predictive-codeing/</guid><description>Contrastive Predictive Coding, aka CPC, is an autoregressive model combined with InfoNCE loss1.
There are two key ideas in CPC:
Autoregressive models in latent space, and InfoNCE loss that combines mutual information and [[NCE]] Noise Contrastive Estimation: NCE Noise contrastive estimation (NCE) objective function is1 $$ \mathcal L = \mathbb E_{x, x^{+}, x^{-}} \left[ - \ln \frac{ C(x, x^{+})}{ C(x,x^{+}) + C(x,x^{-}) } \right], $$ where $x^{+}$ represents data similar to $x$, $x^{-}$ represents data dissimilar to $x$, $C(\cdot, \cdot)$ is a function to compute the similarities. For example, we can use $$ C(x, x^{+}) = e^{ f(x)^T f(x^{+}) }, $$ so that the objective function becomes $$ \mathcal L = \mathbb E_{x, x^{+}, x^{-}} \left[ - \ln \frac{ e^{ … .</description></item><item><title>Deep Infomax</title><link>https://datumorphism.leima.is/wiki/machine-learning/contrastive-models/deep-infomax/</link><pubDate>Wed, 08 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/contrastive-models/deep-infomax/</guid><description>Max Global Mutual Information
Why not just use the global mutual information of the input and encoder output as the objective?
&amp;hellip; maximizing MI between the complete input and the encoder output (i.e.,globalMI) is ofteninsufficient for learning useful representations.
&amp;ndash; Devon et al[^Devon2018]
[[Mutual information]] Mutual Information Mutual information is defined as $$ I(X;Y) = \mathbb E_{p_{XY}} \ln \frac{P_{XY}}{P_X P_Y}. $$ In the case that $X$ and $Y$ are independent variables, we have $P_{XY} = P_X P_Y$, thus $I(X;Y) = 0$. This makes sense as there would be no &amp;ldquo;mutual&amp;rdquo; information if the two variables are independent of each other.</description></item><item><title>Generative Model: Auto-Encoder</title><link>https://datumorphism.leima.is/wiki/machine-learning/generative-models/autoencoder/</link><pubDate>Fri, 13 Aug 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/generative-models/autoencoder/</guid><description>Autoencoders (AE) are machines that encodes inputs into a compact latent space.
The simplest auto-encoder is rather easy to understand.
The loss can be chosen based on the demand, e.g., cross entropy for binary labels.
Notation: dot ($\cdot$)
We use a single vertically centered dot, i.e., $\cdot$, to indicate that the function or machine can take in arguments. A simple autoencoder can be achieved using two neural nets, e.g.,
$$ \begin{align} {\color{green}h} &amp;amp;= {\color{blue}g}{\color{blue}(}{\color{blue}b} + {\color{blue}w} x{\color{blue})} \ \hat x &amp;amp;= {\color{red}\sigma}{\color{red}(c} + {\color{red}v} {\color{green}h}{\color{red})}, \end{align} $$
where in this simple example,
${\color{blue}g(b + w \cdot )}$ is the encoder, and ${\color{red}\sigma(c + v \cdot )}$ is the decoder.</description></item><item><title>Restricted Boltzmann Machine</title><link>https://datumorphism.leima.is/wiki/machine-learning/energy-based-model/restricted-boltzmann-machine/</link><pubDate>Fri, 11 Jun 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/energy-based-model/restricted-boltzmann-machine/</guid><description>Latent variables introduce extra correlations between the nodes in a network. Introducing hidden units can also help us remove the direct connection between some nodes in a Boltzmann machine and create a restricted Boltzmann machine. A restricted Boltzmann machine requires less computation while having some expressing power.
Given Ising like interactions between the nodes, flipping node V1 is likely to also flip node V2 as they are connected through hidden unit H1. They are correlated. Removing the hidden unit leaves us two uncorrelated units.
The Ising Model Given a Ising-like energy function (c.f. [[MaxEnt Model]] MaxEnt Model Maximum Entropy models makes least assumption about the data )</description></item><item><title>Deep Autoregressive Network</title><link>https://datumorphism.leima.is/wiki/machine-learning/neural-networks/deep-autoregressive-networks/</link><pubDate>Mon, 15 Feb 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/neural-networks/deep-autoregressive-networks/</guid><description>There are two levels of autoregressiveness in the DARN network:
Inlayer autoregressive connections of the nodes, Intralayer autoregressive connections of nodes. The network is trained on MDL loss.</description></item><item><title>Wavelet Transform</title><link>https://datumorphism.leima.is/wiki/time-series/wavelets/</link><pubDate>Mon, 07 Dec 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/time-series/wavelets/</guid><description>In general, given a complete set of function $\psi(x; \tilde x)$, we can decompose a function $F(\tilde x)$
$$ F(\tilde x) = \int f(x) \psi(x;\tilde x) dx. $$
The choice of $\psi(x;\tilde x)$ gives us different properties.
Fourier Transform Fourier transform is good for stationary analysis since time is not involved in $F(\omega)$.
$$ F(\omega) = \int_{-\infty}^{\infty} f(t) e^{-i \omega t} dt $$
Short-time Fourier Transform STFT is a Fourier transform with a moving time window $\tau$,
$$ F(\tau,\omega) = \int_{-\infty}^{\infty} f(t) w(t - \tau) e^{-i\omega t} dt. $$
Moving $\tau$ gives us the ability to investigate Fourier components at different time segments (assuming the window function $w(t-\tau)$ is a step function).</description></item><item><title>Parsimony of Models</title><link>https://datumorphism.leima.is/wiki/model-selection/parsimony-of-models/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/model-selection/parsimony-of-models/</guid><description>For models with a lot of parameters, the goodness-of-fit is very likely to be very high. However, it is also likely to generalize bad. So we need measure of generalizability
Here parsinomy gives us a few advantages.
easy to perceive better generalizations</description></item><item><title>Terminal</title><link>https://datumorphism.leima.is/wiki/tools/terminal/</link><pubDate>Tue, 31 Dec 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/tools/terminal/</guid><description>Navigating Some tips to help data scientist navigate faster in terminal.
pushd, popd and dirs pushd to register and change directories: pushd folder_name will change current directory to folder_name and register the folder folder_name in our stack. If no folder name is passed onto the command, it will be default to $HOME folder. popd to go to the last directory in the stack and remove it from the stack. In this example, popd will change the current working directory to folder_name. dirs will list all working directories registered in the stack. The current working directory will always be in the stack.</description></item><item><title>Data Storage</title><link>https://datumorphism.leima.is/wiki/data-warehouse/data-storage/</link><pubDate>Fri, 23 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/data-warehouse/data-storage/</guid><description>tl;dr: Use type safe formats such as HDF5 or parquet
HDF5 BCOLZ &amp;lt;http://bcolz.blosc.org/en/latest/&amp;gt;_ : not designed for multidimentional data. Zarr &amp;lt;https://github.com/alimanfoo/zarr&amp;gt;_ : works with multidimensional data and also parallel computating. Blaze ecosystem &amp;lt;http://blaze.pydata.org/&amp;gt;_ A article that compares HDF5, BCOLZ, and Zarr: To HDF5 and beyond
I also recommend pandas. It is a python module that works very well with data. It even loads HDF5 out of box.</description></item><item><title>Bin Size of Histogram</title><link>https://datumorphism.leima.is/wiki/data-visualization/histogram-bin-size/</link><pubDate>Thu, 22 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/data-visualization/histogram-bin-size/</guid><description>[[Histograms]] Histogram Suppose we check out the burger prices at the stores of Han im Glück, we get a list of numbers. We can arrange the numbers into bins of prices. For example, we can count the number stores that have a price between 10 to 11 euros. are good for understanding the distribution of your data.
The Bin Size Problem As an example, we will use the following series as an example.
[1.45,2.20,0.75,1.23,1.25,1.25,3.09,1.99,2.00,0.78,1.32,2.25,3.15,3.85,0.52,0.99,1.38,1.75,1.21,1.75] If we use bin size 1, we get this spiky chart and it is not so informing.
We could also set bin size to 2.</description></item><item><title>Correlation Coefficient and Covariance for Numeric Data</title><link>https://datumorphism.leima.is/wiki/statistics/correlation-coefficient/</link><pubDate>Sun, 18 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/statistics/correlation-coefficient/</guid><description>Covariances Correlation coefficient is also known as the Pearson&amp;rsquo;s product moment coefficient. Review of Standard Deviation For a series of data A, we have the standard deviations
$$ \sigma_A = \sqrt{ \frac{ \sum (a_i - \bar A)^2 }{ n } }, $$
where $n$ is the number of elements in series A.
The standard deviation is very easy to understand. It is basically the average Eucleadian distance between the data points and the average value. In this article, we will take another point of view.
Now imagine we have two series $(a_i - \bar A)$ and $(a_j - \bar A)$.</description></item><item><title>Basics of Database</title><link>https://datumorphism.leima.is/wiki/computation/basics-of-database/</link><pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/computation/basics-of-database/</guid><description>NoSQL NoSQL = Not only SQL. The four main types of NoSQL databases are
Key-value store: Amazon Dynamo, memcached, Amazon SimpleDB Column-orient store: Google BigTable, Cassandra Graph database: Neo4j, VertexDB Document database: [[MongoDB]] Basics of MongoDB MongoDB is a document based database Object database: ZODB Database Operations Relations Union: $A\cup B$ Intersection: $A\cap B$ $A - B$ Cartesian Product: $A \times B$ Query Union in database: will combine the data with matching common columns.
NaturalJoin and EquiJoin:
--EquiJoin where we specify what condition is used to join SELECT * FROM table1 JOIN table2 ON (table1.</description></item><item><title>Restrictions of Websites</title><link>https://datumorphism.leima.is/wiki/nodecrawler/restrictions/</link><pubDate>Thu, 19 Jul 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/nodecrawler/restrictions/</guid><description>Beware that scraping data off websites is neither always allowed nor as easy as a few lines of code. The preceding articles enable you to scrape many data, however, man websites have counter measures. In this article, we will be dealing with some of the common ones.
Request Frequency Some websites have limitations on the frequency of API requests. The solution to this is simply a brief pause after each request. In Node.js, the function setInterval enables this.
// ... require packages here // define the function fetch to get data const fetch = (aid) =&amp;gt; superagent .get(&amp;#39;https://api.bilibili.com/x/web-interface/archive/stat&amp;#39;) .query({ aid:aid }) .</description></item><item><title>Data Structure: Graph</title><link>https://datumorphism.leima.is/wiki/algorithms/data-structure-graph/</link><pubDate>Tue, 27 Mar 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/algorithms/data-structure-graph/</guid><description>mind the data structure: here comes the graph</description></item><item><title>The Python Language: Performance</title><link>https://datumorphism.leima.is/wiki/programming-languages/python/performance/</link><pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/programming-languages/python/performance/</guid><description>Read the references for performance.
The message:
Use comprehensions Use generators</description></item><item><title>DeepAR</title><link>https://datumorphism.leima.is/wiki/forecasting/deepar/</link><pubDate>Sat, 09 Jul 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/forecasting/deepar/</guid><description>Focus on</description></item><item><title>Information Bottleneck</title><link>https://datumorphism.leima.is/wiki/learning-theory/information-bottleneck/</link><pubDate>Sat, 30 Apr 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/learning-theory/information-bottleneck/</guid><description>Information Bottleneck In a [[induction-deduction framework]] Induction, Deduction, and Transduction , for a given training dataset
$$ \{X, Y\}, $$
a prediction Markov chain1
$$ X \to \hat X \to Y, $$
where $\hat X$ is supposed to be the minimal sufficient statistics of $X$. $\hat X$ is the minimal data that can still represent the relation between $X$ and $Y$, i.e., $I(X;Y)$, the [[mutual information]] Mutual Information Mutual information is defined as $$ I(X;Y) = \mathbb E_{p_{XY}} \ln \frac{P_{XY}}{P_X P_Y}. $$ In the case that $X$ and $Y$ are independent variables, we have $P_{XY} = P_X P_Y$, thus $I(X;Y) = 0$.</description></item><item><title>State Space Models</title><link>https://datumorphism.leima.is/wiki/time-series/state-space-models/</link><pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/time-series/state-space-models/</guid><description>State space model is an important category of model for sequential data. Through simple assumptions, state space models can achieve quite complicated distributions.
To model a sequence, we can use the joint probability of all the nodes,
$$ p(x_1, x_2, \cdots, x_N), $$
where $x_i$ are the nodes in the sequence.
Orders We can introduce different order of dependencies on the past.
The simplest model for the sequence is assuming i.i.d..
Zeroth OrderEach node is independent of each other
To model the dependencies in the sequence, we can assume a node depends on the previous nodes. The first-order model assume that node $x_{i+1}$ only depends on node $x_i$.</description></item><item><title>The Python Language: Packaging</title><link>https://datumorphism.leima.is/wiki/programming-languages/python/packaging/</link><pubDate>Fri, 27 Aug 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/programming-languages/python/packaging/</guid><description>The official documentation has pages about building python packages1. Torborg also compiled a series of pages and examples about building a package2. In this note, I only provide some tips.
Private Python Packages We can easily setup a private pypi service (e.g., pypicloud).
Install Packages from Private Pypi To install packages inline, use
pip install -i https://$PYPI_USER:$PYPI_PWD@your.pypi.url/simple/ durst==0.0.5 To install packages from requirements.txt use,
pip install -r requirements.txt --trusted-host https://$PYPI_USER:$PYPI_PWD@your.pypi.url/simple --extra-index-url https://$PYPI_USER:$PYPI_PWD@your.pypi.url/simple Publishing Packages Using GitHub Actions We can easily publish python packages using GitHub Actions. Here is an example.
name: Publish Package to Private Pypi on: [push] jobs: build-n-publish: runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v2 - name: Set up Python uses: actions/setup-python@v2 with: python-version: 3.</description></item><item><title>Variational Auto-Encoder</title><link>https://datumorphism.leima.is/wiki/machine-learning/generative-models/variational-autoencoder/</link><pubDate>Fri, 13 Aug 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/generative-models/variational-autoencoder/</guid><description>Variational Auto-Encoder (VAE) is very different from [[Generative Model: Auto-Encoder]] Generative Model: Auto-Encoder Autoencoders (AE) are machines that encodes inputs into a compact latent space. The simplest auto-encoder is rather easy to understand. The loss can be chosen based on the demand, e.g., cross entropy for binary labels. Notation: dot ($\cdot$) We use a single vertically centered dot, i.e., $\cdot$, to indicate that the function or machine can take in arguments. A simple autoencoder can be achieved using two neural nets, e.g., $$ \begin{align} {\color{green}h} &amp;amp;= … . In VAE, we introduce a variational distribution $q$ to help us work out the weighted integral after introducing the latent space variable $z$,</description></item><item><title>MDL and Neural Networks</title><link>https://datumorphism.leima.is/wiki/model-selection/mdl-and-neural-networks/</link><pubDate>Sun, 14 Feb 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/model-selection/mdl-and-neural-networks/</guid><description>Minimum Description Length ( [[MDL]] Minimum Description Length MDL is a measure of how well a model compresses data by minimizing the combined cost of the description of the model and the misfit. ) can be used to construct a concise network. A fully connected network has great expressing power but it is easily overfitting.
One strategy is to apply constraints to the networks:
Limit the connections; Shared weights in subgroups of the network; Constrain the weights using some probability distributions. By minimizing the MDL of the network and the misfits on the data, we can build a concise network.</description></item><item><title>Histogram</title><link>https://datumorphism.leima.is/wiki/data-visualization/histogram/</link><pubDate>Tue, 20 Aug 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/data-visualization/histogram/</guid><description>Suppose we check out the burger prices at the stores of Han im Glück, we get a list of numbers. We can arrange the numbers into bins of prices. For example, we can count the number stores that have a price between 10 to 11 euros.</description></item><item><title>Basics of SQL</title><link>https://datumorphism.leima.is/wiki/computation/basics-of-sql/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/computation/basics-of-sql/</guid><description>Adding a new field to data:
Relational: requires a new column Non-Relational: just add the field to one single document, thus can be easily decentralized. Basics and Background SQL: Structured Query Language
Relational Database:
usually in tables rows are called records columns are certain types of data. Data types of rows are specified: INTEGER TEXT DATE REAL, real numbers NULL &amp;hellip; RDBMS: Relational Database Management System, most RDBMS use SQL as the query language. SQLite is one of the RDBMS.
SQLite: open source and minimal MySQL: powerful and popular, also open source, controlled by Oracle, not really scalable.</description></item><item><title>Normalization Methods for Numeric Data</title><link>https://datumorphism.leima.is/wiki/statistics/normalization-methods/</link><pubDate>Sun, 18 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/statistics/normalization-methods/</guid><description>Normalization of data is critical for statistical analysis and feature engineering.
Min-max Normalization This method is linear and straightforward.
Suppose we are analyzing series A, with elements $a_i$. We already know the min and max of the series, $a_{min}$ and $a_{max}$.
Now we would like to normalize the series to be within the range $[a_{min}', a_{max}']$. We simply solve the value of $a' _ i$ in $$ \frac{(a'i - a{min}')}{ ( a'{max} - a'{min} ) } = \frac{(a_i - a_{min})}{ ( a_{max} - a_{min} ) }, $$ where everything on the right hand side is known and $a_{min}'$ and $a_{max}'$ are chosen as the new min and max to be scaled to.</description></item><item><title>Optimization</title><link>https://datumorphism.leima.is/wiki/nodecrawler/optimization/</link><pubDate>Thu, 19 Jul 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/nodecrawler/optimization/</guid><description>In this article, we will be optimizing the crawler to get better performance.
Batch Jobs In the article about using MongoDB as data storage, we write the data to database whenever we get it. In practice, this is not efficient at all. Here comes the batch jobs. It would be much better if one write to database with batch jobs.
If you recall, the code we used to write to database is
// ...other code localdb.test.save(data, (err, res)=&amp;gt;{ // do something }) The function save takes in not only one entry of document but an array of documents:
const array = [] for(let i = INI_ID ; i &amp;lt; MAX_ID; i++){ // fetch data from website const data = fetchData(i) array.</description></item><item><title>Git</title><link>https://datumorphism.leima.is/wiki/tools/git/</link><pubDate>Wed, 22 Jun 2016 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/tools/git/</guid><description>Git Services GitHub Bitbucket GitLab Using Git with GUI There are huge amounts of git commands! There are also a lot of GUIs if you don&amp;rsquo;t like command line.
GitHub Desktop GitKraken SourceTree &amp;hellip; Useful Commands To check all the commits related to a file, use git log -u. Try out git log -g before determining which reflog to deal with. To compare the changes with the last commit, use git diff --cached HEAD~1. Useful Alias A very useful article here: Must Have Git Aliases: Advanced Examples.
git config --global alias.unstage 'reset HEAD --' git config --global alias.</description></item><item><title>Hidden Markov Model</title><link>https://datumorphism.leima.is/wiki/time-series/hidden-markov-model/</link><pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/time-series/hidden-markov-model/</guid><description>The hidden Markov model, HMM, is a type of [[State Space Models]] State Space Models The state space model is an important category of models for sequential data such as time series 1.
HMM Bishop2006 Christpher M. Bishop. Pattern Recognition and Machine Learning. Springer-Verlag New York; 2006. &amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Some ML Workflow Frameworks</title><link>https://datumorphism.leima.is/wiki/tools/ml-flow-frameworks/</link><pubDate>Wed, 13 Jan 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/tools/ml-flow-frameworks/</guid><description>Metaflow Docs
A framework for jupyter notebook data scientists.
Work locally on notebooks. Python environment management using conda. Work in the cloud with Sagemaker. Tasks Methods Comments Code Scripts/Jupyter Notebook Datastore local + S3 metaflow.S3 Compute local + AWS Batch Metadata metaflow service Metadata specifies flow executions: Flows, Runs, Steps, Tasks, and Artifacts. Scheduling AWS Step Functions Deployment AWS Demo from metaflow import FlowSpec, step class BranchFlow(FlowSpec): @step def start(self): self.next(self.a, self.b) @step def a(self): self.</description></item><item><title>Boxplot</title><link>https://datumorphism.leima.is/wiki/data-visualization/boxplots/</link><pubDate>Tue, 20 Aug 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/data-visualization/boxplots/</guid><description>Example The Whiskers in Boxplot They are the outlier data points.
Outliers are determined using the interquatile range (IQR, i.e., 25 percentile to 75 percentile.). We usually the lowest data point within 1.5 IQR range below the 25 percentile or the data point within 1.5 IQR range above the 75 percentile.</description></item><item><title>Linear Regression</title><link>https://datumorphism.leima.is/wiki/statistics/linear-regression/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/statistics/linear-regression/</guid><description>In this article, we will use the Einstein summation convention. For example, $$ X_{ij}\beta_ j $$ is equivalent to $$ \sum_j X_{ij}\beta_ j $$ In statistics, we have at least three categories of quantities:
data and labels abstract theoretical quantities parameters and predictions of models The convention is that quantities with $\hat {}$ are the model quantities. Sometimes we do not distinguish the abstract theoretical quantities and model quantities.
If it is necessary to use different notations for the abstract theoretical quantities and the model quantities, we would use bold symbols ($\mathbf Y$) or latin sub/super indices ( $Y_a$ ) for theoretical quantities and greek letters ( $Y_\alpha$ ) for model quantities.</description></item><item><title>Basics of MongoDB</title><link>https://datumorphism.leima.is/wiki/computation/basics-of-mongodb/</link><pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/computation/basics-of-mongodb/</guid><description>This MongoDB Cheatsheet is my best friend.
MongoDB Concepts Documents Collections: just like tables in SQL. Database MongoShell Some examples:
// show the databases show dbs // show collections show collections //set any database to current database use database_name // insert entry db.database_name.insert( an_object_2_be_the_entry ) // read document db.database_name.findOne({'some_field':'value_of_field'}) db.database_name.fidn() // prettify db.database_name.find().pretty()</description></item><item><title>Time Series Data Augmentation</title><link>https://datumorphism.leima.is/wiki/time-series/data-augmentation/</link><pubDate>Mon, 27 Jun 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/time-series/data-augmentation/</guid><description/></item><item><title>Cookiecutter</title><link>https://datumorphism.leima.is/wiki/tools/cookiecutter/</link><pubDate>Fri, 27 Aug 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/tools/cookiecutter/</guid><description>Cookiecutter is a good tool to setup a scaffold for a data science project. cookiecutter-data-science is a very good template to use.
If some specific (internal) packages are needed for almost every package, fork cookiecutter-data-science and make some changes for future use. For example, one might use [[mkdocs]] Documentation Documenting my data science project using sphinx or mkdocs-material instead of [[sphinx]] Documentation Documenting my data science project using sphinx or mkdocs-material . Swap out sphinx for mkdocs if needed.</description></item><item><title>Statistical Sign Test</title><link>https://datumorphism.leima.is/wiki/statistical-hypothesis-testing/sign-test/</link><pubDate>Sun, 20 Jan 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/statistical-hypothesis-testing/sign-test/</guid><description>We have a small dataset, but it doesn&amp;rsquo;t satisfy the t-test conditions. Then we would use as little assumptions as possible.
Wine Taste Suppose we have two bottles of wine, one of them is 300 euros while the other is 100 euros.
Now we ask the question:
Does expensive wine taste better?
We find 10 experts and give them some experiments. The result is recorded then processed into the following table.
expert # expensive is better 1 yes 2 no 3 yes 4 yes 5 yes 6 no 7 yes 8 yes Naively, we could simply count the number of yes and find the probability of yes in this sample, i.</description></item><item><title>Documentation</title><link>https://datumorphism.leima.is/wiki/tools/documentation/</link><pubDate>Sat, 28 Aug 2021 00:03:10 +0200</pubDate><guid>https://datumorphism.leima.is/wiki/tools/documentation/</guid><description>I would vote for two very different documentation tools for a data science project,
sphinx docs, and squidfunk/mkdocs-material. Sphinx docs Sphinx docs is a mature and stable. I love reStructuredText as the syntax as it is very versatile. It supports math, figures with captions, admonitions, cross reference, auto doc from docstrings, cross project cross referencing, pdf generation, etc.
reStructuredText is not the only choice
We can also use markdown by choosing the markdown parser. squidfunk/mkdocs-material squidfunk&amp;rsquo;s mkdocs-material is a rather light-weight but is also growing to be versatile now. The engine is mkdocs. mkdocs-material is a theme but provides a lot of useful and easy to use elements.</description></item><item><title>Dashboards</title><link>https://datumorphism.leima.is/wiki/data-visualization/dashboards/</link><pubDate>Fri, 27 Aug 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/data-visualization/dashboards/</guid><description>Building interactive dashboards is not easy task. However, with the right tool, we can build a prototype fast.
Theories Dashboard building seems to be a task to build whatever charts the business would like to add.
However, theories are required to build quality dashboards1.
AmNeumarkt/253
I wrote a comment about this: AmNeumarkt/253.
Creating visualizations seems to be a creative task. At least for entry-level visualization tasks, we follow our hearts and build whatever is needed. However, visualizations are made for different purposes. Some visualizations are simply explorations and for us to get some feelings on the data. Some others are built for the validation of hypotheses.</description></item><item><title>Mann-Whitney U Test</title><link>https://datumorphism.leima.is/wiki/statistical-hypothesis-testing/mann-whitney-u-test/</link><pubDate>Sun, 20 Jan 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/statistical-hypothesis-testing/mann-whitney-u-test/</guid><description>Mann-Whitney U is good at [[hypothesis testing]] Statistical Hypothesis Testing hypothesis testing is about the probability of alternative hypothesis if the null hypothesis is true, or even more general heavy-tailed data.</description></item><item><title>Describing Multi-dimensional Data</title><link>https://datumorphism.leima.is/wiki/statistics/multidimensional-data/</link><pubDate>Mon, 03 Dec 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/statistics/multidimensional-data/</guid><description>Descriptions of Multidimensional Data Dispersion Matrix As defined in Correlation Coefficient and Covariance for Numeric Data, covariance is about the variance of two series. This property makes it easy to generalize it to multidimensional data.
The generalized quantity is named as dispersion matrix. Suppose we have a $p$ dimensional dataset $X$,
index $x_1$ $x_2$ &amp;hellip; $x_p$ 1 2.3 12.3 83.2 9.3 &amp;hellip; &amp;hellip; &amp;hellip; &amp;hellip; &amp;hellip; N 3.1 5.6 23.6 8.2 We could then calculate the pairwise covariance between the different dimensions.
$x_1$ $x_2$ &amp;hellip; $x_p$ $x_1$ $x_2$ &amp;hellip; $x_p$</description></item><item><title>Data Storage</title><link>https://datumorphism.leima.is/wiki/data-engeering-for-data-scientist/data-storage/</link><pubDate>Wed, 05 May 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/data-engeering-for-data-scientist/data-storage/</guid><description> Many of the example are from the book by Adreas Kretz. Find the link to the book in the references section. Types of Storage and Data Here is a list 1
Files S3 Message Queues Kinesis Relational DB MySQL Postgres Non-relational DB Document Store MongoDB DocumentDB Key-Value Store HBase Redis Kretz2019 The Data Engineering Cookbook &amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Comparison of MLOps Frameworks</title><link>https://datumorphism.leima.is/wiki/mlops/comparison-of-frameworks/</link><pubDate>Wed, 05 May 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/mlops/comparison-of-frameworks/</guid><description>Metaflow Docs
A framework for jupyter notebook data scientists.
Work locally on notebooks. Python environment management using conda. Work in the cloud with Sagemaker. Tasks Methods Comments Code Scripts/Jupyter Notebook Datastore local + S3 metaflow.S3 Compute local + AWS Batch Metadata metaflow service Metadata specifies flow executions: Flows, Runs, Steps, Tasks, and Artifacts. Scheduling AWS Step Functions Deployment AWS Demo
from metaflow import FlowSpec, step class BranchFlow(FlowSpec): @step def start(self): self.next(self.a, self.b) @step def a(self): self.</description></item><item><title>Data Processing</title><link>https://datumorphism.leima.is/wiki/data-engeering-for-data-scientist/data-processing/</link><pubDate>Wed, 05 May 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/data-engeering-for-data-scientist/data-processing/</guid><description>Many of the example are from the book by Adreas Kretz. Find the link to the book in the references section. Batch Process Kretz recommend to start from batch processing and move to streaming if needed 1.
Stream Process Three methods to stream data
At Least Once:  message gets processed once or multiple times never dropped e.g., time-based GPS data in fleet management, if the stream data has the same timestamp, then we just override the existing data, we do not care how many times the data is being processed or streamed. At Most Once: okay to drop a message only processed once at max e.</description></item><item><title>Signal Processing</title><link>https://datumorphism.leima.is/wiki/algorithms/singal-processing/</link><pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/algorithms/singal-processing/</guid><description>There are many fascinating ideas in signal processing.</description></item><item><title>Data Processing - (Py)Spark</title><link>https://datumorphism.leima.is/wiki/tools/data-processing-spark/</link><pubDate>Mon, 31 Jan 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/tools/data-processing-spark/</guid><description>Spark uses Resilient Distributed Dataset (RDD).
Spark Clusters In one cluster, we have a driver is responsible for managing the tasks, result consolidation, and also shared data access1.
PySpark PySpark provides
SparkContext SparkSession A spark dataframe is immutable. This makes it tricky to update a dataframe.
Useful Commands Get Session In pyspark, we can also get or create the session using the following method.
pyspark.sql.SparkSession.builder.getOrCreate() List all Tables A pyspark.sql.SparkSession has property catalog and can be used to list the tables.
pyspark.sql.SparkSession.catalog.listTables() Run SQL Query Given a SQL query query, we can query the table using .</description></item><item><title>Signal Processing: Audio Basics</title><link>https://datumorphism.leima.is/wiki/algorithms/signal-processing-audio/</link><pubDate>Thu, 29 Mar 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/algorithms/signal-processing-audio/</guid><description>Keywords Harmonic structure of sound Parson code of music Linear time-invariant theory Autocorrelation Noise Chirps DCT compression Discrete Fourier transform filtering convolution Linear Time-Invariant System We describe the system with $Y(t) = f(X(t))$, where $X(t)$ is the input, and $Y(t)$ is the output.
Linear: $f(a X_1(t) + b X_2(t)) = a f(X_1(t)) + b f(X_2(t))$ Time-invariant: input $X(t+\Delta t)$ will produce the shifted signal $Y(t+\Delta t)$. LTI systems are memory systems, casual, real, and stable. Stable means the output won&amp;rsquo;t reach infinite if the input is finite. It&amp;rsquo;s bounded.
Impulse Response Suppose we have a impulse $X(t) = I(t)$, and output $h(t)$.</description></item><item><title>Basics of MapReduce</title><link>https://datumorphism.leima.is/wiki/algorithms/map-reduce/</link><pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/algorithms/map-reduce/</guid><description>Centralized servers are not efficient for big data. Querying and processing data on centralized servers would reach bottleneck of the servers.
MapReduce is used to solve these problems of big data. The two videos are .
Map: take series of key-value pairs and divide them into groups. Reduce: recombine the key-value pairs Checkout the code challenges of MapReduce on HackerRank.</description></item><item><title>Scale Up</title><link>https://datumorphism.leima.is/wiki/data-engeering-for-data-scientist/scale-up/</link><pubDate>Wed, 05 May 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/data-engeering-for-data-scientist/scale-up/</guid><description>Many of the example are from the book by Adreas Kretz. Find the link to the book in the references section. Scaling Up Storage Scaling Up SQL DB SAN: Storage Area Network Use multiple servers on the DB storage to make the query faster.
Good for Read-only DB Not convinient to update DB Hadoop Hadoop:
Distributed storage Analysis 4 core modules:
Hadoop common background functionalities HDFS Divide into blocks Distribute [[MapReduce]] Basics of MapReduce mapreduce Old tech YARN Resource management The Hadoop Ecosystem:</description></item><item><title>Data Generating Processes for Time Series Data</title><link>https://datumorphism.leima.is/cards/time-series/generating-process/</link><pubDate>Mon, 04 Jul 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/time-series/generating-process/</guid><description/></item><item><title>PySpark: Beware of Python Mutable Objects</title><link>https://datumorphism.leima.is/til/data/pyspark.beware-of-mutable-objects/</link><pubDate>Wed, 20 Apr 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/data/pyspark.beware-of-mutable-objects/</guid><description>.file { display: block } I created this example notebook to demonstrate the potential danger when dealing with mutable objects in pyspark udfs.
https://gist.github.com/emptymalei/07ba6716d0e2d815ebb64adce25dee72
In the above notebook, we can see that python lists in udfs are behaving like just pointers. For group in the aggregation, we see that the lists in the same values in column b are behaving like the same list, thus pointer like.
To solve this problem, we can do a few things.
Cache the dataframe after aggregation.
sdf_2 = sdf.groupby(&amp;#34;language&amp;#34;, &amp;#34;b&amp;#34;).agg(F.max(&amp;#34;b&amp;#34;).alias(&amp;#34;combined&amp;#34;)).cache() Make a copy of the mutable object.</description></item><item><title>Conformal Time Series Forecasting</title><link>https://datumorphism.leima.is/wiki/forecasting/probablistic/conformal-time-series-forecasting/</link><pubDate>Tue, 19 Apr 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/forecasting/probablistic/conformal-time-series-forecasting/</guid><description>Conformal time series forecasting is a probabilistic forecasting method using [[Conformal Prediction]] Conformal Prediction Conformal prediction is a method to sequentially predict consistent confidence intervals using nonconformity measures. .
For any given model $\mathcal M$, conformal time series forecasting trains on a training dataset $\mathcal D_{\text{Train}}$ then calculates a [[Confidence Interval]] Confidence Interval Estimates from a sample can be entitled a confidence interval using a calibration dataset $\mathcal D_{\text{Calibration}}$. The confidence interval is directly used for inference. This framework is called the inductive conformal prediction (ICP).
Induction, Deduction, and Transduction How to Forecast the Confidence Interval For a dataset $\mathcal D$, we split it, e.</description></item><item><title>Induction, Deduction, and Transduction</title><link>https://datumorphism.leima.is/cards/machine-learning/learning-theories/induction-deduction-transduction/</link><pubDate>Tue, 19 Apr 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/learning-theories/induction-deduction-transduction/</guid><description/></item><item><title>Graph Edge Sampling</title><link>https://datumorphism.leima.is/cards/graph/graph-edge-sampling/</link><pubDate>Sun, 20 Mar 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/graph/graph-edge-sampling/</guid><description>Edge sampling is a technique to deal with weighted edges in large [[graph]] What is Graph Graph A graph $\mathcal G$ has nodes $\mathcal V$ and edges $\mathcal E$, $$ \mathcal G = ( \mathcal V, \mathcal E). $$ Edges Edges are relations between nodes. For $u\in \mathcal V$ and $v\in \mathcal V$, if there is an edge between them, then $(u, v)\in \mathcal E$. Representations of Graph There are different representations of a graph. Adjacency Matrix A adjacency matrix of a graph represents the nodes using row and column indices and edges using elements of the matrix. For simple … .</description></item><item><title>Continuous Ranked Probability Score - CRPS</title><link>https://datumorphism.leima.is/cards/time-series/crps/</link><pubDate>Fri, 18 Mar 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/time-series/crps/</guid><description>The Continuous Ranked Probability Score, known as CRPS, is a score to measure how a proposed distribution approximates the data, without knowledge about the true distributions of the data.
Definition CRPS is defined as1
$$ CRPS(P, x_a) = \int_{-\infty}^\infty \lVert P(x) - H(x - x_a) \rVert_2 dx, $$
where
$x_a$ is the true value of $x$, P(x) is our proposed cumulative distribution for $x$, $H(x)$ is the Heaviside step function $$ H(x) = \begin{cases} 1, &amp;\qquad x=0\\ 0, &amp;\qquad x\leq 0\\ \end{cases} $$
$\lVert \cdot \rVert_2$ is the L2 norm. Explain it The formula looks abstract on first sight, but it becomes crystal clear once we understand it.</description></item><item><title>PySpark: Compare Two Schemas</title><link>https://datumorphism.leima.is/til/data/pyspark-schema-comparison/</link><pubDate>Mon, 14 Feb 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/data/pyspark-schema-comparison/</guid><description>To compare two dataframe schemas in [[PySpark]] Data Processing - (Py)Spark Processing Data using (Py)Spark , we can utilize the set operations in python.
def schema_diff(schema1, schema2): return { &amp;#39;fields_in_1_not_2&amp;#39;: set(schema1) - set(schema2), &amp;#39;fields_in_2_not_1&amp;#39;: set(schema2) - set(schema1) }</description></item><item><title>StemGNN</title><link>https://datumorphism.leima.is/reading/stemgnn/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/reading/stemgnn/</guid><description>What problem is StemGNN solving: intra-series temporal pattern: DFT Each series inter-series correlations At each step, the interactions between nodes reversible operator Example problem Covid cases: DE, AT, NL, &amp;hellip; Predicting each country without considering the interactions between them Or introduce the people flow between them GFT: Completes DFT as it takes care of the inter-series correlations one extra slide for this topic Convolutions on Graphs one extra slide for this topic Graph Basics How to build the graph &amp;ldquo;self-attention&amp;rdquo;: outer product of key, query, as the adjacency matrix key, query are of length # of ndoes Weights and Biases LC 1DConv GLU FC Experiments Traffic adjacency matrix neighbouring sensors have higher correlations Covid Neighbouring countries have higher correlation Spetral analysis: Some eigenvectors have clear meanings Week spots Paper and code are not consistent https://github.</description></item><item><title>Convolutions Using Fourier Transform</title><link>https://datumorphism.leima.is/cards/math/convolution-and-fourier-transform/</link><pubDate>Sat, 04 Dec 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/convolution-and-fourier-transform/</guid><description>Convolution
$$ (f*h)(x) = \int \mathrm d f(y) h(x-y), $$
is equivalent to
$$ \mathcal F^{-1}\left[ \mathcal F[ f(x) ] \circ \mathcal F[h(x)] \right], $$
with $\mathcal F$ being the Fourier transform, i.e.,
$$ \mathcal F[f(x)] = \int \mathrm d x f(x) e^{-2\pi i x s}. $$
Proof
One could prove it using the Fourier integral theorem,
$$ f(x) = \iint dy d\xi f(y)e^{2\pi i (x-y)\xi}. $$
Derivation Given
$$ \begin{align} \mathcal F_s \left(f(y)\right) &amp;= \int dy f(y) e^{-2\pi i y s}, \\ \mathcal F_s \left(h(y)\right) &amp;= \int dz h(z) e^{-2\pi i z s}, \end{align} $$
we have</description></item><item><title>Graph Convolution Operator</title><link>https://datumorphism.leima.is/cards/graph/graph-convolution-operator/</link><pubDate>Thu, 25 Nov 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/graph/graph-convolution-operator/</guid><description>For a given graph $\mathcal G$, we have an attribute on each node, denoted as $f_v$. All the node attributes put together can be written as a list $\mathbf f\to (f_{v_1}, f_{v_2}, \cdots, f_{v_N})$.
Convolution on graph is combining attributes on nodes with their neighbors'. The [[adjacency matrix]] Graph Adjacency Matrix A graph $\mathcal G$ can be represented with an adjacency matrix $\mathbf A$. There are some nice and clear examples on wikipedia1, for example, $$ \begin{pmatrix} 2 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0\\ 1 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0\\ 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 1\\ 1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \end{pmatrix} $$ for the graph Public Domain, Link $\mathbf A$ applied on all node attributes $\mathbf f$ is such an operation, i.</description></item><item><title>Centered Kernel Alignment (CKA)</title><link>https://datumorphism.leima.is/cards/machine-learning/measurement/centered-kernel-alignment/</link><pubDate>Mon, 08 Nov 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/measurement/centered-kernel-alignment/</guid><description>Centered Kernel Alignment (CKA) is a similarity metric designed to measure the similarity of between representations of features in neural networks1.
Definition of CKA CKA is based on the [[Hilbert-Schmidt Independence Criterion (HSIC)]] Hilbert-Schmidt Independence Criterion (HSIC) Given two kernels of the feature representations $K=k(x,x)$ and $L=l(y,y)$, HSIC is defined as12 $$ \operatorname{HSIC}(K, L) = \frac{1}{(n-1)^2} \operatorname{tr}( K H L H ), $$ where $x$, $y$ are the representations of features, $n$ is the dimension of the representation of the features, $H$ is the so-called [[centering matrix]] Centering Matrix Useful when centering a vector around its mean . We can choose different kernel functions $k$ and $l$.</description></item><item><title>Centering Matrix</title><link>https://datumorphism.leima.is/cards/math/statistics-centering-matrix/</link><pubDate>Mon, 08 Nov 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/statistics-centering-matrix/</guid><description>Given a vector $v$, with mean value of its elements $m$, we can center the vector by subtracting the mean $m$ from each element,
import numpy as np n = 10 v = np.random.randn(n) v_c = v - v.mean() This operation is easy and obvious. However, the formalism is not elegant. In some cases, we would like to formulate the process of centering the elements as operators,
$$ v_c = \operatorname{\hat H}v. $$
In this case, the operator $\operatorname{\hat H}$ is simply a matrix
$$ \operatorname{\hat H} \to I_n - \frac{1}{n} J_n, $$
where $n$ is the dimension of the vector $v$, $I_n$ is a identity matrix, $J_n$ is a matrix of all $1$s.</description></item><item><title>Hilbert-Schmidt Independence Criterion (HSIC)</title><link>https://datumorphism.leima.is/cards/machine-learning/measurement/hilbert-schmidt-independence-criterion/</link><pubDate>Mon, 08 Nov 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/measurement/hilbert-schmidt-independence-criterion/</guid><description>Given two kernels of the feature representations $K=k(x,x)$ and $L=l(y,y)$, HSIC is defined as12
$$ \operatorname{HSIC}(K, L) = \frac{1}{(n-1)^2} \operatorname{tr}( K H L H ), $$
where
$x$, $y$ are the representations of features, $n$ is the dimension of the representation of the features, $H$ is the so-called [[centering matrix]] Centering Matrix Useful when centering a vector around its mean . We can choose different kernel functions $k$ and $l$. For example, if $k$ and $l$ are linear kernels, we have $k(x, y) = l(x, y) = x \cdot y$. In this linear case, HSIC is simply $\parallel\operatorname{cov}(x^T,y^T) \parallel^2_{\text{Frobenius}}$.</description></item><item><title>Differential Learning Rates in PyTorch</title><link>https://datumorphism.leima.is/til/machine-learning/pytorch/pytorch-differential-learning-rates/</link><pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/machine-learning/pytorch/pytorch-differential-learning-rates/</guid><description>Using different learning rates in different layers of our artificial neural network.</description></item><item><title>Learning Rate</title><link>https://datumorphism.leima.is/cards/machine-learning/practice/learning-rate/</link><pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/practice/learning-rate/</guid><description>Finding a suitable learning rate for our model training is crucial.
A safe but time wasting option is to use search on a grid of parameters. However, there are smarter moves.
Karpathy’s Constant
An empirical learning rate $3^{-4}$ for Adms, aka, Karpathy's constant, was started as a tweet by Andrei Karpathy. 3e-4 is the best learning rate for Adam, hands down.
&amp;mdash; Andrej Karpathy (@karpathy) November 24, 2016 (i just wanted to make sure that people understand that this is a joke...)
&amp;mdash; Andrej Karpathy (@karpathy) November 24, 2016 Smarter Method A smarter method is to start with small learning rate and increase it on each mini-batch, then observe the loss vs learning rate (mini-batch in this case).</description></item><item><title>Shatter</title><link>https://datumorphism.leima.is/cards/machine-learning/learning-theories/set-shatter/</link><pubDate>Wed, 27 Oct 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/learning-theories/set-shatter/</guid><description>Given a set $\mathcal S$, and a class (collection of sets) $\mathcal H$.
For any subset of $\mathcal S$, denoted as $\mathcal s$, if we have an element of class $\mathcal H$, denoted as $\mathcal h$, that leads to1
$$ \mathcal h \cap \mathcal S = \mathcal s. $$
Since the power set of $\mathcal S$ ($P(\mathcal S)$) contains all the possible subsets of $\mathcal S$, we can also rephrase the concept using power set. If we can find the power set $P(\mathcal S)$ by looking into intersections of elements $\mathcal h$ of $\mathcal H$ ($\mathcal h\in \mathcal H$), then we say $\mathcal H$ shatters $\mathcal S$ 1.</description></item><item><title>Graph Global Overlap Measure: Katz Index</title><link>https://datumorphism.leima.is/cards/graph/graph-global-overlap-katz-index/</link><pubDate>Sun, 26 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/graph/graph-global-overlap-katz-index/</guid><description>The Katz index is
$$ \mathbf S_{\text{Katz}}[u,v] = \sum_{i=1}^\infty \beta^i \mathbf A^i[u, v], $$
where $\mathbf A^i[u, v]$ is the matrix $\mathbf A$ to the $i$th power. Some for $\beta^i$. The Katz index describes the similarity between of node $u$ and node $v$.
Do not confuse power with contravariant indices
For readers familiar with tensor notations, it might be confusing. We some times use contravariant indices on the top right of the tensor notation.
But here ${}^{i}$ means to the $i$th power.
The index is proved to be the following
$$ \mathbf S_{\text{Katz}} = (\mathbf I - \beta \mathbf A)^{-1} - \mathbf I.</description></item><item><title>Graph Global Overlap Measure: Leicht-Holme-Newman Index</title><link>https://datumorphism.leima.is/cards/graph/graph-global-overlap-leicht-holme-newman-similarity/</link><pubDate>Sun, 26 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/graph/graph-global-overlap-leicht-holme-newman-similarity/</guid><description>The LHN index is a normalized similarity index.
From Katz Index to LHN Index [[Katz Index]] Graph Global Overlap Measure: Katz Index The Katz index is $$ \mathbf S_{\text{Katz}}[u,v] = \sum_{i=1}^\infty \beta^i \mathbf A^i[u, v], $$ where $\mathbf A^i[u, v]$ is the matrix $\mathbf A$ to the $i$th power. Some for $\beta^i$. The Katz index describes the similarity between of node $u$ and node $v$. Do not confuse power with contravariant indices For readers familiar with tensor notations, it might be confusing. We some times use contravariant indices on the top right of the tensor notation. But here ${}^{i}$ means to the $i$th … has a knob to tune the punishment towards longer paths.</description></item><item><title>Graph Global Overlap Measure: Random Walk Similarity</title><link>https://datumorphism.leima.is/cards/graph/graph-global-overlap-random-walk-similarity/</link><pubDate>Sun, 26 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/graph/graph-global-overlap-random-walk-similarity/</guid><description>Random Walk Construct a stochastic transfer matrix $P$ by normalizing the adjacency matrix $\mathbf A$ using the node degrees of the target nodes,
$$ \mathbf P = \mathbf A \mathbf D^{-1}, $$
where $\mathbf A$ is the [[adjacency matrix]] Graph Adjacency Matrix A graph $\mathcal G$ can be represented with an adjacency matrix $\mathbf A$. There are some nice and clear examples on wikipedia1, for example, $$ \begin{pmatrix} 2 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0\\ 1 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0\\ 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 1\\ 1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \end{pmatrix} $$ for the graph Public Domain, Link and $\mathbf D$ is a diagonalized matrix with the diagonal elements being the degrees.</description></item><item><title>Graph Isomorphism</title><link>https://datumorphism.leima.is/cards/graph/graph-isomorphism/</link><pubDate>Sun, 26 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/graph/graph-isomorphism/</guid><description>For two graphs, $\mathcal G$ and $\mathcal H$, the two graphs are isomorphism on the following condition
$$ u, v \text{ adjacent in } G \iff u, v \text{ adjacent in } H. $$
An algorithm to find approximate isomorphism is the [[Weisfeiler Lehman Method]] Weisfeiler-Lehman Kernel The Weisfeiler-Lehman kernel is an iterative integration of neighborhood information. We initialize the labels for each node using its own node degree. At each step, we take the neighboring node degrees to form a [[multiset]] Multiset, mset or bag A bag is a set in which duplicate elements are allowed. An ordered bag is a list that we use in programming.</description></item><item><title>Graph Local Overlap Measure: Adamic Adar Index</title><link>https://datumorphism.leima.is/cards/graph/graph-local-overlap-adamic-adar-index/</link><pubDate>Sun, 26 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/graph/graph-local-overlap-adamic-adar-index/</guid><description>The Adamic Adar (AA) index is1
$$ \mathbf S_{\text{AA}}[v_1,v_2] = \sum_{u\in\mathcal N(u) \cap \mathcal N(v)} \frac{1}{\log d_u}, $$
where $d_u$ is the node degree of node $u$ and $\mathcal N(u)$ is the neighbor nodes of $u$.
If two nodes have shared neighbor, the degree of the neighbors will be at least 2. So it is safe to use $1/\log d_u$.
Adamic2003 Adamic LA, Adar E. Friends and neighbors on the Web. Soc Networks. 2003;25: 211–230. doi:10.1016/S0378-8733(03)00009-1 &amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Graph Local Overlap Measure: Resource Allocation Index</title><link>https://datumorphism.leima.is/cards/graph/graph-local-overlap-resource-allocation-index/</link><pubDate>Sun, 26 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/graph/graph-local-overlap-resource-allocation-index/</guid><description>The Resource Allocation (RA) index is
$$ \mathbf S_{\text{RA}}[v_1,v_2] = \sum_{u\in\mathcal N(u) \cap \mathcal N(v)} \frac{1}{d_u}, $$
where $d_u$ is the node degree of node $u$ and $\mathcal N(u)$ is the neighbor nodes of $u$.</description></item><item><title>Graph Local Overlap Measure: Salton Index</title><link>https://datumorphism.leima.is/cards/graph/graph-local-overlap-salton-index/</link><pubDate>Sun, 26 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/graph/graph-local-overlap-salton-index/</guid><description>The Salton index is
$$ \mathbf S_{\text{Salton}}[u,v] = \frac{ 2\lvert \mathcal N (u) \cap \mathcal N(v) \rvert }{ \sqrt{d_u d_v}}, $$
where $d_u$ is the node degree of node $u$ and $\mathcal N(u)$ is the neighbor nodes of $u$.</description></item><item><title>Graph Local Overlap Measure: Sorensen Index</title><link>https://datumorphism.leima.is/cards/graph/graph-local-overlap-sorensen-index/</link><pubDate>Sun, 26 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/graph/graph-local-overlap-sorensen-index/</guid><description>The Sorensen index is
$$ \mathbf S_{\text{Sorensen}}[u,v] = \frac{ 2\lvert \mathcal N (u) \cap \mathcal N(v) \rvert }{ d_u + d_v}, $$
where $d_u$ is the node degree of node $u$ and $\mathcal N(u)$ is the neighbor nodes of $u$.</description></item><item><title>Betweenness Centrality of a Graph</title><link>https://datumorphism.leima.is/cards/graph/graph-betweenness-centrality/</link><pubDate>Sat, 25 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/graph/graph-betweenness-centrality/</guid><description>Betweenness centrality of a node $v$ is measurement of how likely the shortest path between two nodes $u_s$ and $u_t$ is gonna pass through node $v$,
$$ c(v) = \sum_{v\neq u_s\neq u_t} \frac{\sigma_{u_su_t}(v) }{\sigma_{u_su_t}}, $$
where $\sigma_{u_su_t}(v)$ is the number of shortest path between $u_s$ and $u_t$, and passing through $u$, while $\sigma_{u_su_t}$ is the number of shortest path between $u_s$ and $u_t$.
A figure from wikipedia demonstrates this idea well. The nodes on the outreach have smaller betweenness centrality, while the nodes in the core have higher betweenness centrality.
Source: Wikipedia
Outreach and Core
It is almost like cheating using the work &amp;ldquo;outreach&amp;rdquo; and &amp;ldquo;core&amp;rdquo; here.</description></item><item><title>Eigenvector Centrality of a Graph</title><link>https://datumorphism.leima.is/cards/graph/graph-eigenvector-centrality/</link><pubDate>Sat, 25 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/graph/graph-eigenvector-centrality/</guid><description>Given a graph with adjacency matrix $\mathbf A$, the eigenvector centrality is
$$ \mathbf e_u = \frac{1}{\lambda} \sum_{v\in\mathcal V} \mathbf A[u,v] \mathbf e_v, \qquad \forall u \in \mathcal V. $$
Why is it called Eigenvector Centrality
The definition is equivalent to
$$ \lambda \mathbf e = \mathbf A\mathbf e. $$
Power Iteration The solution to $\mathbf e$ is the eigenvector that corresponds to the largest eigenvalue $\lambda_1$. Power iteration method can help us get this eigenvector, i.e., the $^{(t+1)}$ iteration is related to the previous iteration $^{(t)}$, through the following relation,
$$ \mathbf e^{(t+1)} = \mathbf A \mathbf e^{(t)}.</description></item><item><title>Graph Adjacency Matrix</title><link>https://datumorphism.leima.is/cards/graph/graph-adjacency-matrix/</link><pubDate>Sat, 25 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/graph/graph-adjacency-matrix/</guid><description>A graph $\mathcal G$ can be represented with an adjacency matrix $\mathbf A$. There are some nice and clear examples on wikipedia1, for example,
$$ \begin{pmatrix} 2 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0\\ 1 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0\\ 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 1\\ 1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \end{pmatrix} $$
for the graph
Public Domain, Link</description></item><item><title>Graph Clustering Coefficient</title><link>https://datumorphism.leima.is/cards/graph/graph-local-clustering-coefficient/</link><pubDate>Sat, 25 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/graph/graph-local-clustering-coefficient/</guid><description>Local Clustering Coefficient $$ c_u = \frac{ \lvert (v_1,v_2)\in \mathcal E: v_1, v_2 \in \mathcal N(u) \rvert}{ \color{red}{d_n \choose 2} }, $$
where $\color{red}{d_n \choose 2}$ means all the possible combinations of neighbor nodes, and $\mathcal N(u)$ is the set of nodes that are neighbor to $u$.
Closed Triangles Ego Graph
Counting the closed triangles of the ego graph of a node and normalize it by the total possible number of triangles is also a measure of clustering coefficients.
If the ego graph of $u$ is fully connected, we have $c_u=1$; If the ego graph of $u$ is a star, we have $c_u=0$.</description></item><item><title>Graph Cuts</title><link>https://datumorphism.leima.is/cards/graph/graph-cuts/</link><pubDate>Sat, 25 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/graph/graph-cuts/</guid><description>Cut For a subset of nodes $\mathcal A\subset \mathcal V$, the rest of nodes can be denoted as $\bar {\mathcal A} = \mathcal V \setminus \mathcal A$. In other words, $\mathcal A \cup \bar {\mathcal A} = \mathcal V$ and $\mathcal A \cap \bar {\mathcal A} = \emptyset$. That being said, the nodes can be partitioned into two subsets, $\mathcal A$ and $\bar {\mathcal A}$. The cut of this partition is defined as the total number of edges between them,
$$ \operatorname{Cut} \left( \mathcal A, \bar{\mathcal A} \right) = \frac{1}{2} \left( \lvert (u, v)\in \mathcal E: u\in \mathcal A, v\in \bar{\mathcal A} \rvert + \lvert (u, v)\in \mathcal E: u\in \bar{\mathcal A}, v\in {\mathcal A} \rvert \right).</description></item><item><title>Graph Laplacians</title><link>https://datumorphism.leima.is/cards/graph/graph-laplacians/</link><pubDate>Sat, 25 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/graph/graph-laplacians/</guid><description>Laplacian is a useful representation of graphs. The unnormalized Laplacian is
$$ \mathbf L = \mathbf D - \mathbf A, $$
where $\mathbf A$ is the [[adjacency matrix]] Graph Adjacency Matrix A graph $\mathcal G$ can be represented with an adjacency matrix $\mathbf A$. There are some nice and clear examples on wikipedia1, for example, $$ \begin{pmatrix} 2 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0\\ 1 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0\\ 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 1\\ 1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \end{pmatrix} $$ for the graph Public Domain, Link and $\mathbf D$ is the degree matrix, i.</description></item><item><title>Heterophily on Graph</title><link>https://datumorphism.leima.is/cards/graph/heterophily/</link><pubDate>Sat, 25 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/graph/heterophily/</guid><description>Heterophily is the tendency to differ from others. Heterophily on a graph is the tendency to connect to nodes that are different from itself, e.g., nodes with different attributes have higher probability of edge.</description></item><item><title>Homophily on Graph</title><link>https://datumorphism.leima.is/cards/graph/homophily/</link><pubDate>Sat, 25 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/graph/homophily/</guid><description> Homophily is the principle that a contact between similar people occurs at ahigher rate than among dissimilar people &amp;ndash; McPherson20011
McPherson2001 McPherson M, Smith-Lovin L, Cook JM. Birds of a Feather: Homophily in Social Networks. Annu Rev Sociol. 2001;27: 415–444. doi:10.1146/annurev.soc.27.1.415 &amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Node Degree</title><link>https://datumorphism.leima.is/cards/graph/node-degree/</link><pubDate>Sat, 25 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/graph/node-degree/</guid><description>Node degree of a node $u$
$$ d_u = \sum_{v\in \mathcal V} A[u,v], $$
where $A$ is the adjacency matrix.</description></item><item><title>Structural Equivalence on Graph</title><link>https://datumorphism.leima.is/cards/graph/structural-equivalence/</link><pubDate>Sat, 25 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/graph/structural-equivalence/</guid><description>Structural Equivalence means that nodes with similar neighborhood structures will share similar attributes.</description></item><item><title>Weisfeiler-Lehman Kernel</title><link>https://datumorphism.leima.is/cards/graph/graph-weisfeiler-lehman-kernel/</link><pubDate>Sat, 25 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/graph/graph-weisfeiler-lehman-kernel/</guid><description>The Weisfeiler-Lehman kernel is an iterative integration of neighborhood information.
We initialize the labels for each node using its own node degree. At each step, we take the neighboring node degrees to form a [[multiset]] Multiset, mset or bag A bag is a set in which duplicate elements are allowed. An ordered bag is a list that we use in programming. . At step $K$, we have the multisets for each node. Those multisets at each node can be processed to form an representation of the graph which is in turn used to calculate statistics of the graph.</description></item><item><title>Initialize Artificial Neural Networks</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/neural-networks-initialization/</link><pubDate>Thu, 23 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/neural-networks-initialization/</guid><description>The weights are better if they1
are zero centered, and have similar variance across layers. Why
If we have very different variances across layers, we will need a different learning rate for each layer for our optimization. Setting the variances to be on the same scale, we can use a global learning rate for the whole network. Variance is related to the input size of the layer Suppose we are using a simple linear activation, $\sigma(x) = \alpha x$. For a series of inputs $x_j$, the outputs $y_i$ are
$$ y_i = \sum_{j} w_{ij} x_j. $$</description></item><item><title>Alignment and Uniformity</title><link>https://datumorphism.leima.is/cards/machine-learning/embedding/alignment-and-uniformity/</link><pubDate>Sat, 11 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/embedding/alignment-and-uniformity/</guid><description>A good representation should be able to
separate different instances, and cluster similar instances. Wang et al proposed two concepts that matches the above two ideas, alignment and uniformity, on a hypersphere1.
From Wang et al
Wang T, Isola P. Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2005.10242&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Cross Entropy</title><link>https://datumorphism.leima.is/cards/information/cross-entropy/</link><pubDate>Sat, 04 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/information/cross-entropy/</guid><description>Cross entropy is1
$$ H(p, q) = \mathbb E_{p} \left[ -\log q \right]. $$
Cross entropy $H(p, q)$ can also be decomposed,
$$ H(p, q) = H(p) + \operatorname{D}_{\mathrm{KL}} \left( p \parallel q \right), $$
where $H(p)$ is the [[entropy of $P$]] Shannon Entropy Shannon entropy $S$ is the expectation of information content $I(X)=-\log \left(p\right)$1, \begin{equation} H(p) = \mathbb E_{p}\left[ -\log \left(p\right) \right]. \end{equation} shannon_entropy_wiki Contributors to Wikimedia projects. Entropy (information theory). In: Wikipedia [Internet]. 29 Aug 2021 [cited 4 Sep 2021]. Available: https://en.wikipedia.org/wiki/Entropy_(information_theory) &amp;#160;&amp;#x21a9;&amp;#xfe0e; and $\operatorname{D}_{\mathrm{KL}}$ is the [[KL Divergence]] KL Divergence Kullback–Leibler divergence indicates the differences between two distributions .</description></item><item><title>f-Divergence</title><link>https://datumorphism.leima.is/cards/information/f-divergence/</link><pubDate>Sat, 04 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/information/f-divergence/</guid><description>The f-divergence is defined as1
$$ \operatorname{D}_f = \int f\left(\frac{p}{q}\right) q\mathrm d\mu, $$
where $p$ and $q$ are two densities and $\mu$ is a reference distribution.
Requirements on the generating function
The generating function $f$ is required to
be convex, and $f(1) =0$. For $f(x) = x \log x$ with $x=p/q$, f-divergence is reduced to the KL divergence
$$ \begin{align} &amp;\int f\left(\frac{p}{q}\right) q\mathrm d\mu \\ =&amp; \int \frac{p}{q} \log \left( \frac{p}{q} \right) \mathrm d\mu \\ =&amp; \int p \log \left( \frac{p}{q} \right) \mathrm d\mu. \end{align} $$
For more special cases of f-divergence, please refer to wikipedia1.</description></item><item><title>Jensen-Shannon Divergence</title><link>https://datumorphism.leima.is/cards/information/jensen-shannon-divergence/</link><pubDate>Sat, 04 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/information/jensen-shannon-divergence/</guid><description>The Jensen-Shannon divergence is a symmetric divergence of distributions $P$ and $Q$,
$$ \operatorname{D}_{\text{JS}} = \frac{1}{2} \left[ \operatorname{D}_{\text{KL}} \left(P \bigg\Vert \frac{P+Q}{2} \right) + \operatorname{D}_{\text{KL}} \left(Q \bigg\Vert \frac{P+Q}{2}\right) \right], $$
where $\operatorname{D}_{\text{KL}}$ is the [[KL Divergence]] KL Divergence Kullback–Leibler divergence indicates the differences between two distributions .</description></item><item><title>Shannon Entropy</title><link>https://datumorphism.leima.is/cards/information/shannon-entropy/</link><pubDate>Sat, 04 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/information/shannon-entropy/</guid><description>Shannon entropy $S$ is the expectation of information content $I(X)=-\log \left(p\right)$1,
\begin{equation} H(p) = \mathbb E_{p}\left[ -\log \left(p\right) \right]. \end{equation}
shannon_entropy_wiki Contributors to Wikimedia projects. Entropy (information theory). In: Wikipedia [Internet]. 29 Aug 2021 [cited 4 Sep 2021]. Available: https://en.wikipedia.org/wiki/Entropy_(information_theory) &amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>VSCode Setup Tests when Module is in a Different Folder</title><link>https://datumorphism.leima.is/til/misc/vscode/vscode-setup-python-tests-with-module-in-src-folder/</link><pubDate>Tue, 31 Aug 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/misc/vscode/vscode-setup-python-tests-with-module-in-src-folder/</guid><description>Use .env file</description></item><item><title>VSCode Terminal Python Can Not Activate Conda on Mac</title><link>https://datumorphism.leima.is/til/misc/vscode/vscode-terminal-python-not-conda-mac/</link><pubDate>Tue, 31 Aug 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/misc/vscode/vscode-terminal-python-not-conda-mac/</guid><description>Enable your key repeat in vscode on mac</description></item><item><title>Postgres Timezone Conversions</title><link>https://datumorphism.leima.is/til/data/postgres.timezone-conversion/</link><pubDate>Fri, 27 Aug 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/data/postgres.timezone-conversion/</guid><description>Pitfals of timezone conversion in Postgres</description></item><item><title>Mutual Information</title><link>https://datumorphism.leima.is/cards/information/mutual-information/</link><pubDate>Fri, 13 Aug 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/information/mutual-information/</guid><description>Mutual information is defined as
$$ I(X;Y) = \mathbb E_{p_{XY}} \ln \frac{P_{XY}}{P_X P_Y}. $$
In the case that $X$ and $Y$ are independent variables, we have $P_{XY} = P_X P_Y$, thus $I(X;Y) = 0$. This makes sense as there would be no &amp;ldquo;mutual&amp;rdquo; information if the two variables are independent of each other.
Entropy and Cross Entropy Mutual information is closely related to entropy. A simple decomposition shows that
$$ I(X;Y) = H(X) - H(X\mid Y), $$
which is the reduction of uncertainty in $X$ after observing $Y$.
KL Divergence This definition of mutual information is equivalent to the following [[KL Divergence]] KL Divergence Kullback–Leibler divergence indicates the differences between two distributions ,</description></item><item><title>Noise Contrastive Estimation: NCE</title><link>https://datumorphism.leima.is/cards/machine-learning/learning-theories/noise-contrastive-estimation/</link><pubDate>Fri, 13 Aug 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/learning-theories/noise-contrastive-estimation/</guid><description>Noise contrastive estimation (NCE) objective function is1
$$ \mathcal L = \mathbb E_{x, x^{+}, x^{-}} \left[ - \ln \frac{ C(x, x^{+})}{ C(x,x^{+}) + C(x,x^{-}) } \right], $$
where
$x^{+}$ represents data similar to $x$, $x^{-}$ represents data dissimilar to $x$, $C(\cdot, \cdot)$ is a function to compute the similarities. For example, we can use
$$ C(x, x^{+}) = e^{ f(x)^T f(x^{+}) }, $$
so that the objective function becomes
$$ \mathcal L = \mathbb E_{x, x^{+}, x^{-}} \left[ - \ln \frac{ e^{ f(x)^T f(x^{+}) } }{ e^{ f(x)^T f(x^{+}) } + e^{ f(x)^T f(x^{-}) } } \right]. $$</description></item><item><title>Self-supervised Learning: Generative or Constrastive</title><link>https://datumorphism.leima.is/reading/self-supervised-learning-generative-or-contrastive-2006.08218/</link><pubDate>Fri, 13 Aug 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/reading/self-supervised-learning-generative-or-contrastive-2006.08218/</guid><description>Review of self-supervised learning.</description></item><item><title>The log-sum-exp Trick</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/log-sum-exp-trick/</link><pubDate>Wed, 28 Jul 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/log-sum-exp-trick/</guid><description>The cross entropy for a binary class is
$$ p \ln \hat p + (1-p) \ln (1-\hat p), $$
where $p$ is the probability of the label A and $\hat p$ is the predicted probability of label A. Since we have binary classes, $p$ is either 1 or 0. However, the predicted probabilities can be any value between $[0,1]$.
Probability
For a very simple case, $\hat p$ might be a sigmoid like expression with exponential in it,
$$ p \sim \frac{1}{1 + \exp(-x)}, $$
where $x$ is some kind of input or intermediate input.
The problem is, exponentials may blow up if $p\to 0$.</description></item><item><title>Managing path using pathlib in Python</title><link>https://datumorphism.leima.is/til/programming/python/python-managing-paths-using-pathlib-is-easier/</link><pubDate>Thu, 15 Jul 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/python/python-managing-paths-using-pathlib-is-easier/</guid><description>Since Python 3.4
pathlib is object oriented. It is more elegant than os.path. For example, if we need the parent folders of the currrent file, we need os.path.dirname(),
import os print(f&amp;#34;file: {__file__}&amp;#34;) # file: main.py # Using os.path os__file_absolute_path = os.path.abspath(__file__) print(f&amp;#34;Using os.path:: file absolute path: {os__file_absolute_path}&amp;#34;) # Using os.path:: file absolute path: /home/runner/pathlib/main.py os__file_in_folder = os.path.dirname(os__file_absolute_path) print(f&amp;#34;Using os.path:: file is in folder: {os__file_in_folder}&amp;#34;) # Using os.path:: file is in folder: /home/runner/pathlib It is much more easier to get the folder using pathlib.
from pathlib import Path print(f&amp;#34;file: {__file__}&amp;#34;) # file: main.py # Using pathlib path__file = Path(__file__) print(f&amp;#34;Using pathlib:: path__file: {path__file}; using .</description></item><item><title>Box-Cox Transformation</title><link>https://datumorphism.leima.is/cards/statistics/box-cox/</link><pubDate>Tue, 13 Jul 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/box-cox/</guid><description>Box-Cox transformation is a power transformation that involves logs and powers. It transforms data into normal distributions.
The Box-Cox transformation is defined as
$$ y_i^{(\lambda)} = \begin{cases} \lambda ^{-1} (y_i^\lambda - 1) &amp; \quad \text{if } \lambda \neq 0\\ \log(y_i) &amp; \quad \text{if } \lambda = 0. \end{cases} $$
By selecting a proper $\lambda$, we get a Guassian distributed data, with a variable mean. The transformation take $y$ to
$$ \rho(y^{(\lambda)}) =\frac{ \exp{\left( -(y^{(\lambda)} - \beta X)^{T} (y^{(\lambda)} - \beta X)/(2\sigma^2) \right) }}{(\sqrt{2\pi \sigma^2})^n} \prod_{i=1}^n \left\lvert \frac{d y_i^{(\lambda )}}{ dy_i } \right\rvert. $$
The term
$$ \prod_{i=1}^n \left\lvert \frac{d y_i^{(\lambda )}}{ dy_i } \right\rvert = \lvert J \rvert $$</description></item><item><title>The Hubbard-Stratonovich Identity</title><link>https://datumorphism.leima.is/cards/math/hubbard-stratonovich-identity/</link><pubDate>Thu, 17 Jun 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/hubbard-stratonovich-identity/</guid><description>The Hubbard version of the Hubbard-Stratonovich identity is1
$$ \begin{align} \exp{\left( a^2 \right)} =&amp; \frac{1}{\sqrt{\pi}} \int_{-\infty}^\infty \mathrm dx\, \exp{ \left( - x^2 - 2 a x \right)}\\ =&amp; \frac{1}{\sqrt{\pi}} \int_{\infty}^{-\infty} \mathrm dx'\, \exp{ \left( - x'^2 + 2 a x' \right)}, \end{align} $$
where we changed the sign of $x$, i.e., $x' = -x$.
In many partition functions, we have expressions like $\exp{\left( a^2/2\right)}$, using the identity, we have
$$ \begin{align} \exp{\left( \frac{a^2}{2} \right)} =&amp; \frac{1}{\sqrt{\pi}} \int_{\infty}^{-\infty} \mathrm dx\, \exp{ \left( - x^2 + \sqrt{2} a x \right)} \\ =&amp; \frac{1}{\sqrt{2\pi}} \int_{\infty}^{-\infty} \mathrm dx'\, \exp{ \left( - \frac{x'^2}{2} + a x' \right)}, \end{align} $$</description></item><item><title>Likelihood</title><link>https://datumorphism.leima.is/cards/statistics/likelihood/</link><pubDate>Wed, 26 May 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/likelihood/</guid><description>For some data points $\{x_i\}$ and a model $\theta$, the likelihood of our data point $x_i$ is $p(x_i\mid \theta)$. To be more specific, the likelihood of all data points is a function of the model $\theta$,
$$ L(\theta) = \Pi_i p(x_i\mid\theta). $$
It should be mentioned that this likelihood is not necessarily a pdf. As an example, we can calculate the likelihood of a Bernoulli distribution for a single event $x$,
$$ L(\theta) = \theta^x (1-\theta)^{(1-x)}. $$
If we are flipping coins, and the head $x=1$ probability is $\theta$, the likelihood for this single event $x=1$ is
$$ L(\theta)=\theta. $$</description></item><item><title>Gaussian Integrals</title><link>https://datumorphism.leima.is/cards/math/gaussian-integrals/</link><pubDate>Tue, 11 May 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/gaussian-integrals/</guid><description>The diagonalized case
$$ \begin{eqnarray} Z_0 &amp;=&amp; \int d^n z \exp\left(-\frac{1}{2} z^\mathrm{T} D z\right) \\ &amp;=&amp; \prod_i \int d z_i \exp\left(-\frac{1}{2} \lambda_i z_i^2\right) \\ &amp;=&amp; \prod_i \sqrt{\frac{2\pi}{\lambda_i}} \\ &amp;=&amp; \sqrt{\frac{(2\pi)^n}{\det A}}. \end{eqnarray} $$
For an arbitrary matrix $A$,
$$ Z_J = \int d^n x \exp\left(-\frac{1}{2} x^\mathrm{T} A x + J^\mathrm{T} x\right). $$
$$ \begin{eqnarray} Z_J &amp;=&amp; \int d^n y \exp\left(-\frac{1}{2} {y}^\mathrm{T} A y + \frac{1}{2} J^\mathrm{T}A^{-1}J\right) \\ &amp;=&amp; \sqrt{\frac{(2\pi)^n}{\det A}} \exp\left(\frac{1}{2} J^\mathrm{T}A^{-1}J\right). \end{eqnarray} $$</description></item><item><title>Cross Validation</title><link>https://datumorphism.leima.is/cards/machine-learning/learning-theories/cross-validation/</link><pubDate>Thu, 06 May 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/learning-theories/cross-validation/</guid><description>Cross validation is a method to estimate the [[risk]] The Learning Problem The learning problem posed by Vapnik:1 Given a sample: $\{z_i\}$ in the probability space $Z$; Assuming a probability measure on the probability space $Z$; Assuming a set of functions $Q(z, \alpha)$ (e.g. loss functions), where $\alpha$ is a set of parameters; A risk functional to be minimized by tunning &amp;ldquo;the handles&amp;rdquo; $\alpha$, $R(\alpha)$. The risk functional is $$ R(\alpha) = \int Q(z, \alpha) \,\mathrm d F(z). $$ A learning problem is the minimization of this risk. Vapnik2000 … .
To perform cross validation, we split the train dataset $\mathcal D$ into $k$ folds, with each fold denoted as $\mathcal D_k$.</description></item><item><title>The Learning Problem</title><link>https://datumorphism.leima.is/cards/machine-learning/learning-theories/learning-problem/</link><pubDate>Thu, 06 May 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/learning-theories/learning-problem/</guid><description>The learning problem posed by Vapnik:1
Given a sample: $\{z_i\}$ in the probability space $Z$; Assuming a probability measure on the probability space $Z$; Assuming a set of functions $Q(z, \alpha)$ (e.g. loss functions), where $\alpha$ is a set of parameters; A risk functional to be minimized by tunning &amp;ldquo;the handles&amp;rdquo; $\alpha$, $R(\alpha)$. The risk functional is
$$ R(\alpha) = \int Q(z, \alpha) \,\mathrm d F(z). $$
A learning problem is the minimization of this risk.
Vapnik2000 Vladimir N. Vapnik. The Nature of Statistical Learning Theory. 2000. doi:10.1007/978-1-4757-3264-1 &amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Explained Variation</title><link>https://datumorphism.leima.is/cards/statistics/explained-variation/</link><pubDate>Wed, 05 May 2021 18:05:47 +0200</pubDate><guid>https://datumorphism.leima.is/cards/statistics/explained-variation/</guid><description>Using [[Fraser information]] Fraser Information The Fraser information is $$ I_F(\theta) = \int g(X) \ln f(X;\theta) , \mathrm d X. $$ When comparing two models, $\theta_0$ and $\theta_1$, the information gain is $$ \propto (F(\theta_1) - F(\theta_0)). $$ The Fraser information is closed related to [[Fisher information]] Fisher Information Fisher information measures the second moment of the model sensitivity with respect to the parameters. , Shannon information, and [[Kullback information]] KL Divergence Kullback–Leibler divergence … , we can define a relative information gain by a model
$$ \rho_C ^2 = 1 - \frac{ \exp( - 2 F(\theta_1) ) }{ \exp( - 2 F(\theta_0) ) }, $$</description></item><item><title>Fraser Information</title><link>https://datumorphism.leima.is/cards/information/fraser-information/</link><pubDate>Wed, 05 May 2021 17:49:12 +0200</pubDate><guid>https://datumorphism.leima.is/cards/information/fraser-information/</guid><description>The Fraser information is
$$ I_F(\theta) = \int g(X) \ln f(X;\theta) , \mathrm d X. $$
When comparing two models, $\theta_0$ and $\theta_1$, the information gain is
$$ \propto (F(\theta_1) - F(\theta_0)). $$
The Fraser information is closed related to [[Fisher information]] Fisher Information Fisher information measures the second moment of the model sensitivity with respect to the parameters. , Shannon information, and [[Kullback information]] KL Divergence Kullback–Leibler divergence indicates the differences between two distributions 1.
Fraser DAS. On Information in Statistics. aoms. 1965;36: 890–896. doi:10.1214/aoms/1177700061&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Fisher Information</title><link>https://datumorphism.leima.is/cards/information/fisher-information/</link><pubDate>Wed, 05 May 2021 17:49:03 +0200</pubDate><guid>https://datumorphism.leima.is/cards/information/fisher-information/</guid><description>Given a probability density model $f(X; \theta)$ for a observable $X$, the amount of information that $X$ carriers regarding the model is called Fisher information.
Given ${\theta}$, the probability of observing the value $X$, i.e., the likelihood is
$$ f(X\mid\theta). $$
To describe the suitability of a model and the observables, we can use a the likelihood $f(X\mid \theta)$. One particular interesting property is the sensitivity of the likelihood in terms of the parameter $\theta$ change. For example, the case on the left is less compatible as we have a large variance in the parameters. The model is not very sensitive to the parameter change.</description></item><item><title>Evidence Lower Bound: ELBO</title><link>https://datumorphism.leima.is/wiki/machine-learning/bayesian/elbo/</link><pubDate>Mon, 12 Apr 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/bayesian/elbo/</guid><description>This article reuses a lot of materials from the references. Please see the references for more details on ELBO. Given a probability distribution density $p(X)$ and a latent variable $Z$, we have the marginalization of the joint probability
$$ \int dZ p(X, Z) = p(X). $$
Using Jensen&amp;rsquo;s Inequality In many models, we are interested in the log probability density $\log p(X)$ which can be decomposed using an auxiliary density of the latent variable $q(Z)$,
$$ \begin{align} \log p(X) =&amp; \log \int dZ p(X, Z) \\ =&amp; \log \int dZ p(X, Z) \frac{q(Z)}{q(Z)} \\ =&amp; \log \int dZ q(Z) \frac{p(X, Z)}{q(Z)} \\ =&amp; \log \mathbb E_q \left[ \frac{p(X, Z)}{q(Z)} \right].</description></item><item><title>Jensen's Inequality</title><link>https://datumorphism.leima.is/cards/math/jensens-inequality/</link><pubDate>Mon, 12 Apr 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/jensens-inequality/</guid><description>Jensen&amp;rsquo;s inequality shows that
$$ f(\mathbb E(X)) \leq \mathbb E(f(X)) $$
for a concave function $f(\cdot)$.</description></item><item><title>Valid Confidence Sets in Multiclass and Multilabel Prediction</title><link>https://datumorphism.leima.is/wiki/machine-learning/classification/valid-confidence-sets-in-multiclass-multilabel-prediction/</link><pubDate>Thu, 08 Apr 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/classification/valid-confidence-sets-in-multiclass-multilabel-prediction/</guid><description>Ask for valid confidence:
&amp;ldquo;Valid&amp;rdquo;: validate for test data, train data, or the generating process? &amp;ldquo;Confidence&amp;rdquo;: $P(Y \notin C(X)) \le \alpha$ To avoid too much attention on data based validation, a framework called conformal inference was proposed by Vovk et al. in 2005,
$n$ observations, desired confidence level $1-\alpha$, construct confidence sets $C(x)$ using conform methods so that the sets capture the underlying the distribution a new pair $(X_{n+1}, Y_{n+1})$ from the same distribution, $P(Y_{n+1}\in C(X_{n+1})) \le 1-\alpha$</description></item><item><title>KL Divergence</title><link>https://datumorphism.leima.is/wiki/machine-learning/basics/kl-divergence/</link><pubDate>Mon, 05 Apr 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/basics/kl-divergence/</guid><description>Given two distributions $p(x)$ and $q(x)$, the Kullback-Leibler divergence is defined as
$$ D_\text{KL}(p(x) \parallel q(x) ) = \int_{-\infty}^\infty p(x) \log\left(\frac{p(x)}{q(x)}\right)\, dx = \mathbb E_{p(x)} \left[\log\left(\frac{p(x)}{q(x)}\right) \right]. $$
Connection to Entropy
Notice that this expression is quite similar to entropy,
$$ H(p(x)) = \int_{-\infty}^{\infty} p(x) \log p(x) , dx. $$
The entropy describes the lower bound of the number of bits (if we use $\log_2$) of how the information can be compressed. By looking at the expression of the KL divergence, we intuitively interpret it as the information loss if we use distribution $q(x)$ to approximate distribution $p(x)$, Kurt2017</description></item><item><title>Hierarchical Classification</title><link>https://datumorphism.leima.is/wiki/machine-learning/classification/hierarchical-classification/</link><pubDate>Tue, 30 Mar 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/classification/hierarchical-classification/</guid><description>Hierarchical Classification Problem Hierarchical classification labels involves hierarchical class labels. The hierarchical class labels maybe predefined or inferred. 1
Class Taxonomy A hierarchical classification problem comes with a class taxonomy.
&amp;ldquo;IS-A&amp;rdquo; operator: $\prec$, &amp;ldquo;IS-NOT-A&amp;rdquo; operator: $\nprec$ A IS-A relationship of the labels $c_a$ class set $C$ is
one root $R$ in the tree, asymmetric, i.e., $c_i \prec c_j$ and $c_j\prec c_i$ can not be both true, anti-reflexive, i.e., $c_i \nprec c_i$, transitive, i.e., $c_i \prec c_j$ and $c_j\prec c_k$ $\Rightarrow$ $c_i \prec c_k$. There are different representations of the hierarchical taxonomies.
Figure 2 in Silla2011, showing the difference between tree taxonomy and DAG taxonomy.</description></item><item><title>Classifier Chains for Multilabel Classification</title><link>https://datumorphism.leima.is/wiki/machine-learning/classification/classifier-chains/</link><pubDate>Wed, 24 Mar 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/classification/classifier-chains/</guid><description>Multi-label problem In some classification problems, we have multilabel labels to be predicted. Many different approaches are proposed to solve such problems.
Algorithm Level Develop algorithms for multilabel problems, such as
Decision trees, AdaBoost. Problem Transformation On problem or data level, we can transform the multi-label problem to one or more single label problems.
Binary Relevance Method Binary relevance method, aka BM, transforms the problem into a single label problem by training a binary classifier for each label.
By doing so, the correlations between the target labels are lost.
Label Combination Method Label combination method (label power-set method), aka CM, combines the labels into single labels.</description></item><item><title>Binning Data Values using Pandas</title><link>https://datumorphism.leima.is/til/programming/pandas/pandas-binning-values/</link><pubDate>Wed, 10 Mar 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/pandas/pandas-binning-values/</guid><description>Use the pd.cut function. The bins argument is using (] are the segments. The official documentation comes with detailed examples.
If pandas is not an option, one could use numpy.digitize to find which bin the elements belong to.</description></item><item><title>Deal with Rare Categories Using Pandas</title><link>https://datumorphism.leima.is/til/data/deal-with-rare-categories-using-pandas/</link><pubDate>Wed, 10 Mar 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/data/deal-with-rare-categories-using-pandas/</guid><description>We will illustrate how to deal with rare categories using pandas mask.
import pandas as pd ############# # Create fake names frequent_names = list(&amp;#39;ABC&amp;#39;) rare_names = list(&amp;#39;DEF&amp;#39;) dataset = sum( [[i]*10 for i in frequent_names] + [[i]*2 for i in rare_names], [] ) # Create a series based on the names series = pd.Series(dataset) print(series) # Find the counts of the names in the series series_counts = series.value_counts() print(series_counts) # Find names that has less than 10 counts # And create a mask mask = series.isin(series_counts.loc[series_counts&amp;lt;10].index) print(mask) # Set these rare names to X series[mask] = &amp;#39;X&amp;#39; # Check the new series print(series.</description></item><item><title>ANOVA</title><link>https://datumorphism.leima.is/wiki/statistics/anova/</link><pubDate>Sun, 07 Mar 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/statistics/anova/</guid><description>In many problems, we have to test if several distributions associated with several groups of experiments are the same. The null hypothesis to be used is
The distributions of several groups are the same.
ANOVA tests the null hypothesis by comparing the variability between groups and within groups. If the variability between groups are significantly larger than the variability within groups, we are more confident that the distributions of different groups are different.
We will use two-group experiments as an example. We use a fake dataset:
Group A $x^A_1$ $x^A_2$ &amp;hellip; $x^A_{N_A}$ Group B $x^B_1$ $x^B_2$ &amp;hellip; $x^B_{N_B}$ Within Group Variability The within group variability is proportional to</description></item><item><title>McCulloch-Pitts Model</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/mcculloch-pitts-model/</link><pubDate>Thu, 25 Feb 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/mcculloch-pitts-model/</guid><description>The McCulloch-Pitts model maps the input $\{x_1, x_2,\cdots, x_i \cdots, x_N \}$ into a scalar $y\in\{1,-1\}$,
$$ y = \operatorname{sign}( w\cdot x - b). $$
Since $w\cdot x - b = 0$ is a hyperplane, the McCulloch-Pitts model separates the state space using this hyperplane. The shift $b$ determines the interception, and $w$ decides the slope.</description></item><item><title>Rosenblatt's Perceptron</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/rosenblatt-perceptron/</link><pubDate>Thu, 25 Feb 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/rosenblatt-perceptron/</guid><description>Rosenblatt&amp;rsquo;s perceptron connects McCulloch-Pitts neurons in levels.
Rosenblatt proposed that we fix all the weights and leave the weights of the last neuron free.
The first few layers but the last layer is used as a transformation of the input data ${x_1, \cdots, x_i, \cdots, x_N}$ into a new space ${z_1, \cdots, z_i, \cdots, z_{N'}}$. The classification is done on the ${z_1, \cdots, z_i, \cdots, z_{N'}}$ space by tuning the last neuron.
Initially, we set $w=0$. At step $k$,
if the sign prediction by the perceptron $( w_k \cdot z_{k+1} )$ is the same as the data $y_{k+1}$, i.</description></item><item><title>ERM: Empirical Risk Minimization</title><link>https://datumorphism.leima.is/cards/machine-learning/learning-theories/empirical-risk-minimization/</link><pubDate>Thu, 18 Feb 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/learning-theories/empirical-risk-minimization/</guid><description>In a [[learning problem]] The Learning Problem The learning problem posed by Vapnik:1 Given a sample: $\{z_i\}$ in the probability space $Z$; Assuming a probability measure on the probability space $Z$; Assuming a set of functions $Q(z, \alpha)$ (e.g. loss functions), where $\alpha$ is a set of parameters; A risk functional to be minimized by tunning &amp;ldquo;the handles&amp;rdquo; $\alpha$, $R(\alpha)$. The risk functional is $$ R(\alpha) = \int Q(z, \alpha) \,\mathrm d F(z). $$ A learning problem is the minimization of this risk. Vapnik2000 … , empirical risk $R$ is a measurement the goodness of fit based on empirical information.</description></item><item><title>SRM: Structural Risk Minimization</title><link>https://datumorphism.leima.is/cards/machine-learning/learning-theories/structural-risk-minimization/</link><pubDate>Thu, 18 Feb 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/learning-theories/structural-risk-minimization/</guid><description>[[ERM]] ERM: Empirical Risk Minimization In a [[learning problem]] The Learning Problem The learning problem posed by Vapnik:1 Given a sample: $\{z_i\}$ in the probability space $Z$; Assuming a probability measure on the probability space $Z$; Assuming a set of functions $Q(z, \alpha)$ (e.g. loss functions), where $\alpha$ is a set of parameters; A risk functional to be minimized by tunning &amp;ldquo;the handles&amp;rdquo; $\alpha$, $R(\alpha)$. The risk functional is $$ R(\alpha) = \int Q(z, \alpha) \,\mathrm d F(z). $$ A learning problem … may lead to overfitting since ERM only selects the model to fit the train data well.</description></item><item><title>Coding Theory Concepts</title><link>https://datumorphism.leima.is/cards/information/coding-theory-concepts/</link><pubDate>Wed, 17 Feb 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/information/coding-theory-concepts/</guid><description>The code function produces code words. The expected length of the code word is limited by the entropy from the source probability $p$.
The Shannon information content, aka self-information, is described by
$$ - \log_2 p(x=a), $$
for the case that $x=a$.
The Shannon entropy is the expected information content for the whole sequence with probability distribution $p(x)$,
$$ \mathcal H = - \sum_x p(x\in X) \log_2 p(x). $$
The Shannon source coding theorem says that for $N$ samples from the source, we can roughly compress it into $N\mathcal H$.</description></item><item><title>Empirical Loss</title><link>https://datumorphism.leima.is/cards/machine-learning/measurement/empirical-loss/</link><pubDate>Sat, 06 Feb 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/measurement/empirical-loss/</guid><description>Given a dataset with records $\{x_i, y_i\}$ and a model $\hat y_i = f(x_i)$ the empirical loss is calculated on all the records
$$ \begin{align} \mathcal L_{E} = \frac{1}{n} \sum_i^n d(y_i, f(x_i)), \end{align} $$
where $d(y_i, f(x_i))$ is the distance defined between $y_i$ and $f(x_i)$.</description></item><item><title>Population Loss</title><link>https://datumorphism.leima.is/cards/machine-learning/measurement/population-loss/</link><pubDate>Sat, 06 Feb 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/measurement/population-loss/</guid><description>Given a dataset with records $\{x_i, y_i\}$ and a model $\hat y_i = f(x_i)$. Suppose we know the actual generating process of the dataset and the joint probability density distribution of all the data points is $p(x, y)$, the population loss is defined on the whole assumed population,
$$ \begin{align} \mathcal L_{P} = \mathop{\mathbb{E}}_{p(x,y)}[ d(y, f(x))], \end{align} $$
where $d(y, f(x))$ is the distance defined between $y$ and $f(x)$.</description></item><item><title>Data File Formats</title><link>https://datumorphism.leima.is/cards/machine-learning/datatypes/data-file-formats/</link><pubDate>Tue, 02 Feb 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/datatypes/data-file-formats/</guid><description>Data storage is diverse. For data on smaller scales, we are mostly dealing with some data files.
work_with_data_files
Efficiencies and Compressions Parquet Parquet is fast. But
Don&amp;rsquo;t use json or list of json as columns. Convert them to strings or binary objects if it is really needed.</description></item><item><title>Machine as a Hologram</title><link>https://datumorphism.leima.is/projects/hologram/</link><pubDate>Sun, 31 Jan 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/projects/hologram/</guid><description>Tutorials on machine learning and data science productivity articles</description></item><item><title>Latent Variable Models</title><link>https://datumorphism.leima.is/wiki/machine-learning/bayesian/latent-variable-models/</link><pubDate>Wed, 27 Jan 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/bayesian/latent-variable-models/</guid><description>In the view of statistics, we know everything about a physical system if we know the probability $p(\mathbf s)$ of all possible states of the physical system $\mathbf s$. Time can also be part of the state specification.
As an example, we will classify fruits into oranges and non oranges. We will have the state vector $\mathbf s = (\text{is orange}, \text{texture } x)$. Our goal is to find the joint probability $p(\text{is orange}, x)$.
The reality, we only have sample data. This sample data usually can not cover all the possible states of the system. Thus a direct calculation to find the joint probability $p(\mathbf s)$ is not feasible.</description></item><item><title>Reparametrization in Expectation Sampling</title><link>https://datumorphism.leima.is/cards/statistics/reparametrization-expectation-sampling/</link><pubDate>Wed, 20 Jan 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/reparametrization-expectation-sampling/</guid><description>The expectation value of a function $f(z)$ over a Guassian distribution $\mathscr N(z;\mu, \sigma)$ is equivalent to the expectation value of $f()$ a Gaussian distribution $\mathscr N(z;\mu=0, \sigma=1)$, i.e.,
$$ {\mathbb E}_{\mathscr N(z; \mu, \sigma)} \left[ f(z) \right] = {\mathbb E}_{\mathscr N(z; 0, 1)} \left[ f() \right] $$
where
$$ \mathscr N = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left( -\frac{(z-\mu)^2}{2\sigma^2}\right). $$
$$ \begin{align} {\mathbb E}_{\mathscr N(z; \mu, \sigma)} \left[ f(z) \right] &amp;= \int \mathrm d z \frac{1}{\sqrt{2\pi\sigma^2}}\exp \left( -\frac{(z-\mu)^2}{2\sigma^2}\right) f(z) \\ &amp;= \int \mathrm dz \frac{1}{\sigma} \frac{1}{\sqrt{2\pi}} \exp \left( -\frac{1}{2} \left(\frac{z-\mu}{\sigma}\right)^2 \right) f(z) \\ &amp;= \int \mathrm d \left( \sigma z' + \mu \right) \frac{1}{\sigma} \frac{1}{\sqrt{2\pi}} \exp \left( -\frac{1}{2} z'^2 \right) f(\sigma z' + \mu) \\ &amp;= \int \mathrm d z' \frac{1}{\sqrt{2\pi}}\exp \left( -\frac{1}{2} z'^2 \right) f(\sigma z' + \mu) \\ &amp;= \int \mathrm d z' \mathscr N(z'; \mu=0, \sigma=1) f(\sigma z' + \mu) \\ &amp;= {\mathbb E}_{\mathscr N(z'; \mu=0, \sigma=1)} \left[ f(\sigma z' + \mu) \right] \end{align} $$</description></item><item><title>Normalizing Flows: An Introduction and Review of Current Methods</title><link>https://datumorphism.leima.is/reading/normalizing-flow-introduction-1908.09257/</link><pubDate>Sun, 17 Jan 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/reading/normalizing-flow-introduction-1908.09257/</guid><description>To generate complicated distributions step by step from a simple and interpretable distribution.</description></item><item><title>A Simple Machine Learning Project Framework</title><link>https://datumorphism.leima.is/blog/data-science/a-simple-machine-learning-framework/</link><pubDate>Tue, 12 Jan 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/blog/data-science/a-simple-machine-learning-framework/</guid><description>The HaferML package is developed following this naive framework.
A simple almost stateless machine learning framework</description></item><item><title>Basics of Redis</title><link>https://datumorphism.leima.is/wiki/computation/basics-of-redis/</link><pubDate>Fri, 08 Jan 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/computation/basics-of-redis/</guid><description>Basics Redis is:
NoSQL KeyValue In memory Data Structure Server binary safe strings lists, sets, sorted sets, hashes bitmaps, hyperloglogs Open source Redis is:
Fast Low CPU Requirement Scalable Redis can be used as:
Cache Analytics Leaderboard Queues Cookie storage Expiring data Messaging High I/O workloads API throttlings How to persist your data
Snapshot AOF: Append Only File Pros:
Redis has both data store and job queue built in Redis is a data structure server so is has flexibe data structures Redis is fast Cons:
Redis used a lot of RAM Redis can be be queued by value Comparisons</description></item><item><title>Audiolization of Covid 19 Data in Europe</title><link>https://datumorphism.leima.is/blog/ruthless/audiolization-of-covid19-in-eu/</link><pubDate>Sun, 03 Jan 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/blog/ruthless/audiolization-of-covid19-in-eu/</guid><description>Here is an audiolization sound track using a sample of covid19 data in Europe. The audio is the result of the audiorepr Python package I wrote.</description></item><item><title>PREP</title><link>https://datumorphism.leima.is/cards/communication/prep/</link><pubDate>Sun, 03 Jan 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/communication/prep/</guid><description>PREP PREP is a framework for making your point.
PREP: Point + Reason + Example + Point Point: Make a point; PREP is a good method. Reason: Give the reason; Because it has a clear logic. Example: Show examples; The famous XYZ did ABC then everyone was convinced. Point: State the point for a conclusion.</description></item><item><title>SCQ-A</title><link>https://datumorphism.leima.is/cards/communication/scq-a/</link><pubDate>Sun, 03 Jan 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/communication/scq-a/</guid><description>SCQ-A SCQ-A: Situation + Conflict + Question + Answer SCA-A is a framework for problem-solving.
Situation: background knowledge, set the stage Complications: what is happening Question: propose your hypothesis Answer: accept or reject the hypothesis</description></item><item><title>WWH</title><link>https://datumorphism.leima.is/cards/communication/wwh/</link><pubDate>Sun, 03 Jan 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/communication/wwh/</guid><description>WWH WWH: What (happened) + Why (this happened) + How (to improve)</description></item><item><title>PyTorch: Initialize Parameters</title><link>https://datumorphism.leima.is/til/machine-learning/pytorch/pytorch-initial-params/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/machine-learning/pytorch/pytorch-initial-params/</guid><description>We can set the parameters in a for loop. We take some of the initialization methods from Lippe1.
To set based on the input dimension of the layer ( [[Initialize Artificial Neural Networks]] Initialize Artificial Neural Networks Initialize a neural network is important for the training and performance. Some initializations simply don&amp;#39;t work, some will degrade the performance of the model. We should choose wisely. ) (normalized initialization),
for name, param in model.named_parameters(): if name.endswith(&amp;#34;.bias&amp;#34;): param.data.fill_(0) else: bound = math.sqrt(6)/math.sqrt(param.shape[0]+param.shape[1]) param.data.uniform_(-bound, bound) or set the parameters based on the input size of each layer
for name, param in model.</description></item><item><title>Graph Creation</title><link>https://datumorphism.leima.is/reading/grammar-of-graphics/graph-creation/</link><pubDate>Tue, 29 Dec 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/reading/grammar-of-graphics/graph-creation/</guid><description>Stages Three stages of making a graph:
Specification Assembly Display Specification Statistical graphic specifications are expressed in six statements
DATA: a set of data operations that create variables from datasets TRANS: variable transformations (e.g., rank) SCALE: scale transformations (e.g., log) COORD: a coordinate system (e.g., polar) ELEMENT: graphs (e.g., points) and their aesthetic attributes (e.g., color) GUIDE: one or more guides (axes, legends, etc.) Assembly Assembling a scene from a specification requires a variety of structures in order to index and link components with each other. One of the structures we can use is a network or a tree.</description></item><item><title>Multiset, mset or bag</title><link>https://datumorphism.leima.is/cards/math/multiset-mset-bag/</link><pubDate>Sun, 27 Dec 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/multiset-mset-bag/</guid><description>A bag is a set in which duplicate elements are allowed.
An ordered bag is a list that we use in programming.</description></item><item><title>Python Class Sequential Inheritance</title><link>https://datumorphism.leima.is/til/programming/python/python-class-inheritance-sequential/</link><pubDate>Thu, 03 Dec 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/python/python-class-inheritance-sequential/</guid><description># An experiment on python super class Base: def __init__(self): print(&amp;#34;Start A&amp;#34;) print(&amp;#34;End A&amp;#34;) class IA(Base): def __init__(self): print(&amp;#34;Start IA&amp;#34;) super(IA, self).__init__() print(&amp;#34;End IA&amp;#34;) class IB(IA): def __init__(self): print(&amp;#34;Start IB&amp;#34;) super(IB, self).__init__() print(&amp;#34;End IB&amp;#34;) print(&amp;#34;Experiment 1:&amp;#34;) ib = IB()</description></item><item><title>Three dots in Python</title><link>https://datumorphism.leima.is/til/programming/python/python-three-dots/</link><pubDate>Thu, 03 Dec 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/python/python-three-dots/</guid><description>Using three dots in Python:
from abc import abstractmethod class A: def __init__(self): self.name = &amp;#34;A&amp;#34; print(&amp;#34;Init&amp;#34;) def three_dots(self): ... @abstractmethod def abs_three_dots(self): ... def raise_it(self): raise Exception(&amp;#34;Not yet done&amp;#34;) a = A() print(&amp;#34;\nthree_dots&amp;#34;) print(a.three_dots()) print(&amp;#34;\nabs_three_dots&amp;#34;) print(a.abs_three_dots()) print(&amp;#34;\nraise_it&amp;#34;) a.raise_it() Returns
three_dots None abs_three_dots None raise_it Traceback (most recent call last): File &amp;quot;main.py&amp;quot;, line 27, in &amp;lt;module&amp;gt; a.raise_it() File &amp;quot;main.py&amp;quot;, line 14, in raise_it raise Exception(&amp;quot;Not yet done&amp;quot;) Exception: Not yet done</description></item><item><title>Ordered Member Functions of a Class in Python</title><link>https://datumorphism.leima.is/til/programming/python/python-class-methods-ordered/</link><pubDate>Wed, 02 Dec 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/python/python-class-methods-ordered/</guid><description># References: # 1. https://stackoverflow.com/questions/48145317/can-i-add-attributes-to-class-methods-in-python from functools import wraps # Define a decorator def attributes(**attrs): &amp;#34;&amp;#34;&amp;#34; Set attributes of member functions in a class. ``` class AGoodClass: def __init__(self): self.size = 0 @attributes(order=1) def first_good_member(self, new): return &amp;#34;first good member&amp;#34; @attributes(order=2) def second_good_member(self, new): return &amp;#34;second good member&amp;#34; ``` References: 1. https://stackoverflow.com/a/48146924/1477359 &amp;#34;&amp;#34;&amp;#34; def decorator(f): @wraps(f) def wrapper(*args, **kwargs): return f(*args, **kwargs) for attr_name, attr_value in attrs.items(): setattr(wrapper, attr_name, attr_value) return wrapper return decorator class AGoodClass: def __init__(self): self.size = 0 @attributes(order=1) def first_good_member(self, new): return &amp;#34;first good member&amp;#34; @attributes(order=2) def second_good_member(self, new): return &amp;#34;second good member&amp;#34; # Test agc = AGoodClass() print(agc.</description></item><item><title>Postgres Optimization in JOIN</title><link>https://datumorphism.leima.is/til/data/postgres.join-begin-with-smallest-cardinality/</link><pubDate>Sat, 28 Nov 2020 11:39:21 +0100</pubDate><guid>https://datumorphism.leima.is/til/data/postgres.join-begin-with-smallest-cardinality/</guid><description>Join tables together starting with the smallest table (table with less cardinality) speeds things up.</description></item><item><title>Deal with NULL in Postgres</title><link>https://datumorphism.leima.is/til/data/postgres.deal-with-null/</link><pubDate>Thu, 26 Nov 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/data/postgres.deal-with-null/</guid><description>Please deal with null carefully.</description></item><item><title>Akaike Information Criterion</title><link>https://datumorphism.leima.is/cards/statistics/aic/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/aic/</guid><description>Suppose we have a model that describes the data generation process behind a dataset. The distribution by the model is denoted as $\hat f$. The actual data generation process is described by a distribution $f$.
We ask the question:
How good is the approximation using $\hat f$?
To be more precise, how much information is lost if we use our model dist $\hat f$ to substitute the actual data generation distribution $f$?
AIC defines this information loss as
$$ \mathrm{AIC} = - 2 \ln p(y|\hat\theta) + 2k $$
$y$: data set $\hat\theta$: parameter of the model that is estimated by maximum-likelihood $\ln p(y|\hat\theta)$: log maximum likelihood (the goodness-of-fit) $k$: number of adjustable model params; $+2k$ is then a penalty.</description></item><item><title>Bayes Factors</title><link>https://datumorphism.leima.is/cards/statistics/bayes-factors/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/bayes-factors/</guid><description>$$ \frac{p(\mathscr M_1|y)}{ p(\mathscr M_2|y) } = \frac{p(\mathscr M_1)}{ p(\mathscr M_2) }\frac{p(y|\mathscr M_1)}{ p(y|\mathscr M_2) } $$
Bayes factor
$$ \mathrm{BF_{12}} = \frac{m(y|\mathscr M_1)}{m(y|\mathscr M_2)} $$
$\mathrm{BF_{12}}$: how many time more likely is model $\mathscr M_1$ than $\mathscr M_2$.</description></item><item><title>Bayesian Information Criterion</title><link>https://datumorphism.leima.is/cards/statistics/bic/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/bic/</guid><description>BIC is Bayesian information criterion, it replaced the $+2k$ term in [[AIC]] Akaike Information Criterion Suppose we have a model that describes the data generation process behind a dataset. The distribution by the model is denoted as $\hat f$. The actual data generation process is described by a distribution $f$. We ask the question: How good is the approximation using $\hat f$? To be more precise, how much information is lost if we use our model dist $\hat f$ to substitute the actual data generation distribution $f$? AIC defines this information loss as $$ \mathrm{AIC} = - 2 \ln p(y|\hat\theta) + … with $k\ln n$ to bring in punishment for the number of parameters of the model based on the number of data records,</description></item><item><title>Fisher Information Approximation</title><link>https://datumorphism.leima.is/cards/statistics/fia/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/fia/</guid><description>FIA is a method to describe the minimum description length ( [[MDL]] Minimum Description Length MDL is a measure of how well a model compresses data by minimizing the combined cost of the description of the model and the misfit. ) of models,
$$ \mathrm{FIA} = -\ln p(y | \hat\theta) + \frac{k}{2} \ln \frac{n}{2\pi} + \ln \int_\Theta \sqrt{ \operatorname{det}[I(\theta)] d\theta } $$
$I(\theta)$: Fisher information matrix of sample size 1. $$I_{i,j}(\theta) = E\left( \frac{\partial \ln p(y| \theta)}{\partial \theta_i}\frac{ \partial \ln p (y | \theta) }{ \partial \theta_j } \right)$$.</description></item><item><title>Kolmogorov Complexity</title><link>https://datumorphism.leima.is/cards/statistics/kolmogorov-complexity/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/kolmogorov-complexity/</guid><description>Description of Data
The measurement of complexity is based on the observation that the compressibility of data doesn&amp;rsquo;t depend on the &amp;ldquo;language&amp;rdquo; used to describe the compression process that much. This makes it possible for us to find a universal language, such as a universal computer language, to quantify the compressibility of the data.
One intuitive idea is to use a programming language to describe the data. If we have a sequence of data,
0,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,&amp;hellip;,9999
It takes a lot of space if we show the complete sequence. However, our math intuition tells us that this is nothing but a list of consecutive numbers from 0 to 9999.</description></item><item><title>Minimum Description Length</title><link>https://datumorphism.leima.is/cards/statistics/mdl/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/mdl/</guid><description>The minimum description length, aka, MDL, is based on the relations between regularity and data compression. (See [[Kolmogorov complexity]] Kolmogorov Complexity Description of Data The measurement of complexity is based on the observation that the compressibility of data doesn&amp;rsquo;t depend on the &amp;ldquo;language&amp;rdquo; used to describe the compression process that much. This makes it possible for us to find a universal language, such as a universal computer language, to quantify the compressibility of the data. One intuitive idea is to use a programming language to describe the data. If we have a sequence of data, … for more about data descriptions.</description></item><item><title>Normalized Maximum Likelihood</title><link>https://datumorphism.leima.is/cards/statistics/nml/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/nml/</guid><description>$$ \mathrm{NML} = \frac{ p(y| \hat \theta(y)) }{ \int_X p( x| \hat \theta (x) ) dx } $$</description></item><item><title>Experiments in Biology</title><link>https://datumorphism.leima.is/blog/ruthless/experiments-in-biology/</link><pubDate>Sun, 01 Nov 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/blog/ruthless/experiments-in-biology/</guid><description> Inspired by @hanlu.io</description></item><item><title>The Science Part in Data Science</title><link>https://datumorphism.leima.is/blog/ruthless/science-part-in-data-science/</link><pubDate>Sat, 31 Oct 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/blog/ruthless/science-part-in-data-science/</guid><description>graph TD; s1(An Idea)--d1{Is this idea in the current literature?}; d1{Is this idea in the current literature?}--|Yes|b1(Fail); d1{Is this idea in the current literature?}--|No|b2[Weeks of work]; b2[Weeks of work]--b1(Fail);</description></item><item><title>Conditional Probability Table</title><link>https://datumorphism.leima.is/cards/statistics/conditional-probability-table/</link><pubDate>Tue, 27 Oct 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/conditional-probability-table/</guid><description>The conditional probability table, aka CPT, is used to calculate conditional probabilities from a dataset.
Given a dataset with features $\mathbf X$ and their corresponding classes $\mathbf Y$, the conditional probabilities of each class given a certain feature value can be calculated using a CPT which in turn can be calculated using a [[contigency table]] Correlation Coefficient and Covariance for Numeric Data Detecting correlations using correlations for numeric data .</description></item><item><title>Pandas Groupby Does Not Guarantee Unique Content in Groupby Columns</title><link>https://datumorphism.leima.is/til/machine-learning/pandas-groupby-caveats/</link><pubDate>Mon, 20 Apr 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/machine-learning/pandas-groupby-caveats/</guid><description>Pandas Groupby Does Not Guarantee Unique Content in Groupby Columns, it also considers the datatypes. Dealing with mixed types requires additional attention.</description></item><item><title>== and is in Python</title><link>https://datumorphism.leima.is/til/programming/python/python-none/</link><pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/python/python-none/</guid><description>== and is are different</description></item><item><title>Bonferroni Correction</title><link>https://datumorphism.leima.is/cards/statistics/bonferroni-correction/</link><pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/bonferroni-correction/</guid><description>In a single hypothesis testing problem, we the [[type I error]] Types of Errors in Statistical Hypothesis Testing We all make mistakes. The question is, what kind of mistakes. : Rejecting the one null hypothesis when it is actually true. Given a threshold $\alpha$, we can find out the interval $\Gamma$ that leads to a probability of rejecting the hypothesis $p\leq\alpha$ (single-sided).
In a [[multiple comparisons problem]] Multiple Comparison Problem In a multiple comparisons problem, we deal with multiple statistical tests simultaneously. Examples We see such problems a lot in IT companies. Suppose we have a website and would like to test if a new design of a button can lead to some changes in five different KPIs (e.</description></item><item><title>Multiple Comparison Problem</title><link>https://datumorphism.leima.is/cards/statistics/multiple-comparison-problem/</link><pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/multiple-comparison-problem/</guid><description>In a multiple comparisons problem, we deal with multiple statistical tests simultaneously.
Examples We see such problems a lot in IT companies. Suppose we have a website and would like to test if a new design of a button can lead to some changes in five different KPIs (e.g., view-to-click rate, click-to-book rate, &amp;hellip;).
In multi-horizon time series forecasting, we sometimes choose to forecast multiple future data points in one shot. To properly find the confidence intervals of our predictions, one approach is the so called conformal prediction method. This becomes a multiple comparisons problem because we have to tell if we can reject at least one true null hypothesis.</description></item><item><title>Arcsine Distribution</title><link>https://datumorphism.leima.is/cards/statistics/distributions/arcsine/</link><pubDate>Sat, 14 Mar 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/distributions/arcsine/</guid><description>Arcsine Distribution The PDF is
$$ \frac{1}{\pi\sqrt{x(1-x)}} $$
for $x\in [0,1]$.
It can also be generalized to
$$ \frac{1}{\pi\sqrt{(x-1)(b-x)}} $$
for $x\in [a,b]$.
Visualize</description></item><item><title>Bernoulli Distribution</title><link>https://datumorphism.leima.is/cards/statistics/distributions/bernoulli/</link><pubDate>Sat, 14 Mar 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/distributions/bernoulli/</guid><description>Two categories with probability $p$ and $1-p$ respectively.
For each experiment, the sample space is $\{A, B\}$. The probability for state $A$ is given by $p$ and the probability for state $B$ is given by $1-p$. The Bernoulli distribution describes the probability of $K$ results with state $s$ being $s=A$ and $N-K$ results with state $s$ being $B$ after $N$ experiments,
$$ P\left(\sum_i^N s_i = K \right) = C _ N^K p^K (1 - p)^{N-K}. $$</description></item><item><title>Beta Distribution</title><link>https://datumorphism.leima.is/cards/statistics/distributions/beta/</link><pubDate>Sat, 14 Mar 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/distributions/beta/</guid><description>Beta Distribution Interact {% include extras/vue.html %}
((makeGraph))</description></item><item><title>Binomial Distribution</title><link>https://datumorphism.leima.is/cards/statistics/distributions/binomial/</link><pubDate>Sat, 14 Mar 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/distributions/binomial/</guid><description>The number of successes in $n$ independent events where each trial has a success rate of $p$.
PMF:
$$ C_n^k p^k (1-p)^{n-k} $$</description></item><item><title>Categorical Distribution</title><link>https://datumorphism.leima.is/cards/statistics/distributions/categorical/</link><pubDate>Sat, 14 Mar 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/distributions/categorical/</guid><description>By generalizing the Bernoulli distribution to $k$ states, we get a categorical distribution. The sample space is $\{s_1, s_2, \cdots, s_k\}$. The corresponding probabilities for each state are $\{p_1, p_2, \cdots, p_k\}$ with the constraint $\sum_{i=1}^k p_i = 1$.</description></item><item><title>Cauchy-Lorentz Distribution</title><link>https://datumorphism.leima.is/cards/statistics/distributions/cauchy/</link><pubDate>Sat, 14 Mar 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/distributions/cauchy/</guid><description>Cauchy-Lorentz Distribution .. ratio of two independent normally distributed random variables with mean zero.
Source: https://en.wikipedia.org/wiki/Cauchy_distribution
Lorentz distribution is frequently used in physics.
PDF:
$$ \frac{1}{\pi\gamma} \left( \frac{\gamma^2}{ (x-x_0)^2 + \gamma^2} \right) $$
The median and mode of the Cauchy-Lorentz distribution is always $x_0$. $\gamma$ is the FWHM.
Visualize</description></item><item><title>Gamma Distribution</title><link>https://datumorphism.leima.is/cards/statistics/distributions/gamma/</link><pubDate>Sat, 14 Mar 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/distributions/gamma/</guid><description>Gamma Distribution PDF:
$$ \frac{\beta^\alpha x^{\alpha-1} e^{-\beta x}}{\Gamma(\alpha)} $$
Visualize</description></item><item><title>Diagnolize Matrices</title><link>https://datumorphism.leima.is/cards/math/diagonalize-matrix/</link><pubDate>Wed, 11 Mar 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/diagonalize-matrix/</guid><description>Given a matrix $\mathbf A$, it is diagonalized using its eigenvectors.
Why are the eigenvectors needed?
Eigenvectors of a matrix $\mathbf A$ are the preferred directions. From the definition of eigenvectors,
$$ \mathbf A \mathbf x = \lambda \mathbf x, $$
we know that the matrix $\mathbf A$ only scales the eigenvectors and no rotations. These directions are special to the matrix $\mathbf A$.
Find the eigenvectors $\mathbf x_i$ of the matrix $\mathbf A$; If we find degerations, the matrix is not diagonalizable. Construct a matrix $\mathbf S = \begin{pmatrix} \mathbf x_1 &amp;amp; \mathbf x_2 &amp;amp; \cdots &amp;amp; \mathbf x_n \end{pmatrix}$; The matrix $\mathbf A$ is diagonalize using $\mathbf S^{-1} \mathbf A \mathbf S = \mathbf {A_D}$</description></item><item><title>Mahalanobis Distance</title><link>https://datumorphism.leima.is/cards/math/mahalanobis-distance/</link><pubDate>Wed, 11 Mar 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/mahalanobis-distance/</guid><description>Mahalanobis distance is a distance calculated using the inverse of the covariance matrix as the metric. For two vectors $\mathbf x$ and $\mathbf y$, the Mahalanobis distance is
$$ d^2 = (x_i - \bar x) g_{ij} (y_j - \bar y), $$
where $g_{ij} = (S^{-1})_{ij}$ and $\mathbf S$ is the covariance matrix.
The covariance is a normalization that mitigates the covariances.</description></item><item><title>Covariance Matrix</title><link>https://datumorphism.leima.is/cards/statistics/covariance-matrix/</link><pubDate>Tue, 10 Mar 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/covariance-matrix/</guid><description>We use Einstein&amp;rsquo;s summation convention. Covariance of two discrete series $A$ and $B$ is defined as
$$ \text{Cov} ({A,B}) = \sigma_{A,B}^2 = \frac{ (a_i - \bar A) (b_i - \bar B) }{ n- 1 }, $$
where $n$ is the length of the series. The normalization factor is set to $1/(n-1)$ to mitigate the bias for small $n$.
One could show that
$$ \mathrm{Cov}({A,B}) = E( A,B ) - \bar A \bar B. $$
At first glance, the square in the definition seems to be only for notation purpose at this point.
Meanwhile, using this idea of the mean of geometric mean, we could easily generalize it to the covariance of three series,</description></item><item><title>Jackknife Resampling</title><link>https://datumorphism.leima.is/cards/statistics/jacknife-resampling/</link><pubDate>Sun, 26 Jan 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/jacknife-resampling/</guid><description>Jackknife resampling is a method for estimation of the mean and higher order moments.
Given a sample $\{x_i\}$ of size $n$ for the distribution $X$, the jackknife resampling estimates the mean by leaving out each data point systematically. $n$ estimations of the mean will be obtained, with each of the estimations $x_i$
$$ \bar x_i = \frac{1}{n-1} \sum_{j\neq i} x_j. $$
The mean of the sample is
$$ \bar x = \frac{1}{n}\sum_i \bar x_i = \frac{1}{n} \sum_i \left(\frac{1}{n-1} \sum_{j\neq i} x_j\right) = \frac{1}{n}\sum_i x_i. $$
The result is consistent with other sample mean methods. Jackknife estimates the variance of the sample</description></item><item><title>CBOW: Continuous Bag of Words</title><link>https://datumorphism.leima.is/cards/machine-learning/embedding/continuous-bag-of-words/</link><pubDate>Thu, 16 Jan 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/embedding/continuous-bag-of-words/</guid><description>Here we encode all words presented in the corpus to demostrate the idea of CBOW. In the real world, we might want to remove some certain words such as the. We use the following quote by Ford in Westworld as an example.
I read a theory once that the human intellect is like peacock feathers. Just an extravagant display intended to attract a mate, just an elaborate mating ritual. But, of course, the peacock can barely fly. It lives in the dirt, pecking insects out of the muck, consoling itself with its great beauty.
The word intended is surrunded by extravagant display in the front and to attract after it.</description></item><item><title>Data Types</title><link>https://datumorphism.leima.is/cards/machine-learning/datatypes/data-types/</link><pubDate>Thu, 16 Jan 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/datatypes/data-types/</guid><description/></item><item><title>Gini Impurity</title><link>https://datumorphism.leima.is/cards/machine-learning/measurement/gini-impurity/</link><pubDate>Thu, 16 Jan 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/measurement/gini-impurity/</guid><description>The code used in this article can be found in this repo. Suppose we have a dataset $\{0,1\}^{10}$, which has 10 records and 2 possible classes of objects $\{0,1\}$ in each record.
The first example we investigate is a pure 0 dataset.
object 0 0 0 0 0 0 0 0 0 0 0 0 For such an all-0 dataset, we would like to define its impurity as 0.</description></item><item><title>Information Gain</title><link>https://datumorphism.leima.is/cards/machine-learning/measurement/information-gain/</link><pubDate>Thu, 16 Jan 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/measurement/information-gain/</guid><description>Information gain is a frequently used metric in calculating the gain during a split in tree-based methods.
First o all, the entropy of a dataset if defined as
$$ S = - sum_i p_i \log p_i - sum_i (1-p_i)\log p_i, $$
where $p_i$ is the probability of a class.
The information gain is the difference between the entropy.
For example, in a decision tree algorithm, we would split a node. Before splitting, we assign a label $m$ to the node,
$$ S_m = - p_m \log p_m - (1-p_m)\log p_m. $$
After the splitting, we have two groups that contributes to the entropy, group $L$ and group $R$,</description></item><item><title>Negative Sampling</title><link>https://datumorphism.leima.is/cards/machine-learning/embedding/negative-sampling/</link><pubDate>Thu, 16 Jan 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/embedding/negative-sampling/</guid><description>Knowledge of [[CBOW]] CBOW: Continuous Bag of Words Use the context to predict the center word or [[skipgram]] skipgram: Continuous skip-gram Use the center word to predict the context is required.
A naive model to train a model of words is to
encode input words and output words using vectors, use the input word vector to predict the output word vector, calculate the errors between predicted output word vector and real output word vector, minimize the errors. However, it is very expensive to project out the output words and calculate the error every time.</description></item><item><title>PAC: Probably Approximately Correct</title><link>https://datumorphism.leima.is/cards/machine-learning/learning-theories/pac/</link><pubDate>Thu, 16 Jan 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/learning-theories/pac/</guid><description/></item><item><title>skipgram: Continuous skip-gram</title><link>https://datumorphism.leima.is/cards/machine-learning/embedding/continuous-skip-gram/</link><pubDate>Thu, 16 Jan 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/embedding/continuous-skip-gram/</guid><description>We use the following quote by Ford in Westworld as an example.
I read a theory once that the human intellect is like peacock feathers. Just an extravagant display intended to attract a mate, just an elaborate mating ritual. But, of course, the peacock can barely fly. It lives in the dirt, pecking insects out of the muck, consoling itself with its great beauty.
The word intended is surrunded by extravagant display in the front and to attract after it. The task is to predict the probability of words around the middle word intended, which are the &amp;lsquo;history words&amp;rsquo; extravagant, display and &amp;lsquo;future words&amp;rsquo; to, attract in our case.</description></item><item><title>Improving Document Ranking with Dual Word Embeddings</title><link>https://datumorphism.leima.is/reading/word2vec-in-out-embedding/</link><pubDate>Sat, 05 Oct 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/reading/word2vec-in-out-embedding/</guid><description>Word2vec produces two embedding spaces, the in-embedding and out-embedding.</description></item><item><title>Switch statement in Python</title><link>https://datumorphism.leima.is/til/programming/python/python-switch-statement/</link><pubDate>Tue, 20 Aug 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/python/python-switch-statement/</guid><description>Love switch statement? We can design a switch statement it in python.</description></item><item><title>Python Tilde Operator</title><link>https://datumorphism.leima.is/til/programming/python/python-tilde-operator/</link><pubDate>Thu, 15 Aug 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/python/python-tilde-operator/</guid><description>tilde operator may not work as you expected</description></item><item><title>Arrays and Dicts in MongoDB</title><link>https://datumorphism.leima.is/til/programming/database/mongodb-array-and-dict/</link><pubDate>Wed, 14 Aug 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/database/mongodb-array-and-dict/</guid><description>Array of dictionaries becomes hard to update in MongoDB.</description></item><item><title>eval in Python is Dangerous</title><link>https://datumorphism.leima.is/til/programming/python/python-eval/</link><pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/python/python-eval/</guid><description>eval is powerful but really dangerous</description></item><item><title>Dealing with Missing Data in Machine Learning</title><link>https://datumorphism.leima.is/wiki/machine-learning/feature-engineering/missing-data/</link><pubDate>Mon, 05 Aug 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/feature-engineering/missing-data/</guid><description>How to Deal with Missing Data Remove Listwise deletion: Remove the whole record; Works if the missing values are random. Removing values causes problem in many aspects. For example, we can not just delete data when applying our models. Replace with most frequent value central tendency: median, mean, etc fixed value: a string etc New Category: define a new category for missing data Convert the column to a binary valued column indicating if the feature is missing or not. Tools pandas sklearn: Imputer @ResidentMario/missingno : visualize missing data</description></item><item><title>Cramér's V</title><link>https://datumorphism.leima.is/cards/statistics/cramers-v/</link><pubDate>Sat, 20 Jul 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/cramers-v/</guid><description/></item><item><title>Kendall Tau Correlation</title><link>https://datumorphism.leima.is/cards/statistics/kendall-correlation-coefficient/</link><pubDate>Sat, 20 Jul 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/kendall-correlation-coefficient/</guid><description>Definition two series of data: $X$ and $Y$ cooccurance of them: $(x_i, x_j)$, and we assume that $i&amp;lt;j$ concordant: $x_i &amp;lt; x_j$ and $y_i &amp;lt; y_j$; $x_i &amp;gt; x_j$ and $y_i &amp;gt; y_j$; denoted as $C$ discordant: $x_i &amp;lt; x_j$ and $y_i &amp;gt; y_j$; $x_i &amp;gt; x_j$ and $y_i &amp;lt; y_j$; denoted as $D$ neither concordant nor discordant: whenever equal sign happens Kendall&amp;rsquo;s tau is defined as
$$ \begin{equation} \tau = \frac{C- D}{\text{all possible pairs of comparison}} = \frac{C- D}{n^2/2 - n/2} \end{equation} $$</description></item><item><title>Bayes' Theorem</title><link>https://datumorphism.leima.is/cards/statistics/bayes-theorem/</link><pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/bayes-theorem/</guid><description>Bayes' Theorem is stated as
$$ P(A\mid B) = \frac{P(B \mid A) P(A)}{P(B)} $$
$P(A\mid B)$: likelihood of A given B $P(A)$: marginal probability of A There is a nice tree diagram for the Bayes' theorem on Wikipedia.
Tree diagram of Bayes' theorem</description></item><item><title>Canonical Decomposition</title><link>https://datumorphism.leima.is/cards/math/canonical-decomposition/</link><pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/canonical-decomposition/</guid><description>I find this slide from Christoph Freudenthaler very useful.
Canonical decomposition visualized by Christoph Freudenthaler</description></item><item><title>Cholesky Decomposition</title><link>https://datumorphism.leima.is/cards/math/cholesky-decomposition/</link><pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/cholesky-decomposition/</guid><description>$$ A = L L^T $$</description></item><item><title>Khatri-Rao Product</title><link>https://datumorphism.leima.is/cards/math/khatri-rao/</link><pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/khatri-rao/</guid><description>$$ \mathbf{A} \ast \mathbf{B} = \left(\mathbf{A}_{ij} \otimes \mathbf{B}_{ij}\right)_{ij} $$</description></item><item><title>Modes and Slices of Tensors</title><link>https://datumorphism.leima.is/cards/math/modes-and-slices-of-tensor/</link><pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/modes-and-slices-of-tensor/</guid><description> Modes of a tensor Slices of a tensor</description></item><item><title>Poisson Process</title><link>https://datumorphism.leima.is/cards/statistics/poisson-process/</link><pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/poisson-process/</guid><description/></item><item><title>SVD: Singular Value Decomposition</title><link>https://datumorphism.leima.is/cards/math/svd/</link><pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/svd/</guid><description>Given a matrix $\mathbf X \to X_{m}^{\phantom{m}n}$, we can decompose it into three matrices
$$ X_{m}^{\phantom{m}n} = U_{m}^{\phantom{m}k} D_{k}^{\phantom{k}l} (V_{n}^{\phantom{n}l} )^{\mathrm T}, $$
where $D_{k}^{\phantom{k}l}$ is diagonal.
Here we have $\mathbf U$ being constructed by the eigenvectors of $\mathbf X \mathbf X^{\mathrm T}$, while $\mathbf V$ is being constructed by the eigenvectors of $\mathbf X^{\mathrm T} \mathbf X$ (which is also the reason we keep the transpose).
I find this slide from Christoph Freudenthaler very useful. The original slide has been added as a reference to this article.
SVD visualized by Christoph Freudenthaler</description></item><item><title>Tucker Decomposition</title><link>https://datumorphism.leima.is/cards/math/tucker-decomposition/</link><pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/tucker-decomposition/</guid><description>I find this slide from Christoph Freudenthaler very useful. For the definition of mode 1/2/3 unfold, please refer to Modes and Slices of Tensors.
Tucker decomposition visualized by Christoph Freudenthaler</description></item><item><title>Frobenius distance</title><link>https://datumorphism.leima.is/cards/math/frobenius-distance/</link><pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/frobenius-distance/</guid><description>Frobenius distance between the matrix $X_{n}^{\phantom{n}k}$ and $H_n^{\phantom{n}r} W_r^{\phantom{r}k}$,
$$ \lVert X_{n}^{\phantom{n}k} - H_n^{\phantom{n}r} W_r^{\phantom{r}k} \rVert^2 \equiv \sum_{n,k} (X_{n}^{\phantom{n}k} - H_n^{\phantom{n}r} W_r^{\phantom{r}k})^2. $$</description></item><item><title>Levenshtein Distance</title><link>https://datumorphism.leima.is/cards/math/levenshtein-distance/</link><pubDate>Sun, 19 May 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/levenshtein-distance/</guid><description>Levenshtein distance calculates the number of operations needed to change one word to another by applying single-character edits (insertions, deletions or substitutions).
The reference explains this concept very well. For consistency, I extracted a paragraph from it which explains the operations in Levenshtein algorithm. The source of the following paragraph is the first reference of this article.
Levenshtein Matrix
Cell (0:1) contains red number 1. It means that we need 1 operation to transform M to an empty string. And it is by deleting M. This is why this number is red. Cell (0:2) contains red number 2. It means that we need 2 operations to transform ME to an empty string.</description></item><item><title>n-gram</title><link>https://datumorphism.leima.is/cards/math/n-gram/</link><pubDate>Sun, 19 May 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/n-gram/</guid><description>n-gram is a method to split words into set of substring elements so that those can be used to match words.
Examples Use the following examples to get your first idea about it. I created two columns so that we could compare the n-grams of two different words side-by-side.
n in n-gram is Word One Clean Word: (( sentenceOneWords )) n-grams: (( sentenceOneWordsnGram )) Word Two Clean Word: (( sentenceTwoWords )) n-grams: (( sentenceTwoWordsnGram )) /*************************/ /** The function nGram is a copy of https://github.com/words/n-gram , under MIT License **/ nGram.</description></item><item><title>Add New Kernels to Jupyter Notebook in Conda Environment</title><link>https://datumorphism.leima.is/til/programming/jupyter-notebook-add-new-kernels-in-conda-env/</link><pubDate>Sun, 12 May 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/jupyter-notebook-add-new-kernels-in-conda-env/</guid><description>Python package or python module autoreloading in jupyter notebook</description></item><item><title>Auto-reload Python Packages or Python Modules in Jupyter Notebook</title><link>https://datumorphism.leima.is/til/programming/jupyter-notebook-autoreload-python-modules-or-packages/</link><pubDate>Sun, 12 May 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/jupyter-notebook-autoreload-python-modules-or-packages/</guid><description>Python package or python module autoreloading in jupyter notebook</description></item><item><title>BigQuery Meta Tables</title><link>https://datumorphism.leima.is/til/data/bigquery-meta-tables/</link><pubDate>Sun, 12 May 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/data/bigquery-meta-tables/</guid><description>Meta tables are very useful when it comes to get bigquery table information programmatically.</description></item><item><title>Calculate Moving Average Using SQL/BigQquery</title><link>https://datumorphism.leima.is/til/data/bigquery-moving-average/</link><pubDate>Sun, 12 May 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/data/bigquery-moving-average/</guid><description>Snippet for calculating moving avg using sql/biguqery</description></item><item><title>Generate a Column of Continuous Dates in BigQuery</title><link>https://datumorphism.leima.is/til/data/bigquery-generate-continuous-dates-as-a-column/</link><pubDate>Sun, 12 May 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/data/bigquery-generate-continuous-dates-as-a-column/</guid><description>Generate a table with a column of continuous dates</description></item><item><title>Get Current User in BigQuery</title><link>https://datumorphism.leima.is/til/data/bigquery-get-current-user/</link><pubDate>Sun, 12 May 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/data/bigquery-get-current-user/</guid><description>BigQuery Current User</description></item><item><title>Materialize the Query Result for Performance</title><link>https://datumorphism.leima.is/til/data/bigquery-materialize-query-results-for-performance/</link><pubDate>Sun, 12 May 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/data/bigquery-materialize-query-results-for-performance/</guid><description>Materialize the query result for multistage queries to make your query faster and lower the costs.</description></item><item><title>Cosine Similarity</title><link>https://datumorphism.leima.is/cards/math/cosine-similarity/</link><pubDate>Mon, 06 May 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/cosine-similarity/</guid><description>As simple as the inner product of two vectors
$$ d_{cos} = \frac{\vec A}{\vert \vec A \vert} \cdot \frac{\vec B }{ \vert \vec B \vert} $$
Examples To use cosine similarity, we have to vectorize the words first. There are many different methods to achieve this. For the purpose of illustrating cosine similarity, we use term frequency.
Term frequency is the occurrence of the words. We do not deal with duplications so duplicate words will have some effect on the similarity.
In principle, we could also use word set for a sentence to remove the effect of duplicate words. In most cases, if a word is repeating, it would indeed make the sentences different.</description></item><item><title>Eigenvalues and Eigenvectors</title><link>https://datumorphism.leima.is/cards/math/eigendecomposition/</link><pubDate>Mon, 06 May 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/eigendecomposition/</guid><description>To find the eigenvectors $\mathbf x$ of a matrix $\mathbf A$, we construct the eigen equation
$$ \mathbf A \mathbf x = \lambda \mathbf x, $$
where $\lambda$ is the eigenvalue.
We rewrite it in the components form,
$$ \begin{equation} A_{ij} x_j = \lambda x_i. \label{eqn-eigen-decomp-def} \end{equation} $$
Mathematically speaking, it is straightforward to find the eigenvectors and eigenvalues.
Eigenvectors are Special Directions Judging from the definition in Eq.($\ref{eqn-eigen-decomp-def}$), the eigenvectors do not change direction under the operation of the matrix $\mathbf A$.
Reconstruct $\mathbf A$ We can reconstruct $\mathbf A$ using the eigenvalues and eigenvectors.
First of all, we will construct a matrix of eigenvectors,</description></item><item><title>Jaccard Similarity</title><link>https://datumorphism.leima.is/cards/math/jaccard-similarity/</link><pubDate>Mon, 06 May 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/jaccard-similarity/</guid><description>Jaccard index is the ratio of the size of the intersect of the set and the size of the union of the set.
$$ J(A, B) = \frac{ \vert A \cap B \vert }{ \vert A \cup B \vert } $$
Jaccard distance $d_J(A,B)$ is defined as
$$ d_J(A,B) = 1 - J(A,B). $$
Properties If the two sets are the same, $A=B$, we have $J(A,B)=1$ or $d_J(A,B)=0$. We have maximum similarity.
If the two sets have nothing in common, we have $J(A,B)=0$ or $d_J(A,B)=1$. We have minimum similarity.
Examples Sentence One Word Set: (( sentenceOneWords )) Sentence Two Word Set: (( sentenceTwoWords )) Intersect: (( intersectWords )) Union: (( unionWords )) Jaccard Index: (( jaccardIndex )) Jaccard Distance: (( jaccardDistance )) Vue.</description></item><item><title>Term Frequency - Inverse Document Frequency</title><link>https://datumorphism.leima.is/cards/math/tf-idf/</link><pubDate>Mon, 06 May 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/tf-idf/</guid><description/></item><item><title>The Art of Data Science</title><link>https://datumorphism.leima.is/reading/art-of-data-science/</link><pubDate>Fri, 19 Apr 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/reading/art-of-data-science/</guid><description>A nice and elegant book on data science</description></item><item><title>Awesome Stuff</title><link>https://datumorphism.leima.is/projects/awesome/</link><pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/projects/awesome/</guid><description>Summarizations, workflows, experiences, fails, etc</description></item><item><title>Blog Posts</title><link>https://datumorphism.leima.is/projects/blog/</link><pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/projects/blog/</guid><description>My blog posts for fun.</description></item><item><title>Combinations</title><link>https://datumorphism.leima.is/cards/math/combinations/</link><pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/combinations/</guid><description>Choose X from N is
$$ C_N^X = \frac{N!}{ X! (N-X)! } $$</description></item><item><title>My Data Wiki</title><link>https://datumorphism.leima.is/projects/wiki/</link><pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/projects/wiki/</guid><description>A collection of my wiki articles related to data.</description></item><item><title>My Knowledge Cards</title><link>https://datumorphism.leima.is/projects/cards/</link><pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/projects/cards/</guid><description>A collection of my snippets of knowledge</description></item><item><title>My Reading Notes</title><link>https://datumorphism.leima.is/projects/reading/</link><pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/projects/reading/</guid><description>A collection of my reading notes</description></item><item><title>TIL</title><link>https://datumorphism.leima.is/projects/til/</link><pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/projects/til/</guid><description>Today I Learned</description></item><item><title>Human Graphical Perception of Quantitative Information in Data Visualization</title><link>https://datumorphism.leima.is/reading/graphical-perception/</link><pubDate>Sun, 17 Mar 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/reading/graphical-perception/</guid><description>Data visualization caveats</description></item><item><title>Add Data Files to Python Package</title><link>https://datumorphism.leima.is/til/programming/python/python-package-including-data-file/</link><pubDate>Wed, 13 Mar 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/python/python-package-including-data-file/</guid><description>Add Data Files to Python Package using manifest.in and setup.py</description></item><item><title>Installing requirements.txt in Conda Environments</title><link>https://datumorphism.leima.is/til/programming/python/python-anaconda-install-requirements/</link><pubDate>Wed, 13 Mar 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/python/python-anaconda-install-requirements/</guid><description>Why is pip install -r requirements.txt not working in conda?</description></item><item><title>Information Theory and Statistical Mechanics</title><link>https://datumorphism.leima.is/reading/statistical-physics-and-information-theory/</link><pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/reading/statistical-physics-and-information-theory/</guid><description>Max entropy principle as a method to infer distributions of statistical systems</description></item><item><title>Flatten 2D List in Python</title><link>https://datumorphism.leima.is/til/programming/python/python-flatten-2d-list/</link><pubDate>Wed, 23 Jan 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/python/python-flatten-2d-list/</guid><description>Flatten 2D list using sum</description></item><item><title>Python Datetime on Different OS</title><link>https://datumorphism.leima.is/til/programming/python/python-datetime-on-different-os/</link><pubDate>Mon, 31 Dec 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/python/python-datetime-on-different-os/</guid><description>Python datetime on different os behaves inconsistently</description></item><item><title>Python If on Numbers</title><link>https://datumorphism.leima.is/til/programming/python/python-if-condition-on-numbers/</link><pubDate>Mon, 31 Dec 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/python/python-if-condition-on-numbers/</guid><description>If on int is dangerous</description></item><item><title>Python Long String</title><link>https://datumorphism.leima.is/til/programming/python/python-long-string/</link><pubDate>Mon, 31 Dec 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/python/python-long-string/</guid><description>Python long string formatting</description></item><item><title>Python Reliable Path to File</title><link>https://datumorphism.leima.is/til/programming/python/python-reliable-path/</link><pubDate>Mon, 31 Dec 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/python/python-reliable-path/</guid><description>Find the actual path to file</description></item><item><title>VSCode on Mac Long Press Keys Not Repeating</title><link>https://datumorphism.leima.is/til/misc/vscode/vscode-on-mac-do-not-repeat/</link><pubDate>Mon, 31 Dec 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/misc/vscode/vscode-on-mac-do-not-repeat/</guid><description>Enable your key repeat in vscode on mac in the terminal</description></item><item><title>Controlled Experiments</title><link>https://datumorphism.leima.is/til/statistics/controlled-experiments/</link><pubDate>Tue, 04 Dec 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/statistics/controlled-experiments/</guid><description>The three levels of controlled experiments</description></item><item><title>BiPolar Sigmoid</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-bi-polar-sigmoid/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-bi-polar-sigmoid/</guid><description>A BiPolar sigmoid function is
$$ \sigma(x) = \frac{1-e^{-x}}{1+e^{-x}}. $$
Visualization Bipolar Sigmoid</description></item><item><title>Conic Section Function</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-conic-section-function/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-conic-section-function/</guid><description> TODO
Expand this article. See references 1.
Dorffner1994 Dorffner G. UNIFIED FRAMEWORK FOR MLPs AND RBFNs: INTRODUCING CONIC SECTION FUNCTION NETWORKS. Cybern Syst. 1994;25: 511–554. doi:10.1080/01969729408902340 &amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>ELU</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-elu/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-elu/</guid><description>Both ReLu and Leaky ReLu have discontinuous derivatives. ELU is smooth for first order derivative, i.e., ELU is class $C^1$.
$$ \begin{cases} x, &amp; \text{if }x=0 \\ \exp(x) - 1, &amp; \text{else.} \end{cases} $$
Visualizations ELU
Derivative of ELU
Code def elu(x, alpha): return torch.where(x &amp;gt; 0, x, torch.exp(x) -1) Full code to generate the data used in this article Full code to generate the data used in this article
from torch import nn import matplotlib.pyplot as plt import torch from typing import Union, Optional from pathlib import Path import json def visualize_activation( x: torch.Tensor, acti: torch.</description></item><item><title>Hyperbolic Tanh</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-hyperbolic-tangent/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-hyperbolic-tangent/</guid><description>$$ \tanh(x) = \frac{\sinh(x)}{\cosh(x)} = \frac{e^{x} - e^{-x}}{e^x + e^{-x}} $$
Hyperbolic tangent</description></item><item><title>Leaky ReLu</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-leaky-relu/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-leaky-relu/</guid><description>ReLu sets all negative regions to 0. Leaky ReLu sets the negative regions to a linear relation with slope $\alpha$,
$$ \begin{cases} x, &amp; \text{if }x=0 \\ \alpha x, &amp; \text{else.} \end{cases} $$
Visualizations Leaky ReLu with $\alpha=0.2$
Derivative of Leaky ReLu with $\alpha=0.2$. Notice that the derivative is $0.2$ for $x&amp;lt;0$.
Code def leaky_relu(x, alpha): return torch.where(x &amp;gt; 0, x, alpha * x) Full code to generate the data used in this article Full code to generate the data used in this article
from torch import nn import matplotlib.pyplot as plt import torch from typing import Union, Optional from pathlib import Path import json def visualize_activation( x: torch.</description></item><item><title>Radial Basis Function</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-radial-basis-function/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-radial-basis-function/</guid><description> Hyperbolic tangentTwo unnormalized Gaussian radial basis functions in one input dimension. The basis function centers are located at x1=0.75 and x2=3.25. Source Unnormalized Radial Basis Functions</description></item><item><title>ReLu</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-relu/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-relu/</guid><description>Rectified Linear Unit (ReLu) is a very popular activation function in deep learning. ReLu is defined as
$$ \begin{cases} x, &amp; \text{if }x=0 \\ 0, &amp; \text{else.} \end{cases} $$
Visualizations ReLu
Derivative of ReLu
Characteristics In trained models, ReLu doesn&amp;rsquo;t preserve the qualitative distributions of values after the activation.
Lippe P. Tutorial 3: Activation Functions — UvA DL Notebooks v1.1 documentation. In: UvA Deep Learning Tutorials [Internet].
Because of the zero values in ReLu, many neurons actually don&amp;rsquo;t participate in any of the tasks as they are just nullified to zeros and provide no gradient.</description></item><item><title>Swish</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-swish/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-swish/</guid><description>Swish is infinitely differentiable, i.e., class $C^\infty$.
$$ x \sigma(x), $$
where $\sigma$ is the [[uni-polar sigmoid]] Uni-Polar Sigmoid Uni-polar sigmoid function and its properties .
Visualizations ELU
Derivative of ELU
Code def swish(x, alpha): return x * torch.sigmoid(x) Full code to generate the data used in this article Full code to generate the data used in this article
from torch import nn import matplotlib.pyplot as plt import torch from typing import Union, Optional from pathlib import Path import json def visualize_activation( x: torch.Tensor, acti: torch.nn.Module, save_path: Optional[Union[str, Path]] = None ) -&amp;gt; dict: &amp;#34;&amp;#34;&amp;#34;Visualize activation function on the domain of x&amp;#34;&amp;#34;&amp;#34; y = acti(x) # Calculate the grad of the activation function x = x.</description></item><item><title>Uni-Polar Sigmoid</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-uni-polar-sigmoid/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-uni-polar-sigmoid/</guid><description>A uni-Polar sigmoid function is
$$ \sigma(x) = \frac{1}{1+e^{-x}}. $$
Visualization Uni-polar Sigmoid function Tricks A very useful trick: $$ 1 - \sigma(x) = \sigma(-x). $$</description></item><item><title>Schaum's Outline of Theories and Problems of Elements of Statistics I and II</title><link>https://datumorphism.leima.is/reading/elements-of-statistics/</link><pubDate>Thu, 01 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/reading/elements-of-statistics/</guid><description>The basics and all of modern statistics</description></item><item><title>Pandas with MultiProcessing</title><link>https://datumorphism.leima.is/til/programming/pandas/pandas-parallel-multiprocessing/</link><pubDate>Sun, 09 Sep 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/pandas/pandas-parallel-multiprocessing/</guid><description>Define number of processes, prs; Split dataframe into prs dataframes; Process each dataframe with one process; Merge processed dataframes into one. A piece of demo code is shown below.
from multiprocessing import Pool from multiprocessing.dummy import Pool as ThreadPool import pandas as pd # Create a dataframe to be processed df = pd.read_csv(&amp;#39;somedata.csv&amp;#39;).reset_index(drop=True) # Define a function to be applied to the dataframe def nice_func(name, age): return (name,age) # Apply to dataframe def apply_to_df(df_chunks): df_chunks[&amp;#39;tupled&amp;#39;] = df_chunks.apply( lambda x: nice_func( x[&amp;#39;host_name&amp;#39;], x[&amp;#39;host_country&amp;#39;]), axis=1 ) return df_chunks print(&amp;#39;finished chunk&amp;#39;) # Divide dataframe to chunks prs = 100 # define the number of processes chunk_size = int(df.</description></item><item><title>Beer and Life Expectancy</title><link>https://datumorphism.leima.is/blog/ruthless/beer-and-life-expectancy/</link><pubDate>Wed, 08 Aug 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/blog/ruthless/beer-and-life-expectancy/</guid><description>This is a post of no analysis at all. Everything in this post is meant for fun.
I moved to Germany a few weeks ago and one of the most astonishing things I noticed is that everyone is drinking so much. Yet the life expectance of Germany is pretty high. So I performed this &amp;ldquo;analysis&amp;rdquo; for fun.
Life expectancy vs beer consumption (L) per capita per year. Data obtained from wikipediaList of countries by life expectancy and List of countries by beer consumption per capita.
As you might be curious about the life expectancy vs the life expectancy, I also made one plot about it.</description></item><item><title>Data Mining: Concepts and Techniques</title><link>https://datumorphism.leima.is/reading/data-mining/</link><pubDate>Wed, 01 Aug 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/reading/data-mining/</guid><description>How data mining was done in the past</description></item><item><title>Fitt's Law</title><link>https://datumorphism.leima.is/til/misc/fitts-law/</link><pubDate>Sun, 22 Jul 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/misc/fitts-law/</guid><description>How fast can you move your mouse to target</description></item><item><title>Copy Scalars and Lists in Python</title><link>https://datumorphism.leima.is/til/programming/python/python-copy-value-or-address/</link><pubDate>Tue, 03 Jul 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/python/python-copy-value-or-address/</guid><description>Python copy values of scalars but addresses of lists</description></item><item><title>Certificate Errors in urllib</title><link>https://datumorphism.leima.is/til/data/python-urllib-ssl/</link><pubDate>Mon, 25 Jun 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/data/python-urllib-ssl/</guid><description>Dealing with errors when scraping data</description></item><item><title>Calculated Columns in Pandas</title><link>https://datumorphism.leima.is/til/programming/pandas/pandas-new-column-from-other/</link><pubDate>Sun, 20 May 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/pandas/pandas-new-column-from-other/</guid><description>Create new columns in pandas</description></item><item><title>tree in Linux</title><link>https://datumorphism.leima.is/til/programming/trees/</link><pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/trees/</guid><description>Trees in computer science</description></item><item><title>Heap on Mac and Linux</title><link>https://datumorphism.leima.is/til/programming/cpp/cpp-heap-mac-linux-diff/</link><pubDate>Tue, 26 Sep 2017 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/cpp/cpp-heap-mac-linux-diff/</guid><description>Some caveats about heap on mac and linux</description></item><item><title>C++ int Multiplication</title><link>https://datumorphism.leima.is/til/programming/cpp/cpp-int-multiply/</link><pubDate>Thu, 21 Sep 2017 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/cpp/cpp-int-multiply/</guid><description>int multiplication in C++ should be processed with caution.</description></item><item><title>CMake Usage</title><link>https://datumorphism.leima.is/til/programming/cmake-usage/</link><pubDate>Thu, 21 Sep 2017 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/cmake-usage/</guid><description>How to use CMake to generate makefiles</description></item><item><title>Allocating Memory for Multidimensional Array in C++</title><link>https://datumorphism.leima.is/til/programming/cpp/cpp-allocating-memory-multidimensional-array/</link><pubDate>Thu, 14 Sep 2017 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/cpp/cpp-allocating-memory-multidimensional-array/</guid><description>Some caveats</description></item><item><title>C++ range-for-statement</title><link>https://datumorphism.leima.is/til/programming/cpp/cpp-range-for-statement/</link><pubDate>Tue, 12 Sep 2017 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/cpp/cpp-range-for-statement/</guid><description>In C++ we can use range-for-statement</description></item><item><title>List All Folders in Linux or Mac</title><link>https://datumorphism.leima.is/til/programming/linux-mac-list-all-folders/</link><pubDate>Tue, 01 Aug 2017 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/linux-mac-list-all-folders/</guid><description>Using ls and tree commands to list folders only</description></item><item><title>Python Default Parameters Tripped Me Up</title><link>https://datumorphism.leima.is/til/programming/python/python-default-parameters-mutable/</link><pubDate>Sat, 03 Jun 2017 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/python/python-default-parameters-mutable/</guid><description>Python default parameters might be changed with each run</description></item><item><title>Some Tests on Matplotlib Backends</title><link>https://datumorphism.leima.is/til/programming/matplotlib-backend/</link><pubDate>Tue, 23 May 2017 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/matplotlib-backend/</guid><description>Matplotlib provides many different backends</description></item><item><title>Mathematica Provides Great PlotTheme Options</title><link>https://datumorphism.leima.is/til/programming/mathematica/mathematica-plottheme/</link><pubDate>Fri, 19 May 2017 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/mathematica/mathematica-plottheme/</guid><description>Amazingly, Mathematica provides an option for plot that automatically generates beautiful plots.</description></item><item><title>Turn a Series Expansion into Function in Mathematica</title><link>https://datumorphism.leima.is/til/programming/mathematica/mathematica-turn-series-into-function/</link><pubDate>Mon, 15 May 2017 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/mathematica/mathematica-turn-series-into-function/</guid><description>Turn a series expansion in Mathematica into a function</description></item><item><title>Overcoming catastrophic forgetting in neural networks</title><link>https://datumorphism.leima.is/reading/overcoming-catastrophic-forgetting-in-neural-networks/</link><pubDate>Sun, 14 May 2017 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/reading/overcoming-catastrophic-forgetting-in-neural-networks/</guid><description>Using a newly defined loss function the authors could implement an idea that achieves the multi-task within one network.</description></item><item><title>Git Asks for Password Whenever I Pull or Push</title><link>https://datumorphism.leima.is/til/programming/git/git-ssh-asking-pwd-everytime/</link><pubDate>Thu, 11 May 2017 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/git/git-ssh-asking-pwd-everytime/</guid><description>My git asks for password every time I pull or push even with ssh configured.</description></item><item><title>Command Line Russian Roulette</title><link>https://datumorphism.leima.is/til/programming/command-line-russian-roulette/</link><pubDate>Tue, 09 May 2017 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/command-line-russian-roulette/</guid><description>Play russian roulette in your command line</description></item><item><title>GNU Screen Key Conflict with Bash</title><link>https://datumorphism.leima.is/til/programming/gnu-screen-key-conflict-with-bash/</link><pubDate>Mon, 08 May 2017 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/gnu-screen-key-conflict-with-bash/</guid><description>GNU screen key conflict with bash can be solved</description></item><item><title>How to Run Mathematica Script in Terminal</title><link>https://datumorphism.leima.is/til/programming/run-mathematica-script-in-terminal/</link><pubDate>Mon, 08 May 2017 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/run-mathematica-script-in-terminal/</guid><description>Using math -run or wolfram -run we could execute a Mathematica script through ssh in terminal.</description></item><item><title>GNUPLOT Inline Output in iterm2</title><link>https://datumorphism.leima.is/til/programming/gnuplot-iterm2-imgcat/</link><pubDate>Fri, 07 Apr 2017 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/gnuplot-iterm2-imgcat/</guid><description>Using gnuplot in iterm2 we can output result inside terminal combined with imgcat</description></item><item><title>Mathematica Exclude Singularities in Plot</title><link>https://datumorphism.leima.is/til/programming/mathematica/mathematica-plot-exclude-singularities/</link><pubDate>Wed, 22 Mar 2017 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/mathematica/mathematica-plot-exclude-singularities/</guid><description>Mathematica Plot might include some non-existant lines sometimes, Exclusions is the potion for it.</description></item><item><title>Passing Function Arguments Through Lists in Mathematica</title><link>https://datumorphism.leima.is/til/programming/mathematica/mathematica-passing-arguments-through-lists/</link><pubDate>Mon, 20 Feb 2017 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/mathematica/mathematica-passing-arguments-through-lists/</guid><description>We can pass a list of arguments using Sequence</description></item><item><title>Git Pull with Submodule</title><link>https://datumorphism.leima.is/til/programming/git/git-pull-with-submodule/</link><pubDate>Fri, 03 Feb 2017 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/git/git-pull-with-submodule/</guid><description>Pull git repo with submodule</description></item><item><title>Positioning textblock in LaTeX Beamer</title><link>https://datumorphism.leima.is/til/programming/latex-beamer-textblock-position/</link><pubDate>Tue, 17 Jan 2017 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/latex-beamer-textblock-position/</guid><description>Positioning textblock in LaTeX Beamer using textpos package and eso pic package</description></item><item><title>Mathematica Different Output Forms</title><link>https://datumorphism.leima.is/til/programming/mathematica/mathematica-different-output-forms/</link><pubDate>Mon, 28 Nov 2016 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/mathematica/mathematica-different-output-forms/</guid><description>Mathematica has many different output forms. Understanding them is extremely helpful when making plots.</description></item><item><title>Git Branch Options</title><link>https://datumorphism.leima.is/til/programming/git/git-branch-details/</link><pubDate>Sun, 27 Nov 2016 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/git/git-branch-details/</guid><description>Some useful options about git branch</description></item><item><title>git pull multi remote</title><link>https://datumorphism.leima.is/til/programming/git/git-pull-multi-remote/</link><pubDate>Tue, 22 Nov 2016 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/git/git-pull-multi-remote/</guid><description>working with multi remote</description></item><item><title>Working Memory and Brain Waves</title><link>https://datumorphism.leima.is/reading/working-memory-and-brain-waves/</link><pubDate>Sun, 20 Nov 2016 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/reading/working-memory-and-brain-waves/</guid><description>Working memory might be related to the background brain waves from theoretical point of view</description></item><item><title>Popularity versus similarity in growing networks</title><link>https://datumorphism.leima.is/reading/popularity-vs-similarity/</link><pubDate>Sun, 06 Nov 2016 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/reading/popularity-vs-similarity/</guid><description>Introduce geometry into the manifold of complex networks</description></item><item><title>Formatting Numbers in Python</title><link>https://datumorphism.leima.is/til/programming/formating-numbers-python/</link><pubDate>Tue, 11 Oct 2016 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/formating-numbers-python/</guid><description>Formatting numbers in python using format</description></item><item><title>Solving Equations Using Differential Transformation Method</title><link>https://datumorphism.leima.is/til/math/differential-transformation-method-solving-equations/</link><pubDate>Tue, 11 Oct 2016 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/math/differential-transformation-method-solving-equations/</guid><description>Differential transformation method can be used to solve differential equation even integro-differential equations.</description></item><item><title>The Great Chrome Dev Tool</title><link>https://datumorphism.leima.is/til/programming/chrome-dev-tool-usage/</link><pubDate>Wed, 28 Sep 2016 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/chrome-dev-tool-usage/</guid><description>How to use the chrome dev tool wisely</description></item><item><title>Start a Simple Server</title><link>https://datumorphism.leima.is/til/programming/start-simple-server/</link><pubDate>Sat, 17 Sep 2016 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/start-simple-server/</guid><description>With one line of python command</description></item><item><title>matplotlib x y limit and aspect ratio</title><link>https://datumorphism.leima.is/til/programming/matplotlib-x-y-limit-and-aspect-ratio/</link><pubDate>Thu, 21 Jul 2016 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/matplotlib-x-y-limit-and-aspect-ratio/</guid><description>matplotlib x y limit and aspect ratio</description></item><item><title>TOP Command</title><link>https://datumorphism.leima.is/til/programming/top/</link><pubDate>Thu, 21 Jul 2016 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/top/</guid><description>Some tips about top command</description></item><item><title>Assigning Values to Multiple Variables</title><link>https://datumorphism.leima.is/til/programming/python/python-assigning-values-to-multiple-variables/</link><pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/python/python-assigning-values-to-multiple-variables/</guid><description>Assigning Values to Multiple Variables</description></item><item><title>gitignore by file size</title><link>https://datumorphism.leima.is/til/programming/git/gitignore-by-file-size/</link><pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/git/gitignore-by-file-size/</guid><description>gitignore by file size</description></item><item><title>HTML Animations Using CSS: AnimateCSS</title><link>https://datumorphism.leima.is/til/programming/html-animate-css/</link><pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/html-animate-css/</guid><description>HTML Animations Using CSS AnimateCSS</description></item><item><title>Import in Python</title><link>https://datumorphism.leima.is/til/programming/import-in-python/</link><pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/import-in-python/</guid><description>Import in Python</description></item><item><title>IPython or Jupyter Notebook Magics</title><link>https://datumorphism.leima.is/til/programming/ipython-or-jupyter-notebook-magics/</link><pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/ipython-or-jupyter-notebook-magics/</guid><description>IPython or Jupyter Notebook Magics</description></item><item><title>LaTeX Automatically Adjust Figure</title><link>https://datumorphism.leima.is/til/programming/latex-automatically-adjust-figure/</link><pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/latex-automatically-adjust-figure/</guid><description>LaTeX Automatically Adjust Figure</description></item><item><title>Mathematica Plot Default Font Style and Ticks Style: BaseStyle</title><link>https://datumorphism.leima.is/til/programming/mathematica/mathematica-plot-basestyle-default-font-style-and-ticks-style/</link><pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/mathematica/mathematica-plot-basestyle-default-font-style-and-ticks-style/</guid><description>Mathematica Plot Default Font Style and Ticks Style BaseStyle</description></item><item><title>Mathematica Smooth Plot</title><link>https://datumorphism.leima.is/til/programming/mathematica/mathematica-smooth-plot/</link><pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/mathematica/mathematica-smooth-plot/</guid><description>Mathematica Smooth Plot</description></item><item><title>Migrating Wordpress to Static</title><link>https://datumorphism.leima.is/til/programming/migrating-wordpress-to-static-site/</link><pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/migrating-wordpress-to-static-site/</guid><description>Migrating Wordpress to Static</description></item><item><title>Open URL using python using webbrowser module</title><link>https://datumorphism.leima.is/til/programming/open-url-using-python-webbrowser-module/</link><pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/open-url-using-python-webbrowser-module/</guid><description>Open URL using python using webbrowser module</description></item><item><title>Python Code Style</title><link>https://datumorphism.leima.is/til/programming/python/python-code-style/</link><pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/python/python-code-style/</guid><description>Code Style of Python Guide.
PEP 20 &amp;ndash; The Zen of Python
1. Beautiful is better than ugly. 2. Explicit is better than implicit. 3. Simple is better than complex. 4. Complex is better than complicated. 5. Flat is better than nested. 6. Sparse is better than dense. 7. Readability counts. 8. Special cases aren&amp;#39;t special enough to break the rules. 9. Although practicality beats purity. 10. Errors should never pass silently. 11. Unless explicitly silenced. 12. In the face of ambiguity, refuse the temptation to guess. 13. There should be one-- and preferably only one --obvious way to do it.</description></item><item><title>Python Creating Lists</title><link>https://datumorphism.leima.is/til/programming/python/python-creating-lists/</link><pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/python/python-creating-lists/</guid><description>Code Style of Python Guide</description></item><item><title>Python enumertate</title><link>https://datumorphism.leima.is/til/programming/python/python-enumerate/</link><pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/python/python-enumerate/</guid><description>Python enumertate function</description></item><item><title>Python List Comprehensions</title><link>https://datumorphism.leima.is/til/programming/python/python-list-comprehensions/</link><pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/python/python-list-comprehensions/</guid><description>Python List Comprehensions</description></item><item><title>Python Making a List</title><link>https://datumorphism.leima.is/til/programming/python/python-making-a-list/</link><pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/python/python-making-a-list/</guid><description>Python Making a List</description></item><item><title>Python Map vs For in Python</title><link>https://datumorphism.leima.is/til/programming/python/python-map-vs-for/</link><pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/python/python-map-vs-for/</guid><description>Python Map vs For in Python</description></item><item><title>Python Onliner: Filter Prime Numbers</title><link>https://datumorphism.leima.is/til/programming/filter-prime-numbers/</link><pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/filter-prime-numbers/</guid><description>Python Onliner Filter Prime Numbers</description></item><item><title>Python Stupid numpy.piecewise</title><link>https://datumorphism.leima.is/til/programming/python/python-stupid-numpy-piecewise/</link><pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/python/python-stupid-numpy-piecewise/</guid><description>Python Stupid numpy.piecewise</description></item><item><title>Python Various Ways of Writing Loops</title><link>https://datumorphism.leima.is/til/programming/python/python-writing-loops/</link><pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/python/python-writing-loops/</guid><description>Python Various Ways of Writing Loops</description></item><item><title>Run a program in the background on ubuntu</title><link>https://datumorphism.leima.is/til/programming/run-program-in-background-ubuntu/</link><pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/run-program-in-background-ubuntu/</guid><description>Run a program in the background on ubuntu</description></item><item><title>snakeviz</title><link>https://datumorphism.leima.is/til/programming/python/python-profile-snakeviz/</link><pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/python/python-profile-snakeviz/</guid><description>Python snakeviz</description></item><item><title>Area Enclosed by a Line</title><link>https://datumorphism.leima.is/til/math/area-enclosed-in-a-line/</link><pubDate>Sun, 15 Feb 2015 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/math/area-enclosed-in-a-line/</guid><description>Calculate the area enclosed by a line</description></item><item><title>Eigensystem of A Special Matrix</title><link>https://datumorphism.leima.is/til/math/eigensystem-of-a-special-matrix/</link><pubDate>Sun, 15 Feb 2015 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/math/eigensystem-of-a-special-matrix/</guid><description>Eigenstates of a very special matrix</description></item><item><title>Feynman Trick</title><link>https://datumorphism.leima.is/til/math/feynman-tricks/</link><pubDate>Sun, 15 Feb 2015 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/math/feynman-tricks/</guid><description>An identity about integral</description></item><item><title>Symmetry of second derivatives</title><link>https://datumorphism.leima.is/til/math/symmetry-of-second-derivatives/</link><pubDate>Sun, 15 Feb 2015 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/math/symmetry-of-second-derivatives/</guid><description>Symmetry of second derivatives</description></item><item><title/><link>https://datumorphism.leima.is/wiki/dynamical-system/integration-of-ode/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/dynamical-system/integration-of-ode/</guid><description/></item><item><title/><link>https://datumorphism.leima.is/wiki/survival-analysis/survival-probability/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/survival-analysis/survival-probability/</guid><description/></item><item><title>About</title><link>https://datumorphism.leima.is/about/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/about/</guid><description>Datumorphism is my notebook about programming, data scraping, statistics, machine learning, and data visualization.
Join our Enki Team Learn, practice, and play together. Programmers are unstoppable. Intelligence Notebook Notes about neuroscience, machine intelligence, and collective intelligence. Lei Ma Visit this page if you would like to know more about me.</description></item><item><title>Cheatsheets</title><link>https://datumorphism.leima.is/awesome/cheatsheets/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/awesome/cheatsheets/</guid><description> Supervised Learning k-Nearest Neighbors [Supervised Learning Classification ] : Linear Regression [Supervised Learning Regression ] : Lasso [Supervised Learning Regression Regularization ] : Ridge [Supervised Learning Regression Regularization ] : ElasticNet [Supervised Learning Regression Regularization ] : Unsupervised Learning k-Means [Unsupervised Learning ] : t-SNE [Unsupervised Learning ] : PCA [Unsupervised Learning Dimension Reduction Feature Selection ] : NMF [Unsupervised Learning ] : Non-negative Matrix Factoring</description></item><item><title>Gridlines in Matplotlib</title><link>https://datumorphism.leima.is/til/programming/matplotlib-gridlines/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/programming/matplotlib-gridlines/</guid><description>Adding gridlines in matplotlib</description></item><item><title>Researchers</title><link>https://datumorphism.leima.is/awesome/researchers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/awesome/researchers/</guid><description> Machine Learning Geoffrey Hinton [machine learning psychology artificial intelligence cognitive science computer science ] : Emeritus Prof. Comp Sci, U.Toronto &amp;amp; Engineering Fellow, Google Max Welling [machine learning graph ] : Professor Machine Learning, University of Amsterdam William L Hamilton [machine learning graph ] : Assistant Professor of Computer Science, McGill University and Mila Yann LeCun [machine learning artificial intelligence ] : Chief AI Scientist at Facebook &amp;amp; Silver Professor at the Courant Institute,</description></item><item><title>Tools</title><link>https://datumorphism.leima.is/awesome/tools/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/awesome/tools/</guid><description>List of Tools Dashboard streamlit [Python ] : Build dashboards, fast, in python plotly dash [Python ] : Build complex dashboards in python ReDash [Python ] : Superset [Python ] : Metabase [Java ] : Google Data Studio [Free Google BigQuery Cloud ] : Google Datastudio is a convinent tool to produce simple yet massive dashboards for the team. Design and Build a Data Warehouse for Business [Courses Warehouse Business ] : Explained Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM) [LSTM RNN ] : JavaScript replacements for Python data science tools [JavaScript Tools Data Science ] : https://github.</description></item><item><title>Typography of this Website</title><link>https://datumorphism.leima.is/typography/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/typography/</guid><description>Basic Syntax This website uses kramdown as the basic syntax. However, a lot of html/css/js has been applied to generate some certain contents or styles.
Math also follows the kramdown syntax.
Notes div {% highlight html %}
Figure with Caption {% highlight html %}
![]({{ site.url }}/assets/programming/chrome-dev-tools-inspect.png) where {{ site.url }} is the configured url of the site.
Alternatively, we can use the set attributes syntax in kramdown.
{% highlight md %} This is a paragraph with some class. The class is specified in the end of the paragraph. {: .notes&amp;ndash;warning} {% endhighlight %}
The results shows as a paragraph with the corresponding class.</description></item></channel></rss>