<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Datumorphism</title><link>/</link><description>Recent content on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Wed, 17 Feb 2021 00:00:00 +0000</lastBuildDate><atom:link href="/index.xml" rel="self" type="application/rss+xml"/><item><title>Gibbs Sampling</title><link>/wiki/monte-carlo/gibbs-sampling/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>/wiki/monte-carlo/gibbs-sampling/</guid><description/></item><item><title>Principles of Design</title><link>/wiki/data-visualization/design/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>/wiki/data-visualization/design/</guid><description>There are many principles of designing a visual representation of data. However, before we understand how data is represented visually, it would benefit us a lot if we understand the basic principles of designing on 2D surface.
Robin&amp;rsquo;s CRAP Robin Williams proposed the four elements of design:
Contrast Repetition Alignment Proximity Contrast Use some contrast to distinguish the elements of different contents.
Repetition Repeat the design of similar elements on the same page and across pages to make sure the readers learn the meaning of the design quickly.</description></item><item><title>Model Selection</title><link>/wiki/model-selection/model-selection/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>/wiki/model-selection/model-selection/</guid><description>Suppose we have a generating process that generates some numbers based on a distribution. Based on a data sample, we could reconstruct some sort of theoretical models to represent the actual generating process.
Which is a Good Model? (1)The black curve represent the generating process. The red rectangle is a very simple model that captures some major samples. The blue step-wise model is capturing more sample data but with more parameters.</description></item><item><title>Receiver Operating Characteristics: ROC</title><link>/wiki/machine-learning/performance/roc/</link><pubDate>Wed, 13 May 2020 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/performance/roc/</guid><description>ROC space is the two-dimensional space spanned by True Positive Rate and False Positive Rate.
ROC Space. The color boxes are indicating the confusion matrices. Green is the fraction of true positive. Orange is the fraction of false positive. Refer to Confusion Matrix for more details.
AUC: Area under Curve TPR = TP Rate FPR = FP Rate The ROC curve is defined by the relation $f(TPR, FPR)$.</description></item><item><title>Tree-based Learning</title><link>/wiki/machine-learning/tree-based/overview/</link><pubDate>Wed, 25 Dec 2019 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/tree-based/overview/</guid><description>Decision tree is an easy-to-interpret method in supervised learning. Though simple, it is being used in some widely used algorithms such as random forest method.</description></item><item><title>Embedding</title><link>/wiki/machine-learning/embedding/overview/</link><pubDate>Sun, 13 Oct 2019 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/embedding/overview/</guid><description/></item><item><title>Factorization</title><link>/wiki/machine-learning/factorization/overview/</link><pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/factorization/overview/</guid><description/></item><item><title>Feature Engineering</title><link>/wiki/machine-learning/feature-engineering/overview/</link><pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/feature-engineering/overview/</guid><description/></item><item><title>Naive Bayes</title><link>/wiki/machine-learning/bayesian/naive-bayes/</link><pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/bayesian/naive-bayes/</guid><description>Naive Bayesian is a classifier using Bayes&amp;#39; Theorem Bayes&amp;rsquo; Theorem is stated as $$ P(A\mid B) = \frac{P(B \mid A) P(A)}{P(B)} $$ $P(A\mid B)$: likelihood of A given B $P(A)$: marginal probability of A There is a nice tree diagram for the Bayes&amp;rsquo; theorem on Wikipedia. Tree diagram of Bayes&amp;rsquo; theorem with &amp;lsquo;naive&amp;rsquo; assumptions.
Problems with Conditional Probability Calculation By definition, the conditional probability of event $\mathbf Y$ given features $\mathbf X$ is $$ \begin{equation} P(\mathbf Y\mid \mathbf X) = \frac{P(\mathbf Y, \mathbf X)}{ P(\mathbf X) }, \label{def-cp-y-given-x} \end{equation} $$ where</description></item><item><title>Confusion Matrix (Contingency Table)</title><link>/wiki/machine-learning/basics/confusion-matrix/</link><pubDate>Fri, 31 May 2019 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/basics/confusion-matrix/</guid><description>Confusion Matrix It is much easier to understand the confusion matrix if we use a binary classification problem as an example. For example, we have a bunch of cat photos and the user labeled &amp;ldquo;cute or not&amp;rdquo; data. Now we are using the labeled data to train a cute-or-not binary classifier.
Then we apply the classifier on the test dataset and we would only find four different kinds of results.</description></item><item><title>Normal Distribution</title><link>/wiki/distributions/normal-distribution/</link><pubDate>Tue, 22 Jan 2019 00:00:00 +0000</pubDate><guid>/wiki/distributions/normal-distribution/</guid><description>Visualization Math The formula of normal distribution is
$$ \begin{equation} e^{ ( (x - \mu) / \sqrt{2} \sigma )^2 } \end{equation} $$
where $\mu$ controls the &amp;ldquo;center&amp;rdquo; or &amp;ldquo;peak&amp;rdquo; of the distribution and $\sigma$ tells us how &amp;ldquo;wide&amp;rdquo; or &amp;ldquo;disperse&amp;rdquo; the distribution is.
To understand the distribution, we take some limits.
$x = \mu$ First of all, when $x = \mu$ we have
$$ e^0 = 1. $$
Notice the argument of the exponential is some squared value and can not be negative.</description></item><item><title>Statistical Hypothesis Testing</title><link>/wiki/statistical-hypothesis-testing/hypothesis-testing/</link><pubDate>Sun, 20 Jan 2019 00:00:00 +0000</pubDate><guid>/wiki/statistical-hypothesis-testing/hypothesis-testing/</guid><description>When we have a sample of the population, we immediately calculate the mean using the sample, say the result is $\mu_0$. Of course, the population mean $\mu_p$ is unknown and probably can never be known.
This specific sample mean $\mu_0$ is nothing but like an advanced educated guess. Then again, how do we know if our this specific sample mean $\mu_0$ is a faithful representation of the population mean? In fact, this question is not limited to mean.</description></item><item><title>Why Estimation Theory</title><link>/wiki/statistical-estimation/why-estimation-theory/</link><pubDate>Sun, 20 Jan 2019 00:00:00 +0000</pubDate><guid>/wiki/statistical-estimation/why-estimation-theory/</guid><description>In statistics, we work with samples. For example, the sample mean is easily calculated. However, it is the population mean that is more valuable.
Suppose we have one sample $S_i$, which is used to calculate the mean of the sample $\mu_i$. We have two key problems to solve at this moment.
Can we use this sample mean $\mu_i$ to represent the population mean $\mu_p$? How good is our estimations? To answer these questions, we need to work out the properties of the samples themselves and work out a theory to instruct us to infer population statistics from sample statistics.</description></item><item><title>What is Statistics</title><link>/wiki/statistics/what-is-statistics/</link><pubDate>Fri, 18 Jan 2019 00:00:00 +0000</pubDate><guid>/wiki/statistics/what-is-statistics/</guid><description>A Case Study We have a problem.
In our lab, we found a huge amount of similar robots on a planet (physical population). To know more about the weight of these robots (statistical population), we first need to choose some of them (physical sample), then obtain the weight of them (statistical sample).
To describe the data, we could calculate the mean of the weight. We found that the mean weight is 93kg (descriptive statistics).</description></item><item><title>Association Rules</title><link>/wiki/pattern-mining/association-rules/</link><pubDate>Sun, 06 Jan 2019 00:00:00 +0000</pubDate><guid>/wiki/pattern-mining/association-rules/</guid><description>Association rule is a method for pattern mining. In this article, we perform an association rule analysis of some demo data.
The Problem Defined Suppose we own a store called KIOSK. Here at KIOSK, we sell 4 different things.
Milk Croissant Coffee Fries We need to know what items are associated with each other when the customers are buying.
We have collected the following data. Beware that this small amount of data might not be enough for a real-world problem.</description></item><item><title>Some Concepts about Data Warehouse</title><link>/wiki/data-warehouse/data-warehouse-concepts/</link><pubDate>Fri, 23 Nov 2018 00:00:00 +0000</pubDate><guid>/wiki/data-warehouse/data-warehouse-concepts/</guid><description>The Three Key Ideas about Warehouse The purpose of the data warehouse should be clear. In most cases, it is for the analysis of data, not for data production.1
Subject-oriented: since data warehouses are for decision-makers, arrange them into subjects makes it much easier to access. Integrated: many sources are integrated for easy analysis Time-variant: observation time should be recorded since the data is also used to analyze the time evolution Nonvolatile: simply for analysis OLTP and OLAP OLTP: online transaction processing OLAP: online analytical processing OLTP OLAP user customer data scientist, managers purpose production analysis content everything cleaner data database entity relation model, application-oriented star/snowflake model, subject-oriented history usually no need to record the history history is crucial query short and frequent read and write read-only and but complicated analysis Scope of Data Warehouse Enterprise warehouse: targeting the whole organization Data mart: for a specific group of people Virtual warehouse: views not tables Fact and Dimension Fact is the value of something specified by the dimension.</description></item><item><title>Artificial Neural Networks</title><link>/wiki/machine-learning/neural-networks/artificial-neural-networks/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/neural-networks/artificial-neural-networks/</guid><description>Artificial neural networks works pretty well for solving some differential equations.
Universal Approximators Maxwell Stinchcombe and Halber White proved that no theoretical constraints for the feedforward networks to approximate any measurable function. In principle, one can use feedforward networks to approximate measurable functions to any accuracy.
However, the convergence slows down if we have a lot of hidden units. There is a balance between accuracy and convergence rate. More hidden units lead to slow convergence but more accuracy.</description></item><item><title>Ordinary Differential Equations</title><link>/wiki/dynamical-system/ordinary-differential-method/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>/wiki/dynamical-system/ordinary-differential-method/</guid><description>For a first order differentiation $\frac{\partial f}{\partial t}$, we might have many finite differencing methods.
Euler Method For linear first ODE,
$$ \frac{dy}{dx} = f(x, y), $$
we can discretize the equation using a step size $\delta x \cdot$ so that the differential equation becomes
$$ \frac{y_{n+1} - y_n }{ \delta x } = f(x_n, y_n), $$
which is also written as
$$ y_{n+1} = y_n + \delta x \cdot f(x_n, y_n).</description></item><item><title>Basics of Computation</title><link>/wiki/computation/basics-of-computation/</link><pubDate>Thu, 13 Sep 2018 00:00:00 +0000</pubDate><guid>/wiki/computation/basics-of-computation/</guid><description>Storage, Precision, Error, etc To have some understanding of how the numbers are processed in computers, we have to understand how the numbers are stored first.
Computers stores everything in binary form 1. Suppose we randomly get some segments in the memory, we have no idea what that stands for since we do not know the type of data it represents.
Some of the most used data types in data science are</description></item><item><title>Introduction to Node Crawler Series</title><link>/wiki/nodecrawler/node-crawler-introduction/</link><pubDate>Sun, 15 Jul 2018 00:00:00 +0000</pubDate><guid>/wiki/nodecrawler/node-crawler-introduction/</guid><description>This is a set of tutorials that will help you with your very first crawler with node.js.
The plan of this tutorial is as follows. First of all, we will write a functional crawler using node.js and dump the data into files or simply print it on screen. In the following article, we will use MongoDB as our data management system and organize our data. Then we will optimize and attack some of the pitfalls.</description></item><item><title>Jupyter Notebook</title><link>/wiki/tools/jupyter/</link><pubDate>Wed, 20 Jun 2018 15:58:49 -0400</pubDate><guid>/wiki/tools/jupyter/</guid><description>Magics %lsmagic will show all the magics, including line magics and cell magics.
Line magics are magics start with one %; Cell magics are magics that can be used in the whole cell even with line breaks, where the cell should start with %%. %env can be used when setting environment variables inside the notebook.
%env MONGO_URI=localhost:27072 %%bash is a cell magic that allows bash commands in the cell.</description></item><item><title>Regular Expression Basics</title><link>/wiki/sugar/regular-experssions/</link><pubDate>Wed, 20 Jun 2018 15:58:49 -0400</pubDate><guid>/wiki/sugar/regular-experssions/</guid><description>List of Keys Anchors at the beginning of line ^ import re p = re.compile('^T', re.I) line = &amp;quot;The email address is this this the do you see&amp;quot; result = p.findall(line) print(result) # ['T'] at the end of the line $ import re p = re.compile('e$', re.I) line = &amp;quot;The email address is this this the do you see&amp;quot; result = p.findall(line) print(result) # ['e'] Character Classes Printable Characters any character .</description></item><item><title>Short-Time-Fourier-Transform</title><link>/wiki/time-series/short-time-fourier-transform/</link><pubDate>Wed, 20 Jun 2018 15:58:49 -0400</pubDate><guid>/wiki/time-series/short-time-fourier-transform/</guid><description>Short-Time-Fourier-Transform We Fourier transform the time series data using a Fourier transform, with some window function
\begin{equation} \tilde Y[n,k] = \sum_m Y[n+m] W[m] e^{-i \lambda_k m}, \end{equation}
where $\lambda_k=2\pi k/N$ and $W[m]$ is the window function at $m$.
References and Notes Cousera</description></item><item><title>Linear Methods</title><link>/wiki/machine-learning/linear/linear-methods/</link><pubDate>Fri, 25 May 2018 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/linear/linear-methods/</guid><description>Solving Classification Problems with Linear Models One simple idea behind classification is to calculate the posterior probability of each class given the variables.
Suppose a dataset have features $F_\alpha$ where $\alpha = 1, 2, \cdots, K$, with corresponding class labels $G_\alpha$. The dataset that provides $N$ datapoints with each deoted as $X_i$. The posterior of the classification is $P(G = G_\alpha \vert X = X_i)$.
A naive idea is to classify the data into two classes $m$ and $n$ using the boundary of a linear model</description></item><item><title>Machine Learning Overview</title><link>/wiki/machine-learning/overview/</link><pubDate>Fri, 25 May 2018 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/overview/</guid><description>What is Machine Learning There are many objectives in machine learning. Two of the most applied objectives are classifications and regressions. In classifications and regression, the following four factors are relevant.
A simple framework of machine learning. The dataset $\tilde{\mathscr D}$ is first encoded by $\mathscr T$, $\mathscr D(\mathbf X, \mathbf Y) = \mathscr T(\tilde{\mathscr D})$. The dataset is feeded into the model, $\bar{\mathbf Y} = f(\mathbf X;\mathbf \theta)$.</description></item><item><title>Unsupervised Learning</title><link>/wiki/machine-learning/unsupervised/overview/</link><pubDate>Fri, 25 May 2018 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/unsupervised/overview/</guid><description>Unsupervised Learning!
Principle components analysis Clustering K-means Clustering Algorithm:
Assign data points to a group Iterate through until no change: Find centroid Find the point that is closest to the centroids. Assign that data point to the corresponding group of the centroids. How Many Groups
The art of chosing K. Hierarchical Clustering Bottom-up hierarchical groups can be read out from the dendrogram.</description></item><item><title>Some Basic Ideas of Algorithms</title><link>/wiki/algorithms/algorithms-basics/</link><pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate><guid>/wiki/algorithms/algorithms-basics/</guid><description>This set of notes on algorithms is not meant to be comprehensive or complete. These notes are being used as a skeleton framework. There are many useful books to learn about algorithms from a utilitarian point of view. I have listed a few in the references section.
Numerical recipes is a very comprehensive book that I used during my PhD. It covers almost all the algorithms you need for scientific computing.</description></item><item><title>The C++ Language</title><link>/wiki/programming-languages/cpp/references/</link><pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate><guid>/wiki/programming-languages/cpp/references/</guid><description>C++!
Books The C++ Programming Language Programming Principles and Practice Using C++ The C++ Primer Lectures C++ Beginners Tutorial 1 (For Absolute Beginners) C++ Programming Introduction to C++ Coursear Course: C++ For C Programmers, Part A Top C++ Courses and Tutorials On SoloLearn: C++ Tutorial Practice SoloLearn provides this code playground that we can use to test c++ codes. There is also repl.it
Libraries For solving differential equations:</description></item><item><title>The Python Language: Basics</title><link>/wiki/programming-languages/python/basics/</link><pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate><guid>/wiki/programming-languages/python/basics/</guid><description>Numbers, Arithmetics Two types of numbers exist,
int float, 15 digits, other digits are float error It is worth noting that in Python 2, we have
print(1.0/3) # will give us float numbers # 0.333333333333 while
print(1/3) # will only give us int # 0 However, this was changed in Python 3.
Variables, Functions, Conditions A variable name should start with either a letter or an underscore.
Variables defined inside a function is local and there is no way to find it or use it outside the function.</description></item><item><title>Principles of Colors</title><link>/wiki/data-visualization/colors/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>/wiki/data-visualization/colors/</guid><description>Basic Concepts of Colors Color Wheel and Color Sphere There are two dimensions in the color wheel:
Hue Saturation When we add another dimension, lightness, to the wheel, we have a color sphere (1, 2).
Many color systems have been invented. Color wheel and color sphere are two examples of them.</description></item><item><title>Goodness-of-fit</title><link>/wiki/model-selection/goodness-of-fit/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>/wiki/model-selection/goodness-of-fit/</guid><description>Does the data agree with the model?
Calculate the distance between data and model predictions. Apply Bayesian methods such as likelihood estimation: likelihood of observing the data if we assume the model; the results will be a set of fitting parameters. &amp;hellip; Why don&amp;rsquo;t we always use goodness-of-fit as a measure of the goodness of a model?
We may experience overfitting. The model may not be intuitive. This is why we would like to balance it with parsimony using some measures of generalizability.</description></item><item><title>Data Types and Level of Measurement in Machine Learning</title><link>/wiki/machine-learning/feature-engineering/data-types/</link><pubDate>Wed, 15 Jan 2020 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/feature-engineering/data-types/</guid><description>Types of Data There are several debatable categorization methods of data.
The first widely spread theory, or level of measurement, is by S. Stevens. The theory categorizes data into four types, nominal, ordinal, interval, and ratio.
Other methods are proposed for other fields of research. For example, N. R. Chrisman proposed a different method for cartography. However, these are not generic enough for data science. They are more general than a specific field of research.</description></item><item><title>Decision Tree</title><link>/wiki/machine-learning/tree-based/decision-tree/</link><pubDate>Wed, 25 Dec 2019 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/tree-based/decision-tree/</guid><description>In this article, we will explain how decision trees work and build a tree by hand.
The code used in this article can be found in this repo. Definition of the problem We will decide whether one should go to work today. In this demo project, we consider the following features.
feature possible values health 0: feeling bad, 1: feeling good weather 0: bad weather, 1: good weather holiday 1: holiday, 0: not holiday For more compact notations, we use the abstract notation $\{0,1\}^3$ to describe a set of three features each with 0 and 1 as possible values.</description></item><item><title>Bayesian Linear Regression</title><link>/wiki/machine-learning/bayesian/bayesian-linear-regression/</link><pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/bayesian/bayesian-linear-regression/</guid><description>Linear Regression and Likelihood The linear estimator $y$ is
$$ \begin{equation} y^n = \beta^m X_m^{\phantom{m}n}. \label{eq-linear-model} \end{equation} $$
As usual, we have redefined our data to get rid of the intercept $\beta^0$.
In ordinary linear models, we find the error being the difference between the target $\hat y$ and the estimator $y$
$$ \epsilon = \hat y - y, $$
which is required to have a minimum absolute value.
In linear regressions, we use least squares to solve the problem.</description></item><item><title>NMF: Nonnegative Matrix Factorizatioin</title><link>/wiki/machine-learning/factorization/nmf/</link><pubDate>Thu, 13 Jun 2019 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/factorization/nmf/</guid><description>Decomposition To make it easier to understand, we start with a data point $\mathbf P$ in a $k$-dimensional space spanned by $k$ basis vectors $\mathbf V^k$. Naturally, we could write down the component decomposition of the point using the basis vectors $\mathbf V^k$,
$$ \mathbf P = P_k \mathbf V^k. $$
This is immediately obvious to us since we have been dealing with rank 2 $(k, 1)$ basis vectors and we are talking about the $k$ coordinates for a point.</description></item><item><title>Word2vec</title><link>/wiki/machine-learning/embedding/word2vec/</link><pubDate>Thu, 13 Jun 2019 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/embedding/word2vec/</guid><description>Word2vec is a word embedding model that learns the probability of some words being neighbours in a sentence $p_{neighbours}(w_i, w_o)$.
Build a dataset of adjacent words. CBOW; skipgram; negative sampling; Encode the words using vectors. Build a model $f(\{\theta_i\})$ to calculate the probability of the words being neighours and improve the parameters $\{\theta_i\}$ using the dataset.</description></item><item><title>Bias-Variance</title><link>/wiki/machine-learning/basics/bias-variance/</link><pubDate>Fri, 07 Jun 2019 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/basics/bias-variance/</guid><description>Bias and Variance Suppose $f(X)$ is a perfect model that represents a &amp;ldquo;tight&amp;rdquo; model of the dataset $(X,Y)$ but some irredicible error $\epsilon$,
$$ \begin{equation} Y = f(X) + \epsilon. \label{dataset-using-true-model} \end{equation} $$
On the other hand, we build another model using a specific method such as k-nearest neighbors, which is denoted as $k(X)$.
Why the two models?
Why are we talking about the perfect model and a model using a specific method?</description></item><item><title>Types of Errors in Statistical Hypothesis Testing</title><link>/wiki/statistical-hypothesis-testing/type-1-error-and-type-2-error/</link><pubDate>Fri, 31 May 2019 00:00:00 +0000</pubDate><guid>/wiki/statistical-hypothesis-testing/type-1-error-and-type-2-error/</guid><description>Type I and Type II Errors In statistical hypothesis testing, we always have a null hypothesis $H_0$ which refers to the statement to be tested. We have two possible conclusions from a hypothesis testing,
to accept the hypothesis, that is concluding that $H_0$ is true, to reject the hypothesis, that is concluding that $H_0$ is false. However, it is possible that our conclusion is not correct. There are four possible results.</description></item><item><title>Amazon CloudWatch Logs</title><link>/wiki/tools/awslogs/</link><pubDate>Mon, 11 Mar 2019 00:00:00 +0000</pubDate><guid>/wiki/tools/awslogs/</guid><description>Why Suppose we have all kinds of pipelines written in different languages, using different tools, and located in different places. It would be frustrating to pull out the logs.
This is why we need a centralized log service, for example cloudwatch.
Sending logs to CloudWatch First of all, send your logs to awslogs. The easies way is to use boto.
Retrieving and Analyzing Logs First of all, we need this: awslogs.</description></item><item><title>Confidence Interval</title><link>/wiki/statistical-estimation/confidence-interval/</link><pubDate>Sun, 20 Jan 2019 00:00:00 +0000</pubDate><guid>/wiki/statistical-estimation/confidence-interval/</guid><description>We will use upper cases for the abstract variable and lower cases for the actual numbers.
Why is Confidence Interval Needed? Suppose I sample the population multiple times, the mean value $\mu_i$ of the sample is calculated for each sample. It is a good question to ask how different these $\mu_i$ are compared to the true mean $\mu_p$ of the population.
In this article, we would need to specify several notations.</description></item><item><title>Jargons</title><link>/wiki/statistics/jargons/</link><pubDate>Sat, 24 Nov 2018 00:00:00 +0000</pubDate><guid>/wiki/statistics/jargons/</guid><description>Accuracy and Precision Accuracy: the measurement compared to the truth Precision: variability of repeated measurements; the more precise, the less variations during each measurement. Accurate Inaccurate Precise Close to true value, small variations in each measurement Far from true value, small variations in each measurement Imprecise Close to true value, large variations in each measurement Far from true value, large variations in each measurement Here is an example.</description></item><item><title>Extract, Transform and Load</title><link>/wiki/data-warehouse/extract-transform-load/</link><pubDate>Fri, 23 Nov 2018 00:00:00 +0000</pubDate><guid>/wiki/data-warehouse/extract-transform-load/</guid><description>ETL Process ETL
ETL
Extract: extract data from sources Transform: transform it to proper format Load: load it to data storage infrastructure E for Extract Should not affect the source system. T for Transform Cleaning Filtering Enriching Splitting Joining L for Load Deal with sync and waiting</description></item><item><title>Partial Differential Equations</title><link>/wiki/dynamical-system/partial-difference-method/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>/wiki/dynamical-system/partial-difference-method/</guid><description>Forward Time Centered Space For $\frac{d f}{d t} = - v \frac{ d f }{ dx }$, we write down the finite difference form 1
$$ \frac{f(t_{n+1}, x_i ) - f(t_n, x_i)}{ \Delta t } = - v \frac{ f(t_n, x_{i+1}) - f(t_n, x_{i-1}) }{ 2\Delta x }. $$
FTCS is an explicit method and is not stable.
Lax Method Change the term $f(t_n, x_i)$ in FTCS to $( f(t_n, x_{i+1}) + f(t_n, x_{i-1}) )/2$ 1.</description></item><item><title>Basics of Programming</title><link>/wiki/computation/basics-of-programming/</link><pubDate>Sun, 23 Sep 2018 00:00:00 +0000</pubDate><guid>/wiki/computation/basics-of-programming/</guid><description>Recursive and Iterative Solving problems with iterative and recursive methods are two quite different approaches, somehow, to the same kind of problems.
Here we will calculate the factorial of $n$. We define two functions using the iterative method and the recursive method.
Run the program on Repl.it.
def recursiveFactorial(n): if n == 0: return 1 else: return n * recursiveFactorial(n - 1) def iterativeFactorial(n): ans = 1 i=1 while i &amp;lt;= n: ans = ans * i i=i+1 return ans print(recursiveFactorial(0)) print(iterativeFactorial(0))</description></item><item><title>Basic Node Crawler</title><link>/wiki/nodecrawler/basic-crawler/</link><pubDate>Sun, 15 Jul 2018 00:00:00 +0000</pubDate><guid>/wiki/nodecrawler/basic-crawler/</guid><description>Prerequisites Nodejs &amp;gt;= 8.9 Overview A model for a crawler is as follows.
A crawler requests data from the server, while the server responds with some data. Here is a graphic illustration
+----------+ +-----------+ | | HTTP Request | | | +----------------&amp;gt; | | Nodejs | | Servers | | &amp;lt;----------------+ | | | HTTP Response | | +----------+ +-----------+ HTTP Requests For a good introduction of HTTP requests, please refer to this video on youtube: Explained HTTP, HTTPS, SSL/TLS API As for the first step, we need to find which url to request.</description></item><item><title>Autoregressive Model</title><link>/wiki/time-series/autoregressive-model/</link><pubDate>Wed, 20 Jun 2018 15:58:49 -0400</pubDate><guid>/wiki/time-series/autoregressive-model/</guid><description>Autoregressive Given a time series ${T^i}$, a simple predictive model can be constructed using an autoregressive model.
$$ \begin{equation} T^t = \sum_{i=1}^p \beta_i T^{t - i} + \beta^t + \beta^0. \end{equation} $$
Such a model is usually called an AR(p) model due to the fact that we are using data back in $p$ steps.
Differential Equation For simplicity we will look at a AR(1) model. Assume the time series has a step size of $dt$, our model can be rewritten as</description></item><item><title>Unsupervised Learning: PCA</title><link>/wiki/machine-learning/unsupervised/pca/</link><pubDate>Fri, 25 May 2018 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/unsupervised/pca/</guid><description>We use the Einstein summation notation in this article. Principal Component Analysis (PCA) is a commonly used trick for dimensionality reduction so that the new features represents most of the variances of the data.
Representations of Dataset In theory, a dataset can be represented by a matrix if we specify the basis. However, the initial given basis is not always the most convinient one. Suppose we find a new set of basis for the dataset, the matrix representation may be simpler and easier to use.</description></item><item><title>Data Structure</title><link>/wiki/algorithms/data-structure/</link><pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate><guid>/wiki/algorithms/data-structure/</guid><description>Dealing with data structure is like dealing with your clothes. Some people randomly drop their clothes somewhere without thinking. But it takes time to retrieve a specific T-shirt. Some people spend more time folding and arranging their clothes. This process makes it easy to find a specific T-shirt. Similar to retrieving clothes, there is always a balance between the computation time (retrieving clothes) and the coding time (folding clothes).
Keywords This section serves as some kind of flashcard keywords.</description></item><item><title>The C++ Language: Basics</title><link>/wiki/programming-languages/cpp/basics/</link><pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate><guid>/wiki/programming-languages/cpp/basics/</guid><description>Make it Work Apart from the traditional way of running C++ code, Jupyter notebook has a clingkernel that make it possibel to run C++ in a Jupyter notebook. Here is the post: Interactive C++ for HPC.
Concepts Namespace Operators: assignment operators (=,+=,-=,*=,/=,%=), increment/decrement operator (++x,x++,--x,x--), relational operators (&amp;gt;,&amp;lt;,&amp;gt;=,&amp;lt;=,==,!=), logicl operators (&amp;amp;&amp;amp;,||,!), left shift (&amp;lt;&amp;lt;), extration operator (&amp;gt;&amp;gt;, or right shift), understand the operator precedence Variables: variable name starts with underscore or latin letters, Pascal case (PascalCase), Camel case (pascalCase) if/else and Loops: if (condition is true ){ then something } Data Types: string (double quote), character (char, 1 byte ASCII character, using single quote), float (4 bytes, always signed), double (8 bytes, always signed), long double (8 or 16 bytes, always signed), singed or unsigned short or long int (signed long int, unsigned int) Pointers: ampersand (&amp;amp;) accesses the address, pointer is variable thus needs to be declared using asterisk (*), can be declared to be int or double or float or char (int \*pt; int\* pt; int * pt;) Functions: overload, recursion Class: identity, atrributes, method/behavior, access specifiers (private or public or protected, by default it is set to private), instantiation of object (creating object), constructor, destructor, encapsulation, scope resolution operator (TheClassYouNeed::somefunction()), selection operator (dot member selection .</description></item><item><title>The Python Language: Decorators</title><link>/wiki/programming-languages/python/decorators/</link><pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate><guid>/wiki/programming-languages/python/decorators/</guid><description>Functions: first-class objects; can be passed around as arguments.
What that tells us about is that functions can be pass into a function or even returned by a function. For example,
def a_decoration_function( yet_another_function ): def wrapper(): print(&amp;#39;Before yet_another_function&amp;#39;) yet_another_function() print(&amp;#39;After yet_another_function&amp;#39;) return wraper def yet_another_function(): print(&amp;#39;This is yet_another_function&amp;#39;) When we execute a_decoration_function, we will have
Before yet_another_function This is yet_another_function After yet_another_function So a decorator is simply a function that takes a function as an argument, adds some salt to it.</description></item><item><title>A Physicist's Crash Course on Artificial Neural Network</title><link>/wiki/machine-learning/neural-networks/physicists-crash-course-neural-network/</link><pubDate>Sat, 02 May 2015 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/neural-networks/physicists-crash-course-neural-network/</guid><description>What is a Neuron What a neuron does is to response when a stimulation is given. This response could be strong or weak or even null. If I would draw a figure, of this behavior, it looks like this.
Neuron response Using simple single neuron responses, we could compose complicated responses. To achieve that, we study the transformations of the response first.
transformations Artificial Neural Network A simple network is a collection of neurons that response to stimulations, which could be the responses of other neurons.</description></item><item><title>Measures of Generalizability</title><link>/wiki/model-selection/measures-of-generalizability/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>/wiki/model-selection/measures-of-generalizability/</guid><description>To measure the generalization, we define a generalization error,
$$ \begin{align} \mathcal G = \mathcal L_{P}(\hat f) - \mathcal L_E(\hat f), \end{align} $$ where $\mathcal L_{P}$ is the population loss, $\mathcal L_E$ is the empirical loss, and $\hat f$ is our model by minimizing the empirical loss.
However, we do not know the actual joint probability $p(x, y)$ of our dataset $\{x_i, y_i\}$. Thus the population loss is not known. In machine learning, we usually use cross validation where we split our dataset into train and test dataset.</description></item><item><title>Random Forest</title><link>/wiki/machine-learning/tree-based/random-forest/</link><pubDate>Wed, 25 Dec 2019 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/tree-based/random-forest/</guid><description>Random forest is an ensemble method based on decision trees. Instead of using one decision tree and model on all the features, the decision tree method can model on a random set of features (feature subspace) using many decision trees and make decisions by democratizing the trees.
Given a proper dataset $\mathscr D(\mathbf X, \mathbf y)$, the ensemble of trees is denoted as ${f_i(\mathbf X)}$, will predict an ensemble of results.</description></item><item><title>Predictions Using Time Series Data</title><link>/wiki/time-series/predictions-time-series-data/</link><pubDate>Fri, 21 Jun 2019 00:00:00 +0000</pubDate><guid>/wiki/time-series/predictions-time-series-data/</guid><description>General Phenological Model for Seasonality In business, time series data $f(t)$ usually carries information about trend $g(t)$ ($g$ is used since trend is usually growth), seasonalities (periodical effects) $p(t)$, holiday effects (structural effects) $s(t)$, etc. We will decompose a time series $f(t)$ into four components
$$ \begin{equation} f(t) = g(t) + p(t) + s(t) + \epsilon(t). \end{equation} $$
To train a model for the predictions, we need to write down the exact models of these three predictable components.</description></item><item><title>Tensor Factorization</title><link>/wiki/machine-learning/factorization/tensor-factorization/</link><pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/factorization/tensor-factorization/</guid><description>Tensors We will be talking about tensors but we will skip the introduction to tensor for now.
In this article, we follow a commonly used convention for tensors in physics, the abstract index notation. We will denote tensors as $T^{ab\cdots}_ {\phantom{ab\cdots}cd\cdots}$, where the latin indices such as $^{a}$ are simply a placebo for the slot for this &amp;ldquo;tensor machine&amp;rdquo;. For a given basis (coordinate system), we can write down the components of this tensor $T^{\alpha\beta\cdots} _ {\phantom{\alpha\beta\cdots}\gamma\delta\cdots}$.</description></item><item><title>Anscombe's quartet</title><link>/wiki/data-visualization/anscombes-quartet/</link><pubDate>Mon, 18 Mar 2019 00:00:00 +0000</pubDate><guid>/wiki/data-visualization/anscombes-quartet/</guid><description>Anscombe&amp;rsquo;s Quartet Anscombe&amp;rsquo;s quartet is a brilliant idea that shows the importance and convenience of visual representation of data.
Anscombe&amp;rsquo;s quartet has four datasets. The values of each dataset are shown below.
x1 = [10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5] y1 = [8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68] x2 = [10.0, 8.0, 13.0, 9.0, 11.0, 14.0, 6.0, 4.0, 12.0, 7.0, 5.</description></item><item><title>OLAP Operations</title><link>/wiki/data-warehouse/olap-operations/</link><pubDate>Fri, 23 Nov 2018 00:00:00 +0000</pubDate><guid>/wiki/data-warehouse/olap-operations/</guid><description>Roll-up or Drill-up The word &amp;lsquo;up&amp;rsquo; in the names refers to going up in concept hierarchies.
For example, we would like to know the revenue of the whole year. However, the record of data is
Date Revenue 2018-01-01 1023 2018-01-02 934 &amp;hellip; &amp;hellip; 2018-12-30 1244 2018-12-31 1302 Roll-up is performed by summing up everything of the column revenue.</description></item><item><title>Finite Element Method</title><link>/wiki/dynamical-system/finite-element-method/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>/wiki/dynamical-system/finite-element-method/</guid><description>Differential Equations and Boundary Conditions Two Types of Boundary Conditions As an example, we have a partial differential equation
$$ \frac{d^2u}{dx^2} + f = 0, $$
which describes a 1D problem.
Dirichlet boundary condition: specify values for $u$, such as $u(0)=u_0$ and $u(L)=u_L$; Neumann boundary condition: specifiy values for $u_{,x}$. If we have only Neumann boundary condition, the solution is not unique. One example for it is tossing a bar, which can have both Neumann BC at both ends but it is moving.</description></item><item><title>Chi-square Correlation Test for Nominal Data</title><link>/wiki/statistics/correlation-analysis-chi-square/</link><pubDate>Sun, 18 Nov 2018 00:00:00 +0000</pubDate><guid>/wiki/statistics/correlation-analysis-chi-square/</guid><description>In this article, we will discuss the chi-square correlation test for detecting correlations between two series.
Steps Find out all the possible values of the two nominal series A and B; Count the co-occurrences of the combinations (A, B); Calculate the expected co-occurrences of the combinations (A, B); Calculate chi-square; Determine whether the hypothesis can be rejected. Define the Series Suppose we are analyzing two series A and B.</description></item><item><title>Basics of Network</title><link>/wiki/computation/basics-of-network/</link><pubDate>Sun, 23 Sep 2018 00:00:00 +0000</pubDate><guid>/wiki/computation/basics-of-network/</guid><description>HTTP Keywords Hyper Text Transfer Protocal: deliver hyper text from server to local browser etc. Based on TCP/IP Current version: HTTP/2 Server - Client Client can request through GET, HEAD, POST, PUT, DELETE, TRACE, OPTIONS, CONNECT, PATCH. Transfer anything defined by Content-Type Connectionless Protocol: doesn&amp;rsquo;t maintain the connection all the time Stateless protocal: A very nice explanation URL Keywords Uniform Resource Locator Interpret each part of this URL: http://abc.</description></item><item><title>Unsupervised Learning: SVM</title><link>/wiki/machine-learning/unsupervised/svm/</link><pubDate>Fri, 17 Aug 2018 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/unsupervised/svm/</guid><description>SVM is calculating a hyperplane to separate the data points into groups according to the label.
Hyperplane A hyperplane is defined to be of the following form
$$ \begin{equation} \boldsymbol{\beta} \cdot \mathbf x = \beta_0. \end{equation} $$
where $\boldsymbol\beta$ is the normal vector to the plane and is required to be constant.
It is straight forward to show that the distance $d$ from an arbitrary point $\mathbf x'$ to the hyperplane is</description></item><item><title>Manage Data Using MongoDB</title><link>/wiki/nodecrawler/manage-data-using-mongodb/</link><pubDate>Wed, 18 Jul 2018 00:00:00 +0000</pubDate><guid>/wiki/nodecrawler/manage-data-using-mongodb/</guid><description>In most cases, databases makes the management of data quite convenient. In this article, we would scrape data using the code we discussed before but write data into MongoDB.
For installation of MongoDB, please refer to the official documentation.
The Code To write data to MongoDB using Node.js, we choose the package mongojs, which provides almost exactly the standard MongoDB syntax.
To install mongojs,
npm i mongojs --save Here is a module that can write data to MongoDB.</description></item><item><title>The Python Language: Multi-Processing</title><link>/wiki/programming-languages/python/multiprocessing/</link><pubDate>Thu, 10 May 2018 00:00:00 +0000</pubDate><guid>/wiki/programming-languages/python/multiprocessing/</guid><description>Python has built-in multiprocessing module in its standard library.
One simple example of using the Pool class is the following.
def myfunc(myfuncargs): &amp;#39;some thing here&amp;#39; with Pool(10) as p: records = p.map(myfunc, myfuncargs) However, there are limitations on this, especially on pickles. Another approach.
from multiprocessing import Pool from multiprocessing.dummy import Pool as ThreadPool with ThreadPool(1) as p: records = p.map(myfunc, myfuncargs) Beware that map function will feed in a list of args to the function.</description></item><item><title>Data Structure: Tree</title><link>/wiki/algorithms/data-structure-tree/</link><pubDate>Tue, 27 Mar 2018 00:00:00 +0000</pubDate><guid>/wiki/algorithms/data-structure-tree/</guid><description>mind the data structure: here comes the tree</description></item><item><title>The C++ Language: Numerical Methods</title><link>/wiki/programming-languages/cpp/numerical/</link><pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate><guid>/wiki/programming-languages/cpp/numerical/</guid><description>Modularize The code should be designed to separate physics or model from numerical methods. Speed vectors are convenient but slow. 1 Do not copy arrays if not necessary. The example would be for a function return. Most of the time, we can pass the pointer of an array to the function and update the array itself without copying anything and no return is needed at all. inline function.</description></item><item><title>GNUPlot</title><link>/wiki/tools/gnuplot/</link><pubDate>Mon, 04 Sep 2017 00:00:00 +0000</pubDate><guid>/wiki/tools/gnuplot/</guid><description>Examples Plot .csv data. Suppose we have data of such.
-0.00999983, 0.99995 -0.0199987, 0.9998 -0.0299955, 0.99955 -0.0399893, 0.9992 -0.0499792, 0.99875 -0.059964, 0.998201 To plot the second column against the first column, we use the using parameter in gnuplot.
gnuplot -e &amp;#34;set terminal png; set datafile separator &amp;#39;,&amp;#39; ; plot &amp;#39;complex.txt&amp;#39; using 1:2&amp;#34; | imgcat # datafile seperator is not always necessary # imgcat is a script in iterm2 on mac</description></item><item><title>Boltzmann Machine</title><link>/wiki/machine-learning/neural-networks/boltzmann-machine/</link><pubDate>Sun, 27 Aug 2017 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/neural-networks/boltzmann-machine/</guid><description>Boltzmann machine is much like a spin glass model in physics. In short words, Boltzmann machine is a machine that has nodes that can take values, and the nodes are connected through some weight. It is just like any other neural nets but with complications and theoretical implications.
Boltzmann Machine and Physics To obtain a good understanding of Boltzmann machine for a physicist, we begin with Ising model. We construct a system of neurons ${ s_i}$ which can take values of 1 or -1, where each pair of them $s_i$ and $s_j$ is connected by weight $J_{ij}$.</description></item><item><title>Deep Autoregressive Network</title><link>/wiki/machine-learning/neural-networks/deep-autoregressive-networks/</link><pubDate>Mon, 15 Feb 2021 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/neural-networks/deep-autoregressive-networks/</guid><description>There are two levels of autoregressiveness in the DARN network:
Inlayer autoregressive connections of the nodes, Intralayer autoregressive connections of nodes. The network is trained on MDL loss.</description></item><item><title>Wavelet Transform</title><link>/wiki/time-series/wavelets/</link><pubDate>Mon, 07 Dec 2020 00:00:00 +0000</pubDate><guid>/wiki/time-series/wavelets/</guid><description>In general, given a complete set of function $\psi(x; \tilde x)$, we can decompose a function $F(\tilde x)$
$$ F(\tilde x) = \int f(x) \psi(x;\tilde x) dx. $$
The choice of $\psi(x;\tilde x)$ gives us different properties.
Fourier Transform Fourier transform is good for stationary analysis since time is not involved in $F(\omega)$.
$$ F(\omega) = \int_{-\infty}^{\infty} f(t) e^{-i \omega t} dt $$
Short-time Fourier Transform STFT is a Fourier transform with a moving time window $\tau$,</description></item><item><title>Parsimony of Models</title><link>/wiki/model-selection/parsimony-of-models/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>/wiki/model-selection/parsimony-of-models/</guid><description>For models with a lot of parameters, the goodness-of-fit is very likely to be very high. However, it is also likely to generalize bad. So we need measure of generalizability
Here parsinomy gives us a few advantages.
easy to perceive better generalizations</description></item><item><title>Terminal</title><link>/wiki/tools/terminal/</link><pubDate>Tue, 31 Dec 2019 00:00:00 +0000</pubDate><guid>/wiki/tools/terminal/</guid><description>Navigating Some tips to help data scientist navigate faster in terminal.
pushd, popd and dirs pushd to register and change directories: pushd folder_name will change current directory to folder_name and register the folder folder_name in our stack. If no folder name is passed onto the command, it will be default to $HOME folder. popd to go to the last directory in the stack and remove it from the stack. In this example, popd will change the current working directory to folder_name.</description></item><item><title>Statistical Sign Test</title><link>/wiki/statistical-hypothesis-testing/sign-test/</link><pubDate>Sun, 20 Jan 2019 00:00:00 +0000</pubDate><guid>/wiki/statistical-hypothesis-testing/sign-test/</guid><description>We have a small dataset, but it doesn&amp;rsquo;t satisfy the t-test conditions. Then we would use as little assumptions as possible.
Wine Taste Suppose we have two bottles of wine, one of them is 300 euros while the other is 100 euros.
Now we ask the question:
Does expensive wine taste better?
We find 10 experts and give them some experiments. The result is recorded then processed into the following table.</description></item><item><title>Data Storage</title><link>/wiki/data-warehouse/data-storage/</link><pubDate>Fri, 23 Nov 2018 00:00:00 +0000</pubDate><guid>/wiki/data-warehouse/data-storage/</guid><description>tl;dr: Use type safe formats such as HDF5 or parquet
HDF5 BCOLZ &amp;lt;http://bcolz.blosc.org/en/latest/&amp;gt;_ : not designed for multidimentional data. Zarr &amp;lt;https://github.com/alimanfoo/zarr&amp;gt;_ : works with multidimensional data and also parallel computating. Blaze ecosystem &amp;lt;http://blaze.pydata.org/&amp;gt;_ A article that compares HDF5, BCOLZ, and Zarr: To HDF5 and beyond
I also recommend pandas. It is a python module that works very well with data. It even loads HDF5 out of box.</description></item><item><title>Bin Size of Histogram</title><link>/wiki/data-visualization/histogram-bin-size/</link><pubDate>Thu, 22 Nov 2018 00:00:00 +0000</pubDate><guid>/wiki/data-visualization/histogram-bin-size/</guid><description>Histograms are good for understanding the distribution of your data.
The Bin Size Problem As an example, we will use the following series as an example.
[1.45,2.20,0.75,1.23,1.25,1.25,3.09,1.99,2.00,0.78,1.32,2.25,3.15,3.85,0.52,0.99,1.38,1.75,1.21,1.75] If we use bin size 1, we get this spiky chart and it is not so informing.
We could also set bin size to 2.
In principle, we could keep tuning the bin size until we get something pretty and informing. But that would be quite depressing.</description></item><item><title>Correlation Coefficient and Covariance for Numeric Data</title><link>/wiki/statistics/correlation-coefficient/</link><pubDate>Sun, 18 Nov 2018 00:00:00 +0000</pubDate><guid>/wiki/statistics/correlation-coefficient/</guid><description>Covariances Correlation coefficient is also known as the Pearson&amp;rsquo;s product moment coefficient. Review of Standard Deviation For a series of data A, we have the standard deviations
$$ \sigma_A = \sqrt{ \frac{ \sum (a_i - \bar A)^2 }{ n } }, $$
where $n$ is the number of elements in series A.
The standard deviation is very easy to understand. It is basically the average Eucleadian distance between the data points and the average value.</description></item><item><title>Basics of Database</title><link>/wiki/computation/basics-of-database/</link><pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate><guid>/wiki/computation/basics-of-database/</guid><description>NoSQL NoSQL = Not only SQL. The four main types of NoSQL databases are
Key-value store: Amazon Dynamo, memcached, Amazon SimpleDB Column-orient store: Google BigTable, Cassandra Graph database: Neo4j, VertexDB Document database: MongoDB Object database: ZODB Database Operations Relations Union: $A\cup B$ Intersection: $A\cap B$ $A - B$ Cartesian Product: $A \times B$ Query Union in database: will combine the data with matching common columns.</description></item><item><title>Restrictions of Websites</title><link>/wiki/nodecrawler/restrictions/</link><pubDate>Thu, 19 Jul 2018 00:00:00 +0000</pubDate><guid>/wiki/nodecrawler/restrictions/</guid><description>Beware that scraping data off websites is neither always allowed nor as easy as a few lines of code. The preceding articles enable you to scrape many data, however, man websites have counter measures. In this article, we will be dealing with some of the common ones.
Request Frequency Some websites have limitations on the frequency of API requests. The solution to this is simply a brief pause after each request.</description></item><item><title>The Python Language: Performance</title><link>/wiki/programming-languages/python/performance/</link><pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate><guid>/wiki/programming-languages/python/performance/</guid><description>Read the references for performance.
The message:
Use comprehensions Use generators</description></item><item><title>MDL and Neural Networks</title><link>/wiki/model-selection/mdl-and-neural-networks/</link><pubDate>Sun, 14 Feb 2021 00:00:00 +0000</pubDate><guid>/wiki/model-selection/mdl-and-neural-networks/</guid><description>Minimum Description Length ( MDL MDL is a measure of how well a model compresses data by minimizing the combined cost of the description of the model and the misfit. ) can be used to construct a concise network. A fully connected network has great expressing power but it is easily overfitting.
One strategy is to apply constraints to the networks:
Limit the connections; Shared weights in subgroups of the network; Constrain the weights using some probability distributions.</description></item><item><title>Boxplot</title><link>/wiki/data-visualization/boxplots/</link><pubDate>Tue, 20 Aug 2019 00:00:00 +0000</pubDate><guid>/wiki/data-visualization/boxplots/</guid><description>The Whiskers in Boxplot They are the outlier data points.
Outliers are determined using the interquatile range (IQR, i.e., 25 percentile to 75 percentile.). We usually the lowest data point within 1.5 IQR range below the 25 percentile or the data point within 1.5 IQR range above the 75 percentile.</description></item><item><title>Mann-Whitney U Test</title><link>/wiki/statistical-hypothesis-testing/mann-whitney-u-test/</link><pubDate>Sun, 20 Jan 2019 00:00:00 +0000</pubDate><guid>/wiki/statistical-hypothesis-testing/mann-whitney-u-test/</guid><description>Mann-Whitney U is good at testing heavy-tailed data.</description></item><item><title>Basics of SQL</title><link>/wiki/computation/basics-of-sql/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>/wiki/computation/basics-of-sql/</guid><description>Adding a new field to data:
Relational: requires a new column Non-Relational: just add the field to one single document, thus can be easily decentralized. Basics and Background SQL: Structured Query Language
Relational Database:
usually in tables rows are called records columns are certain types of data. Data types of rows are specified: INTEGER TEXT DATE REAL, real numbers NULL &amp;hellip; RDBMS: Relational Database Management System, most RDBMS use SQL as the query language.</description></item><item><title>Normalization Methods for Numeric Data</title><link>/wiki/statistics/normalization-methods/</link><pubDate>Sun, 18 Nov 2018 00:00:00 +0000</pubDate><guid>/wiki/statistics/normalization-methods/</guid><description>Normalization of data is critical for statistical analysis and feature engineering.
Min-max Normalization This method is linear and straightforward.
Suppose we are analyzing series A, with elements $a_i$. We already know the min and max of the series, $a_{min}$ and $a_{max}$.
Now we would like to normalize the series to be within the range $[a_{min}', a_{max}']$. We simply solve the value of $a&amp;rsquo; _ i$ in $$ \frac{(a&amp;rsquo;_i - a_{min}')}{ ( a&amp;rsquo;_{max} - a&amp;rsquo;_{min} ) } = \frac{(a_i - a_{min})}{ ( a_{max} - a_{min} ) }, $$ where everything on the right hand side is known and $a_{min}&amp;lsquo;$ and $a_{max}&amp;lsquo;$ are chosen as the new min and max to be scaled to.</description></item><item><title>Optimization</title><link>/wiki/nodecrawler/optimization/</link><pubDate>Thu, 19 Jul 2018 00:00:00 +0000</pubDate><guid>/wiki/nodecrawler/optimization/</guid><description>In this article, we will be optimizing the crawler to get better performance.
Batch Jobs In the article about using MongoDB as data storage, we write the data to database whenever we get it. In practice, this is not efficient at all. Here comes the batch jobs. It would be much better if one write to database with batch jobs.
If you recall, the code we used to write to database is</description></item><item><title>Git</title><link>/wiki/tools/git/</link><pubDate>Wed, 22 Jun 2016 00:00:00 +0000</pubDate><guid>/wiki/tools/git/</guid><description>Git Services GitHub Bitbucket GitLab Using Git with GUI There are huge amounts of git commands! There are also a lot of GUIs if you don&amp;rsquo;t like command line.
GitHub Desktop GitKraken SourceTree &amp;hellip; Useful Commands To check all the commits related to a file, use git log -u. Try out git log -g before determining which reflog to deal with. To compare the changes with the last commit, use git diff --cached HEAD~1.</description></item><item><title>Some ML Workflow Frameworks</title><link>/wiki/tools/ml-flow-frameworks/</link><pubDate>Wed, 13 Jan 2021 00:00:00 +0000</pubDate><guid>/wiki/tools/ml-flow-frameworks/</guid><description>Metaflow Docs
A framework for jupyter notebook data scientists.
Work locally on notebooks. Python environment management using conda. Work in the cloud with Sagemaker. Tasks Methods Comments Code Scripts/Jupyter Notebook Datastore local + S3 metaflow.S3 Compute local + AWS Batch Metadata metaflow service Metadata specifies flow executions: Flows, Runs, Steps, Tasks, and Artifacts. Scheduling AWS Step Functions Deployment AWS Demo from metaflow import FlowSpec, step class BranchFlow(FlowSpec): @step def start(self): self.</description></item><item><title>Linear Regression</title><link>/wiki/statistics/linear-regression/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>/wiki/statistics/linear-regression/</guid><description>In this article, we will use the Einstein summation convention. For example, $$ X_{ij}\beta_ j $$ is equivalent to $$ \sum_j X_{ij}\beta_ j $$ In statistics, we have at least three categories of quantities:
data and labels abstract theoretical quantities parameters and predictions of models The convention is that quantities with $\hat {}$ are the model quantities. Sometimes we do not distinguish the abstract theoretical quantities and model quantities.</description></item><item><title>Basics of MongoDB</title><link>/wiki/computation/basics-of-mongodb/</link><pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate><guid>/wiki/computation/basics-of-mongodb/</guid><description>This MongoDB Cheatsheet is my best friend.
MongoDB Concepts Documents Collections: just like tables in SQL. Database MongoShell Some examples:
// show the databases show dbs // show collections show collections //set any database to current database use database_name // insert entry db.database_name.insert( an_object_2_be_the_entry ) // read document db.database_name.findOne({'some_field':'value_of_field'}) db.database_name.fidn() // prettify db.database_name.find().pretty()</description></item><item><title>Describing Multi-dimensional Data</title><link>/wiki/statistics/multidimensional-data/</link><pubDate>Mon, 03 Dec 2018 00:00:00 +0000</pubDate><guid>/wiki/statistics/multidimensional-data/</guid><description>Descriptions of Multidimensional Data Dispersion Matrix As defined in Correlation Coefficient and Covariance for Numeric Data, covariance is about the variance of two series. This property makes it easy to generalize it to multidimensional data.
The generalized quantity is named as dispersion matrix. Suppose we have a $p$ dimensional dataset $X$,
index $x_1$ $x_2$ &amp;hellip; $x_p$ 1 2.3 12.3 83.2 9.3 &amp;hellip; &amp;hellip; &amp;hellip; &amp;hellip; &amp;hellip; N 3.</description></item><item><title>Signal Processing</title><link>/wiki/algorithms/singal-processing/</link><pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate><guid>/wiki/algorithms/singal-processing/</guid><description>There are many fascinating ideas in signal processing.</description></item><item><title>Signal Processing: Audio Basics</title><link>/wiki/algorithms/signal-processing-audio/</link><pubDate>Thu, 29 Mar 2018 00:00:00 +0000</pubDate><guid>/wiki/algorithms/signal-processing-audio/</guid><description>Keywords Harmonic structure of sound Parson code of music Linear time-invariant theory Autocorrelation Noise Chirps DCT compression Discrete Fourier transform filtering convolution Linear Time-Invariant System We describe the system with $Y(t) = f(X(t))$, where $X(t)$ is the input, and $Y(t)$ is the output.
Linear: $f(a X_1(t) + b X_2(t)) = a f(X_1(t)) + b f(X_2(t))$ Time-invariant: input $X(t+\Delta t)$ will produce the shifted signal $Y(t+\Delta t)$. LTI systems are memory systems, casual, real, and stable.</description></item><item><title>Basics of MapReduce</title><link>/wiki/algorithms/map-reduce/</link><pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate><guid>/wiki/algorithms/map-reduce/</guid><description>Centralized servers are not efficient for big data. Querying and processing data on centralized servers would reach bottleneck of the servers.
MapReduce is used to solve these problems of big data. The two videos are .
Map: take series of key-value pairs and divide them into groups. Reduce: recombine the key-value pairs Checkout the code challenges of MapReduce on HackerRank.</description></item><item><title>Coding Theory Concepts</title><link>/cards/information/coding-theory-concepts/</link><pubDate>Wed, 17 Feb 2021 00:00:00 +0000</pubDate><guid>/cards/information/coding-theory-concepts/</guid><description>The code function produces code words. The expected length of the code word is limited by the entropy from the source probability $p$.
The Shannon information content, aka self-information, is described by
$$ - \log_2 p(x=a), $$ for the case that $x=a$.
The Shannon entropy is the expected information content for the whole sequence with probability distribution $p(x)$,
$$ \mathcal H = - \sum_x p(x\in X) \log_2 p(x). $$ The Shannon source coding theorem says that for $N$ samples from the source, we can roughly compress it into $N\mathcal H$.</description></item><item><title>Empirical Loss</title><link>/cards/machine-learning/measurement/empirical-loss/</link><pubDate>Sat, 06 Feb 2021 00:00:00 +0000</pubDate><guid>/cards/machine-learning/measurement/empirical-loss/</guid><description>Given a dataset with records $\{x_i, y_i\}$ and a model $\hat y_i = f(x_i)$ the empirical loss is calculated on all the records
$$ \begin{align} \mathcal L_{E} = \frac{1}{n} \sum_i^n d(y_i, f(x_i)), \end{align} $$ where $d(y_i, f(x_i))$ is the distance defined between $y_i$ and $f(x_i)$.</description></item><item><title>Population Loss</title><link>/cards/machine-learning/measurement/population-loss/</link><pubDate>Sat, 06 Feb 2021 00:00:00 +0000</pubDate><guid>/cards/machine-learning/measurement/population-loss/</guid><description>Given a dataset with records $\{x_i, y_i\}$ and a model $\hat y_i = f(x_i)$. Suppose we know the actual generating process of the dataset and the joint probability density distribution of all the data points is $p(x, y)$, the population loss is defined on the whole assumed population,
$$ \begin{align} \mathcal L_{P} = \mathop{\mathbb{E}}_{p(x,y)}[ d(y, f(x))], \end{align} $$ where $d(y, f(x))$ is the distance defined between $y$ and $f(x)$.</description></item><item><title>Data File Formats</title><link>/cards/machine-learning/datatypes/data-file-formats/</link><pubDate>Tue, 02 Feb 2021 00:00:00 +0000</pubDate><guid>/cards/machine-learning/datatypes/data-file-formats/</guid><description>Data storage is diverse. For data on smaller scales, we are mostly dealing with some data files.
work_with_data_files
Efficiencies and Compressions Parquet Parquet is fast. But
Don&amp;rsquo;t use json or list of json as columns. Convert them to strings or binary objects if it is really needed.</description></item><item><title>Machine as a Hologram</title><link>/projects/hologram/</link><pubDate>Sun, 31 Jan 2021 00:00:00 +0000</pubDate><guid>/projects/hologram/</guid><description>Tutorials on machine learning and data science productivity articles</description></item><item><title>Latent Variable Models</title><link>/wiki/machine-learning/bayesian/latent-variable-models/</link><pubDate>Wed, 27 Jan 2021 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/bayesian/latent-variable-models/</guid><description>In the view of statistics, we know everything about a physical system if we know the probability $p(\mathbf s)$ of all possible states of the physical system $\mathbf s$. Time can also be part of the state specification.
As an example, we will classify fruits into oranges and non oranges. We will have the state vector $\mathbf s = (\text{is orange}, \text{texture } x)$. Our goal is to find the join probability $p(\text{is orange}, x)$.</description></item><item><title>Reparametrization in Expectation Sampling</title><link>/cards/statistics/reparametrization-expectation-sampling/</link><pubDate>Wed, 20 Jan 2021 00:00:00 +0000</pubDate><guid>/cards/statistics/reparametrization-expectation-sampling/</guid><description>The expectation value of a function $f(z)$ over a Guassian distribution $\mathscr N(z;\mu, \sigma)$ is equivalent to the expectation value of $f()$ a Gaussian distribution $\mathscr N(z;\mu=0, \sigma=1)$, i.e.,
$$ {\mathbb E}_{\mathscr N(z; \mu, \sigma)} \left[ f(z) \right] = {\mathbb E}_{\mathscr N(z; 0, 1)} \left[ f() \right] $$ where
$$ \mathscr N = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left( -\frac{(z-\mu)^2}{2\sigma^2}\right). $$ $$ \begin{align} {\mathbb E}_{\mathscr N(z; \mu, \sigma)} \left[ f(z) \right] &amp;= \int \mathrm d z \frac{1}{\sqrt{2\pi\sigma^2}}\exp \left( -\frac{(z-\mu)^2}{2\sigma^2}\right) f(z) \\ &amp;= \int \mathrm dz \frac{1}{\sigma} \frac{1}{\sqrt{2\pi}} \exp \left( -\frac{1}{2} \left(\frac{z-\mu}{\sigma}\right)^2 \right) f(z) \\ &amp;= \int \mathrm d \left( \sigma z' + \mu \right) \frac{1}{\sigma} \frac{1}{\sqrt{2\pi}} \exp \left( -\frac{1}{2} z'^2 \right) f(\sigma z' + \mu) \\ &amp;= \int \mathrm d z' \frac{1}{\sqrt{2\pi}}\exp \left( -\frac{1}{2} z'^2 \right) f(\sigma z' + \mu) \\ &amp;= \int \mathrm d z' \mathscr N(z'; \mu=0, \sigma=1) f(\sigma z' + \mu) \\ &amp;= {\mathbb E}_{\mathscr N(z'; \mu=0, \sigma=1)} \left[ f(\sigma z' + \mu) \right] \end{align} $$ Define a field $\phi = {\mu, \sigma}$, the derivatives of the reparametrization</description></item><item><title>Normalizing Flows: An Introduction and Review of Current Methods</title><link>/reading/normalizing-flow-introduction-1908.09257/</link><pubDate>Sun, 17 Jan 2021 00:00:00 +0000</pubDate><guid>/reading/normalizing-flow-introduction-1908.09257/</guid><description>To generate complicated distributions step by step from a simple and interpretable distribution.</description></item><item><title>A Simple Machine Learning Project Framework</title><link>/blog/data-science/a-simple-machine-learning-framework/</link><pubDate>Tue, 12 Jan 2021 00:00:00 +0000</pubDate><guid>/blog/data-science/a-simple-machine-learning-framework/</guid><description> A simple almost stateless machine learning framework</description></item><item><title>Basics of Redis</title><link>/wiki/computation/basics-of-redis/</link><pubDate>Fri, 08 Jan 2021 00:00:00 +0000</pubDate><guid>/wiki/computation/basics-of-redis/</guid><description>Basics Redis is:
NoSQL KeyValue In memory Data Structure Server binary safe strings lists, sets, sorted sets, hashes bitmaps, hyperloglogs Open source Redis is:
Fast Low CPU Requirement Scalable Redis can be used as:
Cache Analytics Leaderboard Queues Cookie storage Expiring data Messaging High I/O workloads API throttlings How to persist your data
Snapshot AOF: Append Only File Pros:</description></item><item><title>Audiolization of Covid 19 Data in Europe</title><link>/blog/ruthless/audiolization-of-covid19-in-eu/</link><pubDate>Sun, 03 Jan 2021 00:00:00 +0000</pubDate><guid>/blog/ruthless/audiolization-of-covid19-in-eu/</guid><description>Here is an audiolization sound track using a sample of covid19 data in Europe. The audio is the result of the audiorepr Python package I wrote.</description></item><item><title>PREP</title><link>/cards/communication/prep/</link><pubDate>Sun, 03 Jan 2021 00:00:00 +0000</pubDate><guid>/cards/communication/prep/</guid><description>PREP PREP is a framework for making your point.
PREP: Point + Reason + Example + Point Point: Make a point; PREP is a good method. Reason: Give the reason; Because it has a clear logic. Example: Show examples; The famous XYZ did ABC then everyone was convinced. Point: State the point for a conclusion.</description></item><item><title>SCQ-A</title><link>/cards/communication/scq-a/</link><pubDate>Sun, 03 Jan 2021 00:00:00 +0000</pubDate><guid>/cards/communication/scq-a/</guid><description>SCQ-A SCQ-A: Situation + Conflict + Question + Answer SCA-A is a framework for problem-solving.
Situation: background knowledge, set the stage Complications: what is happening Question: propose your hypothesis Answer: accept or reject the hypothesis</description></item><item><title>WWH</title><link>/cards/communication/wwh/</link><pubDate>Sun, 03 Jan 2021 00:00:00 +0000</pubDate><guid>/cards/communication/wwh/</guid><description>WWH WWH: What (happened) + Why (this happened) + How (to improve)</description></item><item><title>Graph Creation</title><link>/reading/grammar-of-graphics/graph-creation/</link><pubDate>Tue, 29 Dec 2020 00:00:00 +0000</pubDate><guid>/reading/grammar-of-graphics/graph-creation/</guid><description>Stages Three stages of making a graph:
Specification Assembly Display Specification Statistical graphic specifications are expressed in six statements
DATA: a set of data operations that create variables from datasets TRANS: variable transformations (e.g., rank) SCALE: scale transformations (e.g., log) COORD: a coordinate system (e.g., polar) ELEMENT: graphs (e.g., points) and their aesthetic attributes (e.g., color) GUIDE: one or more guides (axes, legends, etc.) Assembly Assembling a scene from a specification requires a variety of structures in order to index and link components with each other.</description></item><item><title>Multiset, mset or bag</title><link>/cards/math/multiset-mset-bag/</link><pubDate>Sun, 27 Dec 2020 00:00:00 +0000</pubDate><guid>/cards/math/multiset-mset-bag/</guid><description>A bag is a set in which duplicate elements are allowed.
An ordered bag is a list that we use in programming.</description></item><item><title>Python Class Sequential Inheritance</title><link>/til/programming/python/python-class-inheritance-sequential/</link><pubDate>Thu, 03 Dec 2020 00:00:00 +0000</pubDate><guid>/til/programming/python/python-class-inheritance-sequential/</guid><description># An experiment on python super class Base: def __init__(self): print(&amp;#34;Start A&amp;#34;) print(&amp;#34;End A&amp;#34;) class IA(Base): def __init__(self): print(&amp;#34;Start IA&amp;#34;) super(IA, self).__init__() print(&amp;#34;End IA&amp;#34;) class IB(IA): def __init__(self): print(&amp;#34;Start IB&amp;#34;) super(IB, self).__init__() print(&amp;#34;End IB&amp;#34;) print(&amp;#34;Experiment 1:&amp;#34;) ib = IB()</description></item><item><title>Three dots in Python</title><link>/til/programming/python/python-three-dots/</link><pubDate>Thu, 03 Dec 2020 00:00:00 +0000</pubDate><guid>/til/programming/python/python-three-dots/</guid><description>Using three dots in Python:
from abc import abstractmethod class A: def __init__(self): self.name = &amp;#34;A&amp;#34; print(&amp;#34;Init&amp;#34;) def three_dots(self): ... @abstractmethod def abs_three_dots(self): ... def raise_it(self): raise Exception(&amp;#34;Not yet done&amp;#34;) a = A() print(&amp;#34;\nthree_dots&amp;#34;) print(a.three_dots()) print(&amp;#34;\nabs_three_dots&amp;#34;) print(a.abs_three_dots()) print(&amp;#34;\nraise_it&amp;#34;) a.raise_it() Returns
three_dots None abs_three_dots None raise_it Traceback (most recent call last): File &amp;quot;main.py&amp;quot;, line 27, in &amp;lt;module&amp;gt; a.raise_it() File &amp;quot;main.py&amp;quot;, line 14, in raise_it raise Exception(&amp;quot;Not yet done&amp;quot;) Exception: Not yet done</description></item><item><title>Ordered Member Functions of a Class in Python</title><link>/til/programming/python/python-class-methods-ordered/</link><pubDate>Wed, 02 Dec 2020 00:00:00 +0000</pubDate><guid>/til/programming/python/python-class-methods-ordered/</guid><description># References: # 1. https://stackoverflow.com/questions/48145317/can-i-add-attributes-to-class-methods-in-python from functools import wraps # Define a decorator def attributes(**attrs): &amp;#34;&amp;#34;&amp;#34; Set attributes of member functions in a class. ``` class AGoodClass: def __init__(self): self.size = 0 @attributes(order=1) def first_good_member(self, new): return &amp;#34;first good member&amp;#34; @attributes(order=2) def second_good_member(self, new): return &amp;#34;second good member&amp;#34; ``` References: 1. https://stackoverflow.com/a/48146924/1477359 &amp;#34;&amp;#34;&amp;#34; def decorator(f): @wraps(f) def wrapper(*args, **kwargs): return f(*args, **kwargs) for attr_name, attr_value in attrs.items(): setattr(wrapper, attr_name, attr_value) return wrapper return decorator class AGoodClass: def __init__(self): self.</description></item><item><title>Postgres Optimization in JOIN</title><link>/til/data/postgres.join-begin-with-smallest-cardinality/</link><pubDate>Sat, 28 Nov 2020 11:39:21 +0100</pubDate><guid>/til/data/postgres.join-begin-with-smallest-cardinality/</guid><description>Join tables together starting with the smallest table (table with less cardinality) speeds things up.</description></item><item><title>Deal with NULL in Postgres</title><link>/til/data/postgres.deal-with-null/</link><pubDate>Thu, 26 Nov 2020 00:00:00 +0000</pubDate><guid>/til/data/postgres.deal-with-null/</guid><description>Please deal with null carefully.</description></item><item><title>Akaike Information Criterion</title><link>/cards/statistics/aic/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>/cards/statistics/aic/</guid><description>Suppose we have a model that describes the data generation process behind a dataset. The distribution by the model is denoted as $\hat f$. The actual data generation process is described by a distribution $f$.
We ask the question:
How good is the approximation using $\hat f$?
To be more precise, how much information is lost if we use our model dist $\hat f$ to substitute the actual data generation distribution $f$?</description></item><item><title>Bayes Factors</title><link>/cards/statistics/bayes-factors/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>/cards/statistics/bayes-factors/</guid><description>$$ \frac{p(\mathscr M_1|y)}{ p(\mathscr M_2|y) } = \frac{p(\mathscr M_1)}{ p(\mathscr M_2) }\frac{p(y|\mathscr M_1)}{ p(y|\mathscr M_2) } $$ Bayes factor
$$ \mathrm{BF_{12}} = \frac{m(y|\mathscr M_1)}{m(y|\mathscr M_2)} $$ $\mathrm{BF_{12}}$: how many time more likely is model $\mathscr M_1$ than $\mathscr M_2$.</description></item><item><title>Bayesian Information Criterion</title><link>/cards/statistics/bic/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>/cards/statistics/bic/</guid><description>BIC is Bayesian information criterion, it replaced the $+2k$ term in AIC Suppose we have a model that describes the data generation process behind a dataset. The distribution by the model is denoted as $\hat f$. The actual data generation process is described by a distribution $f$. We ask the question: How good is the approximation using $\hat f$? To be more precise, how much information is lost if we use our model dist $\hat f$ to substitute the actual data generation distribution $f$?</description></item><item><title>Fisher Information Approximation</title><link>/cards/statistics/fia/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>/cards/statistics/fia/</guid><description>FIA is a method to describe the minimum description length ( MDL MDL is a measure of how well a model compresses data by minimizing the combined cost of the description of the model and the misfit. ) of models,
$$ \mathrm{FIA} = -\ln p(y | \hat\theta) + \frac{k}{2} \ln \frac{n}{2\pi} + \ln \int_\Theta \sqrt{ \operatorname{det}[I(\theta)] d\theta } $$
$I(\theta)$: Fisher information matrix of sample size 1.</description></item><item><title>Kolmogorov Complexity</title><link>/cards/statistics/kolmogorov-complexity/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>/cards/statistics/kolmogorov-complexity/</guid><description>Description of Data
The measurement of complexity is based on the observation that the compressibility of data doesn&amp;rsquo;t depend on the &amp;ldquo;language&amp;rdquo; used to describe the compression process that much. This makes it possible for us to find a universal language, such as a universal computer language, to quantify the compressibility of the data.
One intuitive idea is to use a programming language to describe the data. If we have a sequence of data,</description></item><item><title>Minimum Description Length</title><link>/cards/statistics/mdl/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>/cards/statistics/mdl/</guid><description>The minimum description length, aka, MDL, is based on the relations between regularity and data compression. (See Kolmogorov complexity Description of Data The measurement of complexity is based on the observation that the compressibility of data doesn&amp;rsquo;t depend on the &amp;ldquo;language&amp;rdquo; used to describe the compression process that much. This makes it possible for us to find a universal language, such as a universal computer language, to quantify the compressibility of the data.</description></item><item><title>Normalized Maximum Likelihood</title><link>/cards/statistics/nml/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>/cards/statistics/nml/</guid><description>$$ \mathrm{NML} = \frac{ p(y| \hat \theta(y)) }{ \int_X p( x| \hat \theta (x) ) dx } $$</description></item><item><title>Experiments in Biology</title><link>/blog/ruthless/experiments-in-biology/</link><pubDate>Sun, 01 Nov 2020 00:00:00 +0000</pubDate><guid>/blog/ruthless/experiments-in-biology/</guid><description> Inspired by @hanlu.io</description></item><item><title>The Science Part in Data Science</title><link>/blog/ruthless/science-part-in-data-science/</link><pubDate>Sat, 31 Oct 2020 00:00:00 +0000</pubDate><guid>/blog/ruthless/science-part-in-data-science/</guid><description>graph TD; s1(An Idea)--d1{Is this idea in the current literature?}; d1{Is this idea in the current literature?}--|Yes|b1(Fail); d1{Is this idea in the current literature?}--|No|b2[Weeks of work]; b2[Weeks of work]--b1(Fail);</description></item><item><title>Conditional Probability Table</title><link>/cards/statistics/conditional-probability-table/</link><pubDate>Tue, 27 Oct 2020 00:00:00 +0000</pubDate><guid>/cards/statistics/conditional-probability-table/</guid><description>The conditional probability table, aka CPT, is used to calculate conditional probabilities from a dataset.
Given a dataset with features $\mathbf X$ and their corresponding classes $\mathbf Y$, the conditional probabilities of each class given a certain feature value can be calculated using a CPT which in turn can be calculated using a contigency table Detecting correlations using correlations for numeric data .</description></item><item><title>Pandas Groupby Does Not Guarantee Unique Content in Groupby Columns</title><link>/til/machine-learning/pandas-groupby-caveats/</link><pubDate>Mon, 20 Apr 2020 00:00:00 +0000</pubDate><guid>/til/machine-learning/pandas-groupby-caveats/</guid><description>Pandas Groupby Does Not Guarantee Unique Content in Groupby Columns, it also considers the datatypes. Dealing with mixed types requires additional attention.</description></item><item><title>== and is in Python</title><link>/til/programming/python/python-none/</link><pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate><guid>/til/programming/python/python-none/</guid><description>== and is are different</description></item><item><title>Arcsine Distribution</title><link>/cards/statistics/distributions/arcsine/</link><pubDate>Sat, 14 Mar 2020 00:00:00 +0000</pubDate><guid>/cards/statistics/distributions/arcsine/</guid><description>Arcsine Distribution The PDF is
$$ \frac{1}{\pi\sqrt{x(1-x)}} $$
for $x\in [0,1]$.
It can also be generalized to
$$ \frac{1}{\pi\sqrt{(x-1)(b-x)}} $$
for $x\in [a,b]$.
Visualize</description></item><item><title>Bernoulli Distribution</title><link>/cards/statistics/distributions/bernoulli/</link><pubDate>Sat, 14 Mar 2020 00:00:00 +0000</pubDate><guid>/cards/statistics/distributions/bernoulli/</guid><description>Two categories with probability $p$ and $1-p$ respectively.
For each experiment, the sample space is $\{A, B\}$. The probability for state $A$ is given by $p$ and the probability for state $B$ is given by $1-p$. The Bernoulli distribution describes the probability of $K$ results with state $s$ being $s=A$ and $N-K$ results with state $s$ being $B$ after $N$ experiments,
$$ P\left(\sum_i^N s_i = K \right) = C _ N^K p^K (1 - p)^{N-K}.</description></item><item><title>Beta Distribution</title><link>/cards/statistics/distributions/beta/</link><pubDate>Sat, 14 Mar 2020 00:00:00 +0000</pubDate><guid>/cards/statistics/distributions/beta/</guid><description>Beta Distribution Interact {% include extras/vue.html %}
((makeGraph))</description></item><item><title>Binomial Distribution</title><link>/cards/statistics/distributions/binomial/</link><pubDate>Sat, 14 Mar 2020 00:00:00 +0000</pubDate><guid>/cards/statistics/distributions/binomial/</guid><description>The number of successes in $n$ independent events where each trial has a success rate of $p$.
PMF:
$$ C_n^k p^k (1-p)^{n-k} $$</description></item><item><title>Categorical Distribution</title><link>/cards/statistics/distributions/categorical/</link><pubDate>Sat, 14 Mar 2020 00:00:00 +0000</pubDate><guid>/cards/statistics/distributions/categorical/</guid><description>By generalizing the Bernoulli distribution to $k$ states, we get a categorical distribution. The sample space is $\{s_1, s_2, \cdots, s_k\}$. The corresponding probabilities for each state are $\{p_1, p_2, \cdots, p_k\}$ with the constraint $\sum_{i=1}^k p_i = 1$.</description></item><item><title>Cauchy-Lorentz Distribution</title><link>/cards/statistics/distributions/cauchy/</link><pubDate>Sat, 14 Mar 2020 00:00:00 +0000</pubDate><guid>/cards/statistics/distributions/cauchy/</guid><description>Cauchy-Lorentz Distribution .. ratio of two independent normally distributed random variables with mean zero.
Source: https://en.wikipedia.org/wiki/Cauchy_distribution
Lorentz distribution is frequently used in physics.
PDF:
$$ \frac{1}{\pi\gamma} \left( \frac{\gamma^2}{ (x-x_0)^2 + \gamma^2} \right) $$
The median and mode of the Cauchy-Lorentz distribution is always $x_0$. $\gamma$ is the FWHM.
Visualize</description></item><item><title>Gamma Distribution</title><link>/cards/statistics/distributions/gamma/</link><pubDate>Sat, 14 Mar 2020 00:00:00 +0000</pubDate><guid>/cards/statistics/distributions/gamma/</guid><description>Gamma Distribution PDF:
$$ \frac{\beta^\alpha x^{\alpha-1} e^{-\beta x}}{\Gamma(\alpha)} $$
Visualize</description></item><item><title>Diagnolize Matrices</title><link>/cards/math/diagonalize-matrix/</link><pubDate>Wed, 11 Mar 2020 00:00:00 +0000</pubDate><guid>/cards/math/diagonalize-matrix/</guid><description>Given a matrix $\mathbf A$, it is diagonalized using its eigenvectors.
Why are the eigenvectors needed?
Eigenvectors of a matrix $\mathbf A$ are the preferred directions. From the definition of eigenvectors,
$$ \mathbf A \mathbf x = \lambda \mathbf x, $$
we know that the matrix $\mathbf A$ only scales the eigenvectors and no rotations. These directions are special to the matrix $\mathbf A$.
Find the eigenvectors $\mathbf x_i$ of the matrix $\mathbf A$; If we find degerations, the matrix is not diagonalizable.</description></item><item><title>Mahalanobis Distance</title><link>/cards/math/mahalanobis-distance/</link><pubDate>Wed, 11 Mar 2020 00:00:00 +0000</pubDate><guid>/cards/math/mahalanobis-distance/</guid><description>Mahalanobis distance is a distance calculated using the inverse of the covariance matrix as the metric. For two vectors $\mathbf x$ and $\mathbf y$, the Mahalanobis distance is
$$ d^2 = (x_i - \bar x) g_{ij} (y_j - \bar y), $$
where $g_{ij} = (S^{-1})_{ij}$ and $\mathbf S$ is the covariance matrix.
The covariance is a normalization that mitigates the covariances.</description></item><item><title>Covariance Matrix</title><link>/cards/statistics/covariance-matrix/</link><pubDate>Tue, 10 Mar 2020 00:00:00 +0000</pubDate><guid>/cards/statistics/covariance-matrix/</guid><description>We use Einstein&amp;rsquo;s summation convention. Covariance of two discrete series $A$ and $B$ is defined as
$$ \text{Cov} ({A,B}) = \sigma_{A,B}^2 = \frac{ (a_i - \bar A) (b_i - \bar B) }{ n- 1 }, $$ where $n$ is the length of the series. The normalization factor is set to $1/(n-1)$ to mitigate the bias for small $n$.
One could show that
$$ \mathrm{Cov}({A,B}) = E( A,B ) - \bar A \bar B.</description></item><item><title>Jackknife Resampling</title><link>/cards/statistics/jacknife-resampling/</link><pubDate>Sun, 26 Jan 2020 00:00:00 +0000</pubDate><guid>/cards/statistics/jacknife-resampling/</guid><description>Jackknife resampling is a method for estimation of the mean and higher order moments.
Given a sample $\{x_i\}$ of size $n$ for the distribution $X$, the jackknife resampling estimates the mean by leaving out each data point systematically. $n$ estimations of the mean will be obtained, with each of the estimations $x_i$
$$ \bar x_i = \frac{1}{n-1} \sum_{j\neq i} x_j. $$
The mean of the sample is
$$ \bar x = \frac{1}{n}\sum_i \bar x_i = \frac{1}{n} \sum_i \left(\frac{1}{n-1} \sum_{j\neq i} x_j\right) = \frac{1}{n}\sum_i x_i.</description></item><item><title>CBOW: Continuous Bag of Words</title><link>/cards/machine-learning/embedding/continuous-bag-of-words/</link><pubDate>Thu, 16 Jan 2020 00:00:00 +0000</pubDate><guid>/cards/machine-learning/embedding/continuous-bag-of-words/</guid><description>Here we encode all words presented in the corpus to demostrate the idea of CBOW. In the real world, we might want to remove some certain words such as the. We use the following quote by Ford in Westworld as an example.
I read a theory once that the human intellect is like peacock feathers. Just an extravagant display intended to attract a mate, just an elaborate mating ritual.</description></item><item><title>Data Types</title><link>/cards/machine-learning/datatypes/data-types/</link><pubDate>Thu, 16 Jan 2020 00:00:00 +0000</pubDate><guid>/cards/machine-learning/datatypes/data-types/</guid><description/></item><item><title>Gini Impurity</title><link>/cards/machine-learning/measurement/gini-impurity/</link><pubDate>Thu, 16 Jan 2020 00:00:00 +0000</pubDate><guid>/cards/machine-learning/measurement/gini-impurity/</guid><description>The code used in this article can be found in this repo. Suppose we have a dataset $\{0,1\}^{10}$, which has 10 records and 2 possible classes of objects $\{0,1\}$ in each record.
The first example we investigate is a pure 0 dataset.
object 0 0 0 0 0 0 0 0 0 0 0 0 For such an all-0 dataset, we would like to define its impurity as 0.</description></item><item><title>Information Gain</title><link>/cards/machine-learning/measurement/information-gain/</link><pubDate>Thu, 16 Jan 2020 00:00:00 +0000</pubDate><guid>/cards/machine-learning/measurement/information-gain/</guid><description>Information gain is a frequently used metric in calculating the gain during a split in tree-based methods.
First o all, the entropy of a dataset if defined as
$$ S = - sum_i p_i \log p_i - sum_i (1-p_i)\log p_i, $$ where $p_i$ is the probability of a class.
The information gain is the difference between the entropy.
For example, in a decision tree algorithm, we would split a node. Before splitting, we assign a label $m$ to the node,</description></item><item><title>Negative Sampling</title><link>/cards/machine-learning/embedding/negative-sampling/</link><pubDate>Thu, 16 Jan 2020 00:00:00 +0000</pubDate><guid>/cards/machine-learning/embedding/negative-sampling/</guid><description>Knowledge of CBOW or skipgram is required.
A naive model to train a model of words is to
encode input words and output words using vectors, use the input word vector to predict the output word vector, calculate the errors between predicted output word vector and real output word vector, minimize the errors. However, it is very expensive to prject out the output words and calcualte the error eveytime.</description></item><item><title>PAC: Probably Approximately Correct</title><link>/cards/machine-learning/learning-theories/pac/</link><pubDate>Thu, 16 Jan 2020 00:00:00 +0000</pubDate><guid>/cards/machine-learning/learning-theories/pac/</guid><description/></item><item><title>skipgram: Continuous skip-gram</title><link>/cards/machine-learning/embedding/continuous-skip-gram/</link><pubDate>Thu, 16 Jan 2020 00:00:00 +0000</pubDate><guid>/cards/machine-learning/embedding/continuous-skip-gram/</guid><description>We use the following quote by Ford in Westworld as an example.
I read a theory once that the human intellect is like peacock feathers. Just an extravagant display intended to attract a mate, just an elaborate mating ritual. But, of course, the peacock can barely fly. It lives in the dirt, pecking insects out of the muck, consoling itself with its great beauty.
The word intended is surrunded by extravagant display in the front and to attract after it.</description></item><item><title>Improving Document Ranking with Dual Word Embeddings</title><link>/reading/word2vec-in-out-embedding/</link><pubDate>Sat, 05 Oct 2019 00:00:00 +0000</pubDate><guid>/reading/word2vec-in-out-embedding/</guid><description>Word2vec produces two embedding spaces, the in-embedding and out-embedding.</description></item><item><title>Switch statement in Python</title><link>/til/programming/python/python-switch-statement/</link><pubDate>Tue, 20 Aug 2019 00:00:00 +0000</pubDate><guid>/til/programming/python/python-switch-statement/</guid><description>Love switch statement? We can design a switch statement it in python.</description></item><item><title>Python Tilde Operator</title><link>/til/programming/python/python-tilde-operator/</link><pubDate>Thu, 15 Aug 2019 00:00:00 +0000</pubDate><guid>/til/programming/python/python-tilde-operator/</guid><description>tilde operator may not work as you expected</description></item><item><title>Arrays and Dicts in MongoDB</title><link>/til/programming/database/mongodb-array-and-dict/</link><pubDate>Wed, 14 Aug 2019 00:00:00 +0000</pubDate><guid>/til/programming/database/mongodb-array-and-dict/</guid><description>Array of dictionaries becomes hard to update in MongoDB.</description></item><item><title>eval in Python is Dangerous</title><link>/til/programming/python/python-eval/</link><pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate><guid>/til/programming/python/python-eval/</guid><description>eval is powerful but really dangerous</description></item><item><title>Kendall Tau Correlation</title><link>/cards/statistics/kendall-correlation-coefficient/</link><pubDate>Sat, 20 Jul 2019 00:00:00 +0000</pubDate><guid>/cards/statistics/kendall-correlation-coefficient/</guid><description>Definition two series of data: $X$ and $Y$ cooccurance of them: $(x_i, x_j)$, and we assume that $i&amp;lt;j$ concordant: $x_i &amp;lt; x_j$ and $y_i &amp;lt; y_j$; $x_i &amp;gt; x_j$ and $y_i &amp;gt; y_j$; denoted as $C$ discordant: $x_i &amp;lt; x_j$ and $y_i &amp;gt; y_j$; $x_i &amp;gt; x_j$ and $y_i &amp;lt; y_j$; denoted as $D$ neither concordant nor discordant: whenever equal sign happens Kendall&amp;rsquo;s tau is defined as
$$ \begin{equation} \tau = \frac{C- D}{\text{all possible pairs of comparison}} = \frac{C- D}{n^2/2 - n/2} \end{equation} $$</description></item><item><title>Bayes' Theorem</title><link>/cards/statistics/bayes-theorem/</link><pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate><guid>/cards/statistics/bayes-theorem/</guid><description>Bayes&amp;rsquo; Theorem is stated as
$$ P(A\mid B) = \frac{P(B \mid A) P(A)}{P(B)} $$
$P(A\mid B)$: likelihood of A given B $P(A)$: marginal probability of A There is a nice tree diagram for the Bayes&amp;rsquo; theorem on Wikipedia.
Tree diagram of Bayes&amp;rsquo; theorem</description></item><item><title>Canonical Decomposition</title><link>/cards/math/canonical-decomposition/</link><pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate><guid>/cards/math/canonical-decomposition/</guid><description>I find this slide from Christoph Freudenthaler very useful.
Canonical decomposition visualized by Christoph Freudenthaler</description></item><item><title>Khatri-Rao Product</title><link>/cards/math/khatri-rao/</link><pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate><guid>/cards/math/khatri-rao/</guid><description>$$
\mathbf{A} \ast \mathbf{B} = \left(\mathbf{A}_{ij} \otimes \mathbf{B}_{ij}\right)_{ij}
$$</description></item><item><title>Modes and Slices of Tensors</title><link>/cards/math/modes-and-slices-of-tensor/</link><pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate><guid>/cards/math/modes-and-slices-of-tensor/</guid><description> Modes of a tensor Slices of a tensor</description></item><item><title>Poisson Process</title><link>/cards/statistics/poisson-process/</link><pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate><guid>/cards/statistics/poisson-process/</guid><description/></item><item><title>SVD: Singular Value Decomposition</title><link>/cards/math/svd/</link><pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate><guid>/cards/math/svd/</guid><description>Given a matrix $\mathbf X \to X_{m}^{\phantom{m}n}$, we can decompose it into three matrices
$$ X_{m}^{\phantom{m}n} = U_{m}^{\phantom{m}k} D_{k}^{\phantom{k}l} (V_{n}^{\phantom{n}l} )^{\mathrm T}, $$
where $D_{k}^{\phantom{k}l}$ is diagonal.
Here we have $\mathbf U$ being constructed by the eigenvectors of $\mathbf X \mathbf X^{\mathrm T}$, while $\mathbf V$ is being constructed by the eigenvectors of $\mathbf X^{\mathrm T} \mathbf X$ (which is also the reason we keep the transpose).
I find this slide from Christoph Freudenthaler very useful.</description></item><item><title>Tucker Decomposition</title><link>/cards/math/tucker-decomposition/</link><pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate><guid>/cards/math/tucker-decomposition/</guid><description>I find this slide from Christoph Freudenthaler very useful. For the definition of mode 1/2/3 unfold, please refer to Modes and Slices of Tensors.
Tucker decomposition visualized by Christoph Freudenthaler</description></item><item><title>Levenshtein Distance</title><link>/cards/math/levenshtein-distance/</link><pubDate>Sun, 19 May 2019 00:00:00 +0000</pubDate><guid>/cards/math/levenshtein-distance/</guid><description>Levenshtein distance calculates the number of operations needed to change one word to another by applying single-character edits (insertions, deletions or substitutions).
The reference explains this concept very well. For consistency, I extracted a paragraph from it which explains the operations in Levenshtein algorithm. The source of the following paragraph is the first reference of this article.
Levenshtein Matrix
Cell (0:1) contains red number 1. It means that we need 1 operation to transform M to an empty string.</description></item><item><title>n-gram</title><link>/cards/math/n-gram/</link><pubDate>Sun, 19 May 2019 00:00:00 +0000</pubDate><guid>/cards/math/n-gram/</guid><description>n-gram is a method to split words into set of substring elements so that those can be used to match words.
Examples Use the following examples to get your first idea about it. I created two columns so that we could compare the n-grams of two different words side-by-side.
n in n-gram is Word One Clean Word: (( sentenceOneWords )) n-grams: (( sentenceOneWordsnGram )) Word Two Clean Word: (( sentenceTwoWords )) n-grams: (( sentenceTwoWordsnGram )) /*************************/ /** The function nGram is a copy of https://github.</description></item><item><title>Add New Kernels to Jupyter Notebook in Conda Environment</title><link>/til/programming/jupyter-notebook-add-new-kernels-in-conda-env/</link><pubDate>Sun, 12 May 2019 00:00:00 +0000</pubDate><guid>/til/programming/jupyter-notebook-add-new-kernels-in-conda-env/</guid><description>Python package or python module autoreloading in jupyter notebook</description></item><item><title>Auto-reload Python Packages or Python Modules in Jupyter Notebook</title><link>/til/programming/jupyter-notebook-autoreload-python-modules-or-packages/</link><pubDate>Sun, 12 May 2019 00:00:00 +0000</pubDate><guid>/til/programming/jupyter-notebook-autoreload-python-modules-or-packages/</guid><description>Python package or python module autoreloading in jupyter notebook</description></item><item><title>BigQuery Meta Tables</title><link>/til/data/bigquery-meta-tables/</link><pubDate>Sun, 12 May 2019 00:00:00 +0000</pubDate><guid>/til/data/bigquery-meta-tables/</guid><description>Meta tables are very useful when it comes to get bigquery table information programmatically.</description></item><item><title>Calculate Moving Average Using SQL/BigQquery</title><link>/til/data/bigquery-moving-average/</link><pubDate>Sun, 12 May 2019 00:00:00 +0000</pubDate><guid>/til/data/bigquery-moving-average/</guid><description>Snippet for calculating moving avg using sql/biguqery</description></item><item><title>Generate a Column of Continuous Dates in BigQuery</title><link>/til/data/bigquery-generate-continuous-dates-as-a-column/</link><pubDate>Sun, 12 May 2019 00:00:00 +0000</pubDate><guid>/til/data/bigquery-generate-continuous-dates-as-a-column/</guid><description>Generate a table with a column of continuous dates</description></item><item><title>Get Current User in BigQuery</title><link>/til/data/bigquery-get-current-user/</link><pubDate>Sun, 12 May 2019 00:00:00 +0000</pubDate><guid>/til/data/bigquery-get-current-user/</guid><description>BigQuery Current User</description></item><item><title>Materialize the Query Result for Performance</title><link>/til/data/bigquery-materialize-query-results-for-performance/</link><pubDate>Sun, 12 May 2019 00:00:00 +0000</pubDate><guid>/til/data/bigquery-materialize-query-results-for-performance/</guid><description>Materialize the query result for multistage queries to make your query faster and lower the costs.</description></item><item><title>Cosine Similarity</title><link>/cards/math/cosine-similarity/</link><pubDate>Mon, 06 May 2019 00:00:00 +0000</pubDate><guid>/cards/math/cosine-similarity/</guid><description>As simple as the inner product of two vectors
$$ d_{cos} = \frac{\vec A}{\vert \vec A \vert} \cdot \frac{\vec B }{ \vert \vec B \vert} $$
Examples To use cosine similarity, we have to vectorize the words first. There are many different methods to achieve this. For the purpose of illustrating cosine similarity, we use term frequency.
Term frequency is the occurrence of the words. We do not deal with duplications so duplicate words will have some effect on the similarity.</description></item><item><title>Eigenvalues and Eigenvectors</title><link>/cards/math/eigendecomposition/</link><pubDate>Mon, 06 May 2019 00:00:00 +0000</pubDate><guid>/cards/math/eigendecomposition/</guid><description>To find the eigenvectors $\mathbf x$ of a matrix $\mathbf A$, we construct the eigen equation
$$ \mathbf A \mathbf x = \lambda \mathbf x, $$
where $\lambda$ is the eigenvalue.
We rewrite it in the components form,
$$ \begin{equation} A_{ij} x_j = \lambda x_i. \label{eqn-eigen-decomp-def} \end{equation} $$
Mathematically speaking, it is straightforward to find the eigenvectors and eigenvalues.
Eigenvectors are Special Directions Judging from the definition in Eq.($\ref{eqn-eigen-decomp-def}$), the eigenvectors do not change direction under the operation of the matrix $\mathbf A$.</description></item><item><title>Jaccard Similarity</title><link>/cards/math/jaccard-similarity/</link><pubDate>Mon, 06 May 2019 00:00:00 +0000</pubDate><guid>/cards/math/jaccard-similarity/</guid><description>Jaccard index is the ratio of the size of the intersect of the set and the size of the union of the set.
$$ J(A, B) = \frac{ \vert A \cap B \vert }{ \vert A \cup B \vert } $$
Jaccard distance $d_J(A,B)$ is defined as
$$ d_J(A,B) = 1 - J(A,B). $$
Properties If the two sets are the same, $A=B$, we have $J(A,B)=1$ or $d_J(A,B)=0$. We have maximum similarity.</description></item><item><title>Term Frequency - Inverse Document Frequency</title><link>/cards/math/tf-idf/</link><pubDate>Mon, 06 May 2019 00:00:00 +0000</pubDate><guid>/cards/math/tf-idf/</guid><description/></item><item><title>The Art of Data Science</title><link>/reading/art-of-data-science/</link><pubDate>Fri, 19 Apr 2019 00:00:00 +0000</pubDate><guid>/reading/art-of-data-science/</guid><description>A nice and elegant book on data science</description></item><item><title>Awesome Stuff</title><link>/projects/awesome/</link><pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate><guid>/projects/awesome/</guid><description>Summarizations, workflows, experiences, fails, etc</description></item><item><title>Blog Posts</title><link>/projects/blog/</link><pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate><guid>/projects/blog/</guid><description>My blog posts for fun.</description></item><item><title>Combinations</title><link>/cards/math/combinations/</link><pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate><guid>/cards/math/combinations/</guid><description>Choose X from N is
$$ C_N^X = \frac{N!}{ X! (N-X)! } $$</description></item><item><title>My Data Wiki</title><link>/projects/wiki/</link><pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate><guid>/projects/wiki/</guid><description>A collection of my wiki articles related to data.</description></item><item><title>My Knowledge Cards</title><link>/projects/cards/</link><pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate><guid>/projects/cards/</guid><description>A collection of my snippets of knowledge</description></item><item><title>My Reading Notes</title><link>/projects/reading/</link><pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate><guid>/projects/reading/</guid><description>A collection of my reading notes</description></item><item><title>TIL</title><link>/projects/til/</link><pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate><guid>/projects/til/</guid><description>Today I Learned</description></item><item><title>Human Graphical Perception of Quantitative Information in Data Visualization</title><link>/reading/graphical-perception/</link><pubDate>Sun, 17 Mar 2019 00:00:00 +0000</pubDate><guid>/reading/graphical-perception/</guid><description>Data visualization caveats</description></item><item><title>Add Data Files to Python Package</title><link>/til/programming/python/python-package-including-data-file/</link><pubDate>Wed, 13 Mar 2019 00:00:00 +0000</pubDate><guid>/til/programming/python/python-package-including-data-file/</guid><description>Add Data Files to Python Package using manifest.in and setup.py</description></item><item><title>Installing requirements.txt in Conda Environments</title><link>/til/programming/python/python-anaconda-install-requirements/</link><pubDate>Wed, 13 Mar 2019 00:00:00 +0000</pubDate><guid>/til/programming/python/python-anaconda-install-requirements/</guid><description>Why is pip install -r requirements.txt not working?</description></item><item><title>Information Theory and Statistical Mechanics</title><link>/reading/statistical-physics-and-information-theory/</link><pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate><guid>/reading/statistical-physics-and-information-theory/</guid><description>Max entropy principle as a method to infer distributions of statistical systems</description></item><item><title>Flatten 2D List in Python</title><link>/til/programming/python/python-flatten-2d-list/</link><pubDate>Wed, 23 Jan 2019 00:00:00 +0000</pubDate><guid>/til/programming/python/python-flatten-2d-list/</guid><description>Flatten 2D list using sum</description></item><item><title>Python Datetime on Different OS</title><link>/til/programming/python/python-datetime-on-different-os/</link><pubDate>Mon, 31 Dec 2018 00:00:00 +0000</pubDate><guid>/til/programming/python/python-datetime-on-different-os/</guid><description>Python datetime on different os behaves inconsistently</description></item><item><title>Python If on Numbers</title><link>/til/programming/python/python-if-condition-on-numbers/</link><pubDate>Mon, 31 Dec 2018 00:00:00 +0000</pubDate><guid>/til/programming/python/python-if-condition-on-numbers/</guid><description>If on int is dangerous</description></item><item><title>Python Long String</title><link>/til/programming/python/python-long-string/</link><pubDate>Mon, 31 Dec 2018 00:00:00 +0000</pubDate><guid>/til/programming/python/python-long-string/</guid><description>Python long string formatting</description></item><item><title>Python Reliable Path to File</title><link>/til/programming/python/python-reliable-path/</link><pubDate>Mon, 31 Dec 2018 00:00:00 +0000</pubDate><guid>/til/programming/python/python-reliable-path/</guid><description>Find the actual path to file</description></item><item><title>VSCode on Mac Long Press Keys Not Repeating</title><link>/til/misc/vscode-on-mac-do-not-repeat/</link><pubDate>Mon, 31 Dec 2018 00:00:00 +0000</pubDate><guid>/til/misc/vscode-on-mac-do-not-repeat/</guid><description>Enable your key repeat in vscode on mac</description></item><item><title>Controlled Experiments</title><link>/til/statistics/controlled-experiments/</link><pubDate>Tue, 04 Dec 2018 00:00:00 +0000</pubDate><guid>/til/statistics/controlled-experiments/</guid><description>The three levels of controlled experiments</description></item><item><title>Schaum's Outline of Theories and Problems of Elements of Statistics I and II</title><link>/reading/elements-of-statistics/</link><pubDate>Thu, 01 Nov 2018 00:00:00 +0000</pubDate><guid>/reading/elements-of-statistics/</guid><description>The basics and all of modern statistics</description></item><item><title>Pandas with MultiProcessing</title><link>/til/programming/pandas/pandas-parallel-multiprocessing/</link><pubDate>Sun, 09 Sep 2018 00:00:00 +0000</pubDate><guid>/til/programming/pandas/pandas-parallel-multiprocessing/</guid><description>Define number of processes, prs; Split dataframe into prs dataframes; Process each dataframe with one process; Merge processed dataframes into one. A piece of demo code is shown below.
from multiprocessing import Pool from multiprocessing.dummy import Pool as ThreadPool import pandas as pd # Create a dataframe to be processed df = pd.read_csv(&amp;#39;somedata.csv&amp;#39;).reset_index(drop=True) # Define a function to be applied to the dataframe def nice_func(name, age): return (name,age) # Apply to dataframe def apply_to_df(df_chunks): df_chunks[&amp;#39;tupled&amp;#39;] = df_chunks.</description></item><item><title>Beer and Life Expectancy</title><link>/blog/ruthless/beer-and-life-expectancy/</link><pubDate>Wed, 08 Aug 2018 00:00:00 +0000</pubDate><guid>/blog/ruthless/beer-and-life-expectancy/</guid><description>This is a post of no analysis at all. Everything in this post is meant for fun.
I moved to Germany a few weeks ago and one of the most astonishing things I noticed is that everyone is drinking so much. Yet the life expectance of Germany is pretty high. So I performed this &amp;ldquo;analysis&amp;rdquo; for fun.
Life expectancy vs beer consumption (L) per capita per year. Data obtained from wikipediaList of countries by life expectancy and List of countries by beer consumption per capita.</description></item><item><title>Data Mining: Concepts and Techniques</title><link>/reading/data-mining/</link><pubDate>Wed, 01 Aug 2018 00:00:00 +0000</pubDate><guid>/reading/data-mining/</guid><description>How data mining was done in the past</description></item><item><title>Fitt's Law</title><link>/til/misc/fitts-law/</link><pubDate>Sun, 22 Jul 2018 00:00:00 +0000</pubDate><guid>/til/misc/fitts-law/</guid><description>How fast can you move your mouse to target</description></item><item><title>Copy Scalars and Lists in Python</title><link>/til/programming/python/python-copy-value-or-address/</link><pubDate>Tue, 03 Jul 2018 00:00:00 +0000</pubDate><guid>/til/programming/python/python-copy-value-or-address/</guid><description>Python copy values of scalars but addresses of lists</description></item><item><title>Certificate Errors in urllib</title><link>/til/data/python-urllib-ssl/</link><pubDate>Mon, 25 Jun 2018 00:00:00 +0000</pubDate><guid>/til/data/python-urllib-ssl/</guid><description>Dealing with errors when scraping data</description></item><item><title>Calculated Columns in Pandas</title><link>/til/programming/pandas/pandas-new-column-from-other/</link><pubDate>Sun, 20 May 2018 00:00:00 +0000</pubDate><guid>/til/programming/pandas/pandas-new-column-from-other/</guid><description>Create new columns in pandas</description></item><item><title>tree in Linux</title><link>/til/programming/trees/</link><pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate><guid>/til/programming/trees/</guid><description>Trees in computer science</description></item><item><title>Heap on Mac and Linux</title><link>/til/programming/cpp/cpp-heap-mac-linux-diff/</link><pubDate>Tue, 26 Sep 2017 00:00:00 +0000</pubDate><guid>/til/programming/cpp/cpp-heap-mac-linux-diff/</guid><description>Some caveats about heap on mac and linux</description></item><item><title>C++ int Multiplication</title><link>/til/programming/cpp/cpp-int-multiply/</link><pubDate>Thu, 21 Sep 2017 00:00:00 +0000</pubDate><guid>/til/programming/cpp/cpp-int-multiply/</guid><description>int multiplication in C++ should be processed with caution.</description></item><item><title>CMake Usage</title><link>/til/programming/cmake-usage/</link><pubDate>Thu, 21 Sep 2017 00:00:00 +0000</pubDate><guid>/til/programming/cmake-usage/</guid><description>How to use CMake to generate makefiles</description></item><item><title>Allocating Memory for Multidimensional Array in C++</title><link>/til/programming/cpp/cpp-allocating-memory-multidimensional-array/</link><pubDate>Thu, 14 Sep 2017 00:00:00 +0000</pubDate><guid>/til/programming/cpp/cpp-allocating-memory-multidimensional-array/</guid><description>Some caveats</description></item><item><title>C++ range-for-statement</title><link>/til/programming/cpp/cpp-range-for-statement/</link><pubDate>Tue, 12 Sep 2017 00:00:00 +0000</pubDate><guid>/til/programming/cpp/cpp-range-for-statement/</guid><description>In C++ we can use range-for-statement</description></item><item><title>List All Folders in Linux or Mac</title><link>/til/programming/linux-mac-list-all-folders/</link><pubDate>Tue, 01 Aug 2017 00:00:00 +0000</pubDate><guid>/til/programming/linux-mac-list-all-folders/</guid><description>Using ls and tree commands to list folders only</description></item><item><title>Python Default Parameters Tripped Me Up</title><link>/til/programming/python/python-default-parameters-mutable/</link><pubDate>Sat, 03 Jun 2017 00:00:00 +0000</pubDate><guid>/til/programming/python/python-default-parameters-mutable/</guid><description>Python default parameters might be changed with each run</description></item><item><title>Some Tests on Matplotlib Backends</title><link>/til/programming/matplotlib-backend/</link><pubDate>Tue, 23 May 2017 00:00:00 +0000</pubDate><guid>/til/programming/matplotlib-backend/</guid><description>Matplotlib provides many different backends</description></item><item><title>Mathematica Provides Great PlotTheme Options</title><link>/til/programming/mathematica/mathematica-plottheme/</link><pubDate>Fri, 19 May 2017 00:00:00 +0000</pubDate><guid>/til/programming/mathematica/mathematica-plottheme/</guid><description>Amazingly, Mathematica provides an option for plot that automatically generates beautiful plots.</description></item><item><title>Turn a Series Expansion into Function in Mathematica</title><link>/til/programming/mathematica/mathematica-turn-series-into-function/</link><pubDate>Mon, 15 May 2017 00:00:00 +0000</pubDate><guid>/til/programming/mathematica/mathematica-turn-series-into-function/</guid><description>Turn a series expansion in Mathematica into a function</description></item><item><title>Git Asks for Password Whenever I Pull or Push</title><link>/til/programming/git/git-ssh-asking-pwd-everytime/</link><pubDate>Thu, 11 May 2017 00:00:00 +0000</pubDate><guid>/til/programming/git/git-ssh-asking-pwd-everytime/</guid><description>My git asks for password every time I pull or push even with ssh configured.</description></item><item><title>Command Line Russian Roulette</title><link>/til/programming/command-line-russian-roulette/</link><pubDate>Tue, 09 May 2017 00:00:00 +0000</pubDate><guid>/til/programming/command-line-russian-roulette/</guid><description>Play russian roulette in your command line</description></item><item><title>GNU Screen Key Conflict with Bash</title><link>/til/programming/gnu-screen-key-conflict-with-bash/</link><pubDate>Mon, 08 May 2017 00:00:00 +0000</pubDate><guid>/til/programming/gnu-screen-key-conflict-with-bash/</guid><description>GNU screen key conflict with bash can be solved</description></item><item><title>How to Run Mathematica Script in Terminal</title><link>/til/programming/run-mathematica-script-in-terminal/</link><pubDate>Mon, 08 May 2017 00:00:00 +0000</pubDate><guid>/til/programming/run-mathematica-script-in-terminal/</guid><description>Using math -run or wolfram -run we could execute a Mathematica script through ssh in terminal.</description></item><item><title>GNUPLOT Inline Output in iterm2</title><link>/til/programming/gnuplot-iterm2-imgcat/</link><pubDate>Fri, 07 Apr 2017 00:00:00 +0000</pubDate><guid>/til/programming/gnuplot-iterm2-imgcat/</guid><description>Using gnuplot in iterm2 we can output result inside terminal combined with imgcat</description></item><item><title>Mathematica Exclude Singularities in Plot</title><link>/til/programming/mathematica/mathematica-plot-exclude-singularities/</link><pubDate>Wed, 22 Mar 2017 00:00:00 +0000</pubDate><guid>/til/programming/mathematica/mathematica-plot-exclude-singularities/</guid><description>Mathematica Plot might include some non-existant lines sometimes, Exclusions is the potion for it.</description></item><item><title>Passing Function Arguments Through Lists in Mathematica</title><link>/til/programming/mathematica/mathematica-passing-arguments-through-lists/</link><pubDate>Mon, 20 Feb 2017 00:00:00 +0000</pubDate><guid>/til/programming/mathematica/mathematica-passing-arguments-through-lists/</guid><description>We can pass a list of arguments using Sequence</description></item><item><title>Git Pull with Submodule</title><link>/til/programming/git/git-pull-with-submodule/</link><pubDate>Fri, 03 Feb 2017 00:00:00 +0000</pubDate><guid>/til/programming/git/git-pull-with-submodule/</guid><description>Pull git repo with submodule</description></item><item><title>Positioning textblock in LaTeX Beamer</title><link>/til/programming/latex-beamer-textblock-position/</link><pubDate>Tue, 17 Jan 2017 00:00:00 +0000</pubDate><guid>/til/programming/latex-beamer-textblock-position/</guid><description>Positioning textblock in LaTeX Beamer using textpos package and eso pic package</description></item><item><title>Mathematica Different Output Forms</title><link>/til/programming/mathematica/mathematica-different-output-forms/</link><pubDate>Mon, 28 Nov 2016 00:00:00 +0000</pubDate><guid>/til/programming/mathematica/mathematica-different-output-forms/</guid><description>Mathematica has many different output forms. Understanding them is extremely helpful when making plots.</description></item><item><title>Git Branch Options</title><link>/til/programming/git/git-branch-details/</link><pubDate>Sun, 27 Nov 2016 00:00:00 +0000</pubDate><guid>/til/programming/git/git-branch-details/</guid><description>Some useful options about git branch</description></item><item><title>git pull multi remote</title><link>/til/programming/git/git-pull-multi-remote/</link><pubDate>Tue, 22 Nov 2016 00:00:00 +0000</pubDate><guid>/til/programming/git/git-pull-multi-remote/</guid><description>working with multi remote</description></item><item><title>Popularity versus similarity in growing networks</title><link>/reading/popularity-vs-similarity/</link><pubDate>Sun, 06 Nov 2016 00:00:00 +0000</pubDate><guid>/reading/popularity-vs-similarity/</guid><description>Introduce geometry into the manifold of complex networks</description></item><item><title>Formatting Numbers in Python</title><link>/til/programming/formating-numbers-python/</link><pubDate>Tue, 11 Oct 2016 00:00:00 +0000</pubDate><guid>/til/programming/formating-numbers-python/</guid><description>Formatting numbers in python using format</description></item><item><title>Solving Equations Using Differential Transformation Method</title><link>/til/math/differential-transformation-method-solving-equations/</link><pubDate>Tue, 11 Oct 2016 00:00:00 +0000</pubDate><guid>/til/math/differential-transformation-method-solving-equations/</guid><description>Differential transformation method can be used to solve differential equation even integro-differential equations.</description></item><item><title>The Great Chrome Dev Tool</title><link>/til/programming/chrome-dev-tool-usage/</link><pubDate>Wed, 28 Sep 2016 00:00:00 +0000</pubDate><guid>/til/programming/chrome-dev-tool-usage/</guid><description>How to use the chrome dev tool wisely</description></item><item><title>Start a Simple Server</title><link>/til/programming/start-simple-server/</link><pubDate>Sat, 17 Sep 2016 00:00:00 +0000</pubDate><guid>/til/programming/start-simple-server/</guid><description>With one line of python command</description></item><item><title>matplotlib x y limit and aspect ratio</title><link>/til/programming/matplotlib-x-y-limit-and-aspect-ratio/</link><pubDate>Thu, 21 Jul 2016 00:00:00 +0000</pubDate><guid>/til/programming/matplotlib-x-y-limit-and-aspect-ratio/</guid><description>matplotlib x y limit and aspect ratio</description></item><item><title>TOP Command</title><link>/til/programming/top/</link><pubDate>Thu, 21 Jul 2016 00:00:00 +0000</pubDate><guid>/til/programming/top/</guid><description>Some tips about top command</description></item><item><title>Assigning Values to Multiple Variables</title><link>/til/programming/python/python-assigning-values-to-multiple-variables/</link><pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate><guid>/til/programming/python/python-assigning-values-to-multiple-variables/</guid><description>Assigning Values to Multiple Variables</description></item><item><title>gitignore by file size</title><link>/til/programming/git/gitignore-by-file-size/</link><pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate><guid>/til/programming/git/gitignore-by-file-size/</guid><description>gitignore by file size</description></item><item><title>HTML Animations Using CSS: AnimateCSS</title><link>/til/programming/html-animate-css/</link><pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate><guid>/til/programming/html-animate-css/</guid><description>HTML Animations Using CSS AnimateCSS</description></item><item><title>Import in Python</title><link>/til/programming/import-in-python/</link><pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate><guid>/til/programming/import-in-python/</guid><description>Import in Python</description></item><item><title>IPython or Jupyter Notebook Magics</title><link>/til/programming/ipython-or-jupyter-notebook-magics/</link><pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate><guid>/til/programming/ipython-or-jupyter-notebook-magics/</guid><description>IPython or Jupyter Notebook Magics</description></item><item><title>LaTeX Automatically Adjust Figure</title><link>/til/programming/latex-automatically-adjust-figure/</link><pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate><guid>/til/programming/latex-automatically-adjust-figure/</guid><description>LaTeX Automatically Adjust Figure</description></item><item><title>Mathematica Plot Default Font Style and Ticks Style: BaseStyle</title><link>/til/programming/mathematica/mathematica-plot-basestyle-default-font-style-and-ticks-style/</link><pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate><guid>/til/programming/mathematica/mathematica-plot-basestyle-default-font-style-and-ticks-style/</guid><description>Mathematica Plot Default Font Style and Ticks Style BaseStyle</description></item><item><title>Mathematica Smooth Plot</title><link>/til/programming/mathematica/mathematica-smooth-plot/</link><pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate><guid>/til/programming/mathematica/mathematica-smooth-plot/</guid><description>Mathematica Smooth Plot</description></item><item><title>Migrating Wordpress to Static</title><link>/til/programming/migrating-wordpress-to-static-site/</link><pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate><guid>/til/programming/migrating-wordpress-to-static-site/</guid><description>Migrating Wordpress to Static</description></item><item><title>Open URL using python using webbrowser module</title><link>/til/programming/open-url-using-python-webbrowser-module/</link><pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate><guid>/til/programming/open-url-using-python-webbrowser-module/</guid><description>Open URL using python using webbrowser module</description></item><item><title>Python Code Style</title><link>/til/programming/python/python-code-style/</link><pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate><guid>/til/programming/python/python-code-style/</guid><description>Code Style of Python Guide.
PEP 20 &amp;ndash; The Zen of Python
1. Beautiful is better than ugly. 2. Explicit is better than implicit. 3. Simple is better than complex. 4. Complex is better than complicated. 5. Flat is better than nested. 6. Sparse is better than dense. 7. Readability counts. 8. Special cases aren&amp;#39;t special enough to break the rules. 9. Although practicality beats purity. 10. Errors should never pass silently.</description></item><item><title>Python Creating Lists</title><link>/til/programming/python/python-creating-lists/</link><pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate><guid>/til/programming/python/python-creating-lists/</guid><description>Code Style of Python Guide</description></item><item><title>Python enumertate</title><link>/til/programming/python/python-enumerate/</link><pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate><guid>/til/programming/python/python-enumerate/</guid><description>Python enumertate function</description></item><item><title>Python List Comprehensions</title><link>/til/programming/python/python-list-comprehensions/</link><pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate><guid>/til/programming/python/python-list-comprehensions/</guid><description>Python List Comprehensions</description></item><item><title>Python Making a List</title><link>/til/programming/python/python-making-a-list/</link><pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate><guid>/til/programming/python/python-making-a-list/</guid><description>Python Making a List</description></item><item><title>Python Map vs For in Python</title><link>/til/programming/python/python-map-vs-for/</link><pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate><guid>/til/programming/python/python-map-vs-for/</guid><description>Python Map vs For in Python</description></item><item><title>Python Onliner: Filter Prime Numbers</title><link>/til/programming/filter-prime-numbers/</link><pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate><guid>/til/programming/filter-prime-numbers/</guid><description>Python Onliner Filter Prime Numbers</description></item><item><title>Python Stupid numpy.piecewise</title><link>/til/programming/python/python-stupid-numpy-piecewise/</link><pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate><guid>/til/programming/python/python-stupid-numpy-piecewise/</guid><description>Python Stupid numpy.piecewise</description></item><item><title>Python Various Ways of Writing Loops</title><link>/til/programming/python/python-writing-loops/</link><pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate><guid>/til/programming/python/python-writing-loops/</guid><description>Python Various Ways of Writing Loops</description></item><item><title>Run a program in the background on ubuntu</title><link>/til/programming/run-program-in-background-ubuntu/</link><pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate><guid>/til/programming/run-program-in-background-ubuntu/</guid><description>Run a program in the background on ubuntu</description></item><item><title>snakeviz</title><link>/til/programming/python/python-profile-snakeviz/</link><pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate><guid>/til/programming/python/python-profile-snakeviz/</guid><description>Python snakeviz</description></item><item><title>Area Enclosed by a Line</title><link>/til/math/area-enclosed-in-a-line/</link><pubDate>Sun, 15 Feb 2015 00:00:00 +0000</pubDate><guid>/til/math/area-enclosed-in-a-line/</guid><description>Calculate the area enclosed by a line</description></item><item><title>Eigensystem of A Special Matrix</title><link>/til/math/eigensystem-of-a-special-matrix/</link><pubDate>Sun, 15 Feb 2015 00:00:00 +0000</pubDate><guid>/til/math/eigensystem-of-a-special-matrix/</guid><description>Eigenstates of a very special matrix</description></item><item><title>Feynman Trick</title><link>/til/math/feynman-tricks/</link><pubDate>Sun, 15 Feb 2015 00:00:00 +0000</pubDate><guid>/til/math/feynman-tricks/</guid><description>An identity about integral</description></item><item><title>Symmetry of second derivatives</title><link>/til/math/symmetry-of-second-derivatives/</link><pubDate>Sun, 15 Feb 2015 00:00:00 +0000</pubDate><guid>/til/math/symmetry-of-second-derivatives/</guid><description>Symmetry of second derivatives</description></item><item><title/><link>/wiki/dynamical-system/integration-of-ode/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/wiki/dynamical-system/integration-of-ode/</guid><description/></item><item><title/><link>/wiki/survival-analysis/survival-probability/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/wiki/survival-analysis/survival-probability/</guid><description/></item><item><title>About</title><link>/about/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/about/</guid><description>Datumorphism is my notebook about programming, data scraping, statistics, machine learning, and data visualization.
Join our Enki Team Learn, practice, and play together. Programmers are unstoppable. Intelligence Notebook Notes about neuroscience, machine intelligence, and collective intelligence. Lei Ma Visit this page if you would like to know more about me.</description></item><item><title>Cheatsheets</title><link>/awesome/cheatsheets/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/awesome/cheatsheets/</guid><description> Supervised Learning k-Nearest Neighbors [Supervised Learning Classification ] : Linear Regression [Supervised Learning Regression ] : Lasso [Supervised Learning Regression Regularization ] : Ridge [Supervised Learning Regression Regularization ] : ElasticNet [Supervised Learning Regression Regularization ] : Unsupervised Learning k-Means [Unsupervised Learning ] : t-SNE [Unsupervised Learning ] : PCA [Unsupervised Learning Dimension Reduction Feature Selection ] : NMF [Unsupervised Learning ] : Non-negative Matrix Factoring</description></item><item><title>Curriculum</title><link>/awesome/curriculum/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/awesome/curriculum/</guid><description>Prerequisites Programming Python C++ alternatives: name: R name: Matlab Computer Science These theories make people think faster. They don&amp;rsquo;t pose direct limits on what data scientists can do but they will definitely give data scientists a boost.
Data Structures Complexity Math Some basic understanding of these is absolutely required. Higher levels of these topics will also be listed in details.
Statistics Linear Algebra Calculus Differential Equations EDA Tools These tools are used almost everywhere in data science.</description></item><item><title>Gridlines in Matplotlib</title><link>/til/programming/matplotlib-gridlines/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/til/programming/matplotlib-gridlines/</guid><description>Adding gridlines in matplotlib</description></item><item><title>Researchers</title><link>/awesome/researchers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/awesome/researchers/</guid><description> Machine Learning Geoffrey Hinton [machine learning psychology artificial intelligence cognitive science computer science ] : Emeritus Prof. Comp Sci, U.Toronto &amp;amp; Engineering Fellow, Google</description></item><item><title>Tools</title><link>/awesome/tools/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/awesome/tools/</guid><description>List of Tools Dashboard ReDash [Python ] : Superset [Python ] : Metabase [Java ] : Google Data Studio [Free Google BigQuery Cloud ] : Google Datastudio is a convinent tool to produce simple yet massive dashboards for the team. Design and Build a Data Warehouse for Business [Courses Warehouse Business ] : Explained Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM) [LSTM RNN ] : JavaScript replacements for Python data science tools [JavaScript Tools Data Science ] : https://github.</description></item><item><title>Typography of this Website</title><link>/typography/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/typography/</guid><description>Basic Syntax This website uses kramdown as the basic syntax. However, a lot of html/css/js has been applied to generate some certain contents or styles.
Math also follows the kramdown syntax.
Notes div {% highlight html %}
Figure with Caption {% highlight html %}
![]({{ site.url }}/assets/programming/chrome-dev-tools-inspect.png) where {{ site.url }} is the configured url of the site.
Alternatively, we can use the set attributes syntax in kramdown.
{% highlight md %} This is a paragraph with some class.</description></item><item><title>Workflows</title><link>/awesome/workflows/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/awesome/workflows/</guid><description>The scope of exploratory data analysis is not universally defined. Some of the contents discussed here may have crossed the line. The whole modeling process is never decoupled anyway. Data wrangling is mostly guided by the exploratory data analysis (EDA). In other words, the data cleaning process should be mostly guided by questions from business and stakeholder or out of curiosity.
There are three key components in EDA.</description></item></channel></rss>