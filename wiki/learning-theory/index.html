<!doctype html><html lang=en-us>
<head>
<meta name=generator content="Hugo 0.89.4">
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1">
<title>Learning Theory | Datumorphism | L Ma </title>
<meta name=description content="Root of learning">
<meta name=robots content="noindex">
<meta name=author content="L Ma">
<meta property="og:title" content="Learning Theory">
<meta property="og:description" content="Root of learning">
<meta property="og:type" content="website">
<meta property="og:url" content="https://datumorphism.leima.is/wiki/learning-theory/">
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga('create','UA-140452515-1','auto'),ga('send','pageview'))</script>
<script async src=https://www.google-analytics.com/analytics.js></script>
<link href=https://datumorphism.leima.is/wiki/learning-theory/index.xml rel=alternate type=application/rss+xml title=Datumorphism>
<link rel=canonical href=https://datumorphism.leima.is/wiki/learning-theory/> <link rel="shortcut icon" type=image/png href=/logos/logo-square.png>
<link rel=stylesheet href=/css/bulma.css>
<link rel=stylesheet href=/css/bulma-divider.min.css>
<link rel=stylesheet href=/assets/css/bulma-ribbon.min.css>
<link rel=stylesheet href=/assets/css/tooltip.css>
<link rel=stylesheet href=https://jenil.github.io/bulmaswatch/united/bulmaswatch.min.css>
<link rel=stylesheet href=/css/custom.css>
<script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],tags:'ams',processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:['script','noscript','style','textarea','pre']},svg:{fontCache:'global'}},window.addEventListener('load',a=>{document.querySelectorAll("mjx-container").forEach(function(a){a.parentElement.classList+='has-jax'})})</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<script src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script>
<script>mermaid.initialize({startOnLoad:!0})</script>
<link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css integrity="sha512-9usAa10IRO0HhonpyAIVpjrylPvoDwiPUiKdWk5t3PyolY1cOd4DSE0Ga+ri4AuTroPR5aQvXU9xC6qOPnzFeg==" crossorigin=anonymous referrerpolicy=no-referrer>
<script src=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/js/all.min.js integrity="sha512-yFjZbTYRCJodnuyGlsKamNE/LlEaEAxSUDe5+u61mV8zzqJVFOH7TnULE2/PP/l5vKWpUNnF4VGVkXh3MjgLsg==" crossorigin=anonymous referrerpolicy=no-referrer></script>
</head>
<body>
<header> <nav class="navbar is-transparent">
<div class=navbar-brand>
<a class=navbar-item href=/>
<img src=/site/intelligence.png alt=Datumorphism height=28 style=margin-right:.5em> Datumorphism
</a>
<div class="navbar-burger burger" style=color:#000 data-target=navMenu>
<span></span>
<span></span>
<span></span>
</div>
</div>
<div class=navbar-menu id=navMenu>
<div class=navbar-start>
<div class="navbar-item has-dropdown is-hoverable">
<a href=/projects class=navbar-link>
Notebooks
</a>
<div class=navbar-dropdown>
<a href=https://datumorphism.leima.is/awesome/ class=navbar-item>
Awesome
</a>
<a href=https://datumorphism.leima.is/blog/ class=navbar-item>
Blog
</a>
<a href=https://datumorphism.leima.is/cards/ class=navbar-item>
Cards
</a>
<a href=https://datumorphism.leima.is/hologram/ class=navbar-item>
Hologram
</a>
<a href=https://datumorphism.leima.is/reading/ class=navbar-item>
Reading Notes
</a>
<a href=https://datumorphism.leima.is/til/ class=navbar-item>
TIL
</a>
<a href=https://datumorphism.leima.is/wiki/ class=navbar-item>
Wiki
</a>
</div>
</div>
<div class="navbar-item has-dropdown is-hoverable">
<a class=navbar-item href=https://neuronstar.kausalflow.com/cpe-docs/>
Probability Estimation
</a>
</div>
</div>
<span class=navbar-burger>
<span></span>
<span></span>
<span></span>
</span>
<div class=navbar-end>
<div class=navbar-item>
<a class=navbar-item href=/blog/>
Blog
</a>
<a class=navbar-item href=/amneumarkt/>
AmNeumarkt
</a>
<a class=navbar-item href=/>
<i class="fas fa-search"></i>
</a>
<a class=navbar-item href=/tags/>
<i class="fas fa-tags"></i>
</a>
<a class=navbar-item href=/graph>
<i class="fas fa-project-diagram"></i>
</a>
<a class=navbar-item href=https://t.me/amneumarkt>
<i class="fab fa-telegram"></i>
</a>
<a class=navbar-item target=blank href=https://github.com/datumorphism>
<span class=icon>
<i class="fab fa-github"></i>
</span>
</a>
</div>
</div>
</div>
</nav>
<script>document.addEventListener('DOMContentLoaded',()=>{const a=Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'),0);a.length>0&&a.forEach(a=>{a.addEventListener('click',()=>{const b=a.dataset.target,c=document.getElementById(b);a.classList.toggle('is-active'),c.classList.toggle('is-active')})})})</script> </header>
<main>
<section>
<div class="hero is-primary is-medium">
<div class=hero-body>
<div class="container has-text-centered">
<h1 class="title is-1">
Learning Theory
</h1>
<h2 class="subtitle is-3">
Root of learning
</h2>
</div>
</div>
</div>
</section>
<div class="columns is-fullheight">
<div class=column>
<section class="hero is-default is-bold">
<div class=hero-body>
<div class=container>
<nav class="breadcrumb has-succeeds-separator is-small" aria-label=breadcrumbs>
<ul>
<li>
<a href=https://datumorphism.leima.is/>Datumorphism</a>
</li>
<li>
<a href=https://datumorphism.leima.is/wiki/>Wiki</a>
</li>
<li class=active>
<a href=https://datumorphism.leima.is/wiki/learning-theory/>Learning Theory</a>
</li>
</ul>
</nav>
<div class="columns is-multiline is-variable is-1-mobile is-0-tablet is-3-desktop is-8-widescreen is-2-fullhd is-desktop">
<div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork>
<h1 class=blog-timestamp>
</h1>
<h1 class=title>
<sup>3</sup> <a href=https://datumorphism.leima.is/wiki/learning-theory/vc-dimension/ itemprop=headline> VC Dimension</a>
</h1>
<div style=margin-left:2em>
<span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>
Published: <time datetime=2021-02-21T00:00:00+00:00>2021-02-21</time>
</span>
</div>
<div style=margin-left:2em>
<span style=display:block;font-size:90%><i class="far fa-folder" aria-hidden=true></i> Category: { Learning Theory }
</span>
</div>
<div style=margin-left:2em>
<span style=display:block;font-size:90%><i class="fa fa-hashtag" aria-hidden=true></i> Tags: <div class="tags is-small" style=display:inline>
<a href=/tags/learning-theory style=margin-right:.5em><span class="tag is-warning is-small is-light">#Learning Theory</span></a>
<a href=/tags/basics style=margin-right:.5em><span class="tag is-warning is-small is-light">#Basics</span></a>
</div>
</span>
</div>
<div style=margin-left:2em><span style=display:block;font-size:90%>
<i class="fa fa-paperclip" aria-hidden=true></i> References:</span>
<span style=display:block;font-size:90%;padding-left:1em> - <a href=https://doi.org/10.1007/978-1-4757-3264-1>Vapnik, V. N. (2000). The Nature of Statistical Learning Theory. Springer New York. </a></span>
<span style=display:block;font-size:90%;padding-left:1em> - <a href=https://www.mathematik.uni-muenchen.de/~deckert/teaching/SS17/ATML/>Deckert D-A. Advanced Topics in Machine Learning. In: Advanced Topics in Machine Learning [Internet]. Apr 2017 [cited 17 Oct 2021]. Available: https://www.mathematik.uni-muenchen.de/~deckert/teaching/SS17/ATML/</a></span>
<span style=display:block;font-size:90%;padding-left:1em> - <a href=http://arxiv.org/abs/1611.03530>Zhang C, Bengio S, Hardt M, Recht B, Vinyals O. Understanding deep learning requires rethinking generalization. arXiv [cs.LG]. 2016. Available: http://arxiv.org/abs/1611.03530</a></span>
<span style=display:block;font-size:90%;padding-left:1em> - <a href=https://jeremybernste.in/writing/ml-is-just-statistics>Bernstein J. Machine learning is just statistics + quantifier reversal. In: jeremybernste [Internet]. [cited 1 Nov 2021]. Available: https://jeremybernste.in/writing/ml-is-just-statistics</a></span>
<span style=display:block;font-size:90%;padding-left:1em> - <a href=http://arxiv.org/abs/1901.05353>Guedj B. A Primer on PAC-Bayesian Learning. arXiv [stat.ML]. 2019. Available: http://arxiv.org/abs/1901.05353</a></span>
<span style=display:block;font-size:90%;padding-left:1em> - <a href=https://www.semanticscholar.org/paper/Learning-From-Data-Abu-Mostafa-Magdon-Ismail/1c0ed9ed3201ef381cc392fc3ca91cae6ecfc698>Abu-Mostafa, Yaser S and Magdon-Ismail, Malik and Lin, Hsuan-Tien. Learning from Data. AMLBook; 2012. Available: https://www.semanticscholar.org/paper/Learning-From-Data-Abu-Mostafa-Magdon-Ismail/1c0ed9ed3201ef381cc392fc3ca91cae6ecfc698</a></span>
</div>
<div style=margin-left:2em>
<span style=display:block;font-size:90%>
<i class="far fa-star"></i> Summary: Two of the key elements in a learning problem are:
a set of hypothesis $\mathcal H$, and a set of data samples $\mathcal S$. $\mathcal H$
Inside $\mathcal H$, we have a lot of hypotheses, for example, $\mathcal h$. Given some input, e.g., $x_1$ and $x_2$, we can produce some outputs, e.g., $h(x_1)$ and $h(x_2)$. $\mathcal S$
A sample $\mathcal S$ is a fair sample drawn from all the possible inputs $\mathcal X$, where $\mathcal X$ is called the input space. A dataset $\mathcal S$ can be used as a probe of the hypothesis set $\mathcal H$.</span>
</div>
<div style=margin-left:2em>
<span style=display:block;font-size:90%>
<i class="far fa-list-alt"></i> Pages: 2</span>
</div>
</div>
<div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div>
<div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork>
<h1 class=blog-timestamp>
</h1>
<h1 class=title>
<sup>1</sup> <a href=https://datumorphism.leima.is/wiki/learning-theory/feasibility-of-learning/ itemprop=headline> Feasibility of Learning</a>
</h1>
<div style=margin-left:2em>
<span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>
Published: <time datetime=2021-10-17T00:00:00+00:00>2021-10-17</time>
</span>
</div>
<div style=margin-left:2em>
<span style=display:block;font-size:90%><i class="far fa-folder" aria-hidden=true></i> Category: { Learning Theory }
</span>
</div>
<div style=margin-left:2em>
<span style=display:block;font-size:90%><i class="fa fa-hashtag" aria-hidden=true></i> Tags: <div class="tags is-small" style=display:inline>
<a href=/tags/learning-theory style=margin-right:.5em><span class="tag is-warning is-small is-light">#Learning Theory</span></a>
<a href=/tags/basics style=margin-right:.5em><span class="tag is-warning is-small is-light">#Basics</span></a>
</div>
</span>
</div>
<div style=margin-left:2em><span style=display:block;font-size:90%>
<i class="fa fa-paperclip" aria-hidden=true></i> References:</span>
<span style=display:block;font-size:90%;padding-left:1em> - <a href=https://www.semanticscholar.org/paper/Learning-From-Data-Abu-Mostafa-Magdon-Ismail/1c0ed9ed3201ef381cc392fc3ca91cae6ecfc698>Abu-Mostafa, Yaser S and Magdon-Ismail, Malik and Lin, Hsuan-Tien. Learning from Data. AMLBook; 2012. Available: https://www.semanticscholar.org/paper/Learning-From-Data-Abu-Mostafa-Magdon-Ismail/1c0ed9ed3201ef381cc392fc3ca91cae6ecfc698</a></span>
<span style=display:block;font-size:90%;padding-left:1em> - <a href=https://jeremybernste.in/writing/ml-is-just-statistics>Bernstein J. Machine learning is just statistics + quantifier reversal. In: jeremybernste [Internet]. [cited 1 Nov 2021]. Available: https://jeremybernste.in/writing/ml-is-just-statistics</a></span>
<span style=display:block;font-size:90%;padding-left:1em> - <a href=http://arxiv.org/abs/1901.05353>Guedj B. A Primer on PAC-Bayesian Learning. arXiv [stat.ML]. 2019. Available: http://arxiv.org/abs/1901.05353</a></span>
<span style=display:block;font-size:90%;padding-left:1em> - <a href=http://arxiv.org/abs/1611.03530>Zhang C, Bengio S, Hardt M, Recht B, Vinyals O. Understanding deep learning requires rethinking generalization. arXiv [cs.LG]. 2016. Available: http://arxiv.org/abs/1611.03530</a></span>
</div>
<div style=margin-left:2em>
<span style=display:block;font-size:90%>
<i class="far fa-star"></i> Summary: Why is learning from data even possible? To discuss this problem, we need a framework for learning. Operationally, we can think of learning as the following framework1.
Abu-Mostafa2012
Naive View Naively speaking, a model should have two key properties,
enough capacity to hold the necessary information embedded in the data, and a method to find the combination of parameters so that the model can generate/complete new data. Most neural networks have enough capacity to hold the necessary information in the data2. The problem is, the capacity is so large. Why does backprop even work? How did backprop find a suitable set of parameters that can generalize?</span>
</div>
<div style=margin-left:2em>
<span style=display:block;font-size:90%>
<i class="far fa-list-alt"></i> Pages: 2</span>
</div>
</div>
<div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div>
</div>
</div>
<br>
<div class=container>
</div>
</div>
</section>
</div>
</div>
</main>
<footer>
<footer class=footer>
<div class=container>
<div class="content has-text-centered">
<p>
Created and maintained by <a href=https://leima.is>L Ma</a>.
Acknowledgement: <a href=https://gohugo.io/>Hugo</a>,
<a href=https://themes.gohugo.io/bulma/>Bulma</a>, <a href=https://kausalflow.com>KausalFlow</a>.
<strong>love</strong>.
<br>
<a class=tag href=/about>About</a>
<a class=tag href=/index.xml>Feed</a>
<a class=tag href=/data.json>JSON Data</a>
</p>
</div>
</div>
</footer>
</footer>
<script async type=text/javascript src=/js/bulma.js></script>
</body>
</html>