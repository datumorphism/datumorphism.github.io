<!doctype html><html lang=en-us>
<head>
<meta name=generator content="Hugo 0.89.4">
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1">
<title> VC Dimension | Datumorphism | L Ma </title>
<meta name=author content="L Ma">
<meta property="og:title" content="VC Dimension">
<meta property="og:description" content="Two of the key elements in a learning problem are:
 a set of hypothesis $\mathcal H$, and a set of data samples $\mathcal S$.  $\mathcal H$
 Inside $\mathcal H$, we have a lot of hypotheses, for example, $\mathcal h$. Given some input, e.g., $x_1$ and $x_2$, we can produce some outputs, e.g., $h(x_1)$ and $h(x_2)$.   $\mathcal S$
 A sample $\mathcal S$ is a fair sample drawn from all the possible inputs $\mathcal X$, where $\mathcal X$ is called the input space.   A dataset $\mathcal S$ can be used as a probe of the hypothesis set $\mathcal H$.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://datumorphism.leima.is/wiki/learning-theory/vc-dimension/"><meta property="article:section" content="wiki">
<meta property="article:published_time" content="2021-02-21T00:00:00+00:00">
<meta property="article:modified_time" content="2021-02-21T00:00:00+00:00">
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga('create','UA-140452515-1','auto'),ga('send','pageview'))</script>
<script async src=https://www.google-analytics.com/analytics.js></script>
<link rel=canonical href=https://datumorphism.leima.is/wiki/learning-theory/vc-dimension/> <link rel="shortcut icon" type=image/png href=/logos/logo-square.png>
<link rel=stylesheet href=/css/bulma.css>
<link rel=stylesheet href=/css/bulma-divider.min.css>
<link rel=stylesheet href=/assets/css/bulma-ribbon.min.css>
<link rel=stylesheet href=/assets/css/tooltip.css>
<link rel=stylesheet href=https://jenil.github.io/bulmaswatch/united/bulmaswatch.min.css>
<link rel=stylesheet href=/css/custom.css>
<link rel=stylesheet href=/css/blog-post.css>
<link rel=stylesheet href=/css/code-highlighting/dark.css>
<link rel=stylesheet href=/css/custom.css>
<script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],tags:'ams',processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:['script','noscript','style','textarea','pre']},svg:{fontCache:'global'}},window.addEventListener('load',a=>{document.querySelectorAll("mjx-container").forEach(function(a){a.parentElement.classList+='has-jax'})})</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<script src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script>
<script>mermaid.initialize({startOnLoad:!0})</script>
<link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css integrity="sha512-9usAa10IRO0HhonpyAIVpjrylPvoDwiPUiKdWk5t3PyolY1cOd4DSE0Ga+ri4AuTroPR5aQvXU9xC6qOPnzFeg==" crossorigin=anonymous referrerpolicy=no-referrer>
<script src=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/js/all.min.js integrity="sha512-yFjZbTYRCJodnuyGlsKamNE/LlEaEAxSUDe5+u61mV8zzqJVFOH7TnULE2/PP/l5vKWpUNnF4VGVkXh3MjgLsg==" crossorigin=anonymous referrerpolicy=no-referrer></script>
</head>
<body>
<header> <nav class="navbar is-transparent">
<div class=navbar-brand>
<a class=navbar-item href=/>
<img src=/site/intelligence.png alt=Datumorphism height=28 style=margin-right:.5em> Datumorphism
</a>
<div class="navbar-burger burger" style=color:#000 data-target=navMenu>
<span></span>
<span></span>
<span></span>
</div>
</div>
<div class=navbar-menu id=navMenu>
<div class=navbar-start>
<div class="navbar-item has-dropdown is-hoverable">
<a href=/projects class=navbar-link>
Notebooks
</a>
<div class=navbar-dropdown>
<a href=https://datumorphism.leima.is/awesome/ class=navbar-item>
Awesome
</a>
<a href=https://datumorphism.leima.is/blog/ class=navbar-item>
Blog
</a>
<a href=https://datumorphism.leima.is/cards/ class=navbar-item>
Cards
</a>
<a href=https://datumorphism.leima.is/hologram/ class=navbar-item>
Hologram
</a>
<a href=https://datumorphism.leima.is/reading/ class=navbar-item>
Reading Notes
</a>
<a href=https://datumorphism.leima.is/til/ class=navbar-item>
TIL
</a>
<a href=https://datumorphism.leima.is/wiki/ class=navbar-item>
Wiki
</a>
</div>
</div>
<div class="navbar-item has-dropdown is-hoverable">
<a class=navbar-item href=https://neuronstar.kausalflow.com/cpe-docs/>
Probability Estimation
</a>
</div>
</div>
<span class=navbar-burger>
<span></span>
<span></span>
<span></span>
</span>
<div class=navbar-end>
<div class=navbar-item>
<a class=navbar-item href=/blog/>
Blog
</a>
<a class=navbar-item href=/amneumarkt/>
AmNeumarkt
</a>
<a class=navbar-item href=/>
<i class="fas fa-search"></i>
</a>
<a class=navbar-item href=/tags/>
<i class="fas fa-tags"></i>
</a>
<a class=navbar-item href=/graph>
<i class="fas fa-project-diagram"></i>
</a>
<a class=navbar-item href=https://t.me/amneumarkt>
<i class="fab fa-telegram"></i>
</a>
<a class=navbar-item target=blank href=https://github.com/datumorphism>
<span class=icon>
<i class="fab fa-github"></i>
</span>
</a>
</div>
</div>
</div>
</nav>
<script>document.addEventListener('DOMContentLoaded',()=>{const a=Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'),0);a.length>0&&a.forEach(a=>{a.addEventListener('click',()=>{const b=a.dataset.target,c=document.getElementById(b);a.classList.toggle('is-active'),c.classList.toggle('is-active')})})})</script>
</header>
<main>
<div class=container itemscope itemtype=http://schema.org/BlogPosting>
<meta itemprop=name content="VC Dimension">
<meta itemprop=description content="Two of the key elements in a learning problem are:
 a set of hypothesis $\mathcal H$, and a set of data samples $\mathcal S$.  $\mathcal H$
 Inside $\mathcal H$, we have a lot of hypotheses, for example, $\mathcal h$. Given some input, e.g., $x_1$ and $x_2$, we can produce some outputs, e.g., $h(x_1)$ and $h(x_2)$.   $\mathcal S$
 A sample $\mathcal S$ is a fair sample drawn from all the possible inputs $\mathcal X$, where $\mathcal X$ is called the input space.   A dataset $\mathcal S$ can be used as a probe of the hypothesis set $\mathcal H$."><meta itemprop=datePublished content="2021-02-21T00:00:00+00:00">
<meta itemprop=dateModified content="2021-02-21T00:00:00+00:00">
<meta itemprop=wordCount content="787">
<meta itemprop=keywords content="Learning Theory,Basics,">
<section class=section>
<div class=container>
<article class=post>
<header class=post-header>
<nav class="breadcrumb has-succeeds-separator is-small" aria-label=breadcrumbs>
<ul>
<li>
<a href=https://datumorphism.leima.is/>Datumorphism</a>
</li>
<li>
<a href=https://datumorphism.leima.is/wiki/>Wiki</a>
</li>
<li>
<a href=https://datumorphism.leima.is/wiki/learning-theory/>Learning Theory</a>
</li>
<li class=active>
<a href=https://datumorphism.leima.is/wiki/learning-theory/vc-dimension/>VC Dimension</a>
</li>
</ul>
</nav>
<h1 class="post-title has-text-centered is-size-1" itemprop="name headline">
VC Dimension
</h1>
<h2 class="title is-6 has-text-centered">
<i class="fas fa-tags" style=margin-right:.5em></i>
<a href=/tags/learning-theory><span class="tag is-warning is-small is-light">#Learning Theory</span></a>
<a href=/tags/basics><span class="tag is-warning is-small is-light">#Basics</span></a>
</h2>
</header>
<div class=columns>
<div class="column is-8">
<div class=is-divider data-content=ARTICLE></div>
<div class="content blog-post section" itemprop=articleBody>
<p>Two of the key elements in a learning problem are:</p>
<ul>
<li>a set of hypothesis $\mathcal H$, and</li>
<li>a set of data samples $\mathcal S$.</li>
</ul>
<article class="message is-info is-light">
<div class=message-header>
<p>$\mathcal H$</p>
</div>
<div class=message-body>
Inside $\mathcal H$, we have a lot of hypotheses, for example, $\mathcal h$. Given some input, e.g., $x_1$ and $x_2$, we can produce some outputs, e.g., $h(x_1)$ and $h(x_2)$.
</div>
</article>
<article class="message is-info is-light">
<div class=message-header>
<p>$\mathcal S$</p>
</div>
<div class=message-body>
A sample $\mathcal S$ is a fair sample drawn from all the possible inputs $\mathcal X$, where $\mathcal X$ is called the <strong>input space</strong>.
</div>
</article>
<p>A dataset $\mathcal S$ can be used as a probe of the hypothesis set $\mathcal H$. $\mathcal S$ can be used to probe the learning capacity of $\mathcal H$.</p>
<p>Following this idea, the VC dimension of $\mathcal H$ is defined as the largest dataset size $\operatorname{max}(\lvert \mathcal S \rvert)$ that $\mathcal H$ can
<span class=tooltip><a href=https://datumorphism.leima.is/cards/machine-learning/learning-theories/set-shatter/>
[[shatter]]
</a>
<span class=tooltiptext>
<span class=tooltip_title>Shatter</span>
<span class=tooltip_content>Given a set $\mathcal S$, and a class (collection of sets) $\mathcal H$.
For any subset of $\mathcal S$, denoted as $\mathcal s$, if we have an element of class $\mathcal H$, denoted as $\mathcal h$, that leads to1
$$ \mathcal h \cap \mathcal S = \mathcal s. $$
Since the power set of $\mathcal S$ ($P(\mathcal S)$) contains all the possible subsets of $\mathcal S$, we can also rephrase the concept using power set. If we can find the power set $P(\mathcal S)$ by looking into intersections of …</span>
</span>
</span>
. There are some simple examples on wikipedia: <a href=https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_dimension#Examples>Vapnik–Chervonenkis dimension#Examples</a>.</p>
<h2 id=growth-function-of-mathcal-h>Growth Function of $\mathcal H$</h2>
<p>Given a input space, there exists good $\mathcal H_G$ and bad $\mathcal H_B$.</p>
<ul>
<li>$\mathcal H_G$ is able to distinguish the input using the outputs.</li>
<li>$\mathcal H_B$ produces extremely similar outputs for inputs and makes it hard to distinguish the inputs using the outputs.</li>
</ul>
<p>In a <strong>binary classification</strong> problem, it is easier to understand the goodness of $\mathcal H$. The combinations of outputs when feeding data samples to $\mathcal H$ are called dichotomies, as the elements of $\mathcal H$ are being splitted into two parts based on the <strong>binary classes</strong>. It can be denoted as $\mathcal H(x_1, x_2, \cdots, x_m)$.</p>
<p>In such binary classification problems, dichotomies can be used to understand how good is the hypothesis set $\mathcal H$ on an input space $\mathcal X$.</p>
<p>If we see <strong>more dichotomies</strong>, i.e., larger $\lvert\mathcal H(x_1, x_2, \cdots, x_m)\vert$, over the input space $\mathcal X$, we can think of $\mathcal H$ being more powerful over the input space $\mathcal X$.</p>
<p>For example, if we have $2^m$ dichotomies, that means we can use $\mathcal H$ to distinguish all the $m$ data records since the max possible combinations of labels for these $m$ data records is $2^m$.</p>
<p>The growth function is defined as the maximum $\mathcal H(x_1, x_2, \cdots, x_m)$, if we search through all possible $x_1, x_2, \cdots, x_m$ in the input space $\mathcal X$<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>,</p>
<p>$$
\Pi_{\mathcal H}(m) = \operatorname{max}_{x_1, x_2, \cdots, x_m \in \mathcal X} \lvert \mathcal H(x_1, x_2, \cdots, x_m) \rvert
$$</p>
<h2 id=vc-dimension>VC Dimension</h2>
<p>For binary classification problems, $\Pi_{\mathcal H}(m) \leq 2^m$. For small $m$, it is easier to have $\Pi_{\mathcal H}(m) = 2^m$. It is probably also easy for $m=2$. As $m$ becomes larger, not every $\mathcal H$ can reach $\Pi_{\mathcal H}(m) = 2^m$. At some point, $m=m_t$, we will start to have $\Pi_{\mathcal H}(m) &lt; 2^m$. The VC dimension $d_{\text{VC}}(\mathcal H)$ is this $m_t$,</p>
<p>$$
d_{\text{VC}}(\mathcal H) = m_t.
$$</p>
<p>For some $\mathcal H$, we have $\Pi_{\mathcal H}(m) = 2^m$ for all $m$. These hypothesis sets have $d_{\text{VC}}(\mathcal H) = \infty$.</p>
<h2 id=why-do-we-care-about-the-vc-dimension>Why Do We Care about the VC Dimension</h2>
<p>Why is learning from data feasible? A learning process is the selection of good hypotheses from $\mathcal H$ using a data sample $\mathcal S$.</p>
<p>Given a training data sample $\mathcal S$, we train the model and calculate the in-sample error of the model, $E_{s}$. However, the in-sample error is not necessarily the same as the actual error, or the generalization error/out of sample error, $E_{g}$.</p>
<p>Using the Hoeffding&rsquo;s inequality, one could derive the so called learning bound<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>,</p>
<p>$$
P\left( E_g(h) - E_s(h) \leq \sqrt{ \frac{\log\lvert H\rvert + \log(2/\delta)}{2m} } \right) \geq 1 - \delta, \forall h\in \mathcal H,
$$</p>
<p>where $\mathcal H$ is a finite hypothesis set.</p>
<p>We immediately realize that if $\lvert\mathcal H\rvert \to \infty$, the bound becomes infinity too. The above theorem becomes useless.</p>
<p>Since the VC dimension describes the number of useful hypotheses, we might want to replace the actual hypothesis set size $\lvert \mathcal H\rvert$. This is not exactly correct but we indeed have a similar form</p>
<p>$$
P\left( E_g(h) - E_s(h) \leq \sqrt{ \frac{8d\log(m/d) + 8\log(4/\delta)}{m} } \right) \geq 1 - \delta,
$$</p>
<p>for a binary classification problem.</p>
<section class=footnotes role=doc-endnotes>
<hr>
<ol>
<li id=fn:1 role=doc-endnote>
<p>
<a name=Deckert2017 href=#Deckert2017 style=text-decoration:none>
<span class="tag is-link is-light">Deckert2017</span>
</a>
<a href=https://www.mathematik.uni-muenchen.de/~deckert/teaching/SS17/ATML/ style=text-decoration:none>Deckert D-A. Advanced Topics in Machine Learning. In: Advanced Topics in Machine Learning [Internet]. Apr 2017 [cited 17 Oct 2021]. Available: https://www.mathematik.uni-muenchen.de/~deckert/teaching/SS17/ATML/</a>
&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p>
</li>
</ol>
</section>
</div>
<p><div class="has-text-right is-size-7">
<span class=icon>
<i class="fas fa-pencil-alt"></i>
</span>
Published: <time datetime=2021-02-21T00:00:00+00:00>2021-02-21</time>
by <span itemprop=author>L Ma</span>;
</div></p>
<div class=is-divider></div>
<nav class="pagination is-centered" role=navigation aria-label=pagination>
<a href="https://datumorphism.leima.is/wiki/learning-theory/feasibility-of-learning/?ref=footer" class=pagination-previous>« Feasibility of Learning</a>
</nav>
</div>
<div class="column is-4">
<div class=is-divider data-content="Cite Me"></div>
<div class="box is-size-7 has-text-white has-background-black">
<article class=media>
<div class=media-content>
<div class=content>
<p>
L Ma
(2021). 'VC Dimension', Datumorphism, 02 April. Available at: https://datumorphism.leima.is/wiki/learning-theory/vc-dimension/.
</p>
</div>
</div>
</article>
</div>
<style>#TableOfContents>ul{list-style-type:lower-greek;padding-left:0}#TableOfContents>ul>li ul{list-style-type:none;padding-left:1em}</style>
<div class=is-divider data-content=ToC></div>
<div class="box is-size-7">
<article class=media>
<div class=media-content>
<div class=content>
<details>
<summary><i class="fa-solid fa-list-ol"></i> Table of Contents</summary>
<div>
<div>
<nav id=TableOfContents>
<ul>
<li><a href=#growth-function-of-mathcal-h>Growth Function of $\mathcal H$</a></li>
<li><a href=#vc-dimension>VC Dimension</a></li>
<li><a href=#why-do-we-care-about-the-vc-dimension>Why Do We Care about the VC Dimension</a></li>
</ul>
</nav>
</div>
</div>
</details>
</div>
</div>
</article>
</div>
<script>const el=document.querySelector('details summary');el.onclick=()=>{(function(f,c,d,e,a,b){a=c.createElement(d),b=c.getElementsByTagName(d)[0],a.async=1,a.src=e,b.parentNode.insertBefore(a,b)})(window,document,'script','/js/smoothscroll.js'),el.onclick=null},document.querySelectorAll('#TableOfContents a').forEach(a=>{a.addEventListener('click',()=>{document.querySelector(a.href.slice(a.href.indexOf('#'))).scrollIntoView({behavior:'smooth'})})})</script>
<div class=is-divider data-content=REFERENCES></div>
<div class="box is-size-7">
<article class=media>
<div class=media-content style=width:100%;word-break:break-word>
<div class=content>
<p>
<strong><i class="fa-solid fa-quote-left"></i> References: </strong><br>
<ol>
<li class=has-text-weight-bold>
<a name=Vapnik2000 href=#Vapnik2000 style=text-decoration:none>
<span class="tag is-link is-light">Vapnik2000</span>
</a>
<a href=https://doi.org/10.1007/978-1-4757-3264-1 style=text-decoration:none>Vapnik, V. N. (2000). The Nature of Statistical Learning Theory. Springer New York. </a></li>
<li class=has-text-weight-bold>
<a name=Deckert2017 href=#Deckert2017 style=text-decoration:none>
<span class="tag is-link is-light">Deckert2017</span>
</a>
<a href=https://www.mathematik.uni-muenchen.de/~deckert/teaching/SS17/ATML/ style=text-decoration:none>Deckert D-A. Advanced Topics in Machine Learning. In: Advanced Topics in Machine Learning [Internet]. Apr 2017 [cited 17 Oct 2021]. Available: https://www.mathematik.uni-muenchen.de/~deckert/teaching/SS17/ATML/</a></li>
<li class=has-text-weight-bold>
<a name=Zhang2016 href=#Zhang2016 style=text-decoration:none>
<span class="tag is-link is-light">Zhang2016</span>
</a>
<a href=http://arxiv.org/abs/1611.03530 style=text-decoration:none>Zhang C, Bengio S, Hardt M, Recht B, Vinyals O. Understanding deep learning requires rethinking generalization. arXiv [cs.LG]. 2016. Available: http://arxiv.org/abs/1611.03530</a></li>
<li class=has-text-weight-bold>
<a name=Bernstein2021 href=#Bernstein2021 style=text-decoration:none>
<span class="tag is-link is-light">Bernstein2021</span>
</a>
<a href=https://jeremybernste.in/writing/ml-is-just-statistics style=text-decoration:none>Bernstein J. Machine learning is just statistics + quantifier reversal. In: jeremybernste [Internet]. [cited 1 Nov 2021]. Available: https://jeremybernste.in/writing/ml-is-just-statistics</a></li>
<li class=has-text-weight-bold>
<a name=Guedj2019 href=#Guedj2019 style=text-decoration:none>
<span class="tag is-link is-light">Guedj2019</span>
</a>
<a href=http://arxiv.org/abs/1901.05353 style=text-decoration:none>Guedj B. A Primer on PAC-Bayesian Learning. arXiv [stat.ML]. 2019. Available: http://arxiv.org/abs/1901.05353</a></li>
<li class=has-text-weight-bold>
<a name=Abu-Mostafa2012 href=#Abu-Mostafa2012 style=text-decoration:none>
<span class="tag is-link is-light">Abu-Mostafa2012</span>
</a>
<a href=https://www.semanticscholar.org/paper/Learning-From-Data-Abu-Mostafa-Magdon-Ismail/1c0ed9ed3201ef381cc392fc3ca91cae6ecfc698 style=text-decoration:none>Abu-Mostafa, Yaser S and Magdon-Ismail, Malik and Lin, Hsuan-Tien. Learning from Data. AMLBook; 2012. Available: https://www.semanticscholar.org/paper/Learning-From-Data-Abu-Mostafa-Magdon-Ismail/1c0ed9ed3201ef381cc392fc3ca91cae6ecfc698</a></li>
</ol>
</p>
</div>
</div>
</article>
</div>
<div class=is-divider data-content=CONNECTUME></div>
<div class="box is-size-7">
<article class=media>
<div class=media-content style=width:100%>
<div class=content>
<p>
<strong><i class="fa-solid fa-fingerprint"></i> Current Ref: </strong><br>
<ul><li style=list-style:none>
<div>wiki/learning-theory/vc-dimension.md</div></li>
</ul>
</p>
</div>
</div>
</article>
</div>
<div class="box is-size-7">
<article class=media>
<div class=media-content>
<div class=content>
<p>
<strong><i class="fa-solid fa-turn-down"></i> Dynamics Links from: </strong>
<div>
<a class=box href=https://datumorphism.leima.is/wiki/machine-learning/overview/>
<div class=media-content>
<div class=content>
<div class="title is-6" style="padding-bottom:.8em;border-bottom:1px dotted #999">Machine Learning Overview
<span style=float:right><i class="fa-solid fa-bookmark" style=color:#999></i></span>
</div>
<div style=font-size:80%>
A brief overview of machine learning
</div>
</div>
</div>
</a>
<a class=box href=https://datumorphism.leima.is/cards/machine-learning/learning-theories/set-shatter/>
<div class=media-content>
<div class=content>
<div class="title is-6" style="padding-bottom:.8em;border-bottom:1px dotted #999">Shatter
<span style=float:right><i class="fa-solid fa-bookmark" style=color:#999></i></span>
</div>
<div style=font-size:80%>
Given a set $\mathcal S$, and a class (collection of sets) $\mathcal H$.
For any subset of $\mathcal …
</div>
</div>
</div>
</a>
<a class=box href=https://datumorphism.leima.is/cards/machine-learning/learning-theories/learning-problem/>
<div class=media-content>
<div class=content>
<div class="title is-6" style="padding-bottom:.8em;border-bottom:1px dotted #999">The Learning Problem
<span style=float:right><i class="fa-solid fa-bookmark" style=color:#999></i></span>
</div>
<div style=font-size:80%>
The learning problem posed by Vapnik:1
Given a sample: $\{z_i\}$ in the probability space $Z$; …
</div>
</div>
</div>
</a>
</div>
</p>
</div>
</div>
</article>
</div>
<div class="box is-size-7">
<article class=media>
<div class=media-content>
<div class=content>
<p>
<strong><i class="fa-solid fa-turn-up"></i> Links to: </strong>
<div>
<a class=box href=https://datumorphism.leima.is/cards/machine-learning/learning-theories/set-shatter/>
<div class=media-content>
<div class=content>
<div class="title is-6" style="padding-bottom:.8em;border-bottom:1px dotted #999">Shatter
<span style=float:right><i class="fa-solid fa-bookmark" style=color:#999></i></span>
</div>
<div style=font-size:80%>
Given a set $\mathcal S$, and a class (collection of sets) $\mathcal H$.
For any subset of $\mathcal …
</div>
</div>
</div>
</a>
</div>
</p>
</div>
</div>
</article>
</div>
<div id=comments class=is-divider data-content=COMMENTS>
</div>
<script src=https://giscus.app/client.js data-repo=datumorphism/comments data-repo-id="MDEwOlJlcG9zaXRvcnkxNjU5MDkyNDI=" data-category=Comments data-category-id=DIC_kwDOCeOS-s4B-Zxx data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=1 data-theme=light crossorigin=anonymous async></script>
</div>
</div>
</article>
</div>
</section>
</div>
<div class=navtools>
<a class="button is-primary is-light is-outlined" title alt="Edit this page" href=https://github.com/datumorphism/datumorphism.github.io/edit/hugo/content/wiki/learning-theory/vc-dimension.md target=blank style=position:fixed;bottom:20px;right:10px;border-radius:9999px;width:35px;height:35px>
<i class="fas fa-pencil-alt"></i>
</a>
<a class="button is-primary is-light is-outlined" href=#comments alt=Comments style=position:fixed;bottom:60px;right:10px;border-radius:9999px;width:35px;height:35px>
<i class="far fa-comments"></i>
</a>
</div>
</main>
<footer>
<footer class=footer>
<div class=container>
<div class="content has-text-centered">
<p>
Created and maintained by <a href=https://leima.is>L Ma</a>.
Acknowledgement: <a href=https://gohugo.io/>Hugo</a>,
<a href=https://themes.gohugo.io/bulma/>Bulma</a>, <a href=https://kausalflow.com>KausalFlow</a>.
<strong>love</strong>.
<br>
<a class=tag href=/about>About</a>
<a class=tag href=/index.xml>Feed</a>
<a class=tag href=/data.json>JSON Data</a>
</p>
</div>
</div>
</footer>
</footer>
<script async type=text/javascript src=/js/bulma.js></script>
</body>
</html>