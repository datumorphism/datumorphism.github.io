<!doctype html><html lang=en-us>
<head>
<meta name=generator content="Hugo 0.89.4">
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1">
<title> Variational Auto-Encoder | Datumorphism | L Ma </title>
<meta name=author content="L Ma">
<meta property="og:title" content="Variational Auto-Encoder">
<meta property="og:description" content="Variational Auto-Encoder (VAE) is very different from [[Generative Model: Auto-Encoder]]  Generative Model: Auto-Encoder Autoencoders (AE) are machines that encodes inputs into a compact latent space. The simplest auto-encoder is rather easy to understand. The loss can be chosen based on the demand, e.g., cross entropy for binary labels. Notation: dot ($\cdot$) We use a single vertically centered dot, i.e., $\cdot$, to indicate that the function or machine can take in arguments. A simple autoencoder can be achieved using two neural nets, e.g., $$ \begin{align} {\color{green}h} &= …   . In VAE, we introduce a variational distribution $q$ to help us work out the weighted integral after introducing the latent space variable $z$,">
<meta property="og:type" content="article">
<meta property="og:url" content="https://datumorphism.leima.is/wiki/machine-learning/generative-models/variational-autoencoder/"><meta property="article:section" content="wiki">
<meta property="article:published_time" content="2021-08-13T00:00:00+00:00">
<meta property="article:modified_time" content="2021-08-13T00:00:00+00:00">
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga('create','UA-140452515-1','auto'),ga('send','pageview'))</script>
<script async src=https://www.google-analytics.com/analytics.js></script>
<link rel=canonical href=https://datumorphism.leima.is/wiki/machine-learning/generative-models/variational-autoencoder/> <link rel="shortcut icon" type=image/png href=/logos/logo-square.png>
<link rel=stylesheet href=/css/bulma.css>
<link rel=stylesheet href=/css/bulma-divider.min.css>
<link rel=stylesheet href=/assets/css/bulma-ribbon.min.css>
<link rel=stylesheet href=/assets/css/tooltip.css>
<link rel=stylesheet href=https://jenil.github.io/bulmaswatch/united/bulmaswatch.min.css>
<link rel=stylesheet href=/css/custom.css>
<link rel=stylesheet href=/css/blog-post.css>
<link rel=stylesheet href=/css/code-highlighting/dark.css>
<link rel=stylesheet href=/css/custom.css>
<script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],tags:'ams',processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:['script','noscript','style','textarea','pre']},svg:{fontCache:'global'}},window.addEventListener('load',a=>{document.querySelectorAll("mjx-container").forEach(function(a){a.parentElement.classList+='has-jax'})})</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<script src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script>
<script>mermaid.initialize({startOnLoad:!0})</script>
<link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css integrity="sha512-9usAa10IRO0HhonpyAIVpjrylPvoDwiPUiKdWk5t3PyolY1cOd4DSE0Ga+ri4AuTroPR5aQvXU9xC6qOPnzFeg==" crossorigin=anonymous referrerpolicy=no-referrer>
<script src=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/js/all.min.js integrity="sha512-yFjZbTYRCJodnuyGlsKamNE/LlEaEAxSUDe5+u61mV8zzqJVFOH7TnULE2/PP/l5vKWpUNnF4VGVkXh3MjgLsg==" crossorigin=anonymous referrerpolicy=no-referrer></script>
</head>
<body>
<header> <nav class="navbar is-transparent">
<div class=navbar-brand>
<a class=navbar-item href=/>
<img src=/site/intelligence.png alt=Datumorphism height=28 style=margin-right:.5em> Datumorphism
</a>
<div class="navbar-burger burger" style=color:#000 data-target=navMenu>
<span></span>
<span></span>
<span></span>
</div>
</div>
<div class=navbar-menu id=navMenu>
<div class=navbar-start>
<div class="navbar-item has-dropdown is-hoverable">
<a href=/projects class=navbar-link>
Notebooks
</a>
<div class=navbar-dropdown>
<a href=https://datumorphism.leima.is/awesome/ class=navbar-item>
Awesome
</a>
<a href=https://datumorphism.leima.is/blog/ class=navbar-item>
Blog
</a>
<a href=https://datumorphism.leima.is/cards/ class=navbar-item>
Cards
</a>
<a href=https://datumorphism.leima.is/hologram/ class=navbar-item>
Hologram
</a>
<a href=https://datumorphism.leima.is/reading/ class=navbar-item>
Reading Notes
</a>
<a href=https://datumorphism.leima.is/til/ class=navbar-item>
TIL
</a>
<a href=https://datumorphism.leima.is/wiki/ class=navbar-item>
Wiki
</a>
</div>
</div>
<div class="navbar-item has-dropdown is-hoverable">
<a class=navbar-item href=https://neuronstar.kausalflow.com/cpe-docs/>
Probability Estimation
</a>
</div>
</div>
<span class=navbar-burger>
<span></span>
<span></span>
<span></span>
</span>
<div class=navbar-end>
<div class=navbar-item>
<a class=navbar-item href=/blog/>
Blog
</a>
<a class=navbar-item href=/amneumarkt/>
AmNeumarkt
</a>
<a class=navbar-item href=/>
<i class="fas fa-search"></i>
</a>
<a class=navbar-item href=/tags/>
<i class="fas fa-tags"></i>
</a>
<a class=navbar-item href=/graph>
<i class="fas fa-project-diagram"></i>
</a>
<a class=navbar-item href=https://t.me/amneumarkt>
<i class="fab fa-telegram"></i>
</a>
<a class=navbar-item target=blank href=https://github.com/datumorphism>
<span class=icon>
<i class="fab fa-github"></i>
</span>
</a>
</div>
</div>
</div>
</nav>
<script>document.addEventListener('DOMContentLoaded',()=>{const a=Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'),0);a.length>0&&a.forEach(a=>{a.addEventListener('click',()=>{const b=a.dataset.target,c=document.getElementById(b);a.classList.toggle('is-active'),c.classList.toggle('is-active')})})})</script>
</header>
<main>
<div class=container itemscope itemtype=http://schema.org/BlogPosting>
<meta itemprop=name content="Variational Auto-Encoder">
<meta itemprop=description content="Variational Auto-Encoder (VAE) is very different from [[Generative Model: Auto-Encoder]]  Generative Model: Auto-Encoder Autoencoders (AE) are machines that encodes inputs into a compact latent space. The simplest auto-encoder is rather easy to understand. The loss can be chosen based on the demand, e.g., cross entropy for binary labels. Notation: dot ($\cdot$) We use a single vertically centered dot, i.e., $\cdot$, to indicate that the function or machine can take in arguments. A simple autoencoder can be achieved using two neural nets, e.g., $$ \begin{align} {\color{green}h} &= …   . In VAE, we introduce a variational distribution $q$ to help us work out the weighted integral after introducing the latent space variable $z$,"><meta itemprop=datePublished content="2021-08-13T00:00:00+00:00">
<meta itemprop=dateModified content="2021-08-13T00:00:00+00:00">
<meta itemprop=wordCount content="1046">
<meta itemprop=keywords content="Self-supervised Learning,Generative Model,VAE,Basics,">
<section class=section>
<div class=container>
<article class=post>
<header class=post-header>
<nav class="breadcrumb has-succeeds-separator is-small" aria-label=breadcrumbs>
<ul>
<li>
<a href=https://datumorphism.leima.is/>Datumorphism</a>
</li>
<li>
<a href=https://datumorphism.leima.is/wiki/>Wiki</a>
</li>
<li>
<a href=https://datumorphism.leima.is/wiki/machine-learning/>Machine Learning</a>
</li>
<li>
<a href=https://datumorphism.leima.is/wiki/machine-learning/generative-models/>Generative Models</a>
</li>
<li class=active>
<a href=https://datumorphism.leima.is/wiki/machine-learning/generative-models/variational-autoencoder/>Variational Auto-Encoder</a>
</li>
</ul>
</nav>
<h1 class="post-title has-text-centered is-size-1" itemprop="name headline">
Variational Auto-Encoder
</h1>
<h2 class="title is-6 has-text-centered">
<i class="fas fa-tags" style=margin-right:.5em></i>
<a href=/tags/self-supervised-learning><span class="tag is-warning is-small is-light">#Self-supervised Learning</span></a>
<a href=/tags/generative-model><span class="tag is-warning is-small is-light">#Generative Model</span></a>
<a href=/tags/vae><span class="tag is-warning is-small is-light">#VAE</span></a>
<a href=/tags/basics><span class="tag is-warning is-small is-light">#Basics</span></a>
</h2>
</header>
<div class=columns>
<div class="column is-8">
<div class=is-divider data-content=ARTICLE></div>
<div class="content blog-post section" itemprop=articleBody>
<p>Variational Auto-Encoder (VAE) is very different from
<span class=tooltip><a href=https://datumorphism.leima.is/wiki/machine-learning/generative-models/autoencoder/>
[[Generative Model: Auto-Encoder]]
</a>
<span class=tooltiptext>
<span class=tooltip_title>Generative Model: Auto-Encoder</span>
<span class=tooltip_content>Autoencoders (AE) are machines that encodes inputs into a compact latent space.
The simplest auto-encoder is rather easy to understand.
The loss can be chosen based on the demand, e.g., cross entropy for binary labels.
Notation: dot ($\cdot$)
We use a single vertically centered dot, i.e., $\cdot$, to indicate that the function or machine can take in arguments. A simple autoencoder can be achieved using two neural nets, e.g.,
$$ \begin{align} {\color{green}h} &= …</span>
</span>
</span>
. In VAE, we introduce a variational distribution $q$ to help us work out the weighted integral after introducing the latent space variable $z$,</p>
<p>$$
\begin{align}
\ln p_\theta(x) &= \int \left(\ln p_\theta (x\mid z) \right)p(z) \,\mathrm d z \\
&=  \int \left(\ln\left(\frac{q_{\phi}(z\mid x)}{q_{\phi}(z\mid x)} p_\theta (x\mid z)\right) \right) p(z) \, \mathrm d z
\end{align}
$$</p>
<p>In the above derivation,</p>
<ul>
<li>${}_\theta$ is the model for inference, and</li>
<li>${}_\phi$ is the model for variational approximation.</li>
</ul>
<article class="message is-light is-light">
<div class=message-header>
<p>Tricks</p>
</div>
<div class=message-body>
<ul>
<li>$p_\theta(x\mid z)$ is usually Gaussian distribution of $x$ but with mean parameterized by the latent variable $z$ and the model parameters $\theta$.</li>
<li>The latent space variable $p(z)$ is usually assumed to be a normal distribution.</li>
<li>The marginalization of the latent variable increase the expressive power.</li>
<li>Instead of modeling a complex likelihood $p(x\mid z)$ directly, we only need to model parameters of Gaussian distributions, e.g., a function $f(z, \theta)$ for the mean of the Gaussian distribution.</li>
</ul>
</div>
</article>
<figure><img src=../assets/generative-variational-autoencoder/1606.05908-fig2-gaussian-latent.png alt="From simple distribution in latent space to a more complex distribution. [Doersch2016]"><figcaption>
<p>From simple distribution in latent space to a more complex distribution. [Doersch2016]</p>
</figcaption>
</figure>
<p>The demo looks great. However, sampling from latent space becomes more difficult as the dimension of the latent space increases. We need a more efficient way to sample from the latent space. One solution is to apply the variational method. To to sample $z$, the method uses a model that samples $z$ based on $x$, i.e., introduce a function $q(z\mid x)$ to help us with sampling in latent space.</p>
<p>$$
\begin{align}
\ln p_\theta(x) &= \int \left(\ln p_\theta (x\mid z) \right)p(z) \,\mathrm d z \\
&=  \int \left(\ln\frac{q_{\phi}(z\mid x)}{q_{\phi}(z\mid x)} p_\theta (x\mid z) \right) p(z) \, \mathrm d z \\
&=  \int \left(\ln\frac{q_{\phi}(z\mid x)}{q_{\phi}(z\mid x)} \frac{p_\theta (x, z)}{p (z)} \right) p(z) \, \mathrm d z \\
&= \int dz q(z\mid x) \ln \frac{p(x,z)}{q(z\mid x)} + \int dz q(z\mid x) \ln \frac{q(z\mid x)}{p(z\mid x)} \label{eqn-vae-lnp-sep-q} \\
&= - \left[ D_{\mathrm{KL}} ( q_{\phi}(z\mid x) \mathrel{\Vert} p(z) ) - \mathbb E_q ( \ln p_\theta (x\mid z) ) \right] + D_{\mathrm{KL}}( q(z\mid x)\parallel p(z\mid x) ) \label{eqn-vae-lnp-decompositions} \\
& \geq - \left[ D_{\mathrm{KL}} ( q_{\phi}(z\mid x) \mathrel{\Vert} p(z) ) - \mathbb E_q ( \ln p_\theta (x\mid z) ) \right] \label{eqn-vae-lnp-geq-elbo} \\
&\equiv - F(x) \\
&\equiv \mathcal L .
\end{align}
$$</p>
<p>In the derivation, the row ($\ref{eqn-vae-lnp-sep-q}$) is validate because $\int dz q(z\mid x) = 1$.</p>
<p>The term $F(x)$ is the free energy, while the negative of it, $-F(x)=\mathcal L$, is the so-called
<span class=tooltip><a href=https://datumorphism.leima.is/wiki/machine-learning/bayesian/elbo/>
[[Evidence Lower Bound (ELBO)]]
</a>
<span class=tooltiptext>
<span class=tooltip_title>Evidence Lower Bound: ELBO</span>
<span class=tooltip_content>ELBO is an very important concept in variational methods</span>
</span>
</span>
,</p>
<p>$$
\mathcal L = - D_{\mathrm{KL}} ( q_{\phi}(z\mid x) \mathrel{\Vert} p(z) ) + \mathbb E_q ( \ln p_\theta (x\mid z) ).
$$</p>
<p>From row ($\ref{eqn-vae-lnp-decompositions}$) to ($\ref{eqn-vae-lnp-geq-elbo}$), we dropped the term $D_{\mathrm{KL}}( q(z\mid x)\parallel p(z\mid x) )$ which is always nonnegative. The reason is that we can not maximize this
<span class=tooltip><a href=https://datumorphism.leima.is/wiki/machine-learning/basics/kl-divergence/>
[[KL divergence]]
</a>
<span class=tooltiptext>
<span class=tooltip_title>KL Divergence</span>
<span class=tooltip_content>Kullback–Leibler divergence indicates the differences between two distributions</span>
</span>
</span>
as we do not know $p(z\mid x)$. But the KL divergence is always non-negative. So if we find a $q$ that can maximize $\mathcal L$, then we are also miminizing the KL divergence (with a function $q(z\mid x)$ that is close to $p(z\mid x)$) and maximizing the loglikelihood loss. Now we only need to find a way to maximize $\mathcal L$.</p>
<article class="message is-light is-light">
<div class=message-header>
<p>More about this ELBO</p>
</div>
<div class=message-body>
<p>We do not know $p(x,z)$ either but we can rewrite $\mathcal L$,</p>
<p>\begin{align}
\mathcal L(q) =& \int dz q(z\mid x) \ln\frac{p(x,z)}{q(z\mid x)} \\
=& \int dz q(z\mid x)\ln \frac{p(x\mid z)p(z)}{q(z\mid x)} \\
= & \int dz q(z\mid x) \ln p(x\mid z) + \int dz q(z\mid x) \ln \frac{p(z)}{q(z\mid x)} \\
= & \int dz q(z\mid x) \ln p(x\mid z) - \operatorname{KL} \left( q(z\mid x) \parallel p(z) \right)
\end{align}</p>
<p>Our loss function becomes</p>
<p>$$- \mathcal L(q) = - \mathbb E_{q} \ln {\color{red}p(x\mid z)} + \operatorname{KL} \left( {\color{blue}q(z\mid x) }\parallel p(z) \right),$$</p>
<p>where ${\color{blue}q(z\mid x) }$ is our encoder which encodes data $x$ to the latent data $z$, and ${\color{red}p(x\mid z)}$ is our decoder. The second term ensures our encoder is similar to our priors.</p>
</div>
</article>
<h2 id=using-neural-networks>Using Neural networks</h2>
<p>We model the parameters of the Gaussian distribution $p_\theta(x\mid z)$, e.g., $f(z, \theta)$, using a neural network.</p>
<p>In reality, we choose a gaussian form of the variational functional with the mean and variance depends on the data $x$ and the latent variable $z$</p>
<p>$$
q(z\mid x) = \mathcal N ( \mu(x,z), \Sigma (x,z) ).
$$</p>
<p>We have</p>
<p>$$
\begin{align}
&\ln p_\theta(x\mid z) \\
=& \ln \mathscr N( x\mid f(z, \theta), \sigma^2 I )\\
=& \ln \left( \frac{1}{\sqrt{2\pi \sigma^2}} \exp{\left( -\frac{(x -f(z,\theta)^2)}{\sigma^2} \right)} \right) \\
=& -(x - f(z, \theta))^2 + \mathrm{Const.}
\end{align}
$$</p>
<div class=card>
<details>
<summary class="card-header-title card-toggle card-header">
Why don&rsquo;t we simply draw $q$ from $p(z)$?
</summary>
<div class=card-content>
<div class=content>
If we are sort of minimizing the KL divergence $\operatorname{KL} \left( {\color{blue}q(z\mid x) }\parallel p(z) \right)$ too, why don&rsquo;t we simply draw $q$ from $p(z)$? First of all, we also have to take care of the first term. Secondly, we need a latent space that connects to the actual data for reconstruction.
</div>
</div>
</details>
</div>
<h2 id=structure>Structure</h2>
<figure><img src=../assets/generative-variational-autoencoder/simple-vae.png alt="Structure of VAE"><figcaption>
<p>Structure of VAE</p>
</figcaption>
</figure>
<p>Doersch wrote a very nice tutorial on VAE<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. We can find the detailed structures of VAE.</p>
<p>Another key component of VAE is the
<span class=tooltip><a href=https://datumorphism.leima.is/cards/statistics/reparametrization-expectation-sampling/>
[[reparametrization trick]]
</a>
<span class=tooltiptext>
<span class=tooltip_title>Reparametrization in Expectation Sampling</span>
<span class=tooltip_content>Reparametrize the sampling distribution to simplify the sampling</span>
</span>
</span>
. The variational approximation $q_\phi$ is usually a Gaussian distribution. Once we get the parameters for the Gaussian distribution, we will have to sample from the Gaussian distribution based on the parameters. However, this sampling process prohibits us from propagating errors. The
<span class=tooltip><a href=https://datumorphism.leima.is/cards/statistics/reparametrization-expectation-sampling/>
[[reparametrization trick]]
</a>
<span class=tooltiptext>
<span class=tooltip_title>Reparametrization in Expectation Sampling</span>
<span class=tooltip_content>Reparametrize the sampling distribution to simplify the sampling</span>
</span>
</span>
solves this problem.</p>
<h2 id=loss-explanation>Loss Explanation</h2>
<figure><img src=../assets/generative-variational-autoencoder/vae-loss-explained.png alt="VAE Loss Explained [Doersch2016]"><figcaption>
<p>VAE Loss Explained [Doersch2016]</p>
</figcaption>
</figure>
<section class=footnotes role=doc-endnotes>
<hr>
<ol>
<li id=fn:1 role=doc-endnote>
<p>
<a href=http://arxiv.org/abs/2006.08218 style=text-decoration:none>Liu X, Zhang F, Hou Z, Wang Z, Mian L, Zhang J, et al. Self-supervised Learning: Generative or Contrastive. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2006.08218</a>
<a href=https://www.jeremyjordan.me/variational-autoencoders/ style=text-decoration:none>Jordan J. Variational autoencoders. Jeremy Jordan. 19 Mar 2018. Available: https://www.jeremyjordan.me/variational-autoencoders/. Accessed 22 Aug 2021.</a>
&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p>
</li>
</ol>
</section>
</div>
<p><div class="has-text-right is-size-7">
<span class=icon>
<i class="fas fa-pencil-alt"></i>
</span>
Published: <time datetime=2021-08-13T00:00:00+00:00>2021-08-13</time>
by <span itemprop=author>L Ma</span>;
</div></p>
<div class=is-divider></div>
<nav class="pagination is-centered" role=navigation aria-label=pagination>
<a href="https://datumorphism.leima.is/wiki/machine-learning/generative-models/autoencoder/?ref=footer" class=pagination-previous>« Generative Model: Auto-Encoder</a>
</nav>
</div>
<div class="column is-4">
<div class=is-divider data-content="Cite Me"></div>
<div class="box is-size-7 has-text-white has-background-black">
<article class=media>
<div class=media-content>
<div class=content>
<p>
L Ma
(2021). 'Variational Auto-Encoder', Datumorphism, 08 April. Available at: https://datumorphism.leima.is/wiki/machine-learning/generative-models/variational-autoencoder/.
</p>
</div>
</div>
</article>
</div>
<style>#TableOfContents>ul{list-style-type:lower-greek;padding-left:0}#TableOfContents>ul>li ul{list-style-type:none;padding-left:1em}</style>
<div class=is-divider data-content=ToC></div>
<div class="box is-size-7">
<article class=media>
<div class=media-content>
<div class=content>
<details>
<summary><i class="fa-solid fa-list-ol"></i> Table of Contents</summary>
<div>
<div>
<nav id=TableOfContents>
<ul>
<li><a href=#using-neural-networks>Using Neural networks</a></li>
<li><a href=#structure>Structure</a></li>
<li><a href=#loss-explanation>Loss Explanation</a></li>
</ul>
</nav>
</div>
</div>
</details>
</div>
</div>
</article>
</div>
<script>const el=document.querySelector('details summary');el.onclick=()=>{(function(f,c,d,e,a,b){a=c.createElement(d),b=c.getElementsByTagName(d)[0],a.async=1,a.src=e,b.parentNode.insertBefore(a,b)})(window,document,'script','/js/smoothscroll.js'),el.onclick=null},document.querySelectorAll('#TableOfContents a').forEach(a=>{a.addEventListener('click',()=>{document.querySelector(a.href.slice(a.href.indexOf('#'))).scrollIntoView({behavior:'smooth'})})})</script>
<div class=is-divider data-content=REFERENCES></div>
<div class="box is-size-7">
<article class=media>
<div class=media-content style=width:100%;word-break:break-word>
<div class=content>
<p>
<strong><i class="fa-solid fa-quote-left"></i> References: </strong><br>
<ol>
<li class=has-text-weight-bold><a href=http://arxiv.org/abs/2006.08218 style=text-decoration:none>Liu X, Zhang F, Hou Z, Wang Z, Mian L, Zhang J, et al. Self-supervised Learning: Generative or Contrastive. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2006.08218</a></li>
<li class=has-text-weight-bold>
<a name=Doersch2016 href=#Doersch2016 style=text-decoration:none>
<span class="tag is-link is-light">Doersch2016</span>
</a>
<a href=http://arxiv.org/abs/1606.05908 style=text-decoration:none>Doersch C. Tutorial on Variational Autoencoders. arXiv [stat.ML]. 2016. Available: http://arxiv.org/abs/1606.05908</a></li>
<li class=has-text-weight-bold>
<a name=Kingma2019 href=#Kingma2019 style=text-decoration:none>
<span class="tag is-link is-light">Kingma2019</span>
</a>
<a href=http://arxiv.org/abs/1906.02691 style=text-decoration:none>Kingma DP, Welling M. An Introduction to Variational Autoencoders. arXiv [cs.LG]. 2019. Available: http://arxiv.org/abs/1906.02691</a></li>
<li class=has-text-weight-bold><a href=https://www.jeremyjordan.me/variational-autoencoders/ style=text-decoration:none>Jordan J. Variational autoencoders. Jeremy Jordan. 19 Mar 2018. Available: https://www.jeremyjordan.me/variational-autoencoders/. Accessed 22 Aug 2021.</a></li>
</ol>
</p>
</div>
</div>
</article>
</div>
<div class=is-divider data-content=CONNECTUME></div>
<div class="box is-size-7">
<article class=media>
<div class=media-content style=width:100%>
<div class=content>
<p>
<strong><i class="fa-solid fa-fingerprint"></i> Current Ref: </strong><br>
<ul><li style=list-style:none>
<div>wiki/machine-learning/generative-models/variational-autoencoder.md</div></li>
</ul>
</p>
</div>
</div>
</article>
</div>
<div class="box is-size-7">
<article class=media>
<div class=media-content>
<div class=content>
<p>
<strong><i class="fa-solid fa-turn-down"></i> Dynamics Links from: </strong>
<div>
</div>
</p>
</div>
</div>
</article>
</div>
<div class="box is-size-7">
<article class=media>
<div class=media-content>
<div class=content>
<p>
<strong><i class="fa-solid fa-turn-up"></i> Links to: </strong>
<div>
<a class=box href=https://datumorphism.leima.is/wiki/machine-learning/generative-models/generative/>
<div class=media-content>
<div class=content>
<div class="title is-6" style="padding-bottom:.8em;border-bottom:1px dotted #999">An Introduction to Generative Models
<span style=float:right><i class="fa-solid fa-bookmark" style=color:#999></i></span>
</div>
<div style=font-size:80%>
Discriminative model:
The conditional probability of class label on data (posterior) $p(C_k\mid x)$ …
</div>
</div>
</div>
</a>
<a class=box href=https://datumorphism.leima.is/wiki/machine-learning/generative-models/autoencoder/>
<div class=media-content>
<div class=content>
<div class="title is-6" style="padding-bottom:.8em;border-bottom:1px dotted #999">Generative Model: Auto-Encoder
<span style=float:right><i class="fa-solid fa-bookmark" style=color:#999></i></span>
</div>
<div style=font-size:80%>
Autoencoders (AE) are machines that encodes inputs into a compact latent space.
The simplest …
</div>
</div>
</div>
</a>
<a class=box href=https://datumorphism.leima.is/wiki/machine-learning/bayesian/elbo/>
<div class=media-content>
<div class=content>
<div class="title is-6" style="padding-bottom:.8em;border-bottom:1px dotted #999">Evidence Lower Bound: ELBO
<span style=float:right><i class="fa-solid fa-bookmark" style=color:#999></i></span>
</div>
<div style=font-size:80%>
ELBO is an very important concept in variational methods
</div>
</div>
</div>
</a>
<a class=box href=https://datumorphism.leima.is/wiki/machine-learning/bayesian/latent-variable-models/>
<div class=media-content>
<div class=content>
<div class="title is-6" style="padding-bottom:.8em;border-bottom:1px dotted #999">Latent Variable Models
<span style=float:right><i class="fa-solid fa-bookmark" style=color:#999></i></span>
</div>
<div style=font-size:80%>
Latent variable models brings us new insights on identifying the patterns of some sample data.
</div>
</div>
</div>
</a>
<a class=box href=https://datumorphism.leima.is/cards/statistics/reparametrization-expectation-sampling/>
<div class=media-content>
<div class=content>
<div class="title is-6" style="padding-bottom:.8em;border-bottom:1px dotted #999">Reparametrization in Expectation Sampling
<span style=float:right><i class="fa-solid fa-bookmark" style=color:#999></i></span>
</div>
<div style=font-size:80%>
Reparametrize the sampling distribution to simplify the sampling
</div>
</div>
</div>
</a>
<a class=box href=https://datumorphism.leima.is/wiki/machine-learning/basics/kl-divergence/>
<div class=media-content>
<div class=content>
<div class="title is-6" style="padding-bottom:.8em;border-bottom:1px dotted #999">KL Divergence
<span style=float:right><i class="fa-solid fa-bookmark" style=color:#999></i></span>
</div>
<div style=font-size:80%>
Kullback–Leibler divergence indicates the differences between two distributions
</div>
</div>
</div>
</a>
</div>
</p>
</div>
</div>
</article>
</div>
<div id=comments class=is-divider data-content=COMMENTS>
</div>
<script src=https://giscus.app/client.js data-repo=datumorphism/comments data-repo-id="MDEwOlJlcG9zaXRvcnkxNjU5MDkyNDI=" data-category=Comments data-category-id=DIC_kwDOCeOS-s4B-Zxx data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=1 data-theme=light crossorigin=anonymous async></script>
</div>
</div>
</article>
</div>
</section>
</div>
<div class=navtools>
<a class="button is-primary is-light is-outlined" title alt="Edit this page" href=https://github.com/datumorphism/datumorphism.github.io/edit/hugo/content/wiki/machine-learning/generative-models/variational-autoencoder.md target=blank style=position:fixed;bottom:20px;right:10px;border-radius:9999px;width:35px;height:35px>
<i class="fas fa-pencil-alt"></i>
</a>
<a class="button is-primary is-light is-outlined" href=#comments alt=Comments style=position:fixed;bottom:60px;right:10px;border-radius:9999px;width:35px;height:35px>
<i class="far fa-comments"></i>
</a>
</div>
</main>
<footer>
<footer class=footer>
<div class=container>
<div class="content has-text-centered">
<p>
Created and maintained by <a href=https://leima.is>L Ma</a>.
Acknowledgement: <a href=https://gohugo.io/>Hugo</a>,
<a href=https://themes.gohugo.io/bulma/>Bulma</a>, <a href=https://kausalflow.com>KausalFlow</a>.
<strong>love</strong>.
<br>
<a class=tag href=/about>About</a>
<a class=tag href=/index.xml>Feed</a>
<a class=tag href=/data.json>JSON Data</a>
</p>
</div>
</div>
</footer>
</footer>
<script async type=text/javascript src=/js/bulma.js></script>
</body>
</html>