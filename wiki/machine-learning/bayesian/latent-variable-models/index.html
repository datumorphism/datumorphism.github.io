<!doctype html><html lang=en-us><head><meta name=generator content="Hugo 0.69.2"><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><title>Latent Variable Models | Datumorphism | Lei Ma</title><meta name=description content="Latent variable models brings us new insights on identifying the patterns of some sample data."><meta name=author content="Lei Ma"><meta property="og:title" content="Latent Variable Models"><meta property="og:description" content="Latent variable models brings us new insights on identifying the patterns of some sample data."><meta property="og:type" content="article"><meta property="og:url" content="https://datumorphism.leima.is/wiki/machine-learning/bayesian/latent-variable-models/"><meta property="article:published_time" content="2021-01-27T00:00:00+00:00"><meta property="article:modified_time" content="2021-01-27T00:00:00+00:00"><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-140452515-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><link rel=canonical href=https://datumorphism.leima.is/wiki/machine-learning/bayesian/latent-variable-models/><link rel="shortcut icon" type=image/png href=/logos/logo-square.png><link rel=stylesheet href=/css/bulma.css><link rel=stylesheet href=/css/bulma-divider.min.css><link rel=stylesheet href=/assets/css/bulma-ribbon.min.css><link rel=stylesheet href=/assets/css/tooltip.css><link rel=stylesheet href=https://jenil.github.io/bulmaswatch/united/bulmaswatch.min.css><link rel=stylesheet href=/css/custom.css><link rel=stylesheet href=/css/blog-post.css><link rel=stylesheet href=/css/code-highlighting/dark.css><link rel=stylesheet href=/css/custom.css><link rel=stylesheet href=https://unpkg.com/applause-button/dist/applause-button.css><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],tags:'ams',processEscapes:true,processEnvironments:true},options:{skipHtmlTags:['script','noscript','style','textarea','pre']},svg:{fontCache:'global'}};window.addEventListener('load',(event)=>{document.querySelectorAll("mjx-container").forEach(function(x){x.parentElement.classList+='has-jax'})});</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script>mermaid.initialize({startOnLoad:true});</script><script src=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/js/all.min.js integrity="sha256-KzZiKy0DWYsnwMF+X1DvQngQ2/FxF7MF3Ff72XcpuPs=" crossorigin=anonymous></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css integrity="sha256-h20CPZ0QyXlBuAw7A+KluUYx/3pK+c7lYEpqLTlxjYQ=" crossorigin=anonymous></head><body><header><nav class="navbar is-transparent"><div class=navbar-brand><a class=navbar-item href=/><img src=/site/intelligence.png alt=Datumorphism height=28 style=margin-right:.5em> Datumorphism</a><div class="navbar-burger burger" style=color:#000 data-target=navMenu><span></span><span></span><span></span></div></div><div class=navbar-menu id=navMenu><div class=navbar-start><div class="navbar-item has-dropdown is-hoverable"><a href=/projects class=navbar-link>Notebooks</a><div class=navbar-dropdown><a href=https://datumorphism.leima.is/awesome/ class=navbar-item>Awesome</a>
<a href=https://datumorphism.leima.is/blog/ class=navbar-item>Blog</a>
<a href=https://datumorphism.leima.is/cards/ class=navbar-item>Cards</a>
<a href=https://datumorphism.leima.is/hologram/ class=navbar-item>Hologram</a>
<a href=https://datumorphism.leima.is/reading/ class=navbar-item>Reading Notes</a>
<a href=https://datumorphism.leima.is/til/ class=navbar-item>TIL</a>
<a href=https://datumorphism.leima.is/wiki/ class=navbar-item>Wiki</a></div></div></div><span class=navbar-burger><span></span><span></span><span></span></span><div class=navbar-end><div class=navbar-item><a class=navbar-item href=/blog/>Blog</a>
<a class=navbar-item href=/amneumarkt/>AmNeumarkt</a>
<a class=navbar-item href=/><i class="fas fa-search"></i></a><a class=navbar-item href=/tags/><i class="fas fa-tags"></i></a><a class=navbar-item href=/graph><i class="fas fa-project-diagram"></i></a><a class=navbar-item href=https://t.me/amneumarkt><i class="fab fa-telegram"></i></a><a class=navbar-item target=blank href=https://github.com/datumorphism><span class=icon><i class="fab fa-github"></i></span></a></div></div></div></nav><script>document.addEventListener('DOMContentLoaded',()=>{const $navbarBurgers=Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'),0);if($navbarBurgers.length>0){$navbarBurgers.forEach(el=>{el.addEventListener('click',()=>{const target=el.dataset.target;const $target=document.getElementById(target);el.classList.toggle('is-active');$target.classList.toggle('is-active');});});}});</script></header><main><div class=container itemscope itemtype=http://schema.org/BlogPosting><meta itemprop=name content="Latent Variable Models"><meta itemprop=description content="Latent variable models brings us new insights on identifying the patterns of some sample data."><meta itemprop=datePublished content="2021-01-27T00:00:00+00:00"><meta itemprop=dateModified content="2021-01-27T00:00:00+00:00"><meta itemprop=wordCount content="918"><meta itemprop=keywords content="latent variable model,variational autoencoder,normalizing flow,"><section class=section><div class=container><article class=post><header class=post-header><nav class="breadcrumb has-succeeds-separator is-small" aria-label=breadcrumbs><ul><li><a href=https://datumorphism.leima.is/>Datumorphism</a></li><li><a href=https://datumorphism.leima.is/wiki/>Wiki</a></li><li><a href=https://datumorphism.leima.is/wiki/machine-learning/>Machine Learning</a></li><li><a href=https://datumorphism.leima.is/wiki/machine-learning/bayesian/>Bayesian Methods</a></li><li class=active><a href=https://datumorphism.leima.is/wiki/machine-learning/bayesian/latent-variable-models/>Latent Variable Models</a></li></ul></nav><h1 class="post-title has-text-centered is-size-1" itemprop="name headline">Latent Variable Models</h1><h2 class="title is-6 has-text-centered"><i class="fas fa-tags" style=margin-right:.5em></i><a href=/tags/latent-variable-model><span class="tag is-warning is-small is-light">#latent variable model</span></a>
<a href=/tags/variational-autoencoder><span class="tag is-warning is-small is-light">#variational autoencoder</span></a>
<a href=/tags/normalizing-flow><span class="tag is-warning is-small is-light">#normalizing flow</span></a></h2></header><div class=columns><div class="column is-8"><div class=is-divider data-content=ARTICLE></div><div class="content blog-post section" itemprop=articleBody><p>In the view of statistics, we know everything about a physical system if we know the probability $p(\mathbf s)$ of all possible states of the physical system $\mathbf s$. Time can also be part of the state specification.</p><blockquote><p>As an example, we will classify fruits into oranges and non oranges. We will have the state vector $\mathbf s = (\text{is orange}, \text{texture } x)$. Our goal is to find the joint probability $p(\text{is orange}, x)$.</p></blockquote><p>The reality, we only have sample data. This sample data usually can not cover all the possible states of the system. Thus a direct calculation to find the joint probability $p(\mathbf s)$ is not feasible. We do not have so much information nor computing power.</p><blockquote><p>A formal description is to set the generating process on a measure space ${Y, \Sigma_Y}$ with measure $\phi$. We will need to define this space using a sample drawn from it.</p></blockquote><p>Luckily, our world follows some simple rules. The data is very likely to be generated by a simple generating process. So we can estimate the generating process behind the sample data.</p><blockquote><p>Introduce a Gaussian mixture model to describe the observed data points,</p><p>$$
g(y) = \pi \phi(\mu_1, \sigma_1) + (1 - \pi) \phi(\mu_2, \sigma_2),
$$</p><p>with $\phi$ being the normal distribution density. Given data as our prior and $g$ as the model, we can write down the likelihood, $p(\pi, \mu_1, \sigma_1, \mu_2, \sigma_2\mid y)$</p><p>To find the parameters, we could maximum the log-likelihood, in principle. However, this is not that easy as it involves some sum of logs,</p><p>$$
\sum_{\text{all data points}}\log\left(\pi \phi(\mu_1, \sigma_1) + (1 - \pi) \phi(\mu_2, \sigma_2) \right).
$$</p><p>The trick is to introduce latent variable that will not be part of the model but helps us solve the problem. One of the latent variables we can use is a discrete variable that tells us which Gaussian compoent the data is associated with, which is denoted as $\Delta_i\in {0,1}$ for data point $y_i$. This is not known from the data. With the latent variable, the log-likelihood simplifies and we can apply the Expectation-Maximization method, aka EM method. The EM method tackles this problem by introducing an iterative process.</p></blockquote><p>With this latent variable $\mathbf z$, we have the marginalized probability</p><p>$$
p(\mathbf s) = \int p( \mathbf s \mid \mathbf z ) p(\mathbf z) d\mathbf z.
$$</p><p>This is a mixture model of infinite components (in Hilbert space).</p><blockquote><p>Instead of inferring $p(\mathbf s)$, we infer $p( \mathbf s \mid \mathbf z )$ and $p(\mathbf z)$ as well as $p(\mathbf z \mid \mathbf s)$ for the parameters.</p><p>Between $\mathbf x$ and $\mathbf z$, we have</p><ul><li>prior $p(\mathbf z)$,</li><li>likelihood $p(\mathbf s \mid \mathbf z)$,</li><li>posterior $p(\mathbf z \mid \mathbf s)$.</li></ul></blockquote><p>In the Bayesian world, we will separate the state vector $\mathbf s$ into the observables $y$ and the model $\theta$ and use Maximum Likelihood Estimation (MLE) or Maximum A Posteriori (MAP) to find our parameters.</p><blockquote><p>The model based on MLE is</p><p>$$
\begin{align}
\theta_{\mathrm{MLE}}
&= \mathop{\mathrm{arg,max}}\limits_{\theta} \log p(y \mid \theta) \\<br>&= \mathop{\mathrm{arg,max}}\limits_{\theta} \log \prod_{y_i\in Y} p(y_i \mid \theta) \\<br>&= \mathop{\mathrm{arg,max}}\limits_{\theta} \sum_{y_i\in Y} \log p(y_i \mid \theta)
\end{align}
$$</p><p>The model based on MAP is
$$
\begin{align}
\theta_{\mathrm{MAP}}
&= \mathop{\mathrm{arg,max}}\limits_{\theta} p(y \mid \theta) p(\theta) \\<br>&= \mathop{\mathrm{arg,max}}\limits_{\theta} \log p(y \mid \theta) + \log p(\theta) \\<br>&= \mathop{\mathrm{arg,max}}\limits_{\theta} \log \prod_{y_i\in Y} p(y_i \mid \theta) + \log p(\theta) \\<br>&= \mathop{\mathrm{arg,max}}\limits_{\theta} \sum_{y_i\in Y} \log p(y_i \mid \theta) + \log p(\theta)
\end{align}
$$</p></blockquote><p>For example, the likelihood is the margin probability</p><p>$$
p(y\mid \theta) = \int p( y, \mathbf z \mid \theta ) p(\mathbf z) d\mathbf z.
$$</p><p>To work out the integral $p(y\mid \theta)$, numerical methods such as <strong>Monte Carlo</strong> can be utilized. Monte Carlo takes a discrete point of view and find a fair multiset sample ${\cdots, \mathbf z^i, \cdots}$ of the latent space $\mathbf z$. However, the sample space of $\mathbf z$ is usually quite large. To solve this problem, importance sampling comes to the save. Instead of evaluating</p><p>$$
p(y\mid \theta) = \int p( y \mid \theta, \mathbf z ) p(\mathbf z) d\mathbf z,
$$</p><p>we rewrite it as</p><p>$$
p(y\mid \theta) = \int p(y \mid \theta, \mathbf z ) p(\mathbf z) d\mathbf z = \int p(y \mid \theta, \mathbf z ) \frac{f(\mathbf z\mid y)}{f(\mathbf z\mid y)} (p(\mathbf z) d\mathbf z) = \int \frac{p( y \mid \theta, \mathbf z ) p(\mathbf z)}{f(\mathbf z\mid y)} ( f(\mathbf z\mid y)d\mathbf z),
$$
where $f(\mathbf z\mid y)$ is our proposed sampling weight in the $\mathbf z$ space. By carefully choosing $f(\mathbf z\mid y)$, one could simplify the numerical integration.</p><p>Our journey doesn&rsquo;t end here. Using Bayes&rsquo; theorem,</p><p>$$
p(y\mid \theta) = \int \frac{p( y \mid \theta, \mathbf z ) p(\mathbf z)}{f(\mathbf z\mid y)} ( f(\mathbf z\mid y)d\mathbf z) = \int \frac{p( \mathbf z\mid \theta, y ) p(y)}{f(\mathbf z\mid y)} ( f(\mathbf z\mid y)d\mathbf z) .
$$</p><p>The KL divergence</p><p>$$
\begin{align}
KL\left( f(\mathbf{z} \mid y) || p(\mathbf{z} \mid \theta, y) \right) = -\mathcal{L} (y; \theta, f) + \log p(\mathbf{x}\mid \theta)
\end{align}
$$</p><p>Since KL divergence is nonnegative and $\log p(\mathbf{x}\mid \theta)$ is independent of $f$, $\mathcal{L} (y; \theta, f)$ serves as an lower bound of $\log p(\mathbf{x}\mid \theta)$, aka Evidence-lower bound (ELBO).
Instead of dealing with likelihood, we can maximize the ELBO. This is the tricky part. a minimal ELBO due to the choice of $f$ leads to a minimal KL divergence which in turn indicates that $f$ and $p(\mathbf z\mid\theta,y)$ are similar. This is the magic of many methods such as EM algorithm and variational encoders.</p></div><p><div class="has-text-right is-size-7"><span class=icon><i class="fas fa-pencil-alt"></i></span>Published: <time datetime=2021-01-27T00:00:00+00:00>2021-01-27</time>
by <span itemprop=author>Lei Ma</span>;</div></p><div class=is-divider></div><nav class="pagination is-centered" role=navigation aria-label=pagination><a href="https://datumorphism.leima.is/wiki/machine-learning/bayesian/elbo/?ref=footer" class=pagination-previous>« Evidence Lower Bound: ELBO</a></nav></div><div class="column is-4"><div class=is-divider data-content="Cite Me"></div><div class="box is-size-7 has-text-white has-background-black"><article class=media><div class=media-content><div class=content><p>LM (2021). 'Latent Variable Models', Datumorphism, 01 April. Available at: https://datumorphism.leima.is/wiki/machine-learning/bayesian/latent-variable-models/.</p></div></div></article></div><style>#TableOfContents>ul{list-style-type:lower-greek;padding-left:0}#TableOfContents>ul>li ul{list-style-type:none;padding-left:1em}</style><script>const el=document.querySelector('details summary')
el.onclick=()=>{(function(l,o,a,d,e,r){e=o.createElement(a),r=o.getElementsByTagName(a)[0];e.async=1;e.src=d;r.parentNode.insertBefore(e,r)})(window,document,'script','/js/smoothscroll.js');el.onclick=null}
document.querySelectorAll('#TableOfContents a').forEach(link=>{link.addEventListener('click',()=>{document.querySelector(link.href.slice(link.href.indexOf('#'))).scrollIntoView({behavior:'smooth'})})})</script><div class=is-divider data-content=REFERENCES></div><div class="box is-size-7"><article class=media><div class=media-content><div class=content><p><strong>References:</strong><br><ol><li class=has-text-weight-bold><a href style=text-decoration:none>Trevor Hastie, Robert Tibshirani, J. F. (2004). The Elements of Statistical Learning. Springer Science & Business Media.</a></li><li class=has-text-weight-bold><a href style=text-decoration:none>Christpher M. Bishop. (2006). Pattern Recognition and Machine Learning. Springer-Verlag New York.</a></li><li class=has-text-weight-bold><a href=http://akosiorek.github.io/ml/2018/03/14/what_is_wrong_with_vaes.html style=text-decoration:none>What is wrong with VAEs?</a></li></ol></p></div></div></article></div><div class=is-divider data-content=CONNECTUME></div><div class="box is-size-7"><article class=media><div class=media-content style=width:100%><div class=content><p><strong>Current Ref:</strong><br><ul><li style=list-style:none><div>wiki/machine-learning/bayesian/latent-variable-models.md</div></li></ul></p></div></div></article></div><div class="box is-size-7"><article class=media><div class=media-content><div class=content><p><strong>Links to:</strong><div><a class=box href=https://datumorphism.leima.is/cards/statistics/bayes-theorem/><div class=media-content><div class=content><h6>Bayes' Theorem</h6><p>Bayes&rsquo; Theorem is stated as
$$ P(A\mid B) = \frac{P(B \mid A) P(A)}{P(B)} $$
$P(A\mid B)$: …</p></div></div></a><a class=box href=https://datumorphism.leima.is/wiki/machine-learning/basics/kl-divergence/><div class=media-content><div class=content><h6>KL Divergence</h6><p>Kullback–Leibler divergence indicates the differences between two distributions</p></div></div></a><a class=box href=https://datumorphism.leima.is/cards/statistics/likelihood/><div class=media-content><div class=content><h6>Likelihood</h6><p>Likelihood is not necessarily a pdf</p></div></div></a></div></p></div></div></article></div><div class="box is-size-7"><article class=media><div class=media-content><div class=content><p><strong>Links from:</strong><div><a class=box href=https://datumorphism.leima.is/wiki/machine-learning/generative-models/variational-autoencoder/><div class=media-content><div class=content><h6>Variational Auto-Encoder</h6><p>Variational Auto-Encoder (VAE) is very different from Generative Model: Auto-Encoder Generative …</p></div></div></a><a class=box href=https://datumorphism.leima.is/wiki/machine-learning/bayesian/elbo/><div class=media-content><div class=content><h6>Evidence Lower Bound: ELBO</h6><p>ELBO is an very important concept in variational methods</p></div></div></a><a class=box href=https://datumorphism.leima.is/wiki/machine-learning/basics/kl-divergence/><div class=media-content><div class=content><h6>KL Divergence</h6><p>Kullback–Leibler divergence indicates the differences between two distributions</p></div></div></a><a class=box href=https://datumorphism.leima.is/cards/statistics/reparametrization-expectation-sampling/><div class=media-content><div class=content><h6>Reparametrization in Expectation Sampling</h6><p>Reparametrize the sampling distribution to simplify the sampling</p></div></div></a><a class=box href=https://datumorphism.leima.is/reading/normalizing-flow-introduction-1908.09257/><div class=media-content><div class=content><h6>Normalizing Flows: An Introduction and Review of Current Methods</h6><p>To generate complicated distributions step by step from a simple and interpretable distribution.</p></div></div></a></div></p></div></div></article></div><div id=comments class=is-divider data-content=COMMENTS></div><script src=https://giscus.app/client.js data-repo=datumorphism/comments data-repo-id="MDEwOlJlcG9zaXRvcnkxNjU5MDkyNDI=" data-category=Comments data-category-id=DIC_kwDOCeOS-s4B-Zxx data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=1 data-theme=light crossorigin=anonymous async></script></div></div></article></div></section></div><div class=navtools><a class="button is-primary is-light is-outlined" alt="Edit this page" href=https://github.com/datumorphism/datumorphism.github.io/edit/hugo/content/wiki/machine-learning/bayesian/latent-variable-models.md target=blank style=position:fixed;bottom:20px;right:10px;border-radius:9999px;width:35px;height:35px><i class="fas fa-pencil-alt"></i></a><a class="button is-primary is-light is-outlined" href=#comments alt=Comments style=position:fixed;bottom:60px;right:10px;border-radius:9999px;width:35px;height:35px><i class="far fa-comments"></i></a></div></main><footer><footer class=footer><div class=container><div class="content has-text-centered"><p>Created and maintained by <a href=https://leima.is>Lei Ma</a>.
Acknowledgement: <a href=https://gohugo.io/>Hugo</a>,
<a href=https://themes.gohugo.io/bulma/>Bulma</a>, <a href=https://kausalflow.com>KausalFlow</a>.
<strong>love</strong>.<br><a class=tag href=/about>About</a>
<a class=tag href=/index.xml>Feed</a>
<a class=tag href=/data.json>JSON Data</a></p></div></div></footer><script src=https://unpkg.com/applause-button/dist/applause-button.js></script></footer><script async type=text/javascript src=/js/bulma.js></script></body></html>