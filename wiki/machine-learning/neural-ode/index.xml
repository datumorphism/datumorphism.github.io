<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Neural ODE on Datumorphism</title><link>https://datumorphism.leima.is/wiki/machine-learning/neural-ode/</link><description>Recent content in Neural ODE on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Sat, 13 Aug 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://datumorphism.leima.is/wiki/machine-learning/neural-ode/index.xml" rel="self" type="application/rss+xml"/><item><title>Neural ODE</title><link>https://datumorphism.leima.is/wiki/machine-learning/neural-ode/neural-ode-basics/</link><pubDate>Sat, 13 Aug 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/neural-ode/neural-ode-basics/</guid><description>In [[neural networks]] Artificial Neural Networks Simple artificial neural networks using multilayer perceptron , residual connections is a popular architecture to build very deep neural networks1. Apart from residual networks, there are many other designs for deep neural networks23456. These methods share similar ideas that the layered structure in deep neural networks can be treated as a dynamical system and these different architectures are different numerical approaches of solving the dynamical system.
Figure taken from He K, Zhang X, Ren S, Sun J. Deep Residual Learning for Image Recognition. arXiv [cs.CV]. 2015. Available: http://arxiv.org/abs/1512.03385
The degradation problem For deep neural networks, not all deep architectures are able to produce results that are significantly better than shallow architectures.</description></item></channel></rss>