<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Artificial Neural Networks on Datumorphism</title><link>https://datumorphism.leima.is/wiki/machine-learning/neural-networks/</link><description>Recent content in Artificial Neural Networks on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Mon, 15 Feb 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://datumorphism.leima.is/wiki/machine-learning/neural-networks/index.xml" rel="self" type="application/rss+xml"/><item><title>Artificial Neural Networks</title><link>https://datumorphism.leima.is/wiki/machine-learning/neural-networks/artificial-neural-networks/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/neural-networks/artificial-neural-networks/</guid><description>Artificial neural networks works pretty well for solving some differential equations.
Universal Approximators Maxwell Stinchcombe and Halber White proved that no theoretical constraints for the feedforward networks to approximate any measurable function. In principle, one can use feedforward networks to approximate measurable functions to any accuracy.
However, the convergence slows down if we have a lot of hidden units. There is a balance between accuracy and convergence rate. More hidden units lead to slow convergence but more accuracy.
Here is a quick review of the history of this topic.
Kolmogorovâ€™s Theorem
Kolmogorov&amp;rsquo;s theorem shows that one can use a finite number of carefully chosen continuous functions to mix up by sums and multiplication with weights to a continuous multivariable function on a compact set.</description></item><item><title>A Physicist's Crash Course on Artificial Neural Network</title><link>https://datumorphism.leima.is/wiki/machine-learning/neural-networks/physicists-crash-course-neural-network/</link><pubDate>Sat, 02 May 2015 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/neural-networks/physicists-crash-course-neural-network/</guid><description>What is a Neuron What a neuron does is to response when a stimulation is given. This response could be strong or weak or even null. If I would draw a figure, of this behavior, it looks like this.
Neuron response Using simple single neuron responses, we could compose complicated responses. To achieve that, we study the transformations of the response first.
transformations Artificial Neural Network A simple network is a collection of neurons that response to stimulations, which could be the responses of other neurons.
neural network A given input signal is spreaded onto three different neurons.</description></item><item><title>Deep Autoregressive Network</title><link>https://datumorphism.leima.is/wiki/machine-learning/neural-networks/deep-autoregressive-networks/</link><pubDate>Mon, 15 Feb 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/neural-networks/deep-autoregressive-networks/</guid><description>There are two levels of autoregressiveness in the DARN network:
Inlayer autoregressive connections of the nodes, Intralayer autoregressive connections of nodes. The network is trained on MDL loss.</description></item></channel></rss>