<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Artificial Neural Networks on Datumorphism</title><link>https://datumorphism.leima.is/wiki/machine-learning/neural-networks/</link><description>Recent content in Artificial Neural Networks on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Mon, 15 Feb 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://datumorphism.leima.is/wiki/machine-learning/neural-networks/index.xml" rel="self" type="application/rss+xml"/><item><title>Artificial Neural Networks</title><link>https://datumorphism.leima.is/wiki/machine-learning/neural-networks/artificial-neural-networks/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/neural-networks/artificial-neural-networks/</guid><description>Artificial neural networks works pretty well for solving some differential equations.
Universal Approximators Maxwell Stinchcombe and Halber White proved that no theoretical constraints for the feedforward networks to approximate any measurable function. In principle, one can use feedforward networks to approximate measurable functions to any accuracy.
However, the convergence slows down if we have a lot of hidden units. There is a balance between accuracy and convergence rate. More hidden units lead to slow convergence but more accuracy.
Here is a quick review of the history of this topic.
Kolmogorovâ€™s Theorem
Kolmogorov&amp;rsquo;s theorem shows that one can use a finite number of carefully chosen continuous functions to mix up by sums and multiplication with weights to a continuous multivariable function on a compact set.</description></item><item><title>A Physicist's Crash Course on Artificial Neural Network</title><link>https://datumorphism.leima.is/wiki/machine-learning/neural-networks/physicists-crash-course-neural-network/</link><pubDate>Sat, 02 May 2015 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/neural-networks/physicists-crash-course-neural-network/</guid><description>What is a Neuron What a neuron does is to response when a stimulation is given. This response could be strong or weak or even null. If I would draw a figure, of this behavior, it looks like this.
Neuron response Using simple single neuron responses, we could compose complicated responses. To achieve that, we study the transformations of the response first.
transformations Artificial Neural Network A simple network is a collection of neurons that response to stimulations, which could be the responses of other neurons.
neural network A given input signal is spreaded onto three different neurons. The neurons respond to this signal separately then summed together with different weights.</description></item><item><title>Deep Autoregressive Network</title><link>https://datumorphism.leima.is/wiki/machine-learning/neural-networks/deep-autoregressive-networks/</link><pubDate>Mon, 15 Feb 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/neural-networks/deep-autoregressive-networks/</guid><description>There are two levels of autoregressiveness in the DARN network:
Inlayer autoregressive connections of the nodes, Intralayer autoregressive connections of nodes. The network is trained on MDL loss.</description></item><item><title>Layer Norm</title><link>https://datumorphism.leima.is/wiki/machine-learning/neural-networks/layer-norm/</link><pubDate>Mon, 15 Feb 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/neural-networks/layer-norm/</guid><description>Layer norm is a normalization method to enable better training1.
Layer normalization (LayerNorm) is a technique to normalize the distributions of intermediate layers. It enables smoother gradients, faster training, and better generalization accuracy.
Quote from Xu et al. 20191
The key of layer norm is to normalize the input to the layer using the mean and standard deviation.
Layer norm plays two roles in neural networks:
Projects the key vectors onto a hyperplane. Scales the key vectors to have the same length. Xu et al. 2019
Xu2019 Xu J, Sun X, Zhang Z, Zhao G, Lin J. Understanding and Improving Layer Normalization.</description></item><item><title>Batch Norm</title><link>https://datumorphism.leima.is/wiki/machine-learning/neural-networks/batch-norm/</link><pubDate>Mon, 15 Feb 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/neural-networks/batch-norm/</guid><description>Batch norm is a normalization method to relieve the internal covariate shift1.
David Page created a colab notebook to provide some real examples of how batch norm helps with the internal covariance shift.
Rohrer-Batch-Normalization 1.Rohrer B. Batch normalization [Internet]. E2EML School. [cited 2023 Oct 22]. Available from: https://e2eml.school/batch_normalization.html &amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item></channel></rss>