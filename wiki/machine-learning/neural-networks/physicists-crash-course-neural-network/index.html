<!doctype html><html lang=en-us><head><meta name=generator content="Hugo 0.69.2"><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><title>A Physicist's Crash Course on Artificial Neural Network | Datumorphism | Lei Ma</title><meta name=description content="A very very brief introduction to neural network for physicists"><meta name=author content="Lei Ma"><meta property="og:title" content="A Physicist's Crash Course on Artificial Neural Network"><meta property="og:description" content="A very very brief introduction to neural network for physicists"><meta property="og:type" content="article"><meta property="og:url" content="/wiki/machine-learning/neural-networks/physicists-crash-course-neural-network/"><meta property="article:published_time" content="2015-05-02T00:00:00+00:00"><meta property="article:modified_time" content="2015-05-02T00:00:00+00:00"><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-140452515-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><link rel=canonical href=/wiki/machine-learning/neural-networks/physicists-crash-course-neural-network/><link rel="shortcut icon" type=image/png href=/logos/logo-square.png><link rel=stylesheet href=/css/bulma.css><link rel=stylesheet href=/css/bulma-divider.min.css><link rel=stylesheet href=/assets/css/bulma-ribbon.min.css><link rel=stylesheet href=/assets/css/tooltip.css><link rel=stylesheet href=https://jenil.github.io/bulmaswatch/united/bulmaswatch.min.css><link rel=stylesheet href=/css/custom.css><link rel=stylesheet href=/css/blog-post.css><link rel=stylesheet href=/css/code-highlighting/dark.css><link rel=stylesheet href=/css/custom.css><link rel=stylesheet href=https://unpkg.com/applause-button/dist/applause-button.css><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],tags:'ams'},svg:{fontCache:'global'}};</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js></script><script src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script>mermaid.initialize({startOnLoad:true});</script><script src=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/js/all.min.js integrity="sha256-KzZiKy0DWYsnwMF+X1DvQngQ2/FxF7MF3Ff72XcpuPs=" crossorigin=anonymous></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css integrity="sha256-h20CPZ0QyXlBuAw7A+KluUYx/3pK+c7lYEpqLTlxjYQ=" crossorigin=anonymous></head><body><header><nav class="navbar is-transparent"><div class=navbar-brand><a class=navbar-item href=/><img src=/site/intelligence.png alt=Datumorphism height=28 style=margin-right:.5em> Datumorphism</a><div class="navbar-burger burger" data-target=navMenu><span></span><span></span><span></span></div></div><div class=navbar-menu id=navMenu><div class=navbar-start><div class="navbar-item has-dropdown is-hoverable"><a href=/projects class=navbar-link>Notebooks</a><div class=navbar-dropdown><a href=/awesome/ class=navbar-item>Awesome</a>
<a href=/blog/ class=navbar-item>Blog</a>
<a href=/cards/ class=navbar-item>Cards</a>
<a href=/reading/ class=navbar-item>Reading Notes</a>
<a href=/til/ class=navbar-item>TIL</a>
<a href=/wiki/ class=navbar-item>Wiki</a></div></div></div><span class=navbar-burger><span></span><span></span><span></span></span><div class=navbar-end><div class=navbar-item><a class=navbar-item target=blank href=/blog/>Blog</a>
<a class=navbar-item target=blank href=/tags/><i class="fas fa-tags"></i></a><a class=navbar-item target=blank href=/graph><i class="fas fa-project-diagram"></i></a><a class=navbar-item target=blank href=https://github.com/datumorphism><span class=icon><i class="fab fa-github"></i></span></a></div></div></div></nav><script>document.addEventListener('DOMContentLoaded',()=>{const $navbarBurgers=Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'),0);if($navbarBurgers.length>0){$navbarBurgers.forEach(el=>{el.addEventListener('click',()=>{const target=el.dataset.target;const $target=document.getElementById(target);el.classList.toggle('is-active');$target.classList.toggle('is-active');});});}});</script></header><main><div class=container itemscope itemtype=http://schema.org/BlogPosting><meta itemprop=name content="A Physicist's Crash Course on Artificial Neural Network"><meta itemprop=description content="A very very brief introduction to neural network for physicists"><meta itemprop=datePublished content="2015-05-02T00:00:00+00:00"><meta itemprop=dateModified content="2015-05-02T00:00:00+00:00"><meta itemprop=wordCount content="1418"><meta itemprop=keywords content><section class=section><div class=container><article class=post><header class=post-header><nav class="breadcrumb has-succeeds-separator is-small" aria-label=breadcrumbs><ul><li><a href=/>Datumorphism</a></li><li><a href=/wiki/>Wiki</a></li><li><a href=/wiki/machine-learning/>Machine Learning</a></li><li><a href=/wiki/machine-learning/neural-networks/>Artificial Neural Networks</a></li><li class=active><a href=/wiki/machine-learning/neural-networks/physicists-crash-course-neural-network/>A Physicist's Crash Course on Artificial Neural Network</a></li></ul></nav><h1 class="post-title has-text-centered is-size-1" itemprop="name headline">A Physicist's Crash Course on Artificial Neural Network</h1><h2 class="title is-6 has-text-centered"><i class="fas fa-tags" style=margin-right:.5em></i></h2></header><div class=columns><div class="column is-8"><div class=is-divider data-content=NOTIFICATION></div><div class="notification is-warning is-light"><p>This is the notes for a class presentation when I was a student.</p></div><div class=is-divider data-content=ARTICLE></div><div class="content blog-post section" itemprop=articleBody><h2 id=what-is-a-neuron>What is a Neuron</h2><p>What a neuron does is to response when a stimulation is given. This response could be strong or weak or even null. If I would draw a figure, of this behavior, it looks like this.</p><figure><img src=../assets/physicists-crash-course-neural-network/neuron-response.png><figcaption><h4>Neuron response</h4></figcaption></figure><p>Using simple single neuron responses, we could compose complicated responses. To achieve that, we study the transformations of the response first.</p><figure><img src=../assets/physicists-crash-course-neural-network/transformation-activation.png><figcaption><h4>transformations</h4></figcaption></figure><h2 id=artificial-neural-network>Artificial Neural Network</h2><p>A simple network is a collection of neurons that response to stimulations, which could be the responses of other neurons.</p><figure><img src=../assets/physicists-crash-course-neural-network/neural-network-simple.png><figcaption><h4>neural network</h4></figcaption></figure><p>A given input signal is spreaded onto three different neurons. The neurons respond to this signal separately then summed together with different weights. In the language of math, given input $x$, output $y(x)$ is</p><p>$$ y(x) = \sum_{k=1}^{3} x v_k * \text{activation}( w_k * x + u_k ) $$</p><p>where $\text{activation}$ is the activation function, i.e., the response behavior of the neuron. This is a single layer structure.</p><p>A lot of different ways could be used to extend this network.</p><ul><li>Increase the number of neurons on one layer.</li><li>One can extend the number of layers.</li></ul><figure><img src=../assets/physicists-crash-course-neural-network/multilayer.png><figcaption><h4>Multilayer</h4></figcaption></figure><ul><li>We could also include interactions between the neurons.</li><li>Even memory can be simulated.</li></ul><h2 id=how-it-works>How it works</h2><p>Here is an example of how the network works.</p><p>We are going to do two things.</p><ol><li>Find out which temperature is hot which is cold.</li><li>Find out which room is habitable in terms of temperature.</li></ol><p>The first task can be done using one neuron. If a set of parameters are properly chosen, a single neuron will finish the task.</p><p>We have a input temperature and a output that tells us which is high temperature which is low temperature. In the following example, $T_1$ is low temperature, $T_2$, $T_3$ are high temperatures.</p><figure><img src=../assets/physicists-crash-course-neural-network/one-neuron-classification.png><figcaption><h4>one neuron classification</h4></figcaption></figure><p>Suppose we have only two neurons in the network.</p><figure><img src=../assets/physicists-crash-course-neural-network/two-neuron-network.png><figcaption><h4>two neuron classification</h4></figcaption></figure><p>Seen from this example, we can expect neural network to be good at classification.</p><p>And how is this going to help us with the identification of habitable places? Suppose we have three room with temperature $T_1$, $T_2$, $T_3$ respectively. Only $T_2$ falls into the region of high output value which corresponds to the habitable temperature in our net.</p><figure><img src=../assets/physicists-crash-course-neural-network/two-neuron-classification-result.png><figcaption><h4>two neuron classification</h4></figcaption></figure><p>That reminds me of Fourier Analysis. And there is a connection. The activation functions here, which is for a general purpose, are chosen to be universal approximators. These activations can be used to approximate all smooth functions well using a finite number of neurons. Fourier analysis, on the other hand requires infinite to be exact. However in some cases we don&rsquo;t need infinite Fourier terms as we only need a good approximation.</p><h2 id=training>Training</h2><p>We have got a lot of parameters with the set up of the network. The parameters are the degree of freedom we have. <strong>The question is how to get the right parameters.</strong></p><p>The Network NEEDS TRAINING. Just like human learning, the neural network have to be trained using prepared data. One example would be</p><table><thead><tr><th align=center>input</th><th align=center>0</th><th align=center>1</th><th align=center>2</th><th align=center>3</th><th align=center>4</th><th align=center>5</th><th align=center>6</th><th align=center>7</th><th align=center>8</th><th align=center>9</th></tr></thead><tbody><tr><td align=center>output</td><td align=center>1</td><td align=center>0.37</td><td align=center>0.14</td><td align=center>0.050</td><td align=center>0.018</td><td align=center>0.0067</td><td align=center>0.0025</td><td align=center>0.00091</td><td align=center>0.0004</td><td align=center>0.00012</td></tr></tbody></table><p>This set of data, to have some insight, a human would put them on a plot.</p><figure><img src=../assets/physicists-crash-course-neural-network/data-points.png></figure><blockquote><p>Data</p><pre><code>thTrainingData1 = np.array([[0,1,2,3,4,5,6,7,8,9],[1,0.37,0.14,0.050,0.018,0.0067,0.0025,0.00091,0.0004,0.00012]])
plt.figure(figsize=(15,9))
plt.plot(thTrainingData1[0],thTrainingData1[1],'bs')
plt.title(&quot;We have data&quot;)
plt.show()
</code></pre></blockquote><p>A well-trained human immediately recognizes the pattern, which is somewhat close to a exponential decay behavior. However, we wouldn&rsquo;t do any numerical calculation of exponential functions. We just see it because we have seen a lot.</p><p>We also expect that if we have one more data point from the same mechanism to be placed on the graph, it should appear near a line of exponential decay.</p><figure><img src=../assets/physicists-crash-course-neural-network/data-fit.png></figure><blockquote><p>Data</p><pre><code>plt.figure(figsize=(15,9))
plt.plot(thTrainingData1[0],thTrainingData1[1],'bs',label='Old Data')
plt.plot(np.linspace(0,9,100),np.exp(-np.linspace(0,9,100)),'m',label='Exponential')
plt.plot(np.array([1.5]),np.array([0.21]),'ro',label='New Data')
plt.title(&quot;We have got new data&quot;)
plt.legend()
plt.show()
</code></pre></blockquote><p>OK. A red data point appears on the graph. It is close to the line of $f(x)=e^{-x}$. Not exactly on the line but close.</p><p>How do we teach a machine to do that? Train it using these 10 data point we have now.</p><p>We feed in one input, $0$, the net will give us a result. However, the result is not $1$ in general. Then we change the parameters until we have the output as $1$. We feed in another input $1$, and change the parameters until we have $0.37$. Hence the final outcome of this training process is that the machine gives us the right result given input in the data we have know.</p><p>After the training, we get a set of parameters, which should be preserved. This is the structure we need. The machine has learned everything.</p><p>Now keep the parameters fixed. Feed in a number $1.5$, the result from the net should be close to $2.2$. Now the net also learn the behavior of the data, nonetheless without calculating any exponential functions.</p><p>This is amazing. The machine doesn&rsquo;t have any idea of exponential but it can know the trend.</p><h2 id=problems>Problems</h2><p>Just as a human would do, the net can make mistakes. One of them is over-training. If we don&rsquo;t have enough degree of freedom and given to many data points, the network could be over-trained. It is going to be stubborn and resist to give the right trend.</p><h2 id=code-practice>Code Practice</h2><h3 id=solving-a-simple-differential-equation>Solving A Simple Differential Equation</h3><p>The problem to solve is the differential equation $\frac{d}{dt}y(t)= - y(t).$ Using the network, this is $$y_i= 1+t_i v_k f(t_i w_k+u_k).$$</p><p>The procedures are</p><p><strong>Deal with the function first.</strong></p><ol><li><p>The cost is $I=\sum_i\left( \frac{dy_i}{dt}+y_i \right)^2.$ Our purpose is to minimize this cost.</p></li><li><p>To calculate the differential of y, we can write down the explicit expression for it.</p><p>$$\frac{dy}{dt} = v_k f(t w_k+u_k) + t v_k f(tw_k+u_k) (1-f(tw_k+u_k))w_k,$$</p><p>where the function f is defined as a <code>trigf()</code>.</p></li><li><p>So the cost becomes</p><p>$$I = \sum_i \left( v_k f(t w_k+u_k) + t v_k f(tw_k+u_k) (1-f(tw_k+u_k)) w_k + y \right)^2.$$</p></li></ol><pre><code>def cost(v,w,u,t):
    v = np.array(v)   # Don't know why but np.asarray(v) doesn't work here.
    w = np.array(w)
    u = np.array(u)

    fvec = np.array(trigf(t*w + u) )  # This is a vector!!!
    yt = 1 + np.sum ( t * v * fvec  )  # For a given t, this calculates the value of y(t), given the parameters, v, w, u.

    return  ( np.sum (v*fvec + t * v* fvec * ( 1 -  fvec  ) * w ) + yt )   ** 2

    # return np.sum(np.array( v*np.array( trigf( np.array( t*w ) + u ) ) )  +  np.array( t*np.array( v*np.array( trigf(np.array( t*w ) + u)) )  )  * ( 1 - np.array( trigf( np.array( t*w )+u) ) ) * w + ( 1 + np.array( t*np.array( v*np.array( trigf( np.array(t*w)+u ) ) ) ) ) ) # trigf() should return an array with the same length of the input.
</code></pre><p><strong>Caution: a number times an array is not returned as array but instead as list. and list + list doesn&rsquo;t conserved the length of the list!</strong></p><p>Define the trigf() next, usually we use $$trigf(x)=\frac{1}{1+\exp(-x)}$$.</p><pre><code>def trigf(x):
    #return 1/(1+np.exp(-x)) #
    return expit(x)
</code></pre><p>Test cost function:</p><pre><code>test11 = np.ones(30)
cost(np.array([1,1,1]),[1,1,1],[1,1,1],1)
</code></pre><p>Next step is to optimize this cost. To do this we need the derivitive. But anyway let&rsquo;s try a simple minimization first.</p><pre><code>def costTotal(v,w,u,t):
    t = np.array(t)
    costt = 0
    for temp in t:
        costt = costt + cost(v,w,u,temp)

    return costt
</code></pre><p>Test total cost</p><pre><code>test11 = np.ones(30)
tlintest = np.linspace(0,1,2)
print costTotal(np.ones(10),np.ones(10),2*np.ones(10),tlintest)
print costTotal(np.ones(10),np.ones(10),np.ones(10),tlintest)
</code></pre><p>Suppose the parameters are five-dimensional and we have 10 data points.</p><pre><code>tlin = np.linspace(0,5,11)
print tlin
</code></pre><p>Define a list divider that splits an array into three arrays.</p><pre><code>## No need to define such a function! Use np.split(x,3) instead.
np.zeros(30)
</code></pre><pre><code># This is only an example of 2dimensional neural network.
costTotalF = lambda x: costTotal(np.split(x,3)[0],np.split(x,3)[1],np.split(x,3)[2],tlin)

initGuess = np.zeros(30)
# initGuess = np.random.rand(1,30)+2
start1 = timeit.default_timer()
minim=minimize(costTotalF,initGuess,method=&quot;Nelder-Mead&quot;)
# minimize(costTotalF,initGuess,method=&quot;L-BFGS-B&quot;)
# minimize(costTotalF,initGuess,method=&quot;TNC&quot;)
stop1 = timeit.default_timer()

print stop1 - start1
</code></pre><p>It shows that the minimization depends greatly on the initial guess. It is not true for a simple scenario with gradient descent however it could be the case if the landscape is too complicated.</p><h3 id=test-results>Test Results</h3><p>Plot!</p><pre><code>def functionYNNSt(v,w,u,t): # t is a single scalar value

    t = np.array(t)

    return 1 +  np.sum(t * v * trigf( t*w +u ) )



def functionYNN(v,w,u,t):

    t = np.array(t)

    func = np.asarray([])

    for temp in t:
        func = np.append(func, functionYNNSt(v,w,u,temp) )

    return np.array(func)

def functionY(t):

    return np.exp(-t)


xresult=minim.x

# This is the output of neural network
temp14 = np.array([])
for i in np.linspace(0,5,110):
    temp14 = np.append(temp14,functionYNN(np.split(xresult,3)[0],np.split(xresult,3)[1],np.split(xresult,3)[2],np.array([i]))[0])

testTLin = np.linspace(0,5,110)
plt.figure(figsize=(15,9))
plt.plot(testTLin,functionY(testTLin),'bs',label='Exact Solution') # This is the exact solution to the differential equation
plt.plot(testTLin,temp14,'r-',label='ANN Result')
plt.title(&quot;Test the training&quot;)
plt.legend()
plt.show()
</code></pre><figure><img src=../assets/physicists-crash-course-neural-network/fit.png></figure></div><p><div class="has-text-right is-size-7"><span class=icon><i class="fas fa-pencil-alt"></i></span>Published: <time datetime=2015-05-02T00:00:00+00:00>2015-05-02</time>
by <span itemprop=author>Lei Ma</span>;</div></p><div class=is-divider></div><nav class="pagination is-centered" role=navigation aria-label=pagination><a href="/wiki/machine-learning/neural-networks/artificial-neural-networks/?ref=footer" class=pagination-previous>« Artificial Neural Networks</a>
<a class=pagination-next href="/wiki/machine-learning/neural-networks/boltzmann-machine/?ref=footer">Boltzmann Machine »</a></nav></div><div class="column is-4"><style>#TableOfContents>ul{list-style-type:lower-greek;padding-left:0}#TableOfContents>ul>li ul{list-style-type:none;padding-left:1em}</style><div class=is-divider data-content=ToC></div><div class="box is-size-7"><article class=media><div class=media-content><div class=content><details><summary>Table of Contents</summary><div><div><nav id=TableOfContents><ul><li><a href=#what-is-a-neuron>What is a Neuron</a></li><li><a href=#artificial-neural-network>Artificial Neural Network</a></li><li><a href=#how-it-works>How it works</a></li><li><a href=#training>Training</a></li><li><a href=#problems>Problems</a></li><li><a href=#code-practice>Code Practice</a><ul><li><a href=#solving-a-simple-differential-equation>Solving A Simple Differential Equation</a></li><li><a href=#test-results>Test Results</a></li></ul></li></ul></nav></div></div></details></div></div></article></div><script>const el=document.querySelector('details summary')
el.onclick=()=>{(function(l,o,a,d,e,r){e=o.createElement(a),r=o.getElementsByTagName(a)[0];e.async=1;e.src=d;r.parentNode.insertBefore(e,r)})(window,document,'script','/js/smoothscroll.js');el.onclick=null}
document.querySelectorAll('#TableOfContents a').forEach(link=>{link.addEventListener('click',()=>{document.querySelector(link.href.slice(link.href.indexOf('#'))).scrollIntoView({behavior:'smooth'})})})</script><div class=is-divider data-content=REFERENCES></div><div class="box is-size-7"><article class=media><div class=media-content><div class=content><p><strong>References:</strong><br><ol><li class=has-text-weight-bold><a href style=text-decoration:none></a></li></ol></p></div></div></article></div><div class=is-divider data-content=CONNECTUME></div><div class="box is-size-7"><article class=media><div class=media-content><div class=content><p><strong>Current Ref:</strong><br><ul><li style=list-style:none><span class="tag is-primary is-light has-text-weight-bold">wiki/machine-learning/neural-networks/physicists-crash-course-neural-network.md</span></li></ul></p></div></div></article></div><div id=comments class=is-divider data-content=COMMENTS></div><script src=https://utteranc.es/client.js repo=datumorphism/comments issue-term=pathname theme=github-light crossorigin=anonymous async></script></div></div></article></div></section></div><div class=navtools><a class="button is-primary is-light is-outlined" alt="Edit this page" href=https://github.com/datumorphism/datumorphism.github.io/edit/hugo/content/wiki/machine-learning/neural-networks/physicists-crash-course-neural-network.md target=blank style=position:fixed;bottom:20px;right:10px;border-radius:9999px;width:35px;height:35px><i class="fas fa-pencil-alt"></i></a><a class="button is-primary is-light is-outlined" href=#comments alt=Comments style=position:fixed;bottom:60px;right:10px;border-radius:9999px;width:35px;height:35px><i class="far fa-comments"></i></a><style>applause-button .count-container{top:-60%!important}</style><applause-button color=red multiclap=true style="width: 35px; height: 35px;position:fixed;bottom: 100px;right: 10px;"></div></main><footer><footer class=footer><div class=container><div class="content has-text-centered"><p>Created and maintained by <a href=/>Lei Ma</a>.
Acknowledgement: <a href=https://gohugo.io/>Hugo</a>,
<a href=https://themes.gohugo.io/bulma/>Bulma</a>, <a href=https://kausalflow.com>KausalFlow</a>.
<strong>love</strong>.<br><a class=tag href=/about>About</a>
<a class=tag href=/index.xml>Feed</a>
<a class=tag href=/data.json>JSON Data</a></p></div></div></footer><script src=https://unpkg.com/applause-button/dist/applause-button.js></script></footer><script async type=text/javascript src=/js/bulma.js></script></body></html>