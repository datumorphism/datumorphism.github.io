<!doctype html><html lang=en-us><head><meta name=generator content="Hugo 0.69.2"><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><title>Artificial Neural Networks | Datumorphism | Lei Ma</title><meta name=description content="Solving PDEs"><meta name=author content="Lei Ma"><meta property="og:title" content="Artificial Neural Networks"><meta property="og:description" content="Solving PDEs"><meta property="og:type" content="article"><meta property="og:url" content="/wiki/machine-learning/neural-networks/artificial-neural-networks/"><meta property="article:published_time" content="2018-11-19T00:00:00+00:00"><meta property="article:modified_time" content="2018-11-19T00:00:00+00:00"><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-140452515-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><link rel=canonical href=/wiki/machine-learning/neural-networks/artificial-neural-networks/><link rel="shortcut icon" type=image/png href=/logos/logo-square.png><link rel=stylesheet href=/css/bulma.css><link rel=stylesheet href=/css/bulma-divider.min.css><link rel=stylesheet href=/assets/css/bulma-ribbon.min.css><link rel=stylesheet href=/assets/css/tooltip.css><link rel=stylesheet href=https://jenil.github.io/bulmaswatch/united/bulmaswatch.min.css><link rel=stylesheet href=/css/custom.css><link rel=stylesheet href=/css/blog-post.css><link rel=stylesheet href=/css/code-highlighting/dark.css><link rel=stylesheet href=/css/custom.css><link rel=stylesheet href=https://unpkg.com/applause-button/dist/applause-button.css><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],tags:'ams',processEscapes:true,processEnvironments:true},options:{skipHtmlTags:['script','noscript','style','textarea','pre']},svg:{fontCache:'global'}};window.addEventListener('load',(event)=>{document.querySelectorAll("mjx-container").forEach(function(x){x.parentElement.classList+='has-jax'})});</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script>mermaid.initialize({startOnLoad:true});</script><script src=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/js/all.min.js integrity="sha256-KzZiKy0DWYsnwMF+X1DvQngQ2/FxF7MF3Ff72XcpuPs=" crossorigin=anonymous></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css integrity="sha256-h20CPZ0QyXlBuAw7A+KluUYx/3pK+c7lYEpqLTlxjYQ=" crossorigin=anonymous></head><body><header><nav class="navbar is-transparent"><div class=navbar-brand><a class=navbar-item href=/><img src=/site/intelligence.png alt=Datumorphism height=28 style=margin-right:.5em> Datumorphism</a><div class="navbar-burger burger" data-target=navMenu><span></span><span></span><span></span></div></div><div class=navbar-menu id=navMenu><div class=navbar-start><div class="navbar-item has-dropdown is-hoverable"><a href=/projects class=navbar-link>Notebooks</a><div class=navbar-dropdown><a href=/awesome/ class=navbar-item>Awesome</a>
<a href=/blog/ class=navbar-item>Blog</a>
<a href=/cards/ class=navbar-item>Cards</a>
<a href=/hologram/ class=navbar-item>Hologram</a>
<a href=/reading/ class=navbar-item>Reading Notes</a>
<a href=/til/ class=navbar-item>TIL</a>
<a href=/wiki/ class=navbar-item>Wiki</a></div></div></div><span class=navbar-burger><span></span><span></span><span></span></span><div class=navbar-end><div class=navbar-item><a class=navbar-item href=/blog/>Blog</a>
<a class=navbar-item href=/tags/><i class="fas fa-tags"></i></a><a class=navbar-item href=/graph><i class="fas fa-project-diagram"></i></a><a class=navbar-item href=https://t.me/amneumarkt><i class="fab fa-telegram"></i></a><a class=navbar-item target=blank href=https://github.com/datumorphism><span class=icon><i class="fab fa-github"></i></span></a></div></div></div></nav><script>document.addEventListener('DOMContentLoaded',()=>{const $navbarBurgers=Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'),0);if($navbarBurgers.length>0){$navbarBurgers.forEach(el=>{el.addEventListener('click',()=>{const target=el.dataset.target;const $target=document.getElementById(target);el.classList.toggle('is-active');$target.classList.toggle('is-active');});});}});</script></header><main><div class=container itemscope itemtype=http://schema.org/BlogPosting><meta itemprop=name content="Artificial Neural Networks"><meta itemprop=description content="Solving PDEs"><meta itemprop=datePublished content="2018-11-19T00:00:00+00:00"><meta itemprop=dateModified content="2018-11-19T00:00:00+00:00"><meta itemprop=wordCount content="825"><meta itemprop=keywords content="Machine Learning,Artificial Neural Networks,Basics,"><section class=section><div class=container><article class=post><header class=post-header><nav class="breadcrumb has-succeeds-separator is-small" aria-label=breadcrumbs><ul><li><a href=/>Datumorphism</a></li><li><a href=/wiki/>Wiki</a></li><li><a href=/wiki/machine-learning/>Machine Learning</a></li><li><a href=/wiki/machine-learning/neural-networks/>Artificial Neural Networks</a></li><li class=active><a href=/wiki/machine-learning/neural-networks/artificial-neural-networks/>Artificial Neural Networks</a></li></ul></nav><h1 class="post-title has-text-centered is-size-1" itemprop="name headline">Artificial Neural Networks</h1><h2 class="title is-6 has-text-centered"><i class="fas fa-tags" style=margin-right:.5em></i><a href=/tags/machine-learning><span class="tag is-warning is-small is-light">#Machine Learning</span></a>
<a href=/tags/artificial-neural-networks><span class="tag is-warning is-small is-light">#Artificial Neural Networks</span></a>
<a href=/tags/basics><span class="tag is-warning is-small is-light">#Basics</span></a></h2></header><div class=columns><div class="column is-8"><div class=is-divider data-content=ARTICLE></div><div class="content blog-post section" itemprop=articleBody><p>Artificial neural networks works pretty well for solving some differential equations.</p><h2 id=universal-approximators>Universal Approximators</h2><p>Maxwell Stinchcombe and Halber White proved that no theoretical constraints for the feedforward networks to approximate any measurable function. In principle, one can use feedforward networks to approximate measurable functions to any accuracy.</p><p>However, the convergence slows down if we have a lot of hidden units. There is a balance between accuracy and convergence rate. More hidden units lead to slow convergence but more accuracy.</p><p>Here is a quick review of the history of this topic.</p><article class="message is-info is-light"><div class=message-header><p>Kolmogorov’s Theorem</p></div><div class=message-body><p>Kolmogorov&rsquo;s theorem shows that one can use a finite number of carefully chosen continuous functions to mix up by sums and multiplication with weights to a continuous multivariable function on a compact set.</p><p><a href=http://neuron.eng.wayne.edu/tarek/MITbook/chap2/2_3.html>Here is the exact math.</a></p></div></article><ol><li><p>Cybenko 1989</p><p>Cybenko proved that</p><p>$$
\sum_k v_k \sigma(w_k x + u_k)
$$</p><p>is a good approximation of continuous functions because it is dense in continuous function space. In this result, $\sigma$ is a continuous sigmoidal function and the parameters are real.</p></li><li><p>Hornik 1989</p><p>&ldquo;Single hidden layer feedforward networks can approximate any measurable functions arbitrarily well regardless of the activation function, the dimension of the input and the input space environment.&rdquo;</p><p>Reference: <a href=http://deeplearning.cs.cmu.edu/notes/Sonia_Hornik.pdf>http://deeplearning.cs.cmu.edu/notes/Sonia_Hornik.pdf</a></p></li></ol><article class="message is-info is-light"><div class=message-header><p>Dense</p></div><div class=message-body>Set A is dense in set X means that we can use A to arbitarily approximate X. Mathematically for any given element in X, the neighbour of x always has nonzero intersection.</div></article><article class="message is-info is-light"><div class=message-header><p>Measurable Function</p></div><div class=message-body>It means the function is continuous.</div></article><h2 id=activation-functions>Activation Functions</h2><ol><li><p>Uni-Polar Sigmoid Function</p><p>$$
\frac{1}{1+e^{-x}}
$$</p><figure><img src=../assets/artificial-neural-networks/sigmoidFunction.png><figcaption><h4>Sigmoid function</h4></figcaption></figure></li><li><p>Bipolar Sigmoid Function</p><p>$$
\frac{1-e^{-x}}{1+e^{-x}}
$$</p><figure><img src=../assets/artificial-neural-networks/bipolarSigmoid.png><figcaption><h4>Bipolar Sigmoid</h4></figcaption></figure></li><li><p>Hyperbolic Tangent</p><p>$$
\tanh(x) = \frac{\sinh(x)}{\cosh(x)} = \frac{e^{x} - e^{-x}}{e^x + e^{-x}}
$$</p><figure><img src=../assets/artificial-neural-networks/tanh.png><figcaption><h4>Hyperbolic tangent</h4></figcaption></figure></li><li><p>Radial Basis Function</p><figure><img src=../assets/artificial-neural-networks/unnormalized_radial_basis_functions.svg.png alt="Two unnormalized Gaussian radial basis functions in one input dimension. The basis function centers are located at x1=0.75 and x2=3.25. Source Unnormalized Radial Basis Functions"><figcaption><h4>Hyperbolic tangent</h4><p>Two unnormalized Gaussian radial basis functions in one input dimension. The basis function centers are located at x1=0.75 and x2=3.25. Source <a href=https://en.wikipedia.org/wiki/Radial_basis_function#/media/File:Unnormalized_radial_basis_functions.svg>Unnormalized Radial Basis Functions</a></p></figcaption></figure></li><li><p>Conic Section Function</p></li></ol><h2 id=solving-differential-equations>Solving Differential Equations</h2><p>The problem here to solve is</p><p>$$
\frac{d}{dt}y(t)= - y(t),
$$</p><p>with initial condition $y(0)=1$.</p><p>To construct a single layered neural network, the function is decomposed using</p>$$
\begin{align}
y(t_i) & = y(t_0) + t_i v_k f(t_i w_k+u_k) \\
&= 1+t_i v_k f(t_i w_k+u_k) ,
\end{align}
$$<p>where $y(t_0)$ is the initial condition and $k$ is summed over.</p><article class="message is-light is-light"><div class=message-header><p>Articifial Neural Network</p></div><div class=message-body></div></article><p>Presumably this should be the gate controlling trigering of the neuron or not. Therefore the following expit function serves this purpose well,</p><p>$$
f(x) = \frac{1}{1+\exp(-x)}.
$$</p><p>One important reason for chosing this is that a lot of expressions can be calculated analytically and easily.</p><article class="message is-light is-light"><div class=message-header><p>Fermi-Dirac Distribution</p></div><div class=message-body>Aha, the Fermi-Dirac distribution.</div></article><p>With the form of the function to be solved, we can define a cost</p><p>$$
I=\sum_i\left( \frac{dy}{dt}(t_i)+y(t_i) \right)^2,
$$</p><p>which should be minimized to 0 if our struture of networks is optimized for this problem.</p><p>Now the task becomes clear:</p><ol><li>Write down the cost analytically;</li><li>Minimized cost to find structure;</li><li>Substitute back to the function and we are done.</li></ol><h2 id=overfitting>Overfitting</h2><p>It is possible that we could over fit a network so that it works only for the training data. To avoid that, people use several strategies.</p><ol><li>Split data into two parts, one for training and one for testing. <a href="https://www.youtube.com/watch?v=S4ZUwgesjS8">A youtube video</a></li><li>Throw more data in. At least 10 times as many as examples as the DoFs of the model. <a href="https://www.youtube.com/watch?v=S4ZUwgesjS8">A youtube video</a></li><li>Regularization by plugin a artifical term to the cost function, as an example we could add the . <a href="https://www.youtube.com/watch?v=S4ZUwgesjS8">A youtube video</a></li></ol><h2 id=neural-network-and-finite-element-method>Neural Network and Finite Element Method</h2><p>We consider the solution to a differential equation</p><p>$$
\mathcal L \psi - f = 0.
$$</p><p>Neural network is quite similar to finite element method. In terms of finite element method, we can write down a neural network structured form of a function <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></p><p>$$
\psi(x_i) = A(x_i) + F(x_i, \mathcal N_i),
$$</p><p>where $\mathcal N$ is the neural network structure. Specifically,</p><p>$$
\mathcal N_i = \sigma( w_{ij} x_j + u_i ).
$$</p><p>The function is parameterized using the network. Such parameterization is similar to collocation method in finite element method, where multiple basis is used for each location.</p><p>One of the choices of the function $F$ is a linear combination,</p><p>$$
F(x_i, \mathcal N_i) = x_i \mathcal N_i,
$$</p><p>and $A(x_i)$ should take care of the boundary condition.</p><article class="message is-light is-light"><div class=message-header><p>Relation to finite element method</p></div><div class=message-body>This function is similar to the finite element function basis approximation. The goal in finite element method is to find the coefficients of each basis functions to achieve a good approximation. In ANN method, each sigmoid is the analogy to the basis functions, where we are looking for both the coefficients of sigmoids and the parameters of them. These sigmoid functions are some kind of adaptive basis functions.</div></article><p>With such parameterization, the differential equation itself is parameterized such that</p><p>$$
\mathcal L \psi - f = 0,
$$</p><p>such that the minimization should be</p><p>$$
\lvert \mathcal L \psi - f \rvert^2 \to 0
$$</p><p>at each point.</p><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p>Freitag, K. J. (2007). Neural networks and differential equations. <a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></div><p><div class="has-text-right is-size-7"><span class=icon><i class="fas fa-pencil-alt"></i></span>Published: <time datetime=2018-11-19T00:00:00+00:00>2018-11-19</time>
by <span itemprop=author>Lei Ma</span>;</div></p><div class=is-divider></div><nav class="pagination is-centered" role=navigation aria-label=pagination><a class=pagination-next href="/wiki/machine-learning/neural-networks/physicists-crash-course-neural-network/?ref=footer">A Physicist's Crash Course on Artificial Neural... »</a></nav></div><div class="column is-4"><style>#TableOfContents>ul{list-style-type:lower-greek;padding-left:0}#TableOfContents>ul>li ul{list-style-type:none;padding-left:1em}</style><div class=is-divider data-content=ToC></div><div class="box is-size-7"><article class=media><div class=media-content><div class=content><details><summary>Table of Contents</summary><div><div><nav id=TableOfContents><ul><li><a href=#universal-approximators>Universal Approximators</a></li><li><a href=#activation-functions>Activation Functions</a></li><li><a href=#solving-differential-equations>Solving Differential Equations</a></li><li><a href=#overfitting>Overfitting</a></li><li><a href=#neural-network-and-finite-element-method>Neural Network and Finite Element Method</a></li></ul></nav></div></div></details></div></div></article></div><script>const el=document.querySelector('details summary')
el.onclick=()=>{(function(l,o,a,d,e,r){e=o.createElement(a),r=o.getElementsByTagName(a)[0];e.async=1;e.src=d;r.parentNode.insertBefore(e,r)})(window,document,'script','/js/smoothscroll.js');el.onclick=null}
document.querySelectorAll('#TableOfContents a').forEach(link=>{link.addEventListener('click',()=>{document.querySelector(link.href.slice(link.href.indexOf('#'))).scrollIntoView({behavior:'smooth'})})})</script><div class=is-divider data-content=REFERENCES></div><div class="box is-size-7"><article class=media><div class=media-content><div class=content><p><strong>References:</strong><br><ol><li class=has-text-weight-bold><a href=https://doi.org/10.1016/0893-6080%2889%2990020-8 style=text-decoration:none>Hornik, K., Stinchcombe, M., & White, H. (1989). Multilayer feedforward networks are universal approximators. Neural Networks, 2(5), 359–366.</a></li><li class=has-text-weight-bold><a href=https://doi.org/10.1007/BF02551274 style=text-decoration:none>Cybenko, G. (1989). Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals, and Systems, 2(4), 303–314.</a></li><li class=has-text-weight-bold><a href style=text-decoration:none>Freitag, K. J. (2007). Neural networks and differential equations.</a></li><li class=has-text-weight-bold><a href="https://www.youtube.com/watch?v=vq2nnJ4g6N0&t=663s" style=text-decoration:none>Tensorflow and deep learning - without a PhD by Martin Görner</a></li><li class=has-text-weight-bold><a href style=text-decoration:none>Kolmogorov, A. N. (1957). On the Representation of Continuous Functions of Several Variables by Superposition of Continuous Functions of one Variable and Addition, Doklady Akademii. Nauk USSR, 114, 679-681.</a></li><li class=has-text-weight-bold><a href=http://www.sciencedirect.com/science/article/pii/0893608089900208 style=text-decoration:none>Maxwell Stinchcombe, Halbert White (1989). Multilayer feedforward networks are universal approximators. Neural Networks, Vol 2, 5, 359-366.</a></li><li class=has-text-weight-bold><a href=http://www.cscjournals.org/manuscript/Journals/IJAE/volume1/Issue4/IJAE-26.pdf style=text-decoration:none>Performance Analysis of Various Activation Functions in Generalized MLP Architectures of Neural Networks</a></li></ol></p></div></div></article></div><div class=is-divider data-content=CONNECTUME></div><div class="box is-size-7"><article class=media><div class=media-content><div class=content><p><strong>Current Ref:</strong><br><ul><li style=list-style:none><span class="tag is-primary is-light has-text-weight-bold">wiki/machine-learning/neural-networks/artificial-neural-networks.md</span></li></ul></p></div></div></article></div><div id=comments class=is-divider data-content=COMMENTS></div><script src=https://utteranc.es/client.js repo=datumorphism/comments issue-term=pathname theme=github-light crossorigin=anonymous async></script></div></div></article></div></section></div><div class=navtools><a class="button is-primary is-light is-outlined" alt="Edit this page" href=https://github.com/datumorphism/datumorphism.github.io/edit/hugo/content/wiki/machine-learning/neural-networks/artificial-neural-networks.md target=blank style=position:fixed;bottom:20px;right:10px;border-radius:9999px;width:35px;height:35px><i class="fas fa-pencil-alt"></i></a><a class="button is-primary is-light is-outlined" href=#comments alt=Comments style=position:fixed;bottom:60px;right:10px;border-radius:9999px;width:35px;height:35px><i class="far fa-comments"></i></a><style>applause-button .count-container{top:-60%!important}</style><applause-button color=red multiclap=true style="width: 35px; height: 35px;position:fixed;bottom: 100px;right: 10px;"></div></main><footer><footer class=footer><div class=container><div class="content has-text-centered"><p>Created and maintained by <a href=https://leima.is>Lei Ma</a>.
Acknowledgement: <a href=https://gohugo.io/>Hugo</a>,
<a href=https://themes.gohugo.io/bulma/>Bulma</a>, <a href=https://kausalflow.com>KausalFlow</a>.
<strong>love</strong>.<br><a class=tag href=/about>About</a>
<a class=tag href=/index.xml>Feed</a>
<a class=tag href=/data.json>JSON Data</a></p></div></div></footer><script src=https://unpkg.com/applause-button/dist/applause-button.js></script></footer><script async type=text/javascript src=/js/bulma.js></script></body></html>