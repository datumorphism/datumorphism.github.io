<!doctype html><html lang=en-us><head><meta name=generator content="Hugo 0.69.2"><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><title>Artificial Neural Networks | Datumorphism | L Ma</title><meta name=description content="Solving PDEs"><meta name=author content="L Ma"><meta property="og:title" content="Artificial Neural Networks"><meta property="og:description" content="Solving PDEs"><meta property="og:type" content="article"><meta property="og:url" content="https://datumorphism.leima.is/wiki/machine-learning/neural-networks/artificial-neural-networks/"><meta property="article:published_time" content="2018-11-19T00:00:00+00:00"><meta property="article:modified_time" content="2018-11-19T00:00:00+00:00"><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-140452515-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><link rel=canonical href=https://datumorphism.leima.is/wiki/machine-learning/neural-networks/artificial-neural-networks/><link rel="shortcut icon" type=image/png href=/logos/logo-square.png><link rel=stylesheet href=/css/bulma.css><link rel=stylesheet href=/css/bulma-divider.min.css><link rel=stylesheet href=/assets/css/bulma-ribbon.min.css><link rel=stylesheet href=/assets/css/tooltip.css><link rel=stylesheet href=https://jenil.github.io/bulmaswatch/united/bulmaswatch.min.css><link rel=stylesheet href=/css/custom.css><link rel=stylesheet href=/css/blog-post.css><link rel=stylesheet href=/css/code-highlighting/dark.css><link rel=stylesheet href=/css/custom.css><link rel=stylesheet href=https://unpkg.com/applause-button/dist/applause-button.css><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],tags:'ams',processEscapes:true,processEnvironments:true},options:{skipHtmlTags:['script','noscript','style','textarea','pre']},svg:{fontCache:'global'}};window.addEventListener('load',(event)=>{document.querySelectorAll("mjx-container").forEach(function(x){x.parentElement.classList+='has-jax'})});</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script>mermaid.initialize({startOnLoad:true});</script><script src=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/js/all.min.js integrity="sha256-KzZiKy0DWYsnwMF+X1DvQngQ2/FxF7MF3Ff72XcpuPs=" crossorigin=anonymous></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css integrity="sha256-h20CPZ0QyXlBuAw7A+KluUYx/3pK+c7lYEpqLTlxjYQ=" crossorigin=anonymous></head><body><header><nav class="navbar is-transparent"><div class=navbar-brand><a class=navbar-item href=/><img src=/site/intelligence.png alt=Datumorphism height=28 style=margin-right:.5em> Datumorphism</a><div class="navbar-burger burger" style=color:#000 data-target=navMenu><span></span><span></span><span></span></div></div><div class=navbar-menu id=navMenu><div class=navbar-start><div class="navbar-item has-dropdown is-hoverable"><a href=/projects class=navbar-link>Notebooks</a><div class=navbar-dropdown><a href=https://datumorphism.leima.is/awesome/ class=navbar-item>Awesome</a>
<a href=https://datumorphism.leima.is/blog/ class=navbar-item>Blog</a>
<a href=https://datumorphism.leima.is/cards/ class=navbar-item>Cards</a>
<a href=https://datumorphism.leima.is/hologram/ class=navbar-item>Hologram</a>
<a href=https://datumorphism.leima.is/reading/ class=navbar-item>Reading Notes</a>
<a href=https://datumorphism.leima.is/til/ class=navbar-item>TIL</a>
<a href=https://datumorphism.leima.is/wiki/ class=navbar-item>Wiki</a></div></div><div class="navbar-item has-dropdown is-hoverable"><a class=navbar-item href=https://neuronstar.kausalflow.com/cpe-docs/>Probability Estimation</a></div></div><span class=navbar-burger><span></span><span></span><span></span></span><div class=navbar-end><div class=navbar-item><a class=navbar-item href=/blog/>Blog</a>
<a class=navbar-item href=/amneumarkt/>AmNeumarkt</a>
<a class=navbar-item href=/><i class="fas fa-search"></i></a><a class=navbar-item href=/tags/><i class="fas fa-tags"></i></a><a class=navbar-item href=/graph><i class="fas fa-project-diagram"></i></a><a class=navbar-item href=https://t.me/amneumarkt><i class="fab fa-telegram"></i></a><a class=navbar-item target=blank href=https://github.com/datumorphism><span class=icon><i class="fab fa-github"></i></span></a></div></div></div></nav><script>document.addEventListener('DOMContentLoaded',()=>{const $navbarBurgers=Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'),0);if($navbarBurgers.length>0){$navbarBurgers.forEach(el=>{el.addEventListener('click',()=>{const target=el.dataset.target;const $target=document.getElementById(target);el.classList.toggle('is-active');$target.classList.toggle('is-active');});});}});</script></header><main><div class=container itemscope itemtype=http://schema.org/BlogPosting><meta itemprop=name content="Artificial Neural Networks"><meta itemprop=description content="Solving PDEs"><meta itemprop=datePublished content="2018-11-19T00:00:00+00:00"><meta itemprop=dateModified content="2018-11-19T00:00:00+00:00"><meta itemprop=wordCount content="869"><meta itemprop=keywords content="Machine Learning,Artificial Neural Networks,Basics,"><section class=section><div class=container><article class=post><header class=post-header><nav class="breadcrumb has-succeeds-separator is-small" aria-label=breadcrumbs><ul><li><a href=https://datumorphism.leima.is/>Datumorphism</a></li><li><a href=https://datumorphism.leima.is/wiki/>Wiki</a></li><li><a href=https://datumorphism.leima.is/wiki/machine-learning/>Machine Learning</a></li><li><a href=https://datumorphism.leima.is/wiki/machine-learning/neural-networks/>Artificial Neural Networks</a></li><li class=active><a href=https://datumorphism.leima.is/wiki/machine-learning/neural-networks/artificial-neural-networks/>Artificial Neural Networks</a></li></ul></nav><h1 class="post-title has-text-centered is-size-1" itemprop="name headline">Artificial Neural Networks</h1><h2 class="title is-6 has-text-centered"><i class="fas fa-tags" style=margin-right:.5em></i><a href=/tags/machine-learning><span class="tag is-warning is-small is-light">#Machine Learning</span></a>
<a href=/tags/artificial-neural-networks><span class="tag is-warning is-small is-light">#Artificial Neural Networks</span></a>
<a href=/tags/basics><span class="tag is-warning is-small is-light">#Basics</span></a></h2></header><div class=columns><div class="column is-8"><div class=is-divider data-content=ARTICLE></div><div class="content blog-post section" itemprop=articleBody><p>Artificial neural networks works pretty well for solving some differential equations.</p><h2 id=universal-approximators>Universal Approximators</h2><p>Maxwell Stinchcombe and Halber White proved that no theoretical constraints for the feedforward networks to approximate any measurable function. In principle, one can use feedforward networks to approximate measurable functions to any accuracy.</p><p>However, the convergence slows down if we have a lot of hidden units. There is a balance between accuracy and convergence rate. More hidden units lead to slow convergence but more accuracy.</p><p>Here is a quick review of the history of this topic.</p><article class="message is-info is-light"><div class=message-header><p>Kolmogorovâ€™s Theorem</p></div><div class=message-body><p>Kolmogorov&rsquo;s theorem shows that one can use a finite number of carefully chosen continuous functions to mix up by sums and multiplication with weights to a continuous multivariable function on a compact set.</p><p><a href=http://neuron.eng.wayne.edu/tarek/MITbook/chap2/2_3.html>Here is the exact math.</a></p></div></article><ol><li><p>Cybenko 1989</p><p>Cybenko proved that</p><p>$$
\sum_k v_k \sigma(w_k x + u_k)
$$</p><p>is a good approximation of continuous functions because it is dense in continuous function space. In this result, $\sigma$ is a continuous sigmoidal function and the parameters are real.</p></li><li><p>Hornik 1989</p><p>&ldquo;Single hidden layer feedforward networks can approximate any measurable functions arbitrarily well regardless of the activation function, the dimension of the input and the input space environment.&rdquo;</p><p>Reference: <a href=http://deeplearning.cs.cmu.edu/notes/Sonia_Hornik.pdf>http://deeplearning.cs.cmu.edu/notes/Sonia_Hornik.pdf</a></p></li></ol><article class="message is-info is-light"><div class=message-header><p>Dense</p></div><div class=message-body>Set A is dense in set X means that we can use A to arbitarily approximate X. Mathematically for any given element in X, the neighbour of x always has nonzero intersection.</div></article><article class="message is-info is-light"><div class=message-header><p>Measurable Function</p></div><div class=message-body>It means the function is continuous.</div></article><h2 id=activation-functions>Activation Functions</h2><p>There are many activation functions.</p><ul><li><span class=tooltip><a href=https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-uni-polar-sigmoid/>Uni-Polar Sigmoid Function</a>
<span class=tooltiptext><span class=tooltip_title>Uni-Polar Sigmoid</span>
<span class=tooltip_content>Uni-polar sigmoid function and its properties</span></span></span></li><li><span class=tooltip><a href=https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-bi-polar-sigmoid/>Bipolar Sigmoid Function</a>
<span class=tooltiptext><span class=tooltip_title>BiPolar Sigmoid</span>
<span class=tooltip_content>BiPolar sigmoid function and its properties</span></span></span></li><li><span class=tooltip><a href=https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-hyperbolic-tangent/>Hyperbolic Tangent</a>
<span class=tooltiptext><span class=tooltip_title>Hyperbolic Tanh</span>
<span class=tooltip_content>Tanh function and its properties</span></span></span></li><li><span class=tooltip><a href=https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-radial-basis-function/>Radial Basis Function</a>
<span class=tooltiptext><span class=tooltip_title>Radial Basis Function</span>
<span class=tooltip_content>Radial Basis Function function and its properties</span></span></span></li><li><span class=tooltip><a href=https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-conic-section-function/>Conic Section Function</a>
<span class=tooltiptext><span class=tooltip_title>Conic Section Function</span>
<span class=tooltip_content>Conic Section Function and its properties</span></span></span></li><li><span class=tooltip><a href=https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-relu/>ReLu</a>
<span class=tooltiptext><span class=tooltip_title>ReLu</span>
<span class=tooltip_content>Rectified Linear Unit, aka ReLu, and its properties</span></span></span></li><li><span class=tooltip><a href=https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-leaky-relu/>Leaky ReLu</a>
<span class=tooltiptext><span class=tooltip_title>Leaky ReLu</span>
<span class=tooltip_content>Leaky ReLu and its properties</span></span></span></li><li><span class=tooltip><a href=https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-elu/>ELU</a>
<span class=tooltiptext><span class=tooltip_title>ELU</span>
<span class=tooltip_content>ELU and its properties</span></span></span></li><li><span class=tooltip><a href=https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-swish/>Swish</a>
<span class=tooltiptext><span class=tooltip_title>Swish</span>
<span class=tooltip_content>Swish and its properties</span></span></span></li></ul><figure><img src=../assets/artificial-neural-networks/tutorial_notebooks_tutorial3_Activation_Functions_19_0.svg alt="Lippe P. Tutorial 3: Activation Functions â€” UvA DL Notebooks v1.1 documentation. In: UvA Deep Learning Tutorials [Internet]."><figcaption><p><a href=https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial3/Activation_Functions.html>Lippe P. Tutorial 3: Activation Functions â€” UvA DL Notebooks v1.1 documentation. In: UvA Deep Learning Tutorials [Internet]</a>.</p></figcaption></figure><h2 id=solving-differential-equations>Solving Differential Equations</h2><p>The problem here to solve is</p><p>$$
\frac{d}{dt}y(t)= - y(t),
$$</p><p>with initial condition $y(0)=1$.</p><p>To construct a single layered neural network, the function is decomposed using</p><p>$$
\begin{align}
y(t_i) & = y(t_0) + t_i v_k f(t_i w_k+u_k) \\
&= 1+t_i v_k f(t_i w_k+u_k) ,
\end{align}
$$</p><p>where $y(t_0)$ is the initial condition and $k$ is summed over.</p><p>Presumably this should be the gate controlling trigering of the neuron or not. Therefore the following expit function serves this purpose well,</p><p>$$
f(x) = \frac{1}{1+\exp(-x)}.
$$</p><p>One important reason for choosing this is that a lot of expressions can be calculated analytically and easily.</p><article class="message is-light is-light"><div class=message-header><p>Fermi-Dirac Distribution</p></div><div class=message-body>Aha, the Fermi-Dirac distribution.</div></article><p>With the form of the function to be solved, we can define a cost</p><p>$$
I=\sum_i\left( \frac{dy}{dt}(t_i)+y(t_i) \right)^2,
$$</p><p>which should be minimized to 0 if our structure of networks is optimized for this problem.</p><p>Now the task becomes clear:</p><ol><li>Write down the cost analytically;</li><li>Minimized cost to find structure;</li><li>Substitute back to the function and we are done.</li></ol><h2 id=overfitting>Overfitting</h2><p>It is possible that we could over fit a network so that it works only for the training data. To avoid that, people use several strategies.</p><ol><li>Split data into two parts, one for training and one for testing. <a href="https://www.youtube.com/watch?v=S4ZUwgesjS8">A youtube video</a></li><li>Throw more data in. At least 10 times as many as examples as the DoFs of the model. <a href="https://www.youtube.com/watch?v=S4ZUwgesjS8">A youtube video</a></li><li>Regularization by plugin a artificial term to the cost function, as an example we could add the . <a href="https://www.youtube.com/watch?v=S4ZUwgesjS8">A youtube video</a></li></ol><h2 id=neural-network-and-finite-element-method>Neural Network and Finite Element Method</h2><p>We consider the solution to a differential equation</p><p>$$
\mathcal L \psi - f = 0.
$$</p><p>Neural network is quite similar to finite element method. In terms of finite element method, we can write down a neural network structured form of a function <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></p><p>$$
\psi(x_i) = A(x_i) + F(x_i, \mathcal N_i),
$$</p><p>where $\mathcal N$ is the neural network structure. Specifically,</p><p>$$
\mathcal N_i = \sigma( w_{ij} x_j + u_i ).
$$</p><p>The function is parameterized using the network. Such parameterization is similar to collocation method in finite element method, where multiple basis is used for each location.</p><p>One of the choices of the function $F$ is a linear combination,</p><p>$$
F(x_i, \mathcal N_i) = x_i \mathcal N_i,
$$</p><p>and $A(x_i)$ should take care of the boundary condition.</p><article class="message is-light is-light"><div class=message-header><p>Relation to finite element method</p></div><div class=message-body>This function is similar to the finite element function basis approximation. The goal in finite element method is to find the coefficients of each basis functions to achieve a good approximation. In ANN method, each sigmoid is the analogy to the basis functions, where we are looking for both the coefficients of sigmoids and the parameters of them. These sigmoid functions are some kind of adaptive basis functions.</div></article><p>With such parameterization, the differential equation itself is parameterized such that</p><p>$$
\mathcal L \psi - f = 0,
$$</p><p>such that the minimization should be</p><p>$$
\lvert \mathcal L \psi - f \rvert^2 \to 0
$$</p><p>at each point.</p><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p>Freitag, K. J. (2007). Neural networks and differential equations. <a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></div><p><div class="has-text-right is-size-7"><span class=icon><i class="fas fa-pencil-alt"></i></span>Published: <time datetime=2018-11-19T00:00:00+00:00>2018-11-19</time>
by <span itemprop=author>L Ma</span>;</div></p><div class=is-divider></div><nav class="pagination is-centered" role=navigation aria-label=pagination><a class=pagination-next href="https://datumorphism.leima.is/wiki/machine-learning/neural-networks/physicists-crash-course-neural-network/?ref=footer">A Physicist's Crash Course on Artificial Neural... Â»</a></nav></div><div class="column is-4"><div class=is-divider data-content="Cite Me"></div><div class="box is-size-7 has-text-white has-background-black"><article class=media><div class=media-content><div class=content><p>L Ma
(2018). 'Artificial Neural Networks', Datumorphism, 11 April. Available at: https://datumorphism.leima.is/wiki/machine-learning/neural-networks/artificial-neural-networks/.</p></div></div></article></div><style>#TableOfContents>ul{list-style-type:lower-greek;padding-left:0}#TableOfContents>ul>li ul{list-style-type:none;padding-left:1em}</style><div class=is-divider data-content=ToC></div><div class="box is-size-7"><article class=media><div class=media-content><div class=content><details><summary>Table of Contents</summary><div><div><nav id=TableOfContents><ul><li><a href=#universal-approximators>Universal Approximators</a></li><li><a href=#activation-functions>Activation Functions</a></li><li><a href=#solving-differential-equations>Solving Differential Equations</a></li><li><a href=#overfitting>Overfitting</a></li><li><a href=#neural-network-and-finite-element-method>Neural Network and Finite Element Method</a></li></ul></nav></div></div></details></div></div></article></div><script>const el=document.querySelector('details summary')
el.onclick=()=>{(function(l,o,a,d,e,r){e=o.createElement(a),r=o.getElementsByTagName(a)[0];e.async=1;e.src=d;r.parentNode.insertBefore(e,r)})(window,document,'script','/js/smoothscroll.js');el.onclick=null}
document.querySelectorAll('#TableOfContents a').forEach(link=>{link.addEventListener('click',()=>{document.querySelector(link.href.slice(link.href.indexOf('#'))).scrollIntoView({behavior:'smooth'})})})</script><div class=is-divider data-content=REFERENCES></div><div class="box is-size-7"><article class=media><div class=media-content><div class=content><p><strong>References:</strong><br><ol><li class=has-text-weight-bold><a href=https://mitpress.mit.edu/books/fundamentals-artificial-neural-networks style=text-decoration:none>Hassoun MH, Assistant Professor of Computer Engineering Mohamad H Hassoun. Fundamentals of Artificial Neural Networks. MIT Press; 1995. Available: https://mitpress.mit.edu/books/fundamentals-artificial-neural-networks</a></li><li class=has-text-weight-bold><a href=https://link.springer.com/chapter/10.1007%2F11759966_125 style=text-decoration:none>Shenouda EAMA. A Quantitative Comparison of Different MLP Activation Functions in Classification. Advances in Neural Networks - ISNN 2006. Springer Berlin Heidelberg; 2006. pp. 849â€“857. doi:10.1007/11759966_125</a></li><li class=has-text-weight-bold><a href=https://doi.org/10.1016/0893-6080%2889%2990020-8 style=text-decoration:none>Hornik, K., Stinchcombe, M., & White, H. (1989). Multilayer feedforward networks are universal approximators. Neural Networks, 2(5), 359â€“366.</a></li><li class=has-text-weight-bold><a href=https://doi.org/10.1007/BF02551274 style=text-decoration:none>Cybenko, G. (1989). Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals, and Systems, 2(4), 303â€“314.</a></li><li class=has-text-weight-bold><a href style=text-decoration:none>Freitag, K. J. (2007). Neural networks and differential equations.</a></li><li class=has-text-weight-bold><a href="https://www.youtube.com/watch?v=vq2nnJ4g6N0&t=663s" style=text-decoration:none>Tensorflow and deep learning - without a PhD by Martin GÃ¶rner</a></li><li class=has-text-weight-bold><a href style=text-decoration:none>Kolmogorov, A. N. (1957). On the Representation of Continuous Functions of Several Variables by Superposition of Continuous Functions of one Variable and Addition, Doklady Akademii. Nauk USSR, 114, 679-681.</a></li><li class=has-text-weight-bold><a href=http://www.sciencedirect.com/science/article/pii/0893608089900208 style=text-decoration:none>Maxwell Stinchcombe, Halbert White (1989). Multilayer feedforward networks are universal approximators. Neural Networks, Vol 2, 5, 359-366.</a></li><li class=has-text-weight-bold><a href=http://www.cscjournals.org/manuscript/Journals/IJAE/volume1/Issue4/IJAE-26.pdf style=text-decoration:none>Performance Analysis of Various Activation Functions in Generalized MLP Architectures of Neural Networks</a></li><li class=has-text-weight-bold><a name=Lippe href=#Lippe style=text-decoration:none><span class="tag is-link is-light">Lippe</span></a>
<a href=https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial3/Activation_Functions.html style=text-decoration:none>Lippe P. Tutorial 3: Activation Functions â€” UvA DL Notebooks v1.1 documentation. In: UvA Deep Learning Tutorials [Internet]. [cited 23 Sep 2021]. Available: https://uvadlc-notebooks.readthedocs.io</a></li></ol></p></div></div></article></div><div class=is-divider data-content=CONNECTUME></div><div class="box is-size-7"><article class=media><div class=media-content style=width:100%><div class=content><p><strong>Current Ref:</strong><br><ul><li style=list-style:none><div>wiki/machine-learning/neural-networks/artificial-neural-networks.md</div></li></ul></p></div></div></article></div><div class="box is-size-7"><article class=media><div class=media-content><div class=content><p><strong>Links from:</strong><div><a class=box href=https://datumorphism.leima.is/cards/machine-learning/neural-networks/neural-networks-initialization/><div class=media-content><div class=content><h6>Initialize Artificial Neural Networks</h6><p>Initialize a neural network is important for the training and performance. Some initializations â€¦</p></div></div></a><a class=box href=https://datumorphism.leima.is/cards/machine-learning/neural-networks/log-sum-exp-trick/><div class=media-content><div class=content><h6>The log-sum-exp Trick</h6><p>For numerical stability we can use the log-sum-exp trick to calculate some loss such as cross â€¦</p></div></div></a><a class=box href=https://datumorphism.leima.is/cards/machine-learning/neural-networks/mcculloch-pitts-model/><div class=media-content><div class=content><h6>McCulloch-Pitts Model</h6><p>Artificial neuron that separates the state space</p></div></div></a><a class=box href=https://datumorphism.leima.is/cards/machine-learning/neural-networks/rosenblatt-perceptron/><div class=media-content><div class=content><h6>Rosenblatt's Perceptron</h6><p>Connected perceptrons</p></div></div></a><a class=box href=https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-bi-polar-sigmoid/><div class=media-content><div class=content><h6>BiPolar Sigmoid</h6><p>BiPolar sigmoid function and its properties</p></div></div></a><a class=box href=https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-conic-section-function/><div class=media-content><div class=content><h6>Conic Section Function</h6><p>Conic Section Function and its properties</p></div></div></a><a class=box href=https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-elu/><div class=media-content><div class=content><h6>ELU</h6><p>ELU and its properties</p></div></div></a><a class=box href=https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-hyperbolic-tangent/><div class=media-content><div class=content><h6>Hyperbolic Tanh</h6><p>Tanh function and its properties</p></div></div></a><a class=box href=https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-leaky-relu/><div class=media-content><div class=content><h6>Leaky ReLu</h6><p>Leaky ReLu and its properties</p></div></div></a><a class=box href=https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-radial-basis-function/><div class=media-content><div class=content><h6>Radial Basis Function</h6><p>Radial Basis Function function and its properties</p></div></div></a><a class=box href=https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-relu/><div class=media-content><div class=content><h6>ReLu</h6><p>Rectified Linear Unit, aka ReLu, and its properties</p></div></div></a><a class=box href=https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-swish/><div class=media-content><div class=content><h6>Swish</h6><p>Swish and its properties</p></div></div></a><a class=box href=https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-uni-polar-sigmoid/><div class=media-content><div class=content><h6>Uni-Polar Sigmoid</h6><p>Uni-polar sigmoid function and its properties</p></div></div></a></div></p></div></div></article></div><div id=comments class=is-divider data-content=COMMENTS></div><script src=https://giscus.app/client.js data-repo=datumorphism/comments data-repo-id="MDEwOlJlcG9zaXRvcnkxNjU5MDkyNDI=" data-category=Comments data-category-id=DIC_kwDOCeOS-s4B-Zxx data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=1 data-theme=light crossorigin=anonymous async></script></div></div></article></div></section></div><div class=navtools><a class="button is-primary is-light is-outlined" alt="Edit this page" href=https://github.com/datumorphism/datumorphism.github.io/edit/hugo/content/wiki/machine-learning/neural-networks/artificial-neural-networks.md target=blank style=position:fixed;bottom:20px;right:10px;border-radius:9999px;width:35px;height:35px><i class="fas fa-pencil-alt"></i></a><a class="button is-primary is-light is-outlined" href=#comments alt=Comments style=position:fixed;bottom:60px;right:10px;border-radius:9999px;width:35px;height:35px><i class="far fa-comments"></i></a></div></main><footer><footer class=footer><div class=container><div class="content has-text-centered"><p>Created and maintained by <a href=https://leima.is>L Ma</a>.
Acknowledgement: <a href=https://gohugo.io/>Hugo</a>,
<a href=https://themes.gohugo.io/bulma/>Bulma</a>, <a href=https://kausalflow.com>KausalFlow</a>.
<strong>love</strong>.<br><a class=tag href=/about>About</a>
<a class=tag href=/index.xml>Feed</a>
<a class=tag href=/data.json>JSON Data</a></p></div></div></footer><script src=https://unpkg.com/applause-button/dist/applause-button.js></script></footer><script async type=text/javascript src=/js/bulma.js></script></body></html>