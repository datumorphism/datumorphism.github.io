<!doctype html><html lang=en-us><head><meta name=generator content="Hugo 0.69.2"><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><title>Tensor Factorization | Datumorphism | Lei Ma</title><meta name=description content="A generalization of matrix factorization"><meta name=author content="Lei Ma"><meta property="og:title" content="Tensor Factorization"><meta property="og:description" content="A generalization of matrix factorization"><meta property="og:type" content="article"><meta property="og:url" content="/wiki/machine-learning/factorization/tensor-factorization/"><meta property="article:published_time" content="2019-06-17T00:00:00+00:00"><meta property="article:modified_time" content="2019-06-17T00:00:00+00:00"><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-140452515-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><link rel=canonical href=/wiki/machine-learning/factorization/tensor-factorization/><link rel="shortcut icon" type=image/png href=/logos/logo-square.png><link rel=stylesheet href=/css/bulma.css><link rel=stylesheet href=/css/bulma-divider.min.css><link rel=stylesheet href=/assets/css/bulma-ribbon.min.css><link rel=stylesheet href=/assets/css/tooltip.css><link rel=stylesheet href=https://jenil.github.io/bulmaswatch/united/bulmaswatch.min.css><link rel=stylesheet href=/css/custom.css><link rel=stylesheet href=/css/blog-post.css><link rel=stylesheet href=/css/code-highlighting/dark.css><link rel=stylesheet href=/css/custom.css><link rel=stylesheet href=https://unpkg.com/applause-button/dist/applause-button.css><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],tags:'ams'},svg:{fontCache:'global'}};</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js></script><script src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script>mermaid.initialize({startOnLoad:true});</script><script src=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/js/all.min.js integrity="sha256-KzZiKy0DWYsnwMF+X1DvQngQ2/FxF7MF3Ff72XcpuPs=" crossorigin=anonymous></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css integrity="sha256-h20CPZ0QyXlBuAw7A+KluUYx/3pK+c7lYEpqLTlxjYQ=" crossorigin=anonymous></head><body><header><nav class="navbar is-transparent"><div class=navbar-brand><a class=navbar-item href=/><img src=/site/intelligence.png alt=Datumorphism height=28 style=margin-right:.5em> Datumorphism</a><div class="navbar-burger burger" data-target=navMenu><span></span><span></span><span></span></div></div><div class=navbar-menu id=navMenu><div class=navbar-start><div class="navbar-item has-dropdown is-hoverable"><a href=/projects class=navbar-link>Notebooks</a><div class=navbar-dropdown><a href=/awesome/ class=navbar-item>Awesome</a>
<a href=/blog/ class=navbar-item>Blog</a>
<a href=/cards/ class=navbar-item>Cards</a>
<a href=/reading/ class=navbar-item>Reading Notes</a>
<a href=/til/ class=navbar-item>TIL</a>
<a href=/wiki/ class=navbar-item>Wiki</a></div></div></div><span class=navbar-burger><span></span><span></span><span></span></span><div class=navbar-end><div class=navbar-item><a class=navbar-item href=/blog/>Blog</a>
<a class=navbar-item href=/tags/><i class="fas fa-tags"></i></a><a class=navbar-item href=/graph><i class="fas fa-project-diagram"></i></a><a class=navbar-item target=blank href=https://github.com/datumorphism><span class=icon><i class="fab fa-github"></i></span></a></div></div></div></nav><script>document.addEventListener('DOMContentLoaded',()=>{const $navbarBurgers=Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'),0);if($navbarBurgers.length>0){$navbarBurgers.forEach(el=>{el.addEventListener('click',()=>{const target=el.dataset.target;const $target=document.getElementById(target);el.classList.toggle('is-active');$target.classList.toggle('is-active');});});}});</script></header><main><div class=container itemscope itemtype=http://schema.org/BlogPosting><meta itemprop=name content="Tensor Factorization"><meta itemprop=description content="A generalization of matrix factorization"><meta itemprop=datePublished content="2019-06-17T00:00:00+00:00"><meta itemprop=dateModified content="2019-06-17T00:00:00+00:00"><meta itemprop=wordCount content="111"><meta itemprop=keywords content="Machine Learning,Factorization,Tensor,"><section class=section><div class=container><article class=post><header class=post-header><nav class="breadcrumb has-succeeds-separator is-small" aria-label=breadcrumbs><ul><li><a href=/>Datumorphism</a></li><li><a href=/wiki/>Wiki</a></li><li><a href=/wiki/machine-learning/>Machine Learning</a></li><li><a href=/wiki/machine-learning/factorization/>Factorization</a></li><li class=active><a href=/wiki/machine-learning/factorization/tensor-factorization/>Tensor Factorization</a></li></ul></nav><h1 class="post-title has-text-centered is-size-1" itemprop="name headline">Tensor Factorization</h1><h2 class="title is-6 has-text-centered"><i class="fas fa-tags" style=margin-right:.5em></i><a href=/tags/machine-learning><span class="tag is-warning is-small is-light">#Machine Learning</span></a>
<a href=/tags/factorization><span class="tag is-warning is-small is-light">#Factorization</span></a>
<a href=/tags/tensor><span class="tag is-warning is-small is-light">#Tensor</span></a></h2></header><div class=columns><div class="column is-8"><div class=is-divider data-content=ARTICLE></div><div class="content blog-post section" itemprop=articleBody><h2 id=tensors>Tensors</h2><p>We will be talking about tensors but we will skip the introduction to tensor for now.</p><p>In this article, we follow a commonly used convention for tensors in physics, the abstract index notation. We will denote tensors as $T^{ab\cdots}_ {\phantom{ab\cdots}cd\cdots}$, where the latin indices such as $^{a}$ are simply a placebo for the slot for this &ldquo;tensor machine&rdquo;. For a given basis (coordinate system), we can write down the components of this tensor $T^{\alpha\beta\cdots} _ {\phantom{\alpha\beta\cdots}\gamma\delta\cdots}$.</p><article class="message is-info is-light"><div class=message-header><p>Okay, But Why</p></div><div class=message-body>What is usually seen in blog posts is the use of component forms of tensors, $T^{\alpha\beta\cdots}_{\phantom{\alpha\beta\cdots}\gamma\delta\cdots}$. Those are the numbers for a given basis. We would like to keep it general.</div></article></div><p><div class="has-text-right is-size-7"><span class=icon><i class="fas fa-pencil-alt"></i></span>Published: <time datetime=2019-06-17T00:00:00+00:00>2019-06-17</time>
by <span itemprop=author>Lei Ma</span>;</div></p><div class=is-divider></div><nav class="pagination is-centered" role=navigation aria-label=pagination><a href="/wiki/machine-learning/factorization/nmf/?ref=footer" class=pagination-previous>« NMF: Nonnegative Matrix Factorizatioin</a></nav></div><div class="column is-4"><style>#TableOfContents>ul{list-style-type:lower-greek;padding-left:0}#TableOfContents>ul>li ul{list-style-type:none;padding-left:1em}</style><div class=is-divider data-content=ToC></div><div class="box is-size-7"><article class=media><div class=media-content><div class=content><details><summary>Table of Contents</summary><div><div><nav id=TableOfContents><ul><li><a href=#tensors>Tensors</a></li></ul></nav></div></div></details></div></div></article></div><script>const el=document.querySelector('details summary')
el.onclick=()=>{(function(l,o,a,d,e,r){e=o.createElement(a),r=o.getElementsByTagName(a)[0];e.async=1;e.src=d;r.parentNode.insertBefore(e,r)})(window,document,'script','/js/smoothscroll.js');el.onclick=null}
document.querySelectorAll('#TableOfContents a').forEach(link=>{link.addEventListener('click',()=>{document.querySelector(link.href.slice(link.href.indexOf('#'))).scrollIntoView({behavior:'smooth'})})})</script><div class=is-divider data-content=REFERENCES></div><div class="box is-size-7"><article class=media><div class=media-content><div class=content><p><strong>References:</strong><br><ol><li class=has-text-weight-bold><a href=https://doi.org/10.1007/978-3-319-24486-0_2 style=text-decoration:none>Anandkumar, A., Ge, R., Hsu, D., Kakade, S. M., & Telgarsky, M. (2012). Tensor decompositions for learning latent variable models. Journal of Machine Learning Research, 15(1), 2773–2832.</a></li><li class=has-text-weight-bold><a href=https://www.offconvex.org/2015/12/17/tensor-decompositions/ style=text-decoration:none>Tensor Methods in Machine Learning</a></li><li class=has-text-weight-bold><a href=https://en.wikipedia.org/wiki/Penrose_graphical_notation style=text-decoration:none>Penrose graphical notation</a></li><li class=has-text-weight-bold><a href=https://math.stackexchange.com/questions/455478/what-is-the-practical-difference-between-abstract-index-notation-and-ordinary style=text-decoration:none>What is the practical difference between abstract index notation and “ordinary” index notation</a></li><li class=has-text-weight-bold><a href=https://medium.com/@keremturgutlu/tensor-decomposition-fast-cnn-in-your-pocket-f03e9b2a6788 style=text-decoration:none>Tensor Decomposition: Fast CNN in your pocket</a></li></ol></p></div></div></article></div><div class=is-divider data-content=CONNECTUME></div><div class="box is-size-7"><article class=media><div class=media-content><div class=content><p><strong>Current Ref:</strong><br><ul><li style=list-style:none><span class="tag is-primary is-light has-text-weight-bold">wiki/machine-learning/factorization/tensor-factorization.md</span></li></ul></p></div></div></article></div><div class="box is-size-7"><article class=media><div class=media-content><div class=content><p><strong>Links to:</strong><div><a class=box href=/cards/math/canonical-decomposition/><div class=media-content><div class=content><h6>Canonical Decomposition</h6><p>Canonical decomposition</p></div></div></a></div></p></div></div></article></div><div id=comments class=is-divider data-content=COMMENTS></div><script src=https://utteranc.es/client.js repo=datumorphism/comments issue-term=pathname theme=github-light crossorigin=anonymous async></script></div></div></article></div></section></div><div class=navtools><a class="button is-primary is-light is-outlined" alt="Edit this page" href=https://github.com/datumorphism/datumorphism.github.io/edit/hugo/content/wiki/machine-learning/factorization/tensor-factorization.md target=blank style=position:fixed;bottom:20px;right:10px;border-radius:9999px;width:35px;height:35px><i class="fas fa-pencil-alt"></i></a><a class="button is-primary is-light is-outlined" href=#comments alt=Comments style=position:fixed;bottom:60px;right:10px;border-radius:9999px;width:35px;height:35px><i class="far fa-comments"></i></a><style>applause-button .count-container{top:-60%!important}</style><applause-button color=red multiclap=true style="width: 35px; height: 35px;position:fixed;bottom: 100px;right: 10px;"></div></main><footer><footer class=footer><div class=container><div class="content has-text-centered"><p>Created and maintained by <a href=/>Lei Ma</a>.
Acknowledgement: <a href=https://gohugo.io/>Hugo</a>,
<a href=https://themes.gohugo.io/bulma/>Bulma</a>, <a href=https://kausalflow.com>KausalFlow</a>.
<strong>love</strong>.<br><a class=tag href=/about>About</a>
<a class=tag href=/index.xml>Feed</a>
<a class=tag href=/data.json>JSON Data</a></p></div></div></footer><script src=https://unpkg.com/applause-button/dist/applause-button.js></script></footer><script async type=text/javascript src=/js/bulma.js></script></body></html>