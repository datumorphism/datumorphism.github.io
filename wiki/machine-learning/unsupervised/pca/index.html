<!doctype html><html lang=en-us>
<head>
<meta name=generator content="Hugo 0.89.4">
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1">
<title> Unsupervised Learning: PCA | Datumorphism | L Ma </title>
<meta name=description content="Principal component analysis is a method to remove redundancies of the features by looking into the variances.">
<meta name=author content="L Ma">
<meta property="og:title" content="Unsupervised Learning: PCA">
<meta property="og:description" content="Principal component analysis is a method to remove redundancies of the features by looking into the variances.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://datumorphism.leima.is/wiki/machine-learning/unsupervised/pca/"><meta property="article:section" content="wiki">
<meta property="article:published_time" content="2018-05-25T00:00:00+00:00">
<meta property="article:modified_time" content="2018-05-25T00:00:00+00:00">
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga('create','UA-140452515-1','auto'),ga('send','pageview'))</script>
<script async src=https://www.google-analytics.com/analytics.js></script>
<link rel=canonical href=https://datumorphism.leima.is/wiki/machine-learning/unsupervised/pca/> <link rel="shortcut icon" type=image/png href=/logos/logo-square.png>
<link rel=stylesheet href=/css/bulma.css>
<link rel=stylesheet href=/css/bulma-divider.min.css>
<link rel=stylesheet href=/assets/css/bulma-ribbon.min.css>
<link rel=stylesheet href=/assets/css/tooltip.css>
<link rel=stylesheet href=https://jenil.github.io/bulmaswatch/united/bulmaswatch.min.css>
<link rel=stylesheet href=/css/custom.css>
<link rel=stylesheet href=/css/blog-post.css>
<link rel=stylesheet href=/css/code-highlighting/dark.css>
<link rel=stylesheet href=/css/custom.css>
<script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],tags:'ams',processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:['script','noscript','style','textarea','pre']},svg:{fontCache:'global'}},window.addEventListener('load',a=>{document.querySelectorAll("mjx-container").forEach(function(a){a.parentElement.classList+='has-jax'})})</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<script src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script>
<script>mermaid.initialize({startOnLoad:!0})</script>
<link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css integrity="sha512-9usAa10IRO0HhonpyAIVpjrylPvoDwiPUiKdWk5t3PyolY1cOd4DSE0Ga+ri4AuTroPR5aQvXU9xC6qOPnzFeg==" crossorigin=anonymous referrerpolicy=no-referrer>
<script src=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/js/all.min.js integrity="sha512-yFjZbTYRCJodnuyGlsKamNE/LlEaEAxSUDe5+u61mV8zzqJVFOH7TnULE2/PP/l5vKWpUNnF4VGVkXh3MjgLsg==" crossorigin=anonymous referrerpolicy=no-referrer></script>
</head>
<body>
<header> <nav class="navbar is-transparent">
<div class=navbar-brand>
<a class=navbar-item href=/>
<img src=/site/intelligence.png alt=Datumorphism height=28 style=margin-right:.5em> Datumorphism
</a>
<div class="navbar-burger burger" style=color:#000 data-target=navMenu>
<span></span>
<span></span>
<span></span>
</div>
</div>
<div class=navbar-menu id=navMenu>
<div class=navbar-start>
<div class="navbar-item has-dropdown is-hoverable">
<a href=/projects class=navbar-link>
Notebooks
</a>
<div class=navbar-dropdown>
<a href=https://datumorphism.leima.is/awesome/ class=navbar-item>
Awesome
</a>
<a href=https://datumorphism.leima.is/blog/ class=navbar-item>
Blog
</a>
<a href=https://datumorphism.leima.is/cards/ class=navbar-item>
Cards
</a>
<a href=https://datumorphism.leima.is/hologram/ class=navbar-item>
Hologram
</a>
<a href=https://datumorphism.leima.is/reading/ class=navbar-item>
Reading Notes
</a>
<a href=https://datumorphism.leima.is/til/ class=navbar-item>
TIL
</a>
<a href=https://datumorphism.leima.is/wiki/ class=navbar-item>
Wiki
</a>
</div>
</div>
<div class="navbar-item has-dropdown is-hoverable">
<a class=navbar-item href=https://neuronstar.kausalflow.com/cpe-docs/>
Probability Estimation
</a>
</div>
</div>
<span class=navbar-burger>
<span></span>
<span></span>
<span></span>
</span>
<div class=navbar-end>
<div class=navbar-item>
<a class=navbar-item href=/blog/>
Blog
</a>
<a class=navbar-item href=/amneumarkt/>
AmNeumarkt
</a>
<a class=navbar-item href=/>
<i class="fas fa-search"></i>
</a>
<a class=navbar-item href=/tags/>
<i class="fas fa-tags"></i>
</a>
<a class=navbar-item href=/graph>
<i class="fas fa-project-diagram"></i>
</a>
<a class=navbar-item href=https://t.me/amneumarkt>
<i class="fab fa-telegram"></i>
</a>
<a class=navbar-item target=blank href=https://github.com/datumorphism>
<span class=icon>
<i class="fab fa-github"></i>
</span>
</a>
</div>
</div>
</div>
</nav>
<script>document.addEventListener('DOMContentLoaded',()=>{const a=Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'),0);a.length>0&&a.forEach(a=>{a.addEventListener('click',()=>{const b=a.dataset.target,c=document.getElementById(b);a.classList.toggle('is-active'),c.classList.toggle('is-active')})})})</script>
</header>
<main>
<div class=container itemscope itemtype=http://schema.org/BlogPosting>
<meta itemprop=name content="Unsupervised Learning: PCA">
<meta itemprop=description content="Principal component analysis is a method to remove redundancies of the features by looking into the variances."><meta itemprop=datePublished content="2018-05-25T00:00:00+00:00">
<meta itemprop=dateModified content="2018-05-25T00:00:00+00:00">
<meta itemprop=wordCount content="967">
<meta itemprop=keywords content="PCA,Unsupervised Learning,Statistical Learning,Basics,">
<section class=section>
<div class=container>
<article class=post>
<header class=post-header>
<nav class="breadcrumb has-succeeds-separator is-small" aria-label=breadcrumbs>
<ul>
<li>
<a href=https://datumorphism.leima.is/>Datumorphism</a>
</li>
<li>
<a href=https://datumorphism.leima.is/wiki/>Wiki</a>
</li>
<li>
<a href=https://datumorphism.leima.is/wiki/machine-learning/>Machine Learning</a>
</li>
<li>
<a href=https://datumorphism.leima.is/wiki/machine-learning/unsupervised/>Unsupervised Learning</a>
</li>
<li class=active>
<a href=https://datumorphism.leima.is/wiki/machine-learning/unsupervised/pca/>Unsupervised Learning: PCA</a>
</li>
</ul>
</nav>
<h1 class="post-title has-text-centered is-size-1" itemprop="name headline">
Unsupervised Learning: PCA
</h1>
<h2 class="title is-6 has-text-centered">
<i class="fas fa-tags" style=margin-right:.5em></i>
<a href=/tags/pca><span class="tag is-warning is-small is-light">#PCA</span></a>
<a href=/tags/unsupervised-learning><span class="tag is-warning is-small is-light">#Unsupervised Learning</span></a>
<a href=/tags/statistical-learning><span class="tag is-warning is-small is-light">#Statistical Learning</span></a>
<a href=/tags/basics><span class="tag is-warning is-small is-light">#Basics</span></a>
</h2>
</header>
<div class=columns>
<div class="column is-8">
<div class=is-divider data-content=ARTICLE></div>
<div class="content blog-post section" itemprop=articleBody>
<article class="message is-warning is-light">
<div class=message-body>
We use the Einstein summation notation in this article.
</div>
</article>
<p>Principal Component Analysis (PCA) is a commonly used trick for dimensionality reduction so that the new features represents most of the variances of the data.</p>
<div class=card>
<details>
<summary class="card-header-title card-toggle card-header">
Representations of Dataset
</summary>
<div class=card-content>
<div class=content>
<p>In theory, a dataset can be represented by a matrix if we specify the basis. However, the initial given basis is not always the most convinient one. Suppose we find a new set of basis for the dataset, the matrix representation may be simpler and easier to use.</p>
<p>For convenience, we do not distinguish the representation and the abstract dataset in this article.</p>
</div>
</div>
</details>
</div>
<p>Give a dataset of $n$ features and $m$ rows,</p>
<p>$$
\begin{equation}
\mathbf X = \begin{pmatrix}
\mathbf X_1 \\
\mathbf X_2 \\
\cdots \\
\mathbf X_n
\end{pmatrix},
\end{equation}
$$</p>
<p>where each $\mathbf X_i$ is a $m$ dimensional column vector.</p>
<p>We are looking for a $n\times p$ matrix $\mathbf P$ so that</p>
<p>$$
Z_{ik} = X_{ij}P_{jk},
$$</p>
<p>where $i\in [1, m]$, $k\in [1,p]$, and $j\in [1, n]$. This projection will select out $p$ features.</p>
<div class=card>
<details>
<summary class="card-header-title card-toggle card-header">
Vector Notation Formalism
</summary>
<div class=card-content>
<div class=content>
<p>Given features ${\mathbf X_i}$, the new features ${\mathbf Z_i}$ are related through the $\mathbf U$ matrix,</p>
<p>\begin{equation}
\mathbf Z_k = \mathbf X_j P_{jk}.
\end{equation}</p>
</div>
</div>
</details>
</div>
<p>The coefficients $P_{jk}$ are called <strong>loadings</strong>.</p>
<h2 id=what-is-pca>What is PCA</h2>
<p>The transformation $\mathbf P$ select out features that represents the most variances. How are the variances represented? The <a href=/cards/statistics/covariance-matrix/>covariance matrix</a> sheds light on the problem.</p>
<p>In an ideal condition, the covariance matrix of the dataset matrix is diagonalized. This indicates that the dataset matrix features are not correlated, i.e., the basis axes are in the direction of the spread.</p>
<p>In order to perform PCA, we have to acquire a new transformed dataset matrix $\mathbf Z$ so that the covariance matrix $\mathbf {C_{Z}}$ of $\mathbf Z$ is (mostly) diagonalized. It requires</p>
<p>$$
\begin{align}
\mathbf {C_{Z}} =& \frac{\mathbf{Z}^{T} \mathbf {Z} }{n-1} \\
=& \frac{ (\mathbf{X}\mathbf P)^{T} \mathbf {X} \mathbf P}{n-1} \\
=& \frac{\mathbf P^T\mathbf X^T\mathbf X \mathbf P }{n-1} \\
=&  \mathbf P^T\frac{ (\mathbf X^T\mathbf X) }{n-1} \mathbf P.
\label{eqn-pca-cov-mat-diag}
\end{align}
$$</p>
<p><strong>The covariance matrix</strong> $\mathbf {C_{Z}}$ <strong>is diagonalized if choose</strong> $\mathbf P$ <strong>so that it diagonalizes the matrix</strong> $(\mathbf X^T\mathbf X)$, i.e.,</p>
<p>$$
\begin{equation}
(\mathbf X^T\mathbf X) = \mathbf P^T \mathbf {D_{X^TX}} \mathbf P
\end{equation}
$$</p>
<blockquote>
<p>Notice that $\mathbf P \mathbf P^T = \mathbf I$.</p>
</blockquote>
<p>The component $\mathbf Z_i$ that has the largest variance is the first principal component. In most cases, we arrange the transformation $\mathbf P$ so that the first component $\mathbf Z_1$ is the first principal component. The variance of the first principle component is</p>
<p>\begin{equation}
\frac{1}{n}\sum_{n=1}^N z_{n1},
\end{equation}</p>
<p>which is maximized.</p>
<div class=card>
<details>
<summary class="card-header-title card-toggle card-header">
Relation between PCA and Linear Regression
</summary>
<div class=card-content>
<div class=content>
The ESL has a very nice explanation of this geometrically.
</div>
</div>
</details>
</div>
<h2 id=why-is-mathbf-p-related-to-the-covariance-matrix-again>Why is $\mathbf P$ related to the covariance matrix, again?</h2>
<p>The idea behind PCA is to find the mixings of the features so that the new axes give us the largest variance. We could invent an algorithm to explore the whole parameter space (shifts, scales, and rotations of the axes) and find the best parameters. However, this would be rather inefficient. The proof Eq. ($\ref{eqn-pca-cov-mat-diag}$) provides theoretical support.</p>
<div class=card>
<details>
<summary class="card-header-title card-toggle card-header">
Why the covariance matrix?
</summary>
<div class=card-content>
<div class=content>
We need would like to find a way to mix the features so that the new features are mostly decoupled. In other words, we will require the covariance matrix to be almost diagonalized.
</div>
</div>
</details>
</div>
<p>The covariance matrix for ${\mathbf X_i}$ is</p>
<p>$$
\mathbf{C_{X}} = \begin{pmatrix}
\sigma^2_{\mathbf X_1, \mathbf X_1} & \sigma^2_{\mathbf X_1, \mathbf X_2} & \cdots & \sigma^2_{\mathbf X_1, \mathbf X_n} \\
\sigma^2_{\mathbf X_2, \mathbf X_1} & \sigma^2_{\mathbf X_2, \mathbf X_2} & \cdots & \sigma^2_{\mathbf X_1, \mathbf X_n} \\
\vdots & \vdots & \ddots & \vdots \\
\sigma^2_{\mathbf X_n, \mathbf X_1} & \sigma^2_{\mathbf X_n, \mathbf X_2} & \cdots & \sigma^2_{\mathbf X_n, \mathbf X_n}
\end{pmatrix}
$$</p>
<p>Recall that</p>
<p>$$
\sigma_{X_i,X_j}^2 = \frac{ (\mathbf X_i - \bar X_i)^T (\mathbf X_j - \bar X_j) }{ n }.
$$</p>
<p><strong>We shift our data to be centered by the mean</strong>. If we always prepare our data so that the mean is 0, i.e., $\bar X_i = 0$, the covariance could be written as</p>
<p>$$
\sigma_{\mathbf X_i,\mathbf X_j}^2 = \frac{ X_i^T X_j }{ n }.
$$</p>
<p>If we were to calculate the covariance matrix for the new features ${Z_i}$,</p>
<p>$$
\sigma_{Z_i,Z_j}^2 = \frac{ (U_{ik}X_k)^T (U_{jl}X_l) }{ n } = U_{ik}^T U_{jl} \frac{ X_k^TX_l }{ n } .
$$</p>
<p>From this, we could easily derive that a linear transformation for the covariance matrice indicates a linear transformation of the features.</p>
<p>$$
\mathbf{C_{Z}} = \mathbf U^T \mathbf{C_{X}} \mathbf U.
$$</p>
<p>If we were to require the covariance matrix for $\mathbf{C_{Z}}$ to be diagonalized, we would have found that $\mathbf{C_{Z}}$ is composed of the eigenvalues. We could deliberately remove those eigenvectors with small eigenvalues and keep those that help us the most with the variance of the data.</p>
<p>To summarize, we could find the principal components by finding the eigenvectors of the covariance matrix:</p>
<ol>
<li>Standardize the data: shift the data so that the mean is 0.</li>
<li>Calculate the covariance matrix of the features, $\mathbf {C_{X}}$.</li>
<li>Calculate the eigenvectors of the covariance matrix.</li>
<li>Construct the transformation matrix, $\mathbf P$.</li>
<li>Transform the data using $\mathbf U$, $\mathbf Z = \mathbf X \mathbf P$.</li>
<li>Decide the number of principal components $p$ and choose the first $p$ columns of $\mathbf Z$.</li>
</ol>
<h2 id=svd-and-pca>SVD and PCA</h2>
<p>It is straightforward to prove that the principal components are he orthonormal basis that spans the row space of $\mathbf X$ in our setup <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>.</p>
<h2 id=sparse-pca>Sparse PCA</h2>
<p>The principle components are correlated to many original components. In some problems, we prefer principle components that is only related to a few original features. For such problems, we need the Sparse PCA <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>.</p>
<section class=footnotes role=doc-endnotes>
<hr>
<ol>
<li id=fn:1 role=doc-endnote>
<p>Shlens, J. (2003). <a href=https://www.cs.princeton.edu/picasso/mats/PCA-Tutorial-Intuition_jp.pdf>A Tutorial on Principal Component Analysis</a>.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p>
</li>
<li id=fn:2 role=doc-endnote>
<p>Zou H, Hastie T, Tibshirani R. Sparse Principal Component Analysis. J Comput Graph Stat. 2006;15: 265–286. doi:10.1198/106186006X113430&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p>
</li>
</ol>
</section>
</div>
<p><div class="has-text-right is-size-7">
<span class=icon>
<i class="fas fa-pencil-alt"></i>
</span>
Published: <time datetime=2018-05-25T00:00:00+00:00>2018-05-25</time>
by <span itemprop=author>L Ma</span>;
</div></p>
<div class=is-divider></div>
<nav class="pagination is-centered" role=navigation aria-label=pagination>
<a href="https://datumorphism.leima.is/wiki/machine-learning/unsupervised/overview/?ref=footer" class=pagination-previous>« Unsupervised Learning</a>
<a class=pagination-next href="https://datumorphism.leima.is/wiki/machine-learning/unsupervised/svm/?ref=footer">Unsupervised Learning: SVM »</a>
</nav>
</div>
<div class="column is-4">
<div class=is-divider data-content="Cite Me"></div>
<div class="box is-size-7 has-text-white has-background-black">
<article class=media>
<div class=media-content>
<div class=content>
<p>
L Ma
(2018). 'Unsupervised Learning: PCA', Datumorphism, 05 April. Available at: https://datumorphism.leima.is/wiki/machine-learning/unsupervised/pca/.
</p>
</div>
</div>
</article>
</div>
<style>#TableOfContents>ul{list-style-type:lower-greek;padding-left:0}#TableOfContents>ul>li ul{list-style-type:none;padding-left:1em}</style>
<div class=is-divider data-content=ToC></div>
<div class="box is-size-7">
<article class=media>
<div class=media-content>
<div class=content>
<details>
<summary><i class="fa-solid fa-list-ol"></i> Table of Contents</summary>
<div>
<div>
<nav id=TableOfContents>
<ul>
<li><a href=#what-is-pca>What is PCA</a></li>
<li><a href=#why-is-mathbf-p-related-to-the-covariance-matrix-again>Why is $\mathbf P$ related to the covariance matrix, again?</a></li>
<li><a href=#svd-and-pca>SVD and PCA</a></li>
<li><a href=#sparse-pca>Sparse PCA</a></li>
</ul>
</nav>
</div>
</div>
</details>
</div>
</div>
</article>
</div>
<script>const el=document.querySelector('details summary');el.onclick=()=>{(function(f,c,d,e,a,b){a=c.createElement(d),b=c.getElementsByTagName(d)[0],a.async=1,a.src=e,b.parentNode.insertBefore(a,b)})(window,document,'script','/js/smoothscroll.js'),el.onclick=null},document.querySelectorAll('#TableOfContents a').forEach(a=>{a.addEventListener('click',()=>{document.querySelector(a.href.slice(a.href.indexOf('#'))).scrollIntoView({behavior:'smooth'})})})</script>
<div class=is-divider data-content=REFERENCES></div>
<div class="box is-size-7">
<article class=media>
<div class=media-content style=width:100%;word-break:break-word>
<div class=content>
<p>
<strong><i class="fa-solid fa-quote-left"></i> References: </strong><br>
<ol>
<li class=has-text-weight-bold><a href=https://www.cs.princeton.edu/picasso/mats/PCA-Tutorial-Intuition_jp.pdf style=text-decoration:none>Shlens, J. (2003). A Tutorial on Principal Component Analysis</a></li>
</ol>
</p>
</div>
</div>
</article>
</div>
<div class=is-divider data-content=CONNECTUME></div>
<div class="box is-size-7">
<article class=media>
<div class=media-content style=width:100%>
<div class=content>
<p>
<strong><i class="fa-solid fa-fingerprint"></i> Current Ref: </strong><br>
<ul><li style=list-style:none>
<div>wiki/machine-learning/unsupervised/pca.md</div></li>
</ul>
</p>
</div>
</div>
</article>
</div>
<div class="box is-size-7">
<article class=media>
<div class=media-content>
<div class=content>
<p>
<strong><i class="fa-solid fa-turn-down"></i> Dynamic Links from: </strong>
<div>
<a class=box href=https://datumorphism.leima.is/wiki/machine-learning/factorization/nmf/>
<div class=media-content>
<div class=content>
<div class="title is-6" style="padding-bottom:.8em;border-bottom:1px dotted #999">NMF: Nonnegative Matrix Factorizatioin
<span style=float:right><i class="fa-solid fa-bookmark" style=color:#999></i></span>
</div>
<div style=font-size:80%>
Nonnegative Matrix Factorizatioin has a bright future
</div>
</div>
</div>
</a>
<a class=box href=https://datumorphism.leima.is/awesome/curriculum/>
<div class=media-content>
<div class=content>
<div class="title is-6" style="padding-bottom:.8em;border-bottom:1px dotted #999">Curriculum
<span style=float:right><i class="fa-solid fa-bookmark" style=color:#999></i></span>
</div>
<div style=font-size:80%>
The path that I follow
</div>
</div>
</div>
</a>
</div>
</p>
</div>
</div>
</article>
</div>
<div class="box is-size-7">
<article class=media>
<div class=media-content>
<div class=content>
<p>
<strong><i class="fa-solid fa-turn-up"></i> Links to: </strong>
<div>
<a class=box href=https://datumorphism.leima.is/cards/statistics/covariance-matrix/>
<div class=media-content>
<div class=content>
<div class="title is-6" style="padding-bottom:.8em;border-bottom:1px dotted #999">Covariance Matrix
<span style=float:right><i class="fa-solid fa-bookmark" style=color:#999></i></span>
</div>
<div style=font-size:80%>
Also known as the second central moment is a measurement of the spread.
</div>
</div>
</div>
</a>
</div>
</p>
</div>
</div>
</article>
</div>
<div id=comments class=is-divider data-content=COMMENTS>
</div>
<script src=https://giscus.app/client.js data-repo=datumorphism/comments data-repo-id="MDEwOlJlcG9zaXRvcnkxNjU5MDkyNDI=" data-category=Comments data-category-id=DIC_kwDOCeOS-s4B-Zxx data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=1 data-theme=light crossorigin=anonymous async></script>
</div>
</div>
</article>
</div>
</section>
</div>
<div class=navtools>
<a class="button is-primary is-light is-outlined" title alt="Edit this page" href=https://github.com/datumorphism/datumorphism.github.io/edit/hugo/content/wiki/machine-learning/unsupervised/pca.md target=blank style=position:fixed;bottom:20px;right:10px;border-radius:9999px;width:35px;height:35px>
<i class="fas fa-pencil-alt"></i>
</a>
<a class="button is-primary is-light is-outlined" href=#comments alt=Comments style=position:fixed;bottom:60px;right:10px;border-radius:9999px;width:35px;height:35px>
<i class="far fa-comments"></i>
</a>
</div>
</main>
<footer>
<footer class=footer>
<div class=container>
<div class="content has-text-centered">
<p>
Created and maintained by <a href=https://leima.is>L Ma</a>.
Acknowledgement: <a href=https://gohugo.io/>Hugo</a>,
<a href=https://themes.gohugo.io/bulma/>Bulma</a>, <a href=https://kausalflow.com>KausalFlow</a>.
<strong>love</strong>.
<br>
<a class=tag href=/about>About</a>
<a class=tag href=/index.xml>Feed</a>
<a class=tag href=/data.json>JSON Data</a>
</p>
</div>
</div>
</footer>
</footer>
<script async type=text/javascript src=/js/bulma.js></script>
</body>
</html>