<!doctype html><html lang=en-us><head><meta name=generator content="Hugo 0.69.2"><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><title>Bias-Variance | Datumorphism | Lei Ma</title><meta name=description content="Bias Variance Trade off is a key concept in statistical learning"><meta name=author content="Lei Ma"><meta property="og:title" content="Bias-Variance"><meta property="og:description" content="Bias Variance Trade off is a key concept in statistical learning"><meta property="og:type" content="article"><meta property="og:url" content="https://datumorphism.leima.is/wiki/machine-learning/basics/bias-variance/"><meta property="article:published_time" content="2019-06-07T00:00:00+00:00"><meta property="article:modified_time" content="2019-06-07T00:00:00+00:00"><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-140452515-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><link rel=canonical href=https://datumorphism.leima.is/wiki/machine-learning/basics/bias-variance/><link rel="shortcut icon" type=image/png href=/logos/logo-square.png><link rel=stylesheet href=/css/bulma.css><link rel=stylesheet href=/css/bulma-divider.min.css><link rel=stylesheet href=/assets/css/bulma-ribbon.min.css><link rel=stylesheet href=/assets/css/tooltip.css><link rel=stylesheet href=https://jenil.github.io/bulmaswatch/united/bulmaswatch.min.css><link rel=stylesheet href=/css/custom.css><link rel=stylesheet href=/css/blog-post.css><link rel=stylesheet href=/css/code-highlighting/dark.css><link rel=stylesheet href=/css/custom.css><link rel=stylesheet href=https://unpkg.com/applause-button/dist/applause-button.css><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],tags:'ams',processEscapes:true,processEnvironments:true},options:{skipHtmlTags:['script','noscript','style','textarea','pre']},svg:{fontCache:'global'}};window.addEventListener('load',(event)=>{document.querySelectorAll("mjx-container").forEach(function(x){x.parentElement.classList+='has-jax'})});</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script>mermaid.initialize({startOnLoad:true});</script><script src=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/js/all.min.js integrity="sha256-KzZiKy0DWYsnwMF+X1DvQngQ2/FxF7MF3Ff72XcpuPs=" crossorigin=anonymous></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css integrity="sha256-h20CPZ0QyXlBuAw7A+KluUYx/3pK+c7lYEpqLTlxjYQ=" crossorigin=anonymous></head><body><header><nav class="navbar is-transparent"><div class=navbar-brand><a class=navbar-item href=/><img src=/site/intelligence.png alt=Datumorphism height=28 style=margin-right:.5em> Datumorphism</a><div class="navbar-burger burger" style=color:#000 data-target=navMenu><span></span><span></span><span></span></div></div><div class=navbar-menu id=navMenu><div class=navbar-start><div class="navbar-item has-dropdown is-hoverable"><a href=/projects class=navbar-link>Notebooks</a><div class=navbar-dropdown><a href=https://datumorphism.leima.is/awesome/ class=navbar-item>Awesome</a>
<a href=https://datumorphism.leima.is/blog/ class=navbar-item>Blog</a>
<a href=https://datumorphism.leima.is/cards/ class=navbar-item>Cards</a>
<a href=https://datumorphism.leima.is/hologram/ class=navbar-item>Hologram</a>
<a href=https://datumorphism.leima.is/reading/ class=navbar-item>Reading Notes</a>
<a href=https://datumorphism.leima.is/til/ class=navbar-item>TIL</a>
<a href=https://datumorphism.leima.is/wiki/ class=navbar-item>Wiki</a></div></div></div><span class=navbar-burger><span></span><span></span><span></span></span><div class=navbar-end><div class=navbar-item><a class=navbar-item href=/blog/>Blog</a>
<a class=navbar-item href=/tags/><i class="fas fa-tags"></i></a><a class=navbar-item href=/graph><i class="fas fa-project-diagram"></i></a><a class=navbar-item href=https://t.me/amneumarkt><i class="fab fa-telegram"></i></a><a class=navbar-item target=blank href=https://github.com/datumorphism><span class=icon><i class="fab fa-github"></i></span></a></div></div></div></nav><script>document.addEventListener('DOMContentLoaded',()=>{const $navbarBurgers=Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'),0);if($navbarBurgers.length>0){$navbarBurgers.forEach(el=>{el.addEventListener('click',()=>{const target=el.dataset.target;const $target=document.getElementById(target);el.classList.toggle('is-active');$target.classList.toggle('is-active');});});}});</script></header><main><div class=container itemscope itemtype=http://schema.org/BlogPosting><meta itemprop=name content="Bias-Variance"><meta itemprop=description content="Bias Variance Trade off is a key concept in statistical learning"><meta itemprop=datePublished content="2019-06-07T00:00:00+00:00"><meta itemprop=dateModified content="2019-06-07T00:00:00+00:00"><meta itemprop=wordCount content="626"><meta itemprop=keywords content="Statistical Learning,Basics,"><section class=section><div class=container><article class=post><header class=post-header><nav class="breadcrumb has-succeeds-separator is-small" aria-label=breadcrumbs><ul><li><a href=https://datumorphism.leima.is/>Datumorphism</a></li><li><a href=https://datumorphism.leima.is/wiki/>Wiki</a></li><li><a href=https://datumorphism.leima.is/wiki/machine-learning/>Machine Learning</a></li><li><a href=https://datumorphism.leima.is/wiki/machine-learning/basics/>Machine Learning Basics</a></li><li class=active><a href=https://datumorphism.leima.is/wiki/machine-learning/basics/bias-variance/>Bias-Variance</a></li></ul></nav><h1 class="post-title has-text-centered is-size-1" itemprop="name headline">Bias-Variance</h1><h2 class="title is-6 has-text-centered"><i class="fas fa-tags" style=margin-right:.5em></i><a href=/tags/statistical-learning><span class="tag is-warning is-small is-light">#Statistical Learning</span></a>
<a href=/tags/basics><span class="tag is-warning is-small is-light">#Basics</span></a></h2></header><div class=columns><div class="column is-8"><div class=is-divider data-content=ARTICLE></div><div class="content blog-post section" itemprop=articleBody><h2 id=bias-and-variance>Bias and Variance</h2><p>Suppose $f(X)$ is a perfect model that represents a &ldquo;tight&rdquo; model of the dataset $(X,Y)$ but some irredicible error $\epsilon$,</p><p>$$
\begin{equation}
Y = f(X) + \epsilon.
\label{dataset-using-true-model}
\end{equation}
$$</p><p>On the other hand, we build another model using a specific method such as k-nearest neighbors, which is denoted as $k(X)$.</p><article class="message is-info is-light"><div class=message-header><p>Why the two models?</p></div><div class=message-body><p>Why are we talking about the perfect model and a model using a specific method?</p><p>The perfect model $f(X)$ is our ultimate goal, while the model using a specific method $k(X)$ is our effort of approaching the ultimate model.</p></div></article><p>The bias measures the deficit between $k(X)$ and the perfect model $f(X)$,</p><p>$$
\operatorname{Bias}[k(X)] = E[k(X)] - f(X)
$$</p><p>Zero bias means we are matching the perfect model.</p><article class="message is-info is-light"><div class=message-header><p>$E[g(X)]$</p></div><div class=message-body>$E[g(X)]$ is the expectation of the function.</div></article><p>The variance of the model is a measurement of the consistency</p><p>$$
\operatorname{Variance} ( k(X) ) = \operatorname{E} \left( ( k(X) - \operatorname{E}( k(X) ) )^2 \right)
$$</p><p>The larger the variance, the more wiggly the model is.</p><h2 id=mean-square-error>Mean Square Error</h2><p>Bias measures the deficit between the specific model and the perfect model. To measure the deficit between the specific model and the actual data point, we need the Mean Squared Error (MSE).</p><p>The Mean Squared Error (MSE) is defined as</p><p>$$
\begin{equation}
\operatorname{MSE}(X) = \operatorname{E} \left( ( Y - k(X) )^2 \right).
\end{equation}
$$</p><p>This form of expected error can also be used to evaluate models, i.e., calculate expectations by varying models. A straightforward decomposition using equation ($\ref{dataset-using-true-model}$) shows that we have three components in our expected error. There are several ways to derive the decomposition of the expected error. We only show one here.</p><p>$$
\begin{align}
\operatorname{Expected Error}(X) &= \operatorname{E} \left( ( Y - k )^2 \right) \nonumber \\
&=  \operatorname{E} \left( ( f + \epsilon - k )^2 \right) \\
&= \operatorname{E} \left( (f - \operatorname{E} k - (k - \operatorname{k}) + \epsilon)^2 \right) \\
&= \operatorname{E} \left( (f - \operatorname{E} k)^2 + (k - \operatorname{E}k)^2 + \epsilon^2 - (f - \operatorname{E} k)(k - \operatorname{E}k) + (f - \operatorname{E} k) \epsilon - (k - \operatorname{E}k)\epsilon \right) \\
&=  \operatorname{E} \left( (f - \operatorname{E} k)^2\right) + \operatorname{E}\left((k - \operatorname{E}k)^2\right) + \operatorname{E} \left( \epsilon^2 \right) {\color{red}- 2\operatorname{E} \left( (f - \operatorname{E} k)(k - \operatorname{E}k) \right) + 2\operatorname{E} \left( (f - \operatorname{E} k) \epsilon \right) - 2\operatorname{E} \left( (k - \operatorname{E}k)\epsilon \right)} \\
&= (f - \operatorname{E} k)^2 + \operatorname{E}\left((k - \operatorname{E}k)^2\right) + \operatorname{E} \left( \epsilon^2 \right)\\
&= \operatorname{Bias} ( k )^2 + \operatorname{Variance} (k) + \text{Irreducible Error}
\end{align}
$$</p><p>In this derivation, we&rsquo;ve used several relations.</p><ol><li>We used $\operatorname{E} \left( (f - \operatorname{E} k)^2\right) = (f - \operatorname{E} k)^2$ becuase the term $(f - \operatorname{E} k)^2$ is constant thus the expectation value is itself.</li><li>We have dropped the terms in red. They are related to the fact that the irreducible error is required to be zero, $\operatorname{E}(\epsilon)=0$. If it is not zero then the model $f(X)$ is not perfect.<ol><li>$\operatorname{E} \left( (f - \operatorname{E} k)(k - \operatorname{E}k) \right)= (f - \operatorname{E} k)\operatorname{E} \left( (k - \operatorname{E}k) \right)= 0.$</li><li>$\operatorname{E} \left( (f - \operatorname{E} k) \epsilon \right) = (f - \operatorname{E} k) \operatorname{E} \left( \epsilon \right) = 0.$</li><li>$\operatorname{E} \left( (k - \operatorname{E}k)\epsilon \right) = 0.$</li></ol></li></ol><h2 id=bias-variance-tradeoff>Bias-Variance Tradeoff</h2><p>The more parameters we introduce in the model, it is more likely to reduce the bias. However, at some point, the more complexity we have in the model, the more wiggles the model will have. Thus the variance will be larger.</p><article class="message is-light is-light"><div class=message-header><p>Free Parameters</p></div><div class=message-body><p>Fermi once said,</p><blockquote><p>I remember my friend Johnny von Neumann used to say, with four parameters I can fit an elephant, and with five I can make him wiggle his trunk.</p></blockquote><p>There is a <a href=http://lilith.fisica.ufmg.br/~dsoares/fdyson.htm>nice story</a> about Dyson and Fermi behind this.</p></div></article></div><p><div class="has-text-right is-size-7"><span class=icon><i class="fas fa-pencil-alt"></i></span>Published: <time datetime=2019-06-07T00:00:00+00:00>2019-06-07</time>
by <span itemprop=author>Lei Ma</span>;</div></p><div class=is-divider></div><nav class="pagination is-centered" role=navigation aria-label=pagination><a href="https://datumorphism.leima.is/wiki/machine-learning/basics/confusion-matrix/?ref=footer" class=pagination-previous>« Confusion Matrix (Contingency Table)</a>
<a class=pagination-next href="https://datumorphism.leima.is/wiki/machine-learning/basics/kl-divergence/?ref=footer">KL Divergence »</a></nav></div><div class="column is-4"><div class=is-divider data-content="Cite Me"></div><div class="box is-size-7 has-text-white has-background-black"><article class=media><div class=media-content><div class=content><p>Lei Ma
(2019). 'Bias-Variance', Datumorphism, 06 April. Available at: https://datumorphism.leima.is/wiki/machine-learning/basics/bias-variance/.</p></div></div></article></div><style>#TableOfContents>ul{list-style-type:lower-greek;padding-left:0}#TableOfContents>ul>li ul{list-style-type:none;padding-left:1em}</style><div class=is-divider data-content=ToC></div><div class="box is-size-7"><article class=media><div class=media-content><div class=content><details><summary>Table of Contents</summary><div><div><nav id=TableOfContents><ul><li><a href=#bias-and-variance>Bias and Variance</a></li><li><a href=#mean-square-error>Mean Square Error</a></li><li><a href=#bias-variance-tradeoff>Bias-Variance Tradeoff</a></li></ul></nav></div></div></details></div></div></article></div><script>const el=document.querySelector('details summary')
el.onclick=()=>{(function(l,o,a,d,e,r){e=o.createElement(a),r=o.getElementsByTagName(a)[0];e.async=1;e.src=d;r.parentNode.insertBefore(e,r)})(window,document,'script','/js/smoothscroll.js');el.onclick=null}
document.querySelectorAll('#TableOfContents a').forEach(link=>{link.addEventListener('click',()=>{document.querySelector(link.href.slice(link.href.indexOf('#'))).scrollIntoView({behavior:'smooth'})})})</script><div class=is-divider data-content=REFERENCES></div><div class="box is-size-7"><article class=media><div class=media-content><div class=content><p><strong>References:</strong><br><ol><li class=has-text-weight-bold><a href=http://scott.fortmann-roe.com/docs/BiasVariance.html style=text-decoration:none>Understanding the Bias-Variance Tradeoff</a></li><li class=has-text-weight-bold><a href=https://towardsdatascience.com/the-bias-variance-trade-off-explanation-and-demo-8f462f8d6326 style=text-decoration:none>The Bias-Variance trade-off : Explanation and Demo</a></li><li class=has-text-weight-bold><a href=https://web.stanford.edu/~hastie/ElemStatLearn/ style=text-decoration:none>The Element of Statistical Learning</a></li><li class=has-text-weight-bold><a href="https://books.google.de/books?id=qcI_AAAAQBAJ&source=gbs_navlinks_s" style=text-decoration:none>James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning. In Springer Texts in Statistics. Springer Science & Business Media.</a></li></ol></p></div></div></article></div><div class=is-divider data-content=CONNECTUME></div><div class="box is-size-7"><article class=media><div class=media-content style=width:100%><div class=content><p><strong>Current Ref:</strong><br><ul><li style=list-style:none><div>wiki/machine-learning/basics/bias-variance.md</div></li></ul></p></div></div></article></div><div id=comments class=is-divider data-content=COMMENTS></div><script src=https://utteranc.es/client.js repo=datumorphism/comments issue-term=pathname theme=github-light crossorigin=anonymous async></script></div></div></article></div></section></div><div class=navtools><a class="button is-primary is-light is-outlined" alt="Edit this page" href=https://github.com/datumorphism/datumorphism.github.io/edit/hugo/content/wiki/machine-learning/basics/bias-variance.md target=blank style=position:fixed;bottom:20px;right:10px;border-radius:9999px;width:35px;height:35px><i class="fas fa-pencil-alt"></i></a><a class="button is-primary is-light is-outlined" href=#comments alt=Comments style=position:fixed;bottom:60px;right:10px;border-radius:9999px;width:35px;height:35px><i class="far fa-comments"></i></a></div></main><footer><footer class=footer><div class=container><div class="content has-text-centered"><p>Created and maintained by <a href=https://leima.is>Lei Ma</a>.
Acknowledgement: <a href=https://gohugo.io/>Hugo</a>,
<a href=https://themes.gohugo.io/bulma/>Bulma</a>, <a href=https://kausalflow.com>KausalFlow</a>.
<strong>love</strong>.<br><a class=tag href=/about>About</a>
<a class=tag href=/index.xml>Feed</a>
<a class=tag href=/data.json>JSON Data</a></p></div></div></footer><script src=https://unpkg.com/applause-button/dist/applause-button.js></script></footer><script async type=text/javascript src=/js/bulma.js></script></body></html>