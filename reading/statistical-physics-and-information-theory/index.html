<!doctype html><html lang=en-us><head><meta name=generator content="Hugo 0.69.2"><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><title>Information Theory and Statistical Mechanics | Datumorphism | Lei Ma</title><meta name=author content="Lei Ma"><meta property="og:title" content="Information Theory and Statistical Mechanics"><meta property="og:description" content="Max entropy principle as a method to infer distributions of statistical systems"><meta property="og:type" content="article"><meta property="og:url" content="/reading/statistical-physics-and-information-theory/"><meta property="article:published_time" content="2019-03-01T00:00:00+00:00"><meta property="article:modified_time" content="2019-03-01T00:00:00+00:00"><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-140452515-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><link rel=canonical href=/reading/statistical-physics-and-information-theory/><link rel="shortcut icon" type=image/png href=/logos/logo-square.png><link rel=stylesheet href=/css/bulma.css><link rel=stylesheet href=/css/bulma-divider.min.css><link rel=stylesheet href=/assets/css/bulma-ribbon.min.css><link rel=stylesheet href=/assets/css/tooltip.css><link rel=stylesheet href=https://jenil.github.io/bulmaswatch/united/bulmaswatch.min.css><link rel=stylesheet href=/css/custom.css><link rel=stylesheet href=/css/blog-post.css><link rel=stylesheet href=/css/code-highlighting/dark.css><link rel=stylesheet href=/css/custom.css><link rel=stylesheet href=https://unpkg.com/applause-button/dist/applause-button.css><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],tags:'ams',processEscapes:true,processEnvironments:true},options:{skipHtmlTags:['script','noscript','style','textarea','pre']},svg:{fontCache:'global'}};window.addEventListener('load',(event)=>{document.querySelectorAll("mjx-container").forEach(function(x){x.parentElement.classList+='has-jax'})});</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script>mermaid.initialize({startOnLoad:true});</script><script src=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/js/all.min.js integrity="sha256-KzZiKy0DWYsnwMF+X1DvQngQ2/FxF7MF3Ff72XcpuPs=" crossorigin=anonymous></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css integrity="sha256-h20CPZ0QyXlBuAw7A+KluUYx/3pK+c7lYEpqLTlxjYQ=" crossorigin=anonymous></head><body><header><nav class="navbar is-transparent"><div class=navbar-brand><a class=navbar-item href=/><img src=/site/intelligence.png alt=Datumorphism height=28 style=margin-right:.5em> Datumorphism</a><div class="navbar-burger burger" data-target=navMenu><span></span><span></span><span></span></div></div><div class=navbar-menu id=navMenu><div class=navbar-start><div class="navbar-item has-dropdown is-hoverable"><a href=/projects class=navbar-link>Notebooks</a><div class=navbar-dropdown><a href=/awesome/ class=navbar-item>Awesome</a>
<a href=/blog/ class=navbar-item>Blog</a>
<a href=/cards/ class=navbar-item>Cards</a>
<a href=/reading/ class=navbar-item>Reading Notes</a>
<a href=/til/ class=navbar-item>TIL</a>
<a href=/wiki/ class=navbar-item>Wiki</a></div></div></div><span class=navbar-burger><span></span><span></span><span></span></span><div class=navbar-end><div class=navbar-item><a class=navbar-item href=/blog/>Blog</a>
<a class=navbar-item href=/tags/><i class="fas fa-tags"></i></a><a class=navbar-item href=/graph><i class="fas fa-project-diagram"></i></a><a class=navbar-item target=blank href=https://github.com/datumorphism><span class=icon><i class="fab fa-github"></i></span></a></div></div></div></nav><script>document.addEventListener('DOMContentLoaded',()=>{const $navbarBurgers=Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'),0);if($navbarBurgers.length>0){$navbarBurgers.forEach(el=>{el.addEventListener('click',()=>{const target=el.dataset.target;const $target=document.getElementById(target);el.classList.toggle('is-active');$target.classList.toggle('is-active');});});}});</script></header><main><div class=container itemscope itemtype=http://schema.org/BlogPosting><meta itemprop=name content="Information Theory and Statistical Mechanics"><meta itemprop=description content="Max entropy principle as a method to infer distributions of statistical systems"><meta itemprop=datePublished content="2019-03-01T00:00:00+00:00"><meta itemprop=dateModified content="2019-03-01T00:00:00+00:00"><meta itemprop=wordCount content="508"><meta itemprop=keywords content="intelligence,"><section class=section><div class=container><article class=post><header class=post-header><nav class="breadcrumb has-succeeds-separator is-small" aria-label=breadcrumbs><ul><li><a href=/>Datumorphism</a></li><li><a href=/reading/>Reading Notes</a></li><li class=active><a href=/reading/statistical-physics-and-information-theory/>Information Theory and Statistical Mechanics</a></li></ul></nav><h1 class="post-title has-text-centered is-size-1" itemprop="name headline"><i class="far fa-comment-alt"></i>Information Theory and Statistical Mechanics</h1><h2 class="title is-6 has-text-centered"><i class="fas fa-tags" style=margin-right:.5em></i><a href=/tags/intelligence><span class="tag is-warning is-small is-light">#intelligence</span></a></h2></header><div class=columns><div class="column is-8"><div class=is-divider data-content=SUMMARY></div><div class="notification is-light"><p>Max entropy principle as a method to infer distributions of statistical systems</p></div><div class=is-divider data-content=ARTICLE></div><div class="content blog-post section" itemprop=articleBody><h2 id=what-problem-are-we-solving>What problem are we solving?</h2><p>For a large system consisting of small entities, the phase space could be extremely large. In this problem, we assume that only one quantity is measure on a macroscopic level. Then we have to find the values for other measurable quantities.</p><h2 id=line-of-reasoning>Line of Reasoning</h2><p>Jaynes pointed out in this paper that we are solving an insufficient reason problem in statistical physics. What we could measure is some macroscopic quantity, from which we derive other macroscopic quantities. That being said, we know a system with a lot of possible microscopic states, ${ s_i }$ while the probabilities of each microscopic state ${p_i }$ is not know at all. We also know a macroscopic quantity $\langle f(s_i) \rangle$ which is defined as</p><p>$$
\langle f \rangle = \sum_i p_i f(s_i).
$$</p><p>The question that Jaynes asked was the following.</p><p><strong>How are we supposed to find another macroscopic quantities that also depends on the microscopic state of the system?</strong> Say $g(s_i)$.</p><p>It is a quite interesting question for stat mech. In my opinion, it can be generalized. To visualize this problem, we know think of this landscape of the states. Instead of using the state as the dimensions, we use the probabilities as the dimensions since they are unknown. In the end, we have a coordinate system with each dimension as the value of the probabilities ${p_i}$ and the one dimension for the value of $\langle g \rangle (p_i)$ which depends on ${p_i}$. Now we constructed a landscape of $\langle g \rangle (p_i)$. The question is, how do our universe arrange the landscape? Where are we in this landscape if we are at equilibrium?</p><p>In Jaynes&rsquo; paper, he mentioned several crucial problems.</p><ol><li>Do we need to know the landscape formed by ${p_i}$ and $\langle f(x_i)\rangle$? No.</li><li>Do we need to find the exact location of our system? We have to.</li><li>How? Using max entropy principle.</li><li>How to calculate another macroscopic quantity based on one observed macroscopic quantity and the conservation of probability density? Using the probabilities found in the previous step.</li><li>Why max entropy works from the information point of view? Assumping less about the system.</li><li>Why is the result actually predicting measurements even the theory is purely objective? This shows how simple nature is. Going philosophical.</li><li>With the generality of the formalism, what else can we do with it to improve our statistical mechanics power? Based on the information we know, we have different formalism of statistical physics.</li></ol><h2 id=is-this-related-to-mutual-information>Is this related to mutual information?</h2><p>The idea is great.</p><p>However, I think we have to consider another principle. I call this max mutual information principle. Suppose we do not have this max entropy principle. Somehow, we come up with an expression of the average of another quantity. What will happen from a landscape point of view?</p><p>If we construct these these two landscapes:</p><ol><li>$\langle f(x_i)\rangle$ as a function of ${p_i}$,</li><li>and $\langle g(x_i)\rangle$ as a function of ${p_i}$.</li></ol><p>Max mutual information indicates that the ${p_i}$ inferred from the two quantities should be the same. Huh, trivial.</p></div><p><div class="has-text-right is-size-7"><span class=icon><i class="fas fa-pencil-alt"></i></span>Published: <time datetime=2019-03-01T00:00:00+00:00>2019-03-01</time>
by <span itemprop=author>Lei Ma</span>;</div></p><div class=is-divider></div><nav class="pagination is-centered" role=navigation aria-label=pagination><a href="/reading/graphical-perception/?ref=footer" class=pagination-previous>« Human Graphical Perception of Quantitative...</a>
<a class=pagination-next href="/reading/elements-of-statistics/?ref=footer">Schaum's Outline of Theories and Problems of... »</a></nav></div><div class="column is-4"><style>#TableOfContents>ul{list-style-type:lower-greek;padding-left:0}#TableOfContents>ul>li ul{list-style-type:none;padding-left:1em}</style><div class=is-divider data-content=ToC></div><div class="box is-size-7"><article class=media><div class=media-content><div class=content><details><summary>Table of Contents</summary><div><div><nav id=TableOfContents><ul><li><a href=#what-problem-are-we-solving>What problem are we solving?</a></li><li><a href=#line-of-reasoning>Line of Reasoning</a></li><li><a href=#is-this-related-to-mutual-information>Is this related to mutual information?</a></li></ul></nav></div></div></details></div></div></article></div><script>const el=document.querySelector('details summary')
el.onclick=()=>{(function(l,o,a,d,e,r){e=o.createElement(a),r=o.getElementsByTagName(a)[0];e.async=1;e.src=d;r.parentNode.insertBefore(e,r)})(window,document,'script','/js/smoothscroll.js');el.onclick=null}
document.querySelectorAll('#TableOfContents a').forEach(link=>{link.addEventListener('click',()=>{document.querySelector(link.href.slice(link.href.indexOf('#'))).scrollIntoView({behavior:'smooth'})})})</script><div class=is-divider data-content=REFERENCES></div><div class="box is-size-7"><article class=media><div class=media-content><div class=content><p><strong>References:</strong><br><ol><li class=has-text-weight-bold><a href=https://doi.org/10.1103/PhysRev.106.620 style=text-decoration:none>Jaynes, E. T. (1957). Information Theory and Statistical Mechanics. Physical Review, 106(4), 620–630.</a></li></ol></p></div></div></article></div><div class=is-divider data-content=CONNECTUME></div><div class="box is-size-7"><article class=media><div class=media-content><div class=content><p><strong>Current Ref:</strong><br><ul><li style=list-style:none><span class="tag is-primary is-light has-text-weight-bold">reading/statistical-physics-and-information-theory.md</span></li></ul></p></div></div></article></div><div id=comments class=is-divider data-content=COMMENTS></div><script src=https://utteranc.es/client.js repo=datumorphism/comments issue-term=pathname theme=github-light crossorigin=anonymous async></script></div></div></article></div></section></div><div class=navtools><a class="button is-primary is-light is-outlined" alt="Edit this page" href=https://github.com/datumorphism/datumorphism.github.io/edit/hugo/content/reading/statistical-physics-and-information-theory.md target=blank style=position:fixed;bottom:20px;right:10px;border-radius:9999px;width:35px;height:35px><i class="fas fa-pencil-alt"></i></a><a class="button is-primary is-light is-outlined" href=#comments alt=Comments style=position:fixed;bottom:60px;right:10px;border-radius:9999px;width:35px;height:35px><i class="far fa-comments"></i></a><style>applause-button .count-container{top:-60%!important}</style><applause-button color=red multiclap=true style="width: 35px; height: 35px;position:fixed;bottom: 100px;right: 10px;"></div></main><footer><footer class=footer><div class=container><div class="content has-text-centered"><p>Created and maintained by <a href=/>Lei Ma</a>.
Acknowledgement: <a href=https://gohugo.io/>Hugo</a>,
<a href=https://themes.gohugo.io/bulma/>Bulma</a>, <a href=https://kausalflow.com>KausalFlow</a>.
<strong>love</strong>.<br><a class=tag href=/about>About</a>
<a class=tag href=/index.xml>Feed</a>
<a class=tag href=/data.json>JSON Data</a></p></div></div></footer><script src=https://unpkg.com/applause-button/dist/applause-button.js></script></footer><script async type=text/javascript src=/js/bulma.js></script></body></html>