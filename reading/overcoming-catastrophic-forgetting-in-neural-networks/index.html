<!doctype html><html lang=en-us><head><meta name=generator content="Hugo 0.69.2"><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><title>Overcoming catastrophic forgetting in neural networks | Datumorphism | Lei Ma</title><meta name=author content="Lei Ma"><meta property="og:title" content="Overcoming catastrophic forgetting in neural networks"><meta property="og:description" content="Using a newly defined loss function the authors could implement an idea that achieves the multi-task within one network."><meta property="og:type" content="article"><meta property="og:url" content="https://datumorphism.leima.is/reading/overcoming-catastrophic-forgetting-in-neural-networks/"><meta property="article:published_time" content="2017-05-14T00:00:00+00:00"><meta property="article:modified_time" content="2017-05-14T00:00:00+00:00"><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-140452515-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><link rel=canonical href=https://datumorphism.leima.is/reading/overcoming-catastrophic-forgetting-in-neural-networks/><link rel="shortcut icon" type=image/png href=/logos/logo-square.png><link rel=stylesheet href=/css/bulma.css><link rel=stylesheet href=/css/bulma-divider.min.css><link rel=stylesheet href=/assets/css/bulma-ribbon.min.css><link rel=stylesheet href=/assets/css/tooltip.css><link rel=stylesheet href=https://jenil.github.io/bulmaswatch/united/bulmaswatch.min.css><link rel=stylesheet href=/css/custom.css><link rel=stylesheet href=/css/blog-post.css><link rel=stylesheet href=/css/code-highlighting/dark.css><link rel=stylesheet href=/css/custom.css><link rel=stylesheet href=https://unpkg.com/applause-button/dist/applause-button.css><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],tags:'ams',processEscapes:true,processEnvironments:true},options:{skipHtmlTags:['script','noscript','style','textarea','pre']},svg:{fontCache:'global'}};window.addEventListener('load',(event)=>{document.querySelectorAll("mjx-container").forEach(function(x){x.parentElement.classList+='has-jax'})});</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script>mermaid.initialize({startOnLoad:true});</script><script src=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/js/all.min.js integrity="sha256-KzZiKy0DWYsnwMF+X1DvQngQ2/FxF7MF3Ff72XcpuPs=" crossorigin=anonymous></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css integrity="sha256-h20CPZ0QyXlBuAw7A+KluUYx/3pK+c7lYEpqLTlxjYQ=" crossorigin=anonymous></head><body><header><nav class="navbar is-transparent"><div class=navbar-brand><a class=navbar-item href=/><img src=/site/intelligence.png alt=Datumorphism height=28 style=margin-right:.5em> Datumorphism</a><div class="navbar-burger burger" style=color:#000 data-target=navMenu><span></span><span></span><span></span></div></div><div class=navbar-menu id=navMenu><div class=navbar-start><div class="navbar-item has-dropdown is-hoverable"><a href=/projects class=navbar-link>Notebooks</a><div class=navbar-dropdown><a href=https://datumorphism.leima.is/awesome/ class=navbar-item>Awesome</a>
<a href=https://datumorphism.leima.is/blog/ class=navbar-item>Blog</a>
<a href=https://datumorphism.leima.is/cards/ class=navbar-item>Cards</a>
<a href=https://datumorphism.leima.is/hologram/ class=navbar-item>Hologram</a>
<a href=https://datumorphism.leima.is/reading/ class=navbar-item>Reading Notes</a>
<a href=https://datumorphism.leima.is/til/ class=navbar-item>TIL</a>
<a href=https://datumorphism.leima.is/wiki/ class=navbar-item>Wiki</a></div></div></div><span class=navbar-burger><span></span><span></span><span></span></span><div class=navbar-end><div class=navbar-item><a class=navbar-item href=/blog/>Blog</a>
<a class=navbar-item href=/tags/><i class="fas fa-tags"></i></a><a class=navbar-item href=/graph><i class="fas fa-project-diagram"></i></a><a class=navbar-item href=https://t.me/amneumarkt><i class="fab fa-telegram"></i></a><a class=navbar-item target=blank href=https://github.com/datumorphism><span class=icon><i class="fab fa-github"></i></span></a></div></div></div></nav><script>document.addEventListener('DOMContentLoaded',()=>{const $navbarBurgers=Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'),0);if($navbarBurgers.length>0){$navbarBurgers.forEach(el=>{el.addEventListener('click',()=>{const target=el.dataset.target;const $target=document.getElementById(target);el.classList.toggle('is-active');$target.classList.toggle('is-active');});});}});</script></header><main><div class=container itemscope itemtype=http://schema.org/BlogPosting><meta itemprop=name content="Overcoming catastrophic forgetting in neural networks"><meta itemprop=description content="Using a newly defined loss function the authors could implement an idea that achieves the multi-task within one network."><meta itemprop=datePublished content="2017-05-14T00:00:00+00:00"><meta itemprop=dateModified content="2017-05-14T00:00:00+00:00"><meta itemprop=wordCount content="637"><meta itemprop=keywords content="neural network,"><section class=section><div class=container><article class=post><header class=post-header><nav class="breadcrumb has-succeeds-separator is-small" aria-label=breadcrumbs><ul><li><a href=https://datumorphism.leima.is/>Datumorphism</a></li><li><a href=https://datumorphism.leima.is/reading/>Reading Notes</a></li><li class=active><a href=https://datumorphism.leima.is/reading/overcoming-catastrophic-forgetting-in-neural-networks/>Overcoming catastrophic forgetting in neural networks</a></li></ul></nav><h1 class="post-title has-text-centered is-size-1" itemprop="name headline"><i class="far fa-comment-alt"></i>Overcoming catastrophic forgetting in neural networks</h1><h2 class="title is-6 has-text-centered"><i class="fas fa-tags" style=margin-right:.5em></i><a href=/tags/neural-network><span class="tag is-warning is-small is-light">#neural network</span></a></h2></header><div class=columns><div class="column is-8"><div class=is-divider data-content=SUMMARY></div><div class="notification is-light"><p>Using a newly defined loss function the authors could implement an idea that achieves the multi-task within one network.</p></div><div class=is-divider data-content=ARTICLE></div><div class="content blog-post section" itemprop=articleBody><h2 id=the-general-idea>The General Idea</h2><p>During the training of neural networks, the previous tasks would be forgotten significantly if completely new types of data are fed to the network. This is dubbed as <strong>catastrophic forgetting</strong>.</p><p>Naively speaking, the way to overcome this forgetting is to either keep some of the structure in the network or feed the previous data sets together with the new ones. Most of the time the second methods doesn&rsquo;t make sense since it requires larger overall data set as new tasks are added which consumes more computation resources. The authors use the first method.</p><h2 id=the-loss-function>The Loss Function</h2><p>To achieve this, they defined a loss function at step i</p><p>$$
\mathscr L_i = \frac{1}{2}\left( ( W_i x_i - y_i )^2 + \lvert W_i - W_{i-1} \rvert_{M_i} \right),
$$</p><p>where $W_i$ is the weight at step i. The last term is the distance between the weight of the current step and the previous step. Such a loss function requires that the weight in the new steps should NOT change all the weights a lot.</p><p>During a minimization of the new step of training, a traditional method is to minimize $( W_i x_i - y_i )^2$. However, if such a minimization changes too many weights by a large amount, the new loss function $\mathscr L_i$ would be very large and becomes unacceptable. Thus the minimization of the new loss requires the network to use most of the weights in the previous steps while completing the new task.</p><p>The term $\lvert W_i - W_{i-1} \rvert_{M_i}$ is the distance and can be any form that has the meaning of distance. The author tested diagonal elements of Fisher matrix which works well.</p><h2 id=bayesian-analysis>Bayesian Analysis</h2><p>The previous statements contain an assumption that the network has enough degrees of freedom to perform all the tasks. In other words, there exists different sets of parameters that can lead to the same results.</p><p>The Bayesian theorem tells us that</p><p>$$
P(A\vert B) P(B) = P(B\vert A)P(A),
$$</p><p>which has a log form</p><p>$$
\log( P(A\vert B) ) = \log( P(B\vert A) ) + \log(P(A)) - \log(P(B)).
$$</p><p>Back the neural networks, the set of weights $\theta = {W_i}$ are the variables to calculate and the prior is the data set $D$. The Bayesian theorem becomes</p><p>$$
\log( P(\theta\vert D) ) = \log( P(D\vert \theta) ) + \log(P(\theta)) - \log(P(D)).
$$</p><p>For given parameters the calculation probability of data is described by $\log( P(D\vert \theta) )$, which is related to the loss function. This equation says that to calculate the weights for given data set we need to calculate the loss function as well as the priors, in theory.</p><p>For two data sets $D_A$ and $D_B$,</p><p>$$
\log( P(\theta\vert {D_A , D_B}) ) = \log( P(D_B \vert \theta) ) \log( P(\theta\vert D_A)) - \log( P(D_B) ).
$$</p><p>$\log( P(\theta\vert D_A , D_B) )$ tells us about the weights for both data sets. $\log( P(D_B\vert\theta)) $ is the loss function of the second data set $B$. When two data sets are considered we have to consider the weights trained from the first data set.</p><p>As we have seen, this can be achieved by defining a new loss function that contains the change of weights.</p><h2 id=so>So?</h2><p>For a very high dimensional parameter space, the land scape probably has multiple possible configurations that can help to complete one task. The most efficient way to really develop a multi-task network is to find the common configuration for all the data sets. It seems to be hard if we don&rsquo;t train them simultaneously. The method the author used is one of the ways but it is a compromise and doesn&rsquo;t guarantee a common configuration.</p><p>I would be convinced that this is a good method until it can be proven that it selects out the most important weights and fixed them (dropout).</p></div><p><div class="has-text-right is-size-7"><span class=icon><i class="fas fa-pencil-alt"></i></span>Published: <time datetime=2017-05-14T00:00:00+00:00>2017-05-14</time>
by <span itemprop=author>Lei Ma</span>;</div></p><div class=is-divider></div><nav class="pagination is-centered" role=navigation aria-label=pagination><a href="https://datumorphism.leima.is/reading/data-mining/?ref=footer" class=pagination-previous>« Data Mining: Concepts and Techniques</a>
<a class=pagination-next href="https://datumorphism.leima.is/reading/working-memory-and-brain-waves/?ref=footer">Working Memory and Brain Waves »</a></nav></div><div class="column is-4"><div class=is-divider data-content="Cite Me"></div><div class="box is-size-7 has-text-white has-background-black"><article class=media style=overflow-x:scroll><div class=media-content><div class=content><p>Lei Ma (2017). 'Overcoming catastrophic forgetting in neural networks', Datumorphism, 05 April. Available at: https://datumorphism.leima.is/reading/overcoming-catastrophic-forgetting-in-neural-networks/.</p></div></div></article></div><style>#TableOfContents>ul{list-style-type:lower-greek;padding-left:0}#TableOfContents>ul>li ul{list-style-type:none;padding-left:1em}</style><div class=is-divider data-content=ToC></div><div class="box is-size-7"><article class=media><div class=media-content><div class=content><details><summary>Table of Contents</summary><div><div><nav id=TableOfContents><ul><li><a href=#the-general-idea>The General Idea</a></li><li><a href=#the-loss-function>The Loss Function</a></li><li><a href=#bayesian-analysis>Bayesian Analysis</a></li><li><a href=#so>So?</a></li></ul></nav></div></div></details></div></div></article></div><script>const el=document.querySelector('details summary')
el.onclick=()=>{(function(l,o,a,d,e,r){e=o.createElement(a),r=o.getElementsByTagName(a)[0];e.async=1;e.src=d;r.parentNode.insertBefore(e,r)})(window,document,'script','/js/smoothscroll.js');el.onclick=null}
document.querySelectorAll('#TableOfContents a').forEach(link=>{link.addEventListener('click',()=>{document.querySelector(link.href.slice(link.href.indexOf('#'))).scrollIntoView({behavior:'smooth'})})})</script><div class=is-divider data-content=REFERENCES></div><div class="box is-size-7"><article class=media><div class=media-content><div class=content><p><strong>References:</strong><br><ol><li class=has-text-weight-bold><a href=http://www.pnas.org/content/114/13/3521 style=text-decoration:none>Overcoming catastrophic forgetting in neural networks</a></li></ol></p></div></div></article></div><div class=is-divider data-content=CONNECTUME></div><div class="box is-size-7"><article class=media><div class=media-content style=width:100%><div class=content><p><strong>Current Ref:</strong><br><ul><li style=list-style:none><div style=overflow-x:scroll>reading/overcoming-catastrophic-forgetting-in-neural-networks.md</div></li></ul></p></div></div></article></div><div id=comments class=is-divider data-content=COMMENTS></div><script src=https://utteranc.es/client.js repo=datumorphism/comments issue-term=pathname theme=github-light crossorigin=anonymous async></script></div></div></article></div></section></div><div class=navtools><a class="button is-primary is-light is-outlined" alt="Edit this page" href=https://github.com/datumorphism/datumorphism.github.io/edit/hugo/content/reading/overcoming-catastrophic-forgetting-in-neural-networks.md target=blank style=position:fixed;bottom:20px;right:10px;border-radius:9999px;width:35px;height:35px><i class="fas fa-pencil-alt"></i></a><a class="button is-primary is-light is-outlined" href=#comments alt=Comments style=position:fixed;bottom:60px;right:10px;border-radius:9999px;width:35px;height:35px><i class="far fa-comments"></i></a><a class="button is-primary is-light is-outlined" data-lyket-type=updown data-lyket-id=overcoming-catastrophic-forgetting-in-neural-networks data-lyket-namespace=Datumorphism data-lyket-template=reddit style=width:35px;height:35px;position:fixed;bottom:120px;right:10px;border-radius:9999px></a></div></main><footer><footer class=footer><div class=container><div class="content has-text-centered"><p>Created and maintained by <a href=https://leima.is>Lei Ma</a>.
Acknowledgement: <a href=https://gohugo.io/>Hugo</a>,
<a href=https://themes.gohugo.io/bulma/>Bulma</a>, <a href=https://kausalflow.com>KausalFlow</a>.
<strong>love</strong>.<br><a class=tag href=/about>About</a>
<a class=tag href=/index.xml>Feed</a>
<a class=tag href=/data.json>JSON Data</a></p></div></div></footer><script src=https://unpkg.com/applause-button/dist/applause-button.js></script></footer><script src="https://unpkg.com/@lyket/widget@latest/dist/lyket.js?apiKey=4df20b7e32f469fed5dc53f5ab39d8&disableSessionId"></script><script async type=text/javascript src=/js/bulma.js></script></body></html>