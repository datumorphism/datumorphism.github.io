<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Tools on Datumorphism</title><link>https://datumorphism.leima.is/tags/tools/</link><description>Recent content in Tools on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Mon, 31 Jan 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://datumorphism.leima.is/tags/tools/index.xml" rel="self" type="application/rss+xml"/><item><title>Jupyter Notebook</title><link>https://datumorphism.leima.is/wiki/tools/jupyter/</link><pubDate>Wed, 20 Jun 2018 15:58:49 -0400</pubDate><guid>https://datumorphism.leima.is/wiki/tools/jupyter/</guid><description>Magics %lsmagic will show all the magics, including line magics and cell magics.
Line magics are magics start with one %; Cell magics are magics that can be used in the whole cell even with line breaks, where the cell should start with %%. %env can be used when setting environment variables inside the notebook.
%env MONGO_URI=localhost:27072 %%bash is a cell magic that allows bash commands in the cell.
%%bash ls pip install datahubxyz %%time enables timing of functions.
%%time for i in range(1000): i*i %timeit is the corresponding line magic which times the function of the corresponding line.</description></item><item><title>Amazon CloudWatch Logs</title><link>https://datumorphism.leima.is/wiki/tools/awslogs/</link><pubDate>Mon, 11 Mar 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/tools/awslogs/</guid><description>Why Suppose we have all kinds of pipelines written in different languages, using different tools, and located in different places. It would be frustrating to pull out the logs.
This is why we need a centralized log service, for example cloudwatch.
Sending logs to CloudWatch First of all, send your logs to awslogs. The easies way is to use boto.
Retrieving and Analyzing Logs First of all, we need this: awslogs. With the logs sent to cloudwatch, we then could read out the logs using the following command:
awslogs get etl-tools --start='1d ago' --timestamp --output text | grep error This will print out the logs 1 day ago.</description></item><item><title>GNUPlot</title><link>https://datumorphism.leima.is/wiki/tools/gnuplot/</link><pubDate>Mon, 04 Sep 2017 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/tools/gnuplot/</guid><description>Examples Plot .csv data. Suppose we have data of such.
-0.00999983, 0.99995 -0.0199987, 0.9998 -0.0299955, 0.99955 -0.0399893, 0.9992 -0.0499792, 0.99875 -0.059964, 0.998201 To plot the second column against the first column, we use the using parameter in gnuplot.
gnuplot -e &amp;#34;set terminal png; set datafile separator &amp;#39;,&amp;#39; ; plot &amp;#39;complex.txt&amp;#39; using 1:2&amp;#34; | imgcat # datafile seperator is not always necessary # imgcat is a script in iterm2 on mac</description></item><item><title>Terminal</title><link>https://datumorphism.leima.is/wiki/tools/terminal/</link><pubDate>Tue, 31 Dec 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/tools/terminal/</guid><description>Navigating Some tips to help data scientist navigate faster in terminal.
pushd, popd and dirs pushd to register and change directories: pushd folder_name will change current directory to folder_name and register the folder folder_name in our stack. If no folder name is passed onto the command, it will be default to $HOME folder. popd to go to the last directory in the stack and remove it from the stack. In this example, popd will change the current working directory to folder_name. dirs will list all working directories registered in the stack. The current working directory will always be in the stack.</description></item><item><title>Git</title><link>https://datumorphism.leima.is/wiki/tools/git/</link><pubDate>Wed, 22 Jun 2016 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/tools/git/</guid><description>Git Services GitHub Bitbucket GitLab Using Git with GUI There are huge amounts of git commands! There are also a lot of GUIs if you don&amp;rsquo;t like command line.
GitHub Desktop GitKraken SourceTree &amp;hellip; Useful Commands To check all the commits related to a file, use git log -u. Try out git log -g before determining which reflog to deal with. To compare the changes with the last commit, use git diff --cached HEAD~1. Useful Alias A very useful article here: Must Have Git Aliases: Advanced Examples.
git config --global alias.unstage 'reset HEAD --' git config --global alias.</description></item><item><title>Cookiecutter</title><link>https://datumorphism.leima.is/wiki/tools/cookiecutter/</link><pubDate>Fri, 27 Aug 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/tools/cookiecutter/</guid><description>Cookiecutter is a good tool to setup a scaffold for a data science project. cookiecutter-data-science is a very good template to use.
If some specific (internal) packages are needed for almost every package, fork cookiecutter-data-science and make some changes for future use. For example, one might use mkdocs Documentation Documenting my data science project using sphinx or mkdocs-material instead of sphinx Documentation Documenting my data science project using sphinx or mkdocs-material . Swap out sphinx for mkdocs if needed.</description></item><item><title>Documentation</title><link>https://datumorphism.leima.is/wiki/tools/documentation/</link><pubDate>Sat, 28 Aug 2021 00:03:10 +0200</pubDate><guid>https://datumorphism.leima.is/wiki/tools/documentation/</guid><description>I would vote for two very different documentation tools for a data science project,
sphinx docs, and squidfunk/mkdocs-material. Sphinx docs Sphinx docs is a mature and stable. I love reStructuredText as the syntax as it is very versatile. It supports math, figures with captions, admonitions, cross reference, auto doc from docstrings, cross project cross referencing, pdf generation, etc.
reStructuredText is not the only choice
We can also use markdown by choosing the markdown parser. squidfunk/mkdocs-material squidfunk&amp;rsquo;s mkdocs-material is a rather light-weight but is also growing to be versatile now. The engine is mkdocs. mkdocs-material is a theme but provides a lot of useful and easy to use elements.</description></item><item><title>Tools</title><link>https://datumorphism.leima.is/wiki/tools/</link><pubDate>Mon, 31 Jan 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/tools/</guid><description/></item><item><title>Data Processing - (Py)Spark</title><link>https://datumorphism.leima.is/wiki/tools/data-processing-spark/</link><pubDate>Mon, 31 Jan 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/tools/data-processing-spark/</guid><description>Spark uses Resilient Distributed Dataset (RDD).
PySpark PySpark provides
SparkContext SparkSession A spark dataframe is immutable. This makes it tricky to update a dataframe.
Useful Commands Get Session In pyspark, we can also get or create the session using the following method.
pyspark.sql.SparkSession.builder.getOrCreate() List all Tables A pyspark.sql.SparkSession has property catalog and can be used to list the tables.
pyspark.sql.SparkSession.catalog.listTables() Run SQL Query Given a SQL query query, we can query the table using .sql() method of the session.
query = ... pyspark.sql.SparkSession.sql(query) Convert Dataframe to Pandas Dataframe .toPandas() Convert Pandas Dataframe to Spark Dataframe To create a spark dataframe from a pandas dataframe,</description></item></channel></rss>