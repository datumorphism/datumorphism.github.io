<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>PySpark on Datumorphism</title><link>https://datumorphism.leima.is/tags/pyspark/</link><description>Recent content in PySpark on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Mon, 14 Feb 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://datumorphism.leima.is/tags/pyspark/index.xml" rel="self" type="application/rss+xml"/><item><title>Data Processing - (Py)Spark</title><link>https://datumorphism.leima.is/wiki/tools/data-processing-spark/</link><pubDate>Mon, 31 Jan 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/tools/data-processing-spark/</guid><description>Spark uses Resilient Distributed Dataset (RDD).
Spark Clusters In one cluster, we have a driver is responsible for managing the tasks, result consolidation, and also shared data access1.
PySpark PySpark provides
SparkContext SparkSession A spark dataframe is immutable. This makes it tricky to update a dataframe.
Useful Commands Get Session In pyspark, we can also get or create the session using the following method.
pyspark.sql.SparkSession.builder.getOrCreate() List all Tables A pyspark.sql.SparkSession has property catalog and can be used to list the tables.
pyspark.sql.SparkSession.catalog.listTables() Run SQL Query Given a SQL query query, we can query the table using .</description></item><item><title>PySpark: Compare Two Schemas</title><link>https://datumorphism.leima.is/til/data/pyspark-schema-comparison/</link><pubDate>Mon, 14 Feb 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/data/pyspark-schema-comparison/</guid><description>def schema_diff(schema1, schema2): return { &amp;#39;fields_in_1_not_2&amp;#39;: set(schema1) - set(schema2), &amp;#39;fields_in_2_not_1&amp;#39;: set(schema2) - set(schema1) }</description></item></channel></rss>