<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>PySpark on Datumorphism</title><link>https://datumorphism.leima.is/tags/pyspark/</link><description>Recent content in PySpark on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Wed, 20 Apr 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://datumorphism.leima.is/tags/pyspark/index.xml" rel="self" type="application/rss+xml"/><item><title>Data Processing - (Py)Spark</title><link>https://datumorphism.leima.is/wiki/tools/data-processing-spark/</link><pubDate>Mon, 31 Jan 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/tools/data-processing-spark/</guid><description>Spark uses Resilient Distributed Dataset (RDD).
Spark Clusters In one cluster, we have a driver is responsible for managing the tasks, result consolidation, and also shared data access1.
PySpark PySpark provides
SparkContext SparkSession A spark dataframe is immutable. This makes it tricky to update a dataframe.
Useful Commands Get Session In pyspark, we can also get or create the session using the following method.
pyspark.sql.SparkSession.builder.getOrCreate() List all Tables A pyspark.sql.SparkSession has property catalog and can be used to list the tables.
pyspark.sql.SparkSession.catalog.listTables() Run SQL Query Given a SQL query query, we can query the table using .</description></item><item><title>PySpark: Beware of Python Mutable Objects</title><link>https://datumorphism.leima.is/til/data/pyspark.beware-of-mutable-objects/</link><pubDate>Wed, 20 Apr 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/data/pyspark.beware-of-mutable-objects/</guid><description>.file { display: block } I created this example notebook to demonstrate the potential danger when dealing with mutable objects in pyspark udfs.
https://gist.github.com/emptymalei/07ba6716d0e2d815ebb64adce25dee72
In the above notebook, we can see that python lists in udfs are behaving like just pointers. For group in the aggregation, we see that the lists in the same values in column b are behaving like the same list, thus pointer like.
To solve this problem, we can do a few things.
Cache the dataframe after aggregation.
sdf_2 = sdf.groupby(&amp;#34;language&amp;#34;, &amp;#34;b&amp;#34;).agg(F.max(&amp;#34;b&amp;#34;).alias(&amp;#34;combined&amp;#34;)).cache() Make a copy of the mutable object.</description></item><item><title>PySpark: Compare Two Schemas</title><link>https://datumorphism.leima.is/til/data/pyspark-schema-comparison/</link><pubDate>Mon, 14 Feb 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/data/pyspark-schema-comparison/</guid><description>To compare two dataframe schemas in [[PySpark]] Data Processing - (Py)Spark Processing Data using (Py)Spark , we can utilize the set operations in python.
def schema_diff(schema1, schema2): return { &amp;#39;fields_in_1_not_2&amp;#39;: set(schema1) - set(schema2), &amp;#39;fields_in_2_not_1&amp;#39;: set(schema2) - set(schema1) }</description></item></channel></rss>