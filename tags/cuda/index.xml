<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>CUDA on Datumorphism</title><link>https://datumorphism.leima.is/tags/cuda/</link><description>Recent content in CUDA on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Wed, 19 Oct 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://datumorphism.leima.is/tags/cuda/index.xml" rel="self" type="application/rss+xml"/><item><title>CUDA Memory</title><link>https://datumorphism.leima.is/cards/machine-learning/practice/cuda-memory/</link><pubDate>Wed, 19 Oct 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/practice/cuda-memory/</guid><description>CUDA is widely used in deep learning. Though many of deep learning professionals are not exposed to CUDA directly, most people are already using CUDA as frameworks like PyTorch is providing GPU support through CUDA.
To optimize the computational efficiency of our models, knowledge about the data transfer inside the devices is crucial. In this note, we build up the fundamentals of memory transfer for CUDA.
Segmented Memory and Paged Memory CUDA Can not Use Paged Memory A CPU host uses paged memory. However, GPU can not directly take data from paged memory on the host1. Before accessing the data, CUDA has to pin the memory so that the memory is page-locked2.</description></item></channel></rss>