<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Artificial Neuron on Datumorphism</title><link>https://datumorphism.leima.is/tags/artificial-neuron/</link><description>Recent content in Artificial Neuron on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Thu, 25 Feb 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://datumorphism.leima.is/tags/artificial-neuron/index.xml" rel="self" type="application/rss+xml"/><item><title>McCulloch-Pitts Model</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/mcculloch-pitts-model/</link><pubDate>Thu, 25 Feb 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/mcculloch-pitts-model/</guid><description>The McCulloch-Pitts model maps the input $\{x_1, x_2,\cdots, x_i \cdots, x_N \}$ into a scalar $y\in\{1,-1\}$,
$$ y = \operatorname{sign}( w\cdot x - b). $$
Since $w\cdot x - b = 0$ is a hyperplane, the McCulloch-Pitts model separates the state space using this hyperplane. The shift $b$ determines the interception, and $w$ decides the slope.</description></item><item><title>Rosenblatt's Perceptron</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/rosenblatt-perceptron/</link><pubDate>Thu, 25 Feb 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/rosenblatt-perceptron/</guid><description>Rosenblatt&amp;rsquo;s perceptron connects McCulloch-Pitts neurons in levels.
Rosenblatt proposed that we fix all the weights and leave the weights of the last neuron free.
The first few layers but the last layer is used as a transformation of the input data ${x_1, \cdots, x_i, \cdots, x_N}$ into a new space ${z_1, \cdots, z_i, \cdots, z_{N&amp;rsquo;}}$. The classification is done on the ${z_1, \cdots, z_i, \cdots, z_{N&amp;rsquo;}}$ space by tuning the last neuron.</description></item><item><title>BiPolar Sigmoid</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-bi-polar-sigmoid/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-bi-polar-sigmoid/</guid><description>A BiPolar sigmoid function is
$$ \sigma(x) = \frac{1-e^{-x}}{1+e^{-x}}. $$
Visualization Bipolar Sigmoid</description></item><item><title>Conic Section Function</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-conic-section-function/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-conic-section-function/</guid><description> TODO
Expand this article. See references 1.
Dorffner1994 Dorffner G. UNIFIED FRAMEWORK FOR MLPs AND RBFNs: INTRODUCING CONIC SECTION FUNCTION NETWORKS. Cybern Syst. 1994;25: 511â€“554. doi:10.1080/01969729408902340 &amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Hyperbolic Tanh</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-hyperbolic-tangent/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-hyperbolic-tangent/</guid><description>$$ \tanh(x) = \frac{\sinh(x)}{\cosh(x)} = \frac{e^{x} - e^{-x}}{e^x + e^{-x}} $$
Hyperbolic tangent</description></item><item><title>Radial Basis Function</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-radial-basis-function/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-radial-basis-function/</guid><description> Hyperbolic tangentTwo unnormalized Gaussian radial basis functions in one input dimension. The basis function centers are located at x1=0.75 and x2=3.25. Source Unnormalized Radial Basis Functions</description></item><item><title>ReLu</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-relu/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-relu/</guid><description>Rectified Linear Unit (ReLu) is a very popular activation function in deep learning. ReLu is defined as
$$ \begin{cases} x, &amp; \text{if }x=0 \\ 0, &amp; \text{else.} \end{cases} $$
Visualizations ReLu
Derivative of ReLu
Code def relu(self, x): return x * (x &amp;gt; 0) Full code to generate the data used in this article Full code to generate the data used in this article
from torch import nn import matplotlib.</description></item><item><title>Uni-Polar Sigmoid</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-uni-polar-sigmoid/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-uni-polar-sigmoid/</guid><description>A uni-Polar sigmoid function is
$$ \sigma(x) = \frac{1}{1+e^{-x}}. $$
Visualization Uni-polar Sigmoid function Tricks A very useful trick: $$ 1 - \sigma(x) = \sigma(-x). $$</description></item></channel></rss>