<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Artificial Neuron on Datumorphism</title><link>https://datumorphism.leima.is/tags/artificial-neuron/</link><description>Recent content in Artificial Neuron on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Thu, 25 Feb 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://datumorphism.leima.is/tags/artificial-neuron/index.xml" rel="self" type="application/rss+xml"/><item><title>McCulloch-Pitts Model</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/mcculloch-pitts-model/</link><pubDate>Thu, 25 Feb 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/mcculloch-pitts-model/</guid><description>The McCulloch-Pitts model maps the input $\{x_1, x_2,\cdots, x_i \cdots, x_N \}$ into a scalar $y\in\{1,-1\}$,
$$ y = \operatorname{sign}( w\cdot x - b). $$
Since $w\cdot x - b = 0$ is a hyperplane, the McCulloch-Pitts model separates the state space using this hyperplane. The shift $b$ determines the interception, and $w$ decides the slope.</description></item><item><title>Rosenblatt's Perceptron</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/rosenblatt-perceptron/</link><pubDate>Thu, 25 Feb 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/rosenblatt-perceptron/</guid><description>Rosenblatt&amp;rsquo;s perceptron connects McCulloch-Pitts neurons in levels.
Rosenblatt proposed that we fix all the weights and leave the weights of the last neuron free.
The first few layers but the last layer is used as a transformation of the input data ${x_1, \cdots, x_i, \cdots, x_N}$ into a new space ${z_1, \cdots, z_i, \cdots, z_{N&amp;rsquo;}}$. The classification is done on the ${z_1, \cdots, z_i, \cdots, z_{N&amp;rsquo;}}$ space by tuning the last neuron.</description></item><item><title>Uni-Polar Sigmoid</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/uni-polar-sigmoid/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/uni-polar-sigmoid/</guid><description>A uni-Polar sigmoid function is
$$ \sigma(x) = \frac{1}{1+e^{-x}}. $$
Visualization Uni-polar Sigmoid function Tricks A very useful trick: $$ 1 - \sigma(x) = \sigma(-x). $$</description></item></channel></rss>