<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>graph neural networks on Datumorphism</title><link>https://datumorphism.leima.is/tags/graph-neural-networks/</link><description>Recent content in graph neural networks on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Sun, 21 Aug 2022 13:45:33 +0200</lastBuildDate><atom:link href="https://datumorphism.leima.is/tags/graph-neural-networks/index.xml" rel="self" type="application/rss+xml"/><item><title>Over-Smoothing in Graph Neural Networks</title><link>https://datumorphism.leima.is/cards/graph/graph-neural-networks-over-smoothing/</link><pubDate>Sun, 21 Aug 2022 13:45:33 +0200</pubDate><guid>https://datumorphism.leima.is/cards/graph/graph-neural-networks-over-smoothing/</guid><description>Over-smoothing is the problem that the representations on each node of the graph neural networks becomes way too similar to each other.1 In Chapter 7 of Hamilton2020, the author interprets this phenomenon using the lower pass filter theory in signal processing, i.e., multiplying a signal by $\mathbf A^n$ is similar to a low-pass filter when $n$ is large, with $\mathfb A$ being the adjacency matrix.
Hamilton2020 Hamilton WL. Graph Representation Learning. Morgan &amp;amp; Claypool Publishers; 2020. pp. 1â€“159. doi:10.2200/S01045ED1V01Y202009AIM046 &amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item></channel></rss>