<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>GNN on Datumorphism</title><link>https://datumorphism.leima.is/tags/gnn/</link><description>Recent content in GNN on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Mon, 28 Nov 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://datumorphism.leima.is/tags/gnn/index.xml" rel="self" type="application/rss+xml"/><item><title>Mix-hop Propagation in GNN</title><link>https://datumorphism.leima.is/cards/forecasting/gnn-mix-hop-propagation/</link><pubDate>Mon, 28 Nov 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/forecasting/gnn-mix-hop-propagation/</guid><description>The mix-hop propagation layer has two steps1:
information propagation step: $$ \mathbf H^{(k)} = \beta \mathbf H_{in} + (1-\beta)\mathbf L \mathbf H^{(k-1)}, $$
where $\mathbf L= (1+ \operatorname{A}) (\mathbf A + \mathbf I)$. This convolution step tries to disentangle the correlation between the nodes. information selection step: $$ \mathbf H_{out} = \sum_k \mathbf H^{(k)} \mathbf W^{(k)}. $$
See Fig 4 in the paper1.
Wu2020 Wu Z, Pan S, Long G, Jiang J, Chang X, Zhang C. Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2005.11650 &amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Graph Structure Learning in GNN</title><link>https://datumorphism.leima.is/cards/forecasting/gnn-graph-structure-learning/</link><pubDate>Mon, 28 Nov 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/forecasting/gnn-graph-structure-learning/</guid><description>We extract the definitions in Wu et al., 20201. Given node embeddings $\mathbf E_i$1,
$$ \begin{align} \mathbf M_i &amp;= \tanh(\alpha \mathbf E_i \Theta_i) \\ \mathbf A &amp;= \operatorname{ReLU}(\tanh(\alpha (\mathbf M_1 \mathbf M_2^T - \mathbf M_2\mathbf M_1^T))), \end{align} $$
The author also proposed sparse requirement and only take the top-$k$ largest elements in $A$.
Wu2020 Wu Z, Pan S, Long G, Jiang J, Chang X, Zhang C. Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2005.11650 &amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Over-Smoothing in Graph Neural Networks</title><link>https://datumorphism.leima.is/cards/graph/graph-neural-networks-over-smoothing/</link><pubDate>Sun, 21 Aug 2022 13:45:33 +0200</pubDate><guid>https://datumorphism.leima.is/cards/graph/graph-neural-networks-over-smoothing/</guid><description>Over-smoothing is the problem that the representations on each node of the graph neural networks becomes way too similar to each other.1 In Chapter 7 of Hamilton2020, the author interprets this phenomenon using the lower pass filter theory in signal processing, i.e., multiplying a signal by $\mathbf A^n$ is similar to a low-pass filter when $n$ is large, with $\mathbf A$ being the adjacency matrix.
Hamilton2020 Hamilton WL. Graph Representation Learning. Morgan &amp;amp; Claypool Publishers; 2020. pp. 1â€“159. doi:10.2200/S01045ED1V01Y202009AIM046 &amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item></channel></rss>