<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Mutual Information on Datumorphism</title><link>https://datumorphism.leima.is/tags/mutual-information/</link><description>Recent content in Mutual Information on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Fri, 13 Aug 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://datumorphism.leima.is/tags/mutual-information/index.xml" rel="self" type="application/rss+xml"/><item><title>Mutual Information</title><link>https://datumorphism.leima.is/cards/information/mutual-information/</link><pubDate>Fri, 13 Aug 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/information/mutual-information/</guid><description>Mutual information is defined as
$$ I(X;Y) = \mathbb E_{p_{XY}} \ln \frac{P_{XY}}{P_X P_Y}. $$
In the case that $X$ and $Y$ are independent variables, we have $P_{XY} = P_X P_Y$, thus $I(X;Y) = 0$. This makes sense as there would be no &amp;ldquo;mutual&amp;rdquo; information if the two variables are independent of each other.
Entropy and Cross Entropy Mutual information is closely related to entropy. A simple decomposition shows that
$$ I(X;Y) = H(X) - H(X\mid Y), $$</description></item></channel></rss>