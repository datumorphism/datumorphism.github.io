<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Neural Network on Datumorphism</title><link>https://datumorphism.leima.is/tags/neural-network/</link><description>Recent content in Neural Network on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Thu, 23 Sep 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://datumorphism.leima.is/tags/neural-network/index.xml" rel="self" type="application/rss+xml"/><item><title>MaxEnt Model</title><link>https://datumorphism.leima.is/wiki/machine-learning/energy-based-model/maxent-energy-based-model/</link><pubDate>Mon, 31 May 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/energy-based-model/maxent-energy-based-model/</guid><description>The Maximum Entropy model, aka MaxEnt model, is a fascinating generative model as it is based on a very intuitive idea from statistical physics - the Principle of Maximum Entropy.
The Idea The essence of the MaxEnt model is that the underlying probability distribution $p(x)$ of the random variables $x$ should
gives the whole system the largest uncertainty, while producing reasonable observables. Uncertainty The uncertainty of the whole system is described by the Shannon entropy based on the probability distributions $p(x)$,
$$ S[p] = -\operatorname{Tr} p(x) \log p(x). $$
The Shannon entropy Coding Theory Concepts The code function produces code words.</description></item><item><title>Restricted Boltzmann Machine</title><link>https://datumorphism.leima.is/wiki/machine-learning/energy-based-model/restricted-boltzmann-machine/</link><pubDate>Fri, 11 Jun 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/energy-based-model/restricted-boltzmann-machine/</guid><description>Latent variables introduce extra correlations between the nodes in a network. Introducing hidden units can also help us remove the direct connection between some nodes in a Boltzmann machine and create a restricted Boltzmann machine. A restricted Boltzmann machine requires less computation while having some expressing power.
Given Ising like interactions between the nodes, flipping node V1 is likely to also flip node V2 as they are connected through hidden unit H1. They are correlated. Removing the hidden unit leaves us two uncorrelated units.
The Ising Model Given a Ising-like energy function (c.f. MaxEnt Model MaxEnt Model Maximum Entropy models makes least assumption about the data )</description></item><item><title>MDL and Neural Networks</title><link>https://datumorphism.leima.is/wiki/model-selection/mdl-and-neural-networks/</link><pubDate>Sun, 14 Feb 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/model-selection/mdl-and-neural-networks/</guid><description>Minimum Description Length ( MDL Minimum Description Length MDL is a measure of how well a model compresses data by minimizing the combined cost of the description of the model and the misfit. ) can be used to construct a concise network. A fully connected network has great expressing power but it is easily overfitting.
One strategy is to apply constraints to the networks:
Limit the connections; Shared weights in subgroups of the network; Constrain the weights using some probability distributions. By minimizing the MDL of the network and the misfits on the data, we can build a concise network.</description></item><item><title>Initialize Artificial Neural Networks</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/neural-networks-initialization/</link><pubDate>Thu, 23 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/neural-networks-initialization/</guid><description>The weights are better if they1
are zero centered, and have similar variance across layers. Why
If we have very different variances across layers, we will need a different learning rate for each layer for our optimization. Setting the variances to be on the same scale, we can use a global learning rate for the whole network. Variance is related to the input size of the layer Suppose we are using a simple linear activation, $\sigma(x) = \alpha x$. For a series of inputs $x_j$, the outputs $y_i$ are
$$ y_i = \sum_{j} w_{ij} x_j. $$</description></item><item><title>The log-sum-exp Trick</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/log-sum-exp-trick/</link><pubDate>Wed, 28 Jul 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/log-sum-exp-trick/</guid><description>The cross entropy for a binary class is
$$ p \ln \hat p + (1-p) \ln (1-\hat p), $$
where $p$ is the probability of the label A and $\hat p$ is the predicted probability of label A. Since we have binary classes, $p$ is either 1 or 0. However, the predicted probabilities can be any value between $[0,1]$.
Probability
For a very simple case, $\hat p$ might be a sigmoid like expression with exponential in it,
$$ p \sim \frac{1}{1 + \exp(-x)}, $$
where $x$ is some kind of input or intermediate input.
The problem is, exponentials may blow up if $p\to 0$.</description></item><item><title>McCulloch-Pitts Model</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/mcculloch-pitts-model/</link><pubDate>Thu, 25 Feb 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/mcculloch-pitts-model/</guid><description>The McCulloch-Pitts model maps the input $\{x_1, x_2,\cdots, x_i \cdots, x_N \}$ into a scalar $y\in\{1,-1\}$,
$$ y = \operatorname{sign}( w\cdot x - b). $$
Since $w\cdot x - b = 0$ is a hyperplane, the McCulloch-Pitts model separates the state space using this hyperplane. The shift $b$ determines the interception, and $w$ decides the slope.</description></item><item><title>Rosenblatt's Perceptron</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/rosenblatt-perceptron/</link><pubDate>Thu, 25 Feb 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/rosenblatt-perceptron/</guid><description>Rosenblatt&amp;rsquo;s perceptron connects McCulloch-Pitts neurons in levels.
Rosenblatt proposed that we fix all the weights and leave the weights of the last neuron free.
The first few layers but the last layer is used as a transformation of the input data ${x_1, \cdots, x_i, \cdots, x_N}$ into a new space ${z_1, \cdots, z_i, \cdots, z_{N'}}$. The classification is done on the ${z_1, \cdots, z_i, \cdots, z_{N'}}$ space by tuning the last neuron.
Initially, we set $w=0$. At step $k$,
if the sign prediction by the perceptron $( w_k \cdot z_{k+1} )$ is the same as the data $y_{k+1}$, i.</description></item><item><title>BiPolar Sigmoid</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-bi-polar-sigmoid/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-bi-polar-sigmoid/</guid><description>A BiPolar sigmoid function is
$$ \sigma(x) = \frac{1-e^{-x}}{1+e^{-x}}. $$
Visualization Bipolar Sigmoid</description></item><item><title>Conic Section Function</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-conic-section-function/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-conic-section-function/</guid><description> TODO
Expand this article. See references 1.
Dorffner1994 Dorffner G. UNIFIED FRAMEWORK FOR MLPs AND RBFNs: INTRODUCING CONIC SECTION FUNCTION NETWORKS. Cybern Syst. 1994;25: 511â€“554. doi:10.1080/01969729408902340 &amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>ELU</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-elu/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-elu/</guid><description>Both ReLu and Leaky ReLu have discontinuous derivatives. ELU is smooth for first order derivative, i.e., ELU is class $C^1$.
$$ \begin{cases} x, &amp; \text{if }x=0 \\ \exp(x) - 1, &amp; \text{else.} \end{cases} $$
Visualizations ELU
Derivative of ELU
Code def elu(x, alpha): return torch.where(x &amp;gt; 0, x, torch.exp(x) -1) Full code to generate the data used in this article Full code to generate the data used in this article
from torch import nn import matplotlib.pyplot as plt import torch from typing import Union, Optional from pathlib import Path import json def visualize_activation( x: torch.Tensor, acti: torch.</description></item><item><title>Hyperbolic Tanh</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-hyperbolic-tangent/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-hyperbolic-tangent/</guid><description>$$ \tanh(x) = \frac{\sinh(x)}{\cosh(x)} = \frac{e^{x} - e^{-x}}{e^x + e^{-x}} $$
Hyperbolic tangent</description></item><item><title>Leaky ReLu</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-leaky-relu/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-leaky-relu/</guid><description>ReLu sets all negative regions to 0. Leaky ReLu sets the negative regions to a linear relation with slope $\alpha$,
$$ \begin{cases} x, &amp; \text{if }x=0 \\ \alpha x, &amp; \text{else.} \end{cases} $$
Visualizations Leaky ReLu with $\alpha=0.2$
Derivative of Leaky ReLu with $\alpha=0.2$. Notice that the derivative is $0.2$ for $x&amp;lt;0$.
Code def leaky_relu(x, alpha): return torch.where(x &amp;gt; 0, x, alpha * x) Full code to generate the data used in this article Full code to generate the data used in this article
from torch import nn import matplotlib.pyplot as plt import torch from typing import Union, Optional from pathlib import Path import json def visualize_activation( x: torch.</description></item><item><title>Radial Basis Function</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-radial-basis-function/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-radial-basis-function/</guid><description> Hyperbolic tangentTwo unnormalized Gaussian radial basis functions in one input dimension. The basis function centers are located at x1=0.75 and x2=3.25. Source Unnormalized Radial Basis Functions</description></item><item><title>ReLu</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-relu/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-relu/</guid><description>Rectified Linear Unit (ReLu) is a very popular activation function in deep learning. ReLu is defined as
$$ \begin{cases} x, &amp; \text{if }x=0 \\ 0, &amp; \text{else.} \end{cases} $$
Visualizations ReLu
Derivative of ReLu
Characteristics In trained models, ReLu doesn&amp;rsquo;t preserve the qualitative distributions of values after the activation.
Lippe P. Tutorial 3: Activation Functions â€” UvA DL Notebooks v1.1 documentation. In: UvA Deep Learning Tutorials [Internet].
Because of the zero values in ReLu, many neurons actually don&amp;rsquo;t participate in any of the tasks as they are just nullified to zeros and provide no gradient. Such neurons are dead neurons.</description></item><item><title>Swish</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-swish/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-swish/</guid><description>Swish is infinitely differentiable, i.e., class $C^\infty$.
$$ x \sigma(x), $$
where $\sigma$ is the uni-polar sigmoid Uni-Polar Sigmoid Uni-polar sigmoid function and its properties .
Visualizations ELU
Derivative of ELU
Code def swish(x, alpha): return x * torch.sigmoid(x) Full code to generate the data used in this article Full code to generate the data used in this article
from torch import nn import matplotlib.pyplot as plt import torch from typing import Union, Optional from pathlib import Path import json def visualize_activation( x: torch.Tensor, acti: torch.nn.Module, save_path: Optional[Union[str, Path]] = None ) -&amp;gt; dict: &amp;#34;&amp;#34;&amp;#34;Visualize activation function on the domain of x&amp;#34;&amp;#34;&amp;#34; y = acti(x) # Calculate the grad of the activation function x = x.</description></item><item><title>Uni-Polar Sigmoid</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-uni-polar-sigmoid/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-uni-polar-sigmoid/</guid><description>A uni-Polar sigmoid function is
$$ \sigma(x) = \frac{1}{1+e^{-x}}. $$
Visualization Uni-polar Sigmoid function Tricks A very useful trick: $$ 1 - \sigma(x) = \sigma(-x). $$</description></item><item><title>Overcoming catastrophic forgetting in neural networks</title><link>https://datumorphism.leima.is/reading/overcoming-catastrophic-forgetting-in-neural-networks/</link><pubDate>Sun, 14 May 2017 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/reading/overcoming-catastrophic-forgetting-in-neural-networks/</guid><description>Using a newly defined loss function the authors could implement an idea that achieves the multi-task within one network.</description></item></channel></rss>