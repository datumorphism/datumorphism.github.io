<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>PCA on Datumorphism</title><link>https://datumorphism.leima.is/tags/pca/</link><description>Recent content in PCA on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Fri, 25 May 2018 00:00:00 +0000</lastBuildDate><atom:link href="https://datumorphism.leima.is/tags/pca/index.xml" rel="self" type="application/rss+xml"/><item><title>Unsupervised Learning: PCA</title><link>https://datumorphism.leima.is/wiki/machine-learning/unsupervised/pca/</link><pubDate>Fri, 25 May 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/unsupervised/pca/</guid><description>We use the Einstein summation notation in this article. Principal Component Analysis (PCA) is a commonly used trick for dimensionality reduction so that the new features represents most of the variances of the data.
Representations of Dataset In theory, a dataset can be represented by a matrix if we specify the basis. However, the initial given basis is not always the most convinient one. Suppose we find a new set of basis for the dataset, the matrix representation may be simpler and easier to use.
For convenience, we do not distinguish the representation and the abstract dataset in this article.</description></item><item><title>Unsupervised Learning</title><link>https://datumorphism.leima.is/wiki/machine-learning/unsupervised/</link><pubDate>Fri, 17 Aug 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/unsupervised/</guid><description/></item></channel></rss>