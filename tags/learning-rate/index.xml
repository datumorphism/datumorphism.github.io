<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Learning Rate on Datumorphism</title><link>https://datumorphism.leima.is/tags/learning-rate/</link><description>Recent content in Learning Rate on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Mon, 01 Nov 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://datumorphism.leima.is/tags/learning-rate/index.xml" rel="self" type="application/rss+xml"/><item><title>Differential Learning Rates in PyTorch</title><link>https://datumorphism.leima.is/til/machine-learning/pytorch/pytorch-differential-learning-rates/</link><pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/machine-learning/pytorch/pytorch-differential-learning-rates/</guid><description>Using different learning rates in different layers of our artificial neural network.</description></item><item><title>Learning Rate</title><link>https://datumorphism.leima.is/cards/machine-learning/practice/learning-rate/</link><pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/practice/learning-rate/</guid><description>Finding a suitable learning rate for our model training is crucial.
A safe but time wasting option is to use search on a grid of parameters. However, there are smarter moves.
Karpathyâ€™s Constant
An empirical learning rate $3^{-4}$ for Adms, aka, Karpathy's constant, was started as a tweet by Andrei Karpathy. 3e-4 is the best learning rate for Adam, hands down.
&amp;mdash; Andrej Karpathy (@karpathy) November 24, 2016 (i just wanted to make sure that people understand that this is a joke...)
&amp;mdash; Andrej Karpathy (@karpathy) November 24, 2016 Smarter Method A smarter method is to start with small learning rate and increase it on each mini-batch, then observe the loss vs learning rate (mini-batch in this case).</description></item></channel></rss>