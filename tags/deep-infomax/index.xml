<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Deep Infomax on Datumorphism</title><link>https://datumorphism.leima.is/tags/deep-infomax/</link><description>Recent content in Deep Infomax on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Wed, 08 Sep 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://datumorphism.leima.is/tags/deep-infomax/index.xml" rel="self" type="application/rss+xml"/><item><title>Deep Infomax</title><link>https://datumorphism.leima.is/wiki/machine-learning/contrastive-models/deep-infomax/</link><pubDate>Wed, 08 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/contrastive-models/deep-infomax/</guid><description>Max Global Mutual Information
Why not just use the global mutual information of the input and encoder output as the objective?
&amp;hellip; maximizing MI between the complete input and the encoder output (i.e.,globalMI) is ofteninsufficient for learning useful representations.
&amp;ndash; Devon et al[^Devon2018]
Mutual information Mutual Information Mutual information is defined as $$ I(X;Y) = \mathbb E_{p_{XY}} \ln \frac{P_{XY}}{P_X P_Y}. $$ In the case that $X$ and $Y$ are independent variables, we have $P_{XY} = P_X P_Y$, thus $I(X;Y) = 0$. This makes sense as there would be no &amp;ldquo;mutual&amp;rdquo; information if the two variables are independent of each other.</description></item></channel></rss>