<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>self-supervised learning on Datumorphism</title><link>https://datumorphism.leima.is/tags/self-supervised-learning/</link><description>Recent content in self-supervised learning on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Fri, 13 Aug 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://datumorphism.leima.is/tags/self-supervised-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Generative Model: Autoregressive Model</title><link>https://datumorphism.leima.is/wiki/machine-learning/self-supervised-learning/generative-autoregressive-model/</link><pubDate>Fri, 13 Aug 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/self-supervised-learning/generative-autoregressive-model/</guid><description>The likelihood is modeled as
$$ \begin{align} p_\theta (x) &amp;= \Pi_{t=1}^T p_\theta (x_t \mid x_{1:t-1}) \\ &amp;= p_\theta(x_2 \mid x_{1:1}) p_\theta(x_3 \mid x_{1:2}) \cdots p_\theta(x_T \mid x_{1:T-1}) \end{align} $$
Taking the log of it
$$ \ln p_\theta (x) = \sum_{t=1}^T \ln p_\theta (x_t \mid x_{1:t-1}) $$</description></item><item><title>Generative Model: Flow</title><link>https://datumorphism.leima.is/wiki/machine-learning/self-supervised-learning/generative-flow/</link><pubDate>Fri, 13 Aug 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/self-supervised-learning/generative-flow/</guid><description>For a probability density $p(x)$ and a transformation of coordinate $x=g(z)$ or $z=f(x)$, the density can be expressed using the coordinate transformations, i.e.,
$$ \begin{align} p(x) &amp;= \tilde p (f(x)) \lvert \operatorname{det} \operatorname{D} g(f(x)) \rvert^{-1} \\ &amp;= \tilde p(f(x)) \lvert \operatorname{det}\operatorname{D} f(x) \rvert \end{align} $$
where the Jacobian is
$$ \operatorname{D} g(z) \to \frac{\partial }{\partial z} g. $$
The operation $g_{*}\circ \tilde p(z)$ is the push forward of $\tilde p(z)$.</description></item><item><title>Generative Model: Auto-Encoder</title><link>https://datumorphism.leima.is/wiki/machine-learning/self-supervised-learning/generative-autoencoder/</link><pubDate>Fri, 13 Aug 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/self-supervised-learning/generative-autoencoder/</guid><description>The simplest auto-encoder is rather simple.
The loss can be chosen based on the demand, e.g., cross entropy for binary labels.</description></item><item><title>Generative Model: Variational Auto-Encoder</title><link>https://datumorphism.leima.is/wiki/machine-learning/self-supervised-learning/generative-variational-autoencoder/</link><pubDate>Fri, 13 Aug 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/self-supervised-learning/generative-variational-autoencoder/</guid><description>Variational Auto-Encoder (VAE) is very different from Generative Model: Auto-Encoder Generative Model: Auto-Encoder The simplest auto-encoder is rather simple. The loss can be chosen based on the demand, e.g., cross entropy for binary labels. . In VAE, we introduce a variational distribution $q$ to help us work out the weighted integral after introducing the latent space variable $z$,
$$ \begin{align} \ln p_\theta(x) &amp;= \ln \int p_\theta (x\mid z) p(z) \,\mathrm d z \\ &amp;= \ln \int \frac{q_{\phi}(z\mid x)}{q_{\phi}(z\mid x)} p_\theta (x\mid z) p(z) \, \mathrm d z\\ &amp; \geq - \left[ D_{\mathrm{KL}} ( q_{\phi}(z\mid x) \mathrel{\Vert} p(z) ) - \mathbb E_q ( \ln p_\theta (x\mid z) ) \right] \\ &amp;\equiv - F(x).</description></item><item><title>Contrastive Model</title><link>https://datumorphism.leima.is/wiki/machine-learning/self-supervised-learning/contrastive/</link><pubDate>Fri, 13 Aug 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/self-supervised-learning/contrastive/</guid><description>Contrastive models learn to compare1. Contrastive use special objective functions such as NCE Noise Contrastive Estimation: NCE Noise contrastive estimation (NCE) objective function is1 $$ \mathcal L = \mathbb E_{x, x^{+}, x^{-}} \left[ - \ln \frac{ C(x, x^{+})}{ C(x,x^{+}) + C(x,x^{-}) } \right], $$ where $x^{+}$ represents data similar to $x$, $x^{-}$ represents data dissimilar to $x$, $C(\cdot, \cdot)$ is a function to compute the similarities. For example, we can use $$ C(x, x^{+}) = e^{ f(x)^T f(x^{+}) }, $$ so that the objective function becomes .</description></item><item><title>Self-supervised Learning</title><link>https://datumorphism.leima.is/wiki/machine-learning/self-supervised-learning/</link><pubDate>Fri, 13 Aug 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/self-supervised-learning/</guid><description/></item><item><title>Self-supervised Learning: Generative or Constrastive</title><link>https://datumorphism.leima.is/reading/self-supervised-learning-generative-or-contrastive-2006.08218/</link><pubDate>Fri, 13 Aug 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/reading/self-supervised-learning-generative-or-contrastive-2006.08218/</guid><description>Review of self-supervised learning.</description></item></channel></rss>