<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>PyTorch on Datumorphism</title><link>https://datumorphism.leima.is/tags/pytorch/</link><description>Recent content in PyTorch on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Wed, 19 Oct 2022 14:54:29 +0200</lastBuildDate><atom:link href="https://datumorphism.leima.is/tags/pytorch/index.xml" rel="self" type="application/rss+xml"/><item><title>Pytorch Data Parallelism</title><link>https://datumorphism.leima.is/cards/machine-learning/practice/pytorch-data-parallelism/</link><pubDate>Wed, 19 Oct 2022 14:54:29 +0200</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/practice/pytorch-data-parallelism/</guid><description>To train large models using PyTorch, we need to go parallel. There are two commonly used strategies123:
model parallelism, data parallelism, data-model parallelism. Model Parallelism Model parallelism splits the model on different nodes14. We will focus on data parallelism but the key idea is shown in the following illustration.
Model parallelLi X, Zhang G, Li K, Zheng W. Chapter 4 - Deep Learning and Its Parallelization. In: Buyya R, Calheiros RN, Dastjerdi AV, editors. Big Data. Morgan Kaufmann; 2016. pp. 95â€“118. doi:10.1016/B978-0-12-805394-2.00004-0
Data Parallelism Data parallelism creates replicas of the model on each device and use different subsets of training data14.</description></item><item><title>Differential Learning Rates in PyTorch</title><link>https://datumorphism.leima.is/til/machine-learning/pytorch/pytorch-differential-learning-rates/</link><pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/machine-learning/pytorch/pytorch-differential-learning-rates/</guid><description>Using different learning rates in different layers of our artificial neural network.</description></item><item><title>PyTorch: Initialize Parameters</title><link>https://datumorphism.leima.is/til/machine-learning/pytorch/pytorch-initial-params/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/machine-learning/pytorch/pytorch-initial-params/</guid><description>We can set the parameters in a for loop. We take some of the initialization methods from Lippe1.
To set based on the input dimension of the layer ( [[Initialize Artificial Neural Networks]] Initialize Artificial Neural Networks Initialize a neural network is important for the training and performance. Some initializations simply don&amp;#39;t work, some will degrade the performance of the model. We should choose wisely. ) (normalized initialization),
for name, param in model.named_parameters(): if name.endswith(&amp;#34;.bias&amp;#34;): param.data.fill_(0) else: bound = math.sqrt(6)/math.sqrt(param.shape[0]+param.shape[1]) param.data.uniform_(-bound, bound) or set the parameters based on the input size of each layer
for name, param in model.</description></item></channel></rss>