<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Parallel on Datumorphism</title><link>https://datumorphism.leima.is/tags/parallel/</link><description>Recent content in Parallel on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Wed, 19 Oct 2022 14:54:29 +0200</lastBuildDate><atom:link href="https://datumorphism.leima.is/tags/parallel/index.xml" rel="self" type="application/rss+xml"/><item><title>The Python Language: Multi-Processing</title><link>https://datumorphism.leima.is/wiki/programming-languages/python/multiprocessing/</link><pubDate>Thu, 10 May 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/programming-languages/python/multiprocessing/</guid><description>Python has built-in multiprocessing module in its standard library.
One simple example of using the Pool class is the following.
def myfunc(myfuncargs): &amp;#39;some thing here&amp;#39; with Pool(10) as p: records = p.map(myfunc, myfuncargs) However, there are limitations on this, especially on pickles. Another approach.
from multiprocessing import Pool from multiprocessing.dummy import Pool as ThreadPool with ThreadPool(1) as p: records = p.map(myfunc, myfuncargs) Beware that map function will feed in a list of args to the function. So I have to use p.map(myfunc, [arg]) for one arg.</description></item><item><title>Pytorch Data Parallelism</title><link>https://datumorphism.leima.is/cards/machine-learning/practice/pytorch-data-parallelism/</link><pubDate>Wed, 19 Oct 2022 14:54:29 +0200</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/practice/pytorch-data-parallelism/</guid><description>To train large models using PyTorch, we need to go parallel. There are two commonly used strategies123:
model parallelism, data parallelism, data-model parallelism. Model Parallelism Model parallelism splits the model on different nodes14. We will focus on data parallelism but the key idea is shown in the following illustration.
Model parallelLi X, Zhang G, Li K, Zheng W. Chapter 4 - Deep Learning and Its Parallelization. In: Buyya R, Calheiros RN, Dastjerdi AV, editors. Big Data. Morgan Kaufmann; 2016. pp. 95â€“118. doi:10.1016/B978-0-12-805394-2.00004-0
Data Parallelism Data parallelism creates replicas of the model on each device and use different subsets of training data14.</description></item></channel></rss>