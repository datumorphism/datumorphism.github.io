<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Entropy on Datumorphism</title><link>https://datumorphism.leima.is/tags/entropy/</link><description>Recent content in Entropy on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Wed, 17 Feb 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://datumorphism.leima.is/tags/entropy/index.xml" rel="self" type="application/rss+xml"/><item><title>Coding Theory Concepts</title><link>https://datumorphism.leima.is/cards/information/coding-theory-concepts/</link><pubDate>Wed, 17 Feb 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/information/coding-theory-concepts/</guid><description>The code function produces code words. The expected length of the code word is limited by the entropy from the source probability $p$.
The Shannon information content, aka self-information, is described by
$$ - \log_2 p(x=a), $$ for the case that $x=a$.
The Shannon entropy is the expected information content for the whole sequence with probability distribution $p(x)$,
$$ \mathcal H = - \sum_x p(x\in X) \log_2 p(x). $$ The Shannon source coding theorem says that for $N$ samples from the source, we can roughly compress it into $N\mathcal H$.</description></item></channel></rss>