<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>machine learning on Datumorphism</title><link>https://datumorphism.leima.is/tags/machine-learning/</link><description>Recent content in machine learning on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Sat, 12 Nov 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://datumorphism.leima.is/tags/machine-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>The Time Series Forecasting Problem</title><link>https://datumorphism.leima.is/wiki/forecasting/forecasting-problem/</link><pubDate>Sun, 24 Apr 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/forecasting/forecasting-problem/</guid><description>There are many different types of tasks on time series data:
classification, anomaly detection, forecasting. Forecasting Problem A time series forecasting problem can be formulated as the following.
Given a dataset $\mathcal D$, with
$y^{(i)}_t$, the sequential variable to be forecasted, $x^{(i)}_t$, exogenous data for the time series data, $u^{(i)}_t$, some features that can be obtained or planned in advance, where ${}^{(i)}$ indicates the $i$th variable, ${}_ t$ denotes time. In a forecasting task, we use $y^{(i)} _ {t-K:t}$, $x^{(i) _ {t-K:t}}$, and $u^{(i)} _ {t-K:t+H}$, to forecast the future $y^{(i)} _ {t+1:t+H}$.
A model $f$ will use $x^{(i)} _ {t-K:t}$ and $u^{(i)} _ {t-K:t+H}$ to forecast $y^{(i)} _ {t+1:t+H}$.</description></item><item><title>Receiver Operating Characteristics: ROC</title><link>https://datumorphism.leima.is/wiki/machine-learning/performance/roc/</link><pubDate>Wed, 13 May 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/performance/roc/</guid><description>ROC space is the two-dimensional space spanned by True Positive Rate and False Positive Rate.
ROC Space. The color boxes are indicating the confusion matrices. Green is the fraction of true positive. Orange is the fraction of false positive. Refer to Confusion Matrix for more details.
AUC: Area under Curve TPR = TP Rate FPR = FP Rate The ROC curve is defined by the relation $f(TPR, FPR)$. Area under the ROC curve is
$$ \int TPR(FPR) d(FPR) \sim \sum_i TPR_i *\Delta FPR. $$
If AUC = 1, we have TP Rate = 1 for all FP Rate.</description></item><item><title>Embedding</title><link>https://datumorphism.leima.is/wiki/machine-learning/embedding/overview/</link><pubDate>Sun, 13 Oct 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/embedding/overview/</guid><description/></item><item><title>Factorization</title><link>https://datumorphism.leima.is/wiki/machine-learning/factorization/overview/</link><pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/factorization/overview/</guid><description/></item><item><title>Feature Engineering</title><link>https://datumorphism.leima.is/wiki/machine-learning/feature-engineering/overview/</link><pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/feature-engineering/overview/</guid><description/></item><item><title>Naive Bayes</title><link>https://datumorphism.leima.is/wiki/machine-learning/bayesian/naive-bayes/</link><pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/bayesian/naive-bayes/</guid><description>Naive Bayesian is a classifier using [[Bayes&amp;#39; Theorem]] Bayes&amp;#39; Theorem Bayes' Theorem is stated as $$ P(A\mid B) = \frac{P(B \mid A) P(A)}{P(B)} $$ $P(A\mid B)$: likelihood of A given B $P(A)$: marginal probability of A There is a nice tree diagram for the Bayes' theorem on Wikipedia. Tree diagram of Bayes' theorem with &amp;lsquo;naive&amp;rsquo; assumptions.
Problems with Conditional Probability Calculation By definition, the conditional probability of event $\mathbf Y$ given features $\mathbf X$ is $$ \begin{equation} P(\mathbf Y\mid \mathbf X) = \frac{P(\mathbf Y, \mathbf X)}{ P(\mathbf X) }, \label{def-cp-y-given-x} \end{equation} $$
where
$P(\mathbf X)$ is probability of an event having the features $\mathbf X$, $P(\mathbf Y, \mathbf X)$ is the probability of the event $Y$ with features $\mathbf X$.</description></item><item><title>Artificial Neural Networks</title><link>https://datumorphism.leima.is/wiki/machine-learning/neural-networks/artificial-neural-networks/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/neural-networks/artificial-neural-networks/</guid><description>Artificial neural networks works pretty well for solving some differential equations.
Universal Approximators Maxwell Stinchcombe and Halber White proved that no theoretical constraints for the feedforward networks to approximate any measurable function. In principle, one can use feedforward networks to approximate measurable functions to any accuracy.
However, the convergence slows down if we have a lot of hidden units. There is a balance between accuracy and convergence rate. More hidden units lead to slow convergence but more accuracy.
Here is a quick review of the history of this topic.
Kolmogorov’s Theorem
Kolmogorov&amp;rsquo;s theorem shows that one can use a finite number of carefully chosen continuous functions to mix up by sums and multiplication with weights to a continuous multivariable function on a compact set.</description></item><item><title>Machine Learning Overview</title><link>https://datumorphism.leima.is/wiki/machine-learning/overview/</link><pubDate>Fri, 25 May 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/overview/</guid><description>What is Machine Learning Abu-Mostafa, Magdon-Ismail, and Lin summarized machine learning problem using the following chart 1 2. Utilimately, we need to find an approximation $g$ of the true map $f$ from features $\mathcal X$ to targets $\mathcal Y$ on a specific probability distribution of features $P$. This process is done by using an algorithm to select some hypothesis that works.
From the book Learning From Data by Abu-Mostafa, Magdon-Ismail, and Lin. I am using a version by Deckert.
In the core of machine learning models, we have three components3:
Representation: encode data and problem representation, i.</description></item><item><title>Time Series Forecasting with Deep Learning</title><link>https://datumorphism.leima.is/wiki/forecasting/forecasting-with-deep-learning/</link><pubDate>Sun, 24 Apr 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/forecasting/forecasting-with-deep-learning/</guid><description>The Encoder-Decoder Framework Many of the models for [[time series forecasting]] The Time Series Forecasting Problem Forecasting time series using deep learning are following some sort of encoder-decoder architecture.
Encoder: $g_{\text{enc}}(x^{(i)} _ {t-K:t}, u^{(i)} _ {t-K:t}) \to z_t$, Decoder: $g_{\text{dec}}(z_t, u^{(i)} _ {t+1: t+H}) \to y_{t+1:t+H}$.</description></item><item><title>Traditional Methods on Graph</title><link>https://datumorphism.leima.is/wiki/graph/traditional-methods-on-graph/</link><pubDate>Sat, 25 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/graph/traditional-methods-on-graph/</guid><description/></item><item><title>Embedding</title><link>https://datumorphism.leima.is/cards/machine-learning/embedding/</link><pubDate>Sat, 11 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/embedding/</guid><description/></item><item><title>Data Types and Level of Measurement in Machine Learning</title><link>https://datumorphism.leima.is/wiki/machine-learning/feature-engineering/data-types/</link><pubDate>Wed, 15 Jan 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/feature-engineering/data-types/</guid><description>Types of Data There are several debatable categorization methods of data.
The first widely spread theory, or level of measurement, is by S. Stevens. The theory categorizes data into four types, nominal, ordinal, interval, and ratio.
Other methods are proposed for other fields of research. For example, N. R. Chrisman proposed a different method for cartography. However, these are not generic enough for data science. They are more general than a specific field of research.
For machine learning, many statistical data types have been proposed. Some examples of data types and their relations with the level of measurement are shown in the following chart.</description></item><item><title>Bayesian Linear Regression</title><link>https://datumorphism.leima.is/wiki/machine-learning/bayesian/bayesian-linear-regression/</link><pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/bayesian/bayesian-linear-regression/</guid><description>Linear Regression and Likelihood The linear estimator $y$ is
$$ \begin{equation} y^n = \beta^m X_m^{\phantom{m}n}. \label{eq-linear-model} \end{equation} $$
As usual, we have redefined our data to get rid of the intercept $\beta^0$.
In ordinary linear models, we find the error being the difference between the target $\hat y$ and the estimator $y$
$$ \epsilon = \hat y - y, $$
which is required to have a minimum absolute value.
In linear regressions, we use least squares to solve the problem. In Bayesian linear regression, instead of using a deterministic estimator $\beta^m X_m^{\phantom{m}n}$, we assume a Gaussian random estimator
$$ \begin{equation} \mathcal{N}(\mu, \sigma^2) = \mathcal{N}(\beta^m X_m^{\phantom{m}n}, \sigma^2), \end{equation} $$</description></item><item><title>NMF: Nonnegative Matrix Factorizatioin</title><link>https://datumorphism.leima.is/wiki/machine-learning/factorization/nmf/</link><pubDate>Thu, 13 Jun 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/factorization/nmf/</guid><description>Decomposition For simplicity, we start with a data point $\mathbf P$ in a $k$-dimensional space spanned by $k$ basis vectors $\mathbf V^k$. Naturally, we could write down the component decomposition of the point using the basis vectors $\mathbf V^k$,
$$ \mathbf P = P_k \mathbf V^k. $$
This is immediately obvious to us since we have been dealing with rank 2 $(k, 1)$ basis vectors and we are talking about the $k$ coordinates for a point.
This point is represented by a matrix of rank 2 $(k, 1)$ given this basis.
$$ \mathbf P \to \begin{pmatrix} P_1, P_2, \cdots, P_k \end{pmatrix} $$</description></item><item><title>Word2vec</title><link>https://datumorphism.leima.is/wiki/machine-learning/embedding/word2vec/</link><pubDate>Thu, 13 Jun 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/embedding/word2vec/</guid><description>Word2vec is a word embedding model that learns the probability of some words being neighbours in a sentence $p_{neighbours}(w_i, w_o)$.
Build a dataset of adjacent words. CBOW; skipgram; negative sampling; Encode the words using vectors. Build a model $f(\{\theta_i\})$ to calculate the probability of the words being neighours and improve the parameters $\{\theta_i\}$ using the dataset.</description></item><item><title>A Physicist's Crash Course on Artificial Neural Network</title><link>https://datumorphism.leima.is/wiki/machine-learning/neural-networks/physicists-crash-course-neural-network/</link><pubDate>Sat, 02 May 2015 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/neural-networks/physicists-crash-course-neural-network/</guid><description>What is a Neuron What a neuron does is to response when a stimulation is given. This response could be strong or weak or even null. If I would draw a figure, of this behavior, it looks like this.
Neuron response Using simple single neuron responses, we could compose complicated responses. To achieve that, we study the transformations of the response first.
transformations Artificial Neural Network A simple network is a collection of neurons that response to stimulations, which could be the responses of other neurons.
neural network A given input signal is spreaded onto three different neurons.</description></item><item><title>Workflows</title><link>https://datumorphism.leima.is/awesome/workflows/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/awesome/workflows/</guid><description>The scope of exploratory data analysis is not universally defined. Some of the contents discussed here may have crossed the line. The whole modeling process is never decoupled anyway. Data wrangling is mostly guided by the exploratory data analysis (EDA). In other words, the data cleaning process should be mostly guided by questions from business and stakeholder or out of curiosity.
There are three key components in EDA.
Clearly state the purpose of this EDA. Are we asking the right question? Does the dataset fit in memory or shall I use distributed preprocessing? Is the dataset good enough to solve the problem?</description></item><item><title>Learning Theories</title><link>https://datumorphism.leima.is/cards/machine-learning/learning-theories/</link><pubDate>Tue, 19 Apr 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/learning-theories/</guid><description/></item><item><title>Evaluating Time Series Models</title><link>https://datumorphism.leima.is/wiki/forecasting/evalutate-time-series-models/</link><pubDate>Fri, 08 Apr 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/forecasting/evalutate-time-series-models/</guid><description>Evaluating time series models is usually different from most other machine learning tasks as we usually don&amp;rsquo;t have i.i.d. data.
Out-of-sample Out-of-Sample with Sliding Window
If the sliding window size is 1, then we have the simplest out-of-sample holdout scenario.
Prequential Prequential with Gap
Prequential with Growing Train
Prequential with Sliding Blocks
Cross-validation Cross-validation
Cross-validation with Neighbor removed</description></item><item><title>Feature Engineering</title><link>https://datumorphism.leima.is/wiki/machine-learning/feature-engineering/</link><pubDate>Wed, 15 Jan 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/feature-engineering/</guid><description/></item><item><title>Tensor Factorization</title><link>https://datumorphism.leima.is/wiki/machine-learning/factorization/tensor-factorization/</link><pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/factorization/tensor-factorization/</guid><description>Tensors We will be talking about tensors but we will skip the introduction to tensor for now.
In this article, we follow a commonly used convention for tensors in physics, the abstract index notation. We will denote tensors as $T^{ab\cdots}_ {\phantom{ab\cdots}cd\cdots}$, where the latin indices such as $^{a}$ are simply a placebo for the slot for this &amp;ldquo;tensor machine&amp;rdquo;. For a given basis (coordinate system), we can write down the components of this tensor $T^{\alpha\beta\cdots} _ {\phantom{\alpha\beta\cdots}\gamma\delta\cdots}$.
Okay, But Why
What is usually seen in blog posts is the use of component forms of tensors, $T^{\alpha\beta\cdots}_{\phantom{\alpha\beta\cdots}\gamma\delta\cdots}$. Those are the numbers for a given basis.</description></item><item><title>Boltzmann Machine</title><link>https://datumorphism.leima.is/wiki/machine-learning/energy-based-model/boltzmann-machine/</link><pubDate>Sun, 27 Aug 2017 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/energy-based-model/boltzmann-machine/</guid><description>Boltzmann machine is much like a spin glass model in physics. In short words, Boltzmann machine is a machine that has nodes that can take values, and the nodes are connected through some weight. It is just like any other neural nets but with complications and theoretical implications.
Boltzmann machine is usually used as a generative model.
Boltzmann Machine and Physics To obtain a good understanding of Boltzmann machine for a physicist, we begin with Ising model. We construct a system of neurons ${ s_i}$ which can take values of 1 or -1, where each pair of them $s_i$ and $s_j$ is connected by weight $J_{ij}$.</description></item><item><title>Measurement</title><link>https://datumorphism.leima.is/cards/machine-learning/measurement/</link><pubDate>Mon, 08 Nov 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/measurement/</guid><description/></item><item><title>Deep Autoregressive Network</title><link>https://datumorphism.leima.is/wiki/machine-learning/neural-networks/deep-autoregressive-networks/</link><pubDate>Mon, 15 Feb 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/neural-networks/deep-autoregressive-networks/</guid><description>There are two levels of autoregressiveness in the DARN network:
Inlayer autoregressive connections of the nodes, Intralayer autoregressive connections of nodes. The network is trained on MDL loss.</description></item><item><title>DeepAR</title><link>https://datumorphism.leima.is/wiki/forecasting/deepar/</link><pubDate>Sat, 09 Jul 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/forecasting/deepar/</guid><description>Focus on</description></item><item><title>State Space Models</title><link>https://datumorphism.leima.is/wiki/time-series/state-space-models/</link><pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/time-series/state-space-models/</guid><description>State space model is an important category of model for sequential data. Through simple assumptions, state space models can achieve quite complicated distributions.
To model a sequence, we can use the joint probability of all the nodes,
$$ p(x_1, x_2, \cdots, x_N), $$
where $x_i$ are the nodes in the sequence.
Orders We can introduce different order of dependencies on the past.
The simplest model for the sequence is assuming i.i.d..
Zeroth OrderEach node is independent of each other
To model the dependencies in the sequence, we can assume a node depends on the previous nodes. The first-order model assume that node $x_{i+1}$ only depends on node $x_i$.</description></item><item><title>Neural Networks</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/</link><pubDate>Thu, 23 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/</guid><description/></item><item><title>Hidden Markov Model</title><link>https://datumorphism.leima.is/wiki/time-series/hidden-markov-model/</link><pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/time-series/hidden-markov-model/</guid><description>The hidden Markov model, HMM, is a type of [[State Space Models]] State Space Models The state space model is an important category of models for sequential data such as time series 1.
HMM Bishop2006 Christpher M. Bishop. Pattern Recognition and Machine Learning. Springer-Verlag New York; 2006. &amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Some ML Workflow Frameworks</title><link>https://datumorphism.leima.is/wiki/tools/ml-flow-frameworks/</link><pubDate>Wed, 13 Jan 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/tools/ml-flow-frameworks/</guid><description>Metaflow Docs
A framework for jupyter notebook data scientists.
Work locally on notebooks. Python environment management using conda. Work in the cloud with Sagemaker. Tasks Methods Comments Code Scripts/Jupyter Notebook Datastore local + S3 metaflow.S3 Compute local + AWS Batch Metadata metaflow service Metadata specifies flow executions: Flows, Runs, Steps, Tasks, and Artifacts. Scheduling AWS Step Functions Deployment AWS Demo from metaflow import FlowSpec, step class BranchFlow(FlowSpec): @step def start(self): self.next(self.a, self.b) @step def a(self): self.</description></item><item><title>Time Series Data Augmentation</title><link>https://datumorphism.leima.is/wiki/time-series/data-augmentation/</link><pubDate>Mon, 27 Jun 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/time-series/data-augmentation/</guid><description/></item><item><title>Bayesian Methods</title><link>https://datumorphism.leima.is/wiki/machine-learning/bayesian/</link><pubDate>Mon, 12 Apr 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/bayesian/</guid><description/></item><item><title>Classification</title><link>https://datumorphism.leima.is/wiki/machine-learning/classification/</link><pubDate>Thu, 08 Apr 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/classification/</guid><description/></item><item><title>Forecasting</title><link>https://datumorphism.leima.is/wiki/forecasting/</link><pubDate>Sat, 12 Nov 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/forecasting/</guid><description/></item><item><title>Level Set Forecaster</title><link>https://datumorphism.leima.is/wiki/forecasting/probablistic/level-set-forecaster/</link><pubDate>Sat, 12 Nov 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/forecasting/probablistic/level-set-forecaster/</guid><description>This note is a more detailed version of Algorithm 1 in:
Hasson H, Wang B, Januschowski T, Gasthaus J. Probabilistic forecasting: A level-set approach. Adv Neural Inf Process Syst. 2021;34: 6404–6416. Available: https://proceedings.neurips.cc/paper/2021/hash/32b127307a606effdcc8e51f60a45922-Abstract.html.
It maybe hard to comprehend without reading the texts before Algorithm 1.
A level set forecaster converts point forecaster to probabilistic forecasters by constructing the [[level set]] Level Set Level set can be used in ML of the forecaster1.
Given a point forecaster $f(x_1, \cdots, x_d)$ trained on dataset $\mathcal D = {(\mathcal x_i, y_i)}$, we collect the predictions and true values and build a map, $f(x_i) \to [y_{i_1}, y_{i_2}, \cdots, y_{i_m}]$.</description></item><item><title>Conformal Time Series Forecasting</title><link>https://datumorphism.leima.is/wiki/forecasting/probablistic/conformal-time-series-forecasting/</link><pubDate>Tue, 19 Apr 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/forecasting/probablistic/conformal-time-series-forecasting/</guid><description>Conformal time series forecasting is a probabilistic forecasting method using [[Conformal Prediction]] Conformal Prediction Conformal prediction is a method to sequentially predict consistent confidence intervals using nonconformity measures. .
For any given model $\mathcal M$, conformal time series forecasting trains on a training dataset $\mathcal D_{\text{Train}}$ then calculates a [[Confidence Interval]] Confidence Interval Estimates from a sample can be entitled a confidence interval using a calibration dataset $\mathcal D_{\text{Calibration}}$. The confidence interval is directly used for inference. This framework is called the inductive conformal prediction (ICP).
Induction, Deduction, and Transduction How to Forecast the Confidence Interval For a dataset $\mathcal D$, we split it, e.</description></item><item><title>Learning Rate</title><link>https://datumorphism.leima.is/cards/machine-learning/practice/learning-rate/</link><pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/practice/learning-rate/</guid><description>Finding a suitable learning rate for our model training is crucial.
A safe but time wasting option is to use search on a grid of parameters. However, there are smarter moves.
Karpathy’s Constant
An empirical learning rate $3^{-4}$ for Adms, aka, Karpathy's constant, was started as a tweet by Andrei Karpathy. 3e-4 is the best learning rate for Adam, hands down.
&amp;mdash; Andrej Karpathy (@karpathy) November 24, 2016 (i just wanted to make sure that people understand that this is a joke...)
&amp;mdash; Andrej Karpathy (@karpathy) November 24, 2016 Smarter Method A smarter method is to start with small learning rate and increase it on each mini-batch, then observe the loss vs learning rate (mini-batch in this case).</description></item><item><title>Alignment and Uniformity</title><link>https://datumorphism.leima.is/cards/machine-learning/embedding/alignment-and-uniformity/</link><pubDate>Sat, 11 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/embedding/alignment-and-uniformity/</guid><description>A good representation should be able to
separate different instances, and cluster similar instances. Wang et al proposed two concepts that matches the above two ideas, alignment and uniformity, on a hypersphere1.
From Wang et al
Wang T, Isola P. Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2005.10242&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Self-supervised Learning: Generative or Constrastive</title><link>https://datumorphism.leima.is/reading/self-supervised-learning-generative-or-contrastive-2006.08218/</link><pubDate>Fri, 13 Aug 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/reading/self-supervised-learning-generative-or-contrastive-2006.08218/</guid><description>Review of self-supervised learning.</description></item><item><title>Hierarchical Classification</title><link>https://datumorphism.leima.is/wiki/machine-learning/classification/hierarchical-classification/</link><pubDate>Tue, 30 Mar 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/classification/hierarchical-classification/</guid><description>Hierarchical Classification Problem Hierarchical classification labels involves hierarchical class labels. The hierarchical class labels maybe predefined or inferred. 1
Class Taxonomy A hierarchical classification problem comes with a class taxonomy.
&amp;ldquo;IS-A&amp;rdquo; operator: $\prec$, &amp;ldquo;IS-NOT-A&amp;rdquo; operator: $\nprec$ A IS-A relationship of the labels $c_a$ class set $C$ is
one root $R$ in the tree, asymmetric, i.e., $c_i \prec c_j$ and $c_j\prec c_i$ can not be both true, anti-reflexive, i.e., $c_i \nprec c_i$, transitive, i.e., $c_i \prec c_j$ and $c_j\prec c_k$ $\Rightarrow$ $c_i \prec c_k$. There are different representations of the hierarchical taxonomies.
Figure 2 in Silla2011, showing the difference between tree taxonomy and DAG taxonomy.</description></item><item><title>Classifier Chains for Multilabel Classification</title><link>https://datumorphism.leima.is/wiki/machine-learning/classification/classifier-chains/</link><pubDate>Wed, 24 Mar 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/classification/classifier-chains/</guid><description>Multi-label problem In some classification problems, we have multilabel labels to be predicted. Many different approaches are proposed to solve such problems.
Algorithm Level Develop algorithms for multilabel problems, such as
Decision trees, AdaBoost. Problem Transformation On problem or data level, we can transform the multi-label problem to one or more single label problems.
Binary Relevance Method Binary relevance method, aka BM, transforms the problem into a single label problem by training a binary classifier for each label.
By doing so, the correlations between the target labels are lost.
Label Combination Method Label combination method (label power-set method), aka CM, combines the labels into single labels.</description></item><item><title>Machine as a Hologram</title><link>https://datumorphism.leima.is/projects/hologram/</link><pubDate>Sun, 31 Jan 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/projects/hologram/</guid><description>Tutorials on machine learning and data science productivity articles</description></item><item><title>Normalizing Flows: An Introduction and Review of Current Methods</title><link>https://datumorphism.leima.is/reading/normalizing-flow-introduction-1908.09257/</link><pubDate>Sun, 17 Jan 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/reading/normalizing-flow-introduction-1908.09257/</guid><description>To generate complicated distributions step by step from a simple and interpretable distribution.</description></item><item><title>Improving Document Ranking with Dual Word Embeddings</title><link>https://datumorphism.leima.is/reading/word2vec-in-out-embedding/</link><pubDate>Sat, 05 Oct 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/reading/word2vec-in-out-embedding/</guid><description>Word2vec produces two embedding spaces, the in-embedding and out-embedding.</description></item><item><title>Dealing with Missing Data in Machine Learning</title><link>https://datumorphism.leima.is/wiki/machine-learning/feature-engineering/missing-data/</link><pubDate>Mon, 05 Aug 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/feature-engineering/missing-data/</guid><description>How to Deal with Missing Data Remove Listwise deletion: Remove the whole record; Works if the missing values are random. Removing values causes problem in many aspects. For example, we can not just delete data when applying our models. Replace with most frequent value central tendency: median, mean, etc fixed value: a string etc New Category: define a new category for missing data Convert the column to a binary valued column indicating if the feature is missing or not. Tools pandas sklearn: Imputer @ResidentMario/missingno : visualize missing data</description></item><item><title>My Data Wiki</title><link>https://datumorphism.leima.is/projects/wiki/</link><pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/projects/wiki/</guid><description>A collection of my wiki articles related to data.</description></item><item><title>My Knowledge Cards</title><link>https://datumorphism.leima.is/projects/cards/</link><pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/projects/cards/</guid><description>A collection of my snippets of knowledge</description></item><item><title>My Reading Notes</title><link>https://datumorphism.leima.is/projects/reading/</link><pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/projects/reading/</guid><description>A collection of my reading notes</description></item><item><title>TIL</title><link>https://datumorphism.leima.is/projects/til/</link><pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/projects/til/</guid><description>Today I Learned</description></item></channel></rss>