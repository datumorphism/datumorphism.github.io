<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Time Series on Datumorphism</title><link>https://datumorphism.leima.is/tags/time-series/</link><description>Recent content in Time Series on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Mon, 28 Nov 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://datumorphism.leima.is/tags/time-series/index.xml" rel="self" type="application/rss+xml"/><item><title>The Time Series Forecasting Problem</title><link>https://datumorphism.leima.is/wiki/forecasting/forecasting-problem/</link><pubDate>Sun, 24 Apr 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/forecasting/forecasting-problem/</guid><description>There are many different types of tasks on time series data:
classification, anomaly detection, forecasting. Forecasting Problem A time series forecasting problem can be formulated as the following.
Given a dataset $\mathcal D$, with
$y^{(i)}_t$, the sequential variable to be forecasted, $x^{(i)}_t$, exogenous data for the time series data, $u^{(i)}_t$, some features that can be obtained or planned in advance, where ${}^{(i)}$ indicates the $i$th variable, ${}_ t$ denotes time. In a forecasting task, we use $y^{(i)} _ {t-K:t}$, $x^{(i) _ {t-K:t}}$, and $u^{(i)} _ {t-K:t+H}$, to forecast the future $y^{(i)} _ {t+1:t+H}$.
A model $f$ will use $x^{(i)} _ {t-K:t}$ and $u^{(i)} _ {t-K:t+H}$ to forecast $y^{(i)} _ {t+1:t+H}$.</description></item><item><title>Prediction Space in Forecasting</title><link>https://datumorphism.leima.is/cards/forecasting/prediction-space/</link><pubDate>Fri, 08 Apr 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/forecasting/prediction-space/</guid><description>In a forecasting problem, we have
$\mathcal P$, the priors, e.g., price and demand is negatively correlated, $\mathcal D$, available dataset, $Y$, the observations, and $F$, the forecasts. Information Set $\mathcal A$
The priors $\mathcal D$ and the available data $\mathcal P$ can be summarized together as the information set $\mathcal A$. Under a probabilistic view, a forecaster will find out or approximate a CDF $\mathcal F$ such that1
$$ \mathcal F(Y\vert \mathcal D, \mathcal P) \to F. $$
Naively speaking, once the density $\rho(F, Y)$ is determined or estimated, a probabilistic forecaster can be formed.</description></item><item><title>Time Series Forecasting with Deep Learning</title><link>https://datumorphism.leima.is/wiki/forecasting/forecasting-with-deep-learning/</link><pubDate>Sun, 24 Apr 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/forecasting/forecasting-with-deep-learning/</guid><description>The Encoder-Decoder Framework Many of the models for [[time series forecasting]] The Time Series Forecasting Problem Forecasting time series using deep learning are following some sort of encoder-decoder architecture.
Encoder: $g_{\text{enc}}(x^{(i)} _ {t-K:t}, u^{(i)} _ {t-K:t}) \to z_t$, Decoder: $g_{\text{dec}}(z_t, u^{(i)} _ {t+1: t+H}) \to y_{t+1:t+H}$.</description></item><item><title>Time Convolution</title><link>https://datumorphism.leima.is/cards/forecasting/time-convolution/</link><pubDate>Mon, 28 Nov 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/forecasting/time-convolution/</guid><description>The temporal convolution is responsible for capturing temporal patterns in a sequence.
Dilated Temporal Convolution Unit8 has a nice blog about temporal convolution and dilated temporal convolution1. In this
Convolutions Using Fourier Transform Convolution and Fourier transform Dilated Convolution For a convolution $$ f*h(x) = \sum_{s+t=x} f(s) h(t), $$ the dilated version of it is1 $$ f*_l h(x) = \sum_{s+t*l=x} f(s) h(t), $$ where $l$ is the dilation factor. Yu2015 Yu F, Koltun V. Multi-Scale Context Aggregation by Dilated Convolutions. arXiv [cs.CV]. 2015. Available: http://arxiv.org/abs/1511.07122 &amp;#160;&amp;#x21a9;&amp;#xfe0e; Inception A good convolutional network should capture both short-term and long-term patterns in the time series data.</description></item><item><title>Evaluating Time Series Models</title><link>https://datumorphism.leima.is/wiki/forecasting/evalutate-time-series-models/</link><pubDate>Fri, 08 Apr 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/forecasting/evalutate-time-series-models/</guid><description>Evaluating time series models is usually different from most other machine learning tasks as we usually don&amp;rsquo;t have i.i.d. data.
Out-of-sample Out-of-Sample with Sliding Window
If the sliding window size is 1, then we have the simplest out-of-sample holdout scenario.
Prequential Prequential with Gap
Prequential with Growing Train
Prequential with Sliding Blocks
Cross-validation Cross-validation
Cross-validation with Neighbor removed</description></item><item><title>Probabilistic Time Series Forecasting</title><link>https://datumorphism.leima.is/wiki/forecasting/probablistic/</link><pubDate>Sat, 12 Nov 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/forecasting/probablistic/</guid><description/></item><item><title>Wavelet Transform</title><link>https://datumorphism.leima.is/wiki/time-series/wavelets/</link><pubDate>Mon, 07 Dec 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/time-series/wavelets/</guid><description>In general, given a complete set of function $\psi(x; \tilde x)$, we can decompose a function $F(\tilde x)$
$$ F(\tilde x) = \int f(x) \psi(x;\tilde x) dx. $$
The choice of $\psi(x;\tilde x)$ gives us different properties.
Fourier Transform Fourier transform is good for stationary analysis since time is not involved in $F(\omega)$.
$$ F(\omega) = \int_{-\infty}^{\infty} f(t) e^{-i \omega t} dt $$
Short-time Fourier Transform STFT is a Fourier transform with a moving time window $\tau$,
$$ F(\tau,\omega) = \int_{-\infty}^{\infty} f(t) w(t - \tau) e^{-i\omega t} dt. $$
Moving $\tau$ gives us the ability to investigate Fourier components at different time segments (assuming the window function $w(t-\tau)$ is a step function).</description></item><item><title>DeepAR</title><link>https://datumorphism.leima.is/wiki/forecasting/deepar/</link><pubDate>Sat, 09 Jul 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/forecasting/deepar/</guid><description>Focus on</description></item><item><title>State Space Models</title><link>https://datumorphism.leima.is/wiki/time-series/state-space-models/</link><pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/time-series/state-space-models/</guid><description>State space model is an important category of model for sequential data. Through simple assumptions, state space models can achieve quite complicated distributions.
To model a sequence, we can use the joint probability of all the nodes,
$$ p(x_1, x_2, \cdots, x_N), $$
where $x_i$ are the nodes in the sequence.
Orders We can introduce different order of dependencies on the past.
The simplest model for the sequence is assuming i.i.d..
Zeroth OrderEach node is independent of each other
To model the dependencies in the sequence, we can assume a node depends on the previous nodes. The first-order model assume that node $x_{i+1}$ only depends on node $x_i$.</description></item><item><title>Hidden Markov Model</title><link>https://datumorphism.leima.is/wiki/time-series/hidden-markov-model/</link><pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/time-series/hidden-markov-model/</guid><description>The hidden Markov model, HMM, is a type of [[State Space Models]] State Space Models The state space model is an important category of models for sequential data such as time series 1.
HMM Bishop2006 Christpher M. Bishop. Pattern Recognition and Machine Learning. Springer-Verlag New York; 2006. &amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Time Series Forecasting with Tree-based Models</title><link>https://datumorphism.leima.is/wiki/forecasting/tree/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/forecasting/tree/</guid><description/></item><item><title>Time Series Data Augmentation</title><link>https://datumorphism.leima.is/wiki/time-series/data-augmentation/</link><pubDate>Mon, 27 Jun 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/time-series/data-augmentation/</guid><description/></item><item><title>Time Series</title><link>https://datumorphism.leima.is/wiki/time-series/</link><pubDate>Fri, 10 Feb 2023 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/time-series/</guid><description/></item><item><title>Forecasting</title><link>https://datumorphism.leima.is/wiki/forecasting/</link><pubDate>Sat, 12 Nov 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/forecasting/</guid><description/></item><item><title>MTGNN</title><link>https://datumorphism.leima.is/reading/mtgnn/</link><pubDate>Mon, 28 Nov 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/reading/mtgnn/</guid><description>Key Components Time Convolution (TC) Module Time Convolution The temporal convolution is responsible for capturing temporal patterns in a sequence. Graph Convolution Module Mix-hop Propagation in GNN Mix-hop is a strategy to avoid oversmoothing in GNN Graph Structure Learning Layer Graph Structure Learning in GNN We can learn a graph structure without prior knowledge Architecture Wu et al., 2020</description></item><item><title>Level Set Forecaster</title><link>https://datumorphism.leima.is/wiki/forecasting/probablistic/level-set-forecaster/</link><pubDate>Sat, 12 Nov 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/forecasting/probablistic/level-set-forecaster/</guid><description>This note is a more detailed version of Algorithm 1 in:
Hasson H, Wang B, Januschowski T, Gasthaus J. Probabilistic forecasting: A level-set approach. Adv Neural Inf Process Syst. 2021;34: 6404–6416. Available: https://proceedings.neurips.cc/paper/2021/hash/32b127307a606effdcc8e51f60a45922-Abstract.html.
It maybe hard to comprehend without reading the texts before Algorithm 1.
A level set forecaster converts point forecaster to probabilistic forecasters by constructing the [[level set]] Level Set Level set can be used in ML of the forecaster1.
Given a point forecaster $f(x_1, \cdots, x_d)$ trained on dataset $\mathcal D = {(\mathcal x_i, y_i)}$, we collect the predictions and true values and build a map, $f(x_i) \to [y_{i_1}, y_{i_2}, \cdots, y_{i_m}]$.</description></item><item><title>Empirical Correlation Coefficient (CORR)</title><link>https://datumorphism.leima.is/cards/time-series/ts-corr/</link><pubDate>Sun, 21 Aug 2022 20:43:36 +0200</pubDate><guid>https://datumorphism.leima.is/cards/time-series/ts-corr/</guid><description>The Empirical Correlation Coefficient (CORR) is an evaluation metric in time series forecasting,1
$$ \mathrm{CORR} = \frac{1}{N} \sum_{i=1}^N \frac{ \sum_t (y^{(i)}_t - \bar y^{(i)} ) ( \hat y^{(i)}_t -\bar{ \hat y}^{(i)} ) }{ \sqrt{ \sum_t (y^{(i)}_t - \bar y^{(i)} )^2 ( \hat y^{(i)}_t -\bar{\hat y}^{(i)} )^2 } } $$
where $y^{(i)}$ is the $i$th time series, ${} _ t$ denotes the time step $t$, and $\bar y^{(i)}$ is the mean of the $i$th forecasted series, i.e., $\bar y^{(i)} = \operatorname{mean}( y^{(i)} _ { t \in \{T _ f, T _ {f+1}, \cdots T _ {f+H}\} } )$.
Lai2017 Lai G, Chang W-C, Yang Y, Liu H.</description></item><item><title>Root Relative Squared Error (RSE)</title><link>https://datumorphism.leima.is/cards/time-series/ts-rse/</link><pubDate>Sun, 21 Aug 2022 20:43:36 +0200</pubDate><guid>https://datumorphism.leima.is/cards/time-series/ts-rse/</guid><description>The Root Relative Squared Error (RSE) is an evaluation metric in time series forecasting,1
$$ \mathrm{RSE} = \frac{ \sqrt{ \sum_{i, t} ( y^{(i)}_t - \hat y^{(i)}_t )^2 } }{ \sqrt{ \sum_{i, t} ( y^{(i)}_t - \bar y )^2 } } $$
where $y^{(i)}$ is the $i$th time series, ${} _ t$ denotes the time step $t$, and $\bar y$ is the mean of the forecasted series, i.e., $\bar y = \operatorname{mean}(y^{(i\in\{0, 1, \cdots, N\})} _ { t\in \{T _ f, T _ {f+1}, \cdots T _ {f+H}\} })$.
Lai2017 Lai G, Chang W-C, Yang Y, Liu H.</description></item><item><title>Data Generating Processes for Time Series Data</title><link>https://datumorphism.leima.is/cards/time-series/generating-process/</link><pubDate>Mon, 04 Jul 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/time-series/generating-process/</guid><description/></item><item><title>Conformal Time Series Forecasting</title><link>https://datumorphism.leima.is/wiki/forecasting/probablistic/conformal-time-series-forecasting/</link><pubDate>Tue, 19 Apr 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/forecasting/probablistic/conformal-time-series-forecasting/</guid><description>Conformal time series forecasting is a probabilistic forecasting method using [[Conformal Prediction]] Conformal Prediction Conformal prediction is a method to sequentially predict consistent confidence intervals using nonconformity measures. .
For any given model $\mathcal M$, conformal time series forecasting trains on a training dataset $\mathcal D_{\text{Train}}$ then calculates a [[Confidence Interval]] Confidence Interval Estimates from a sample can be entitled a confidence interval using a calibration dataset $\mathcal D_{\text{Calibration}}$. The confidence interval is directly used for inference. This framework is called the inductive conformal prediction (ICP).
Induction, Deduction, and Transduction How to Forecast the Confidence Interval For a dataset $\mathcal D$, we split it, e.</description></item><item><title>Continuous Ranked Probability Score - CRPS</title><link>https://datumorphism.leima.is/cards/time-series/crps/</link><pubDate>Fri, 18 Mar 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/time-series/crps/</guid><description>The Continuous Ranked Probability Score, known as CRPS, is a score to measure how a proposed distribution approximates the data, without knowledge about the true distributions of the data.
Definition CRPS is defined as1
$$ CRPS(P, x_a) = \int_{-\infty}^\infty \lVert P(x) - H(x - x_a) \rVert_2 dx, $$
where
$x_a$ is the true value of $x$, P(x) is our proposed cumulative distribution for $x$, $H(x)$ is the Heaviside step function $$ H(x) = \begin{cases} 1, &amp;\qquad x=0\\ 0, &amp;\qquad x\leq 0\\ \end{cases} $$
$\lVert \cdot \rVert_2$ is the L2 norm. Explain it The formula looks abstract on first sight, but it becomes crystal clear once we understand it.</description></item><item><title>StemGNN</title><link>https://datumorphism.leima.is/reading/stemgnn/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/reading/stemgnn/</guid><description>What problem is StemGNN solving: intra-series temporal pattern: DFT Each series inter-series correlations At each step, the interactions between nodes reversible operator Example problem Covid cases: DE, AT, NL, &amp;hellip; Predicting each country without considering the interactions between them Or introduce the people flow between them GFT: Completes DFT as it takes care of the inter-series correlations one extra slide for this topic Convolutions on Graphs one extra slide for this topic Graph Basics How to build the graph &amp;ldquo;self-attention&amp;rdquo;: outer product of key, query, as the adjacency matrix key, query are of length # of ndoes Weights and Biases LC 1DConv GLU FC Experiments Traffic adjacency matrix neighbouring sensors have higher correlations Covid Neighbouring countries have higher correlation Spetral analysis: Some eigenvectors have clear meanings Week spots Paper and code are not consistent https://github.</description></item><item><title>Box-Cox Transformation</title><link>https://datumorphism.leima.is/cards/statistics/box-cox/</link><pubDate>Tue, 13 Jul 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/box-cox/</guid><description>Box-Cox transformation is a power transformation that involves logs and powers. It transforms data into normal distributions.
The Box-Cox transformation is defined as
$$ y_i^{(\lambda)} = \begin{cases} \lambda ^{-1} (y_i^\lambda - 1) &amp; \quad \text{if } \lambda \neq 0\\ \log(y_i) &amp; \quad \text{if } \lambda = 0. \end{cases} $$
By selecting a proper $\lambda$, we get a Guassian distributed data, with a variable mean. The transformation take $y$ to
$$ \rho(y^{(\lambda)}) =\frac{ \exp{\left( -(y^{(\lambda)} - \beta X)^{T} (y^{(\lambda)} - \beta X)/(2\sigma^2) \right) }}{(\sqrt{2\pi \sigma^2})^n} \prod_{i=1}^n \left\lvert \frac{d y_i^{(\lambda )}}{ dy_i } \right\rvert. $$
The term
$$ \prod_{i=1}^n \left\lvert \frac{d y_i^{(\lambda )}}{ dy_i } \right\rvert = \lvert J \rvert $$</description></item></channel></rss>