<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Numerical on Datumorphism</title><link>https://datumorphism.leima.is/tags/numerical/</link><description>Recent content in Numerical on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Wed, 28 Jul 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://datumorphism.leima.is/tags/numerical/index.xml" rel="self" type="application/rss+xml"/><item><title>The log-sum-exp Trick</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/log-sum-exp-trick/</link><pubDate>Wed, 28 Jul 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/log-sum-exp-trick/</guid><description>The cross entropy for a binary class is
$$ p \ln \hat p + (1-p) \ln (1-\hat p), $$
where $p$ is the probability of the label A and $\hat p$ is the predicted probability of label A. Since we have binary classes, $p$ is either 1 or 0. However, the predicted probabilities can be any value between $[0,1]$.
Probability
For a very simple case, $\hat p$ might be a sigmoid like expression with exponential in it,
$$ p \sim \frac{1}{1 + \exp(-x)}, $$
where $x$ is some kind of input or intermediate input.
The problem is, exponentials may blow up if $p\to 0$.</description></item></channel></rss>