<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>normalizing flow on Datumorphism</title><link>https://datumorphism.leima.is/tags/normalizing-flow/</link><description>Recent content in normalizing flow on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Wed, 27 Jan 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://datumorphism.leima.is/tags/normalizing-flow/index.xml" rel="self" type="application/rss+xml"/><item><title>Latent Variable Models</title><link>https://datumorphism.leima.is/wiki/machine-learning/bayesian/latent-variable-models/</link><pubDate>Wed, 27 Jan 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/bayesian/latent-variable-models/</guid><description>In the view of statistics, we know everything about a physical system if we know the probability $p(\mathbf s)$ of all possible states of the physical system $\mathbf s$. Time can also be part of the state specification.
As an example, we will classify fruits into oranges and non oranges. We will have the state vector $\mathbf s = (\text{is orange}, \text{texture } x)$. Our goal is to find the joint probability $p(\text{is orange}, x)$.
The reality, we only have sample data. This sample data usually can not cover all the possible states of the system. Thus a direct calculation to find the joint probability $p(\mathbf s)$ is not feasible.</description></item><item><title>Cohen's d</title><link>https://datumorphism.leima.is/cards/statistics/cohens-d/</link><pubDate>Wed, 20 Jan 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/cohens-d/</guid><description/></item><item><title>Reparametrization in Expectation Sampling</title><link>https://datumorphism.leima.is/cards/statistics/reparametrization-expectation-sampling/</link><pubDate>Wed, 20 Jan 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/reparametrization-expectation-sampling/</guid><description>The expectation value of a function $f(z)$ over a Guassian distribution $\mathscr N(z;\mu, \sigma)$ is equivalent to the expectation value of $f()$ a Gaussian distribution $\mathscr N(z;\mu=0, \sigma=1)$, i.e.,
$$ {\mathbb E}_{\mathscr N(z; \mu, \sigma)} \left[ f(z) \right] = {\mathbb E}_{\mathscr N(z; 0, 1)} \left[ f() \right] $$
where
$$ \mathscr N = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left( -\frac{(z-\mu)^2}{2\sigma^2}\right). $$
$$ \begin{align} {\mathbb E}_{\mathscr N(z; \mu, \sigma)} \left[ f(z) \right] &amp;= \int \mathrm d z \frac{1}{\sqrt{2\pi\sigma^2}}\exp \left( -\frac{(z-\mu)^2}{2\sigma^2}\right) f(z) \\ &amp;= \int \mathrm dz \frac{1}{\sigma} \frac{1}{\sqrt{2\pi}} \exp \left( -\frac{1}{2} \left(\frac{z-\mu}{\sigma}\right)^2 \right) f(z) \\ &amp;= \int \mathrm d \left( \sigma z' + \mu \right) \frac{1}{\sigma} \frac{1}{\sqrt{2\pi}} \exp \left( -\frac{1}{2} z'^2 \right) f(\sigma z' + \mu) \\ &amp;= \int \mathrm d z' \frac{1}{\sqrt{2\pi}}\exp \left( -\frac{1}{2} z'^2 \right) f(\sigma z' + \mu) \\ &amp;= \int \mathrm d z' \mathscr N(z'; \mu=0, \sigma=1) f(\sigma z' + \mu) \\ &amp;= {\mathbb E}_{\mathscr N(z'; \mu=0, \sigma=1)} \left[ f(\sigma z' + \mu) \right] \end{align} $$</description></item><item><title>Normalizing Flows: An Introduction and Review of Current Methods</title><link>https://datumorphism.leima.is/reading/normalizing-flow-introduction-1908.09257/</link><pubDate>Sun, 17 Jan 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/reading/normalizing-flow-introduction-1908.09257/</guid><description>To generate complicated distributions step by step from a simple and interpretable distribution.</description></item></channel></rss>