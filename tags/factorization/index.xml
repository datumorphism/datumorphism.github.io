<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Factorization on Datumorphism</title><link>/tags/factorization/</link><description>Recent content in Factorization on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Tue, 18 Jun 2019 00:00:00 +0000</lastBuildDate><atom:link href="/tags/factorization/index.xml" rel="self" type="application/rss+xml"/><item><title>NMF: Nonnegative Matrix Factorizatioin</title><link>/wiki/machine-learning/factorization/nmf/</link><pubDate>Thu, 13 Jun 2019 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/factorization/nmf/</guid><description>Decomposition To make it easier to understand, we start with a data point $\mathbf P$ in a $k$-dimensional space spanned by $k$ basis vectors $\mathbf V^k$. Naturally, we could write down the component decomposition of the point using the basis vectors $\mathbf V^k$,
$$ \mathbf P = P_k \mathbf V^k. $$
This is immediately obvious to us since we have been dealing with rank 2 $(k, 1)$ basis vectors and we are talking about the $k$ coordinates for a point.</description></item><item><title>Tensor Factorization</title><link>/wiki/machine-learning/factorization/tensor-factorization/</link><pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/factorization/tensor-factorization/</guid><description>Tensors We will be talking about tensors but we will skip the introduction to tensor for now.
In this article, we follow a commonly used convention for tensors in physics, the abstract index notation. We will denote tensors as $T^{ab\cdots}_ {\phantom{ab\cdots}cd\cdots}$, where the latin indices such as $^{a}$ are simply a placebo for the slot for this &amp;ldquo;tensor machine&amp;rdquo;. For a given basis (coordinate system), we can write down the components of this tensor $T^{\alpha\beta\cdots} _ {\phantom{\alpha\beta\cdots}\gamma\delta\cdots}$.</description></item><item><title>Factorization</title><link>/wiki/machine-learning/factorization/</link><pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/factorization/</guid><description/></item><item><title>Canonical Decomposition</title><link>/cards/math/canonical-decomposition/</link><pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate><guid>/cards/math/canonical-decomposition/</guid><description>I find this slide from Christoph Freudenthaler very useful.
Canonical decomposition visualized by Christoph Freudenthaler</description></item><item><title>SVD: Singular Value Decomposition</title><link>/cards/math/svd/</link><pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate><guid>/cards/math/svd/</guid><description>Given a matrix $\mathbf X \to X_{m}^{\phantom{m}n}$, we can decompose it into three matrices
$$ X_{m}^{\phantom{m}n} = U_{m}^{\phantom{m}k} D_{k}^{\phantom{k}l} (V_{n}^{\phantom{n}l} )^{\mathrm T}, $$
where $D_{k}^{\phantom{k}l}$ is diagonal.
Here we have $\mathbf U$ being constructed by the eigenvectors of $\mathbf X \mathbf X^{\mathrm T}$, while $\mathbf V$ is being constructed by the eigenvectors of $\mathbf X^{\mathrm T} \mathbf X$ (which is also the reason we keep the transpose).
I find this slide from Christoph Freudenthaler very useful.</description></item><item><title>Tucker Decomposition</title><link>/cards/math/tucker-decomposition/</link><pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate><guid>/cards/math/tucker-decomposition/</guid><description>I find this slide from Christoph Freudenthaler very useful. For the definition of mode 1/2/3 unfold, please refer to Modes and Slices of Tensors.
Tucker decomposition visualized by Christoph Freudenthaler</description></item></channel></rss>