<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Distance on Datumorphism</title><link>https://datumorphism.leima.is/tags/distance/</link><description>Recent content in Distance on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Mon, 05 Apr 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://datumorphism.leima.is/tags/distance/index.xml" rel="self" type="application/rss+xml"/><item><title>KL Divergence</title><link>https://datumorphism.leima.is/wiki/machine-learning/basics/kl-divergence/</link><pubDate>Mon, 05 Apr 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/basics/kl-divergence/</guid><description>Given two distributions $p(x)$ and $q(x)$, the Kullback-Leibler divergence is defined as
$$ D_\text{KL}(p(x) \parallel q(x) ) = \int_{-\infty}^\infty p(x) \log\left(\frac{p(x)}{q(x)}\right), dx . $$
Kullback-Leibler divergence is also called relative entropy. In fact, KL divergence can be decomposed into cross entropy $H(p) = -\int_{-\infty}^\infty p(x) \log q(x), dx$ and entropy $H(p) = -\int_{-\infty}^\infty p(x) \log p(x), dx$
$$ D_\text{KL}(p \parallel q) = \int_{-\infty}^\infty p(x) \log p(x), dx - \int_{-\infty}^\infty p(x) \log q(x) , dx = - H(p) + H(p, q), $$</description></item><item><title>Mahalanobis Distance</title><link>https://datumorphism.leima.is/cards/math/mahalanobis-distance/</link><pubDate>Wed, 11 Mar 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/mahalanobis-distance/</guid><description>Mahalanobis distance is a distance calculated using the inverse of the covariance matrix as the metric. For two vectors $\mathbf x$ and $\mathbf y$, the Mahalanobis distance is
$$ d^2 = (x_i - \bar x) g_{ij} (y_j - \bar y), $$
where $g_{ij} = (S^{-1})_{ij}$ and $\mathbf S$ is the covariance matrix.
The covariance is a normalization that mitigates the covariances.</description></item><item><title>Levenshtein Distance</title><link>https://datumorphism.leima.is/cards/math/levenshtein-distance/</link><pubDate>Sun, 19 May 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/levenshtein-distance/</guid><description>Levenshtein distance calculates the number of operations needed to change one word to another by applying single-character edits (insertions, deletions or substitutions).
The reference explains this concept very well. For consistency, I extracted a paragraph from it which explains the operations in Levenshtein algorithm. The source of the following paragraph is the first reference of this article.
Levenshtein Matrix
Cell (0:1) contains red number 1. It means that we need 1 operation to transform M to an empty string.</description></item><item><title>Cosine Similarity</title><link>https://datumorphism.leima.is/cards/math/cosine-similarity/</link><pubDate>Mon, 06 May 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/cosine-similarity/</guid><description>As simple as the inner product of two vectors
$$ d_{cos} = \frac{\vec A}{\vert \vec A \vert} \cdot \frac{\vec B }{ \vert \vec B \vert} $$
Examples To use cosine similarity, we have to vectorize the words first. There are many different methods to achieve this. For the purpose of illustrating cosine similarity, we use term frequency.
Term frequency is the occurrence of the words. We do not deal with duplications so duplicate words will have some effect on the similarity.</description></item><item><title>Jaccard Similarity</title><link>https://datumorphism.leima.is/cards/math/jaccard-similarity/</link><pubDate>Mon, 06 May 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/jaccard-similarity/</guid><description>Jaccard index is the ratio of the size of the intersect of the set and the size of the union of the set.
$$ J(A, B) = \frac{ \vert A \cap B \vert }{ \vert A \cup B \vert } $$
Jaccard distance $d_J(A,B)$ is defined as
$$ d_J(A,B) = 1 - J(A,B). $$
Properties If the two sets are the same, $A=B$, we have $J(A,B)=1$ or $d_J(A,B)=0$. We have maximum similarity.</description></item></channel></rss>