<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Model Selection on Datumorphism</title><link>https://datumorphism.leima.is/tags/model-selection/</link><description>Recent content in Model Selection on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Sun, 14 Feb 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://datumorphism.leima.is/tags/model-selection/index.xml" rel="self" type="application/rss+xml"/><item><title>Model Selection</title><link>https://datumorphism.leima.is/wiki/model-selection/model-selection/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/model-selection/model-selection/</guid><description>Suppose we have a generating process that generates some numbers based on a distribution. Based on a data sample, we could reconstruct some sort of theoretical models to represent the actual generating process.
Which is a Good Model? (1)The black curve represent the generating process. The red rectangle is a very simple model that captures some major samples. The blue step-wise model is capturing more sample data but with more parameters.
In the above example, the red model on the left is not that good in most cases while the blue model seems to be better. In reality, the choice depends on the usage of the model.</description></item><item><title>Goodness-of-fit</title><link>https://datumorphism.leima.is/wiki/model-selection/goodness-of-fit/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/model-selection/goodness-of-fit/</guid><description>Does the data agree with the model?
Calculate the distance between data and model predictions. Apply Bayesian methods such as likelihood estimation: likelihood of observing the data if we assume the model; the results will be a set of fitting parameters. &amp;hellip; Why don&amp;rsquo;t we always use goodness-of-fit as a measure of the goodness of a model?
We may experience overfitting. The model may not be intuitive. This is why we would like to balance it with parsimony using some measures of generalizability.
K-means and overfitting
The overfitting problem is easily demonstrated using the K-means model.
Suppose we use $k=1$, i.</description></item><item><title>Measures of Generalizability</title><link>https://datumorphism.leima.is/wiki/model-selection/measures-of-generalizability/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/model-selection/measures-of-generalizability/</guid><description>To measure the generalization, we define a generalization error,
$$ \begin{align} \mathcal G = \mathcal L_{P}(\hat f) - \mathcal L_E(\hat f), \end{align} $$
where $\mathcal L_{P}$ is the population loss, $\mathcal L_E$ is the empirical loss, and $\hat f$ is our model by minimizing the empirical loss.
However, we do not know the actual joint probability $p(x, y)$ of our dataset $\{x_i, y_i\}$. Thus the population loss is not known. In machine learning, we usually use [[cross validation]] Cross Validation Cross validation is a method to estimate the [[risk]] The Learning Problem The learning problem posed by Vapnik:1 Given a sample: $\{z_i\}$ in the probability space $Z$; Assuming a probability measure on the probability space $Z$; Assuming a set of functions $Q(z, \alpha)$ (e.</description></item><item><title>MDL and Neural Networks</title><link>https://datumorphism.leima.is/wiki/model-selection/mdl-and-neural-networks/</link><pubDate>Sun, 14 Feb 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/model-selection/mdl-and-neural-networks/</guid><description>Minimum Description Length ( [[MDL]] Minimum Description Length MDL is a measure of how well a model compresses data by minimizing the combined cost of the description of the model and the misfit. ) can be used to construct a concise network. A fully connected network has great expressing power but it is easily overfitting.
One strategy is to apply constraints to the networks:
Limit the connections; Shared weights in subgroups of the network; Constrain the weights using some probability distributions. By minimizing the MDL of the network and the misfits on the data, we can build a concise network. Based on the [[Source Coding Theorem]] Coding Theory Concepts The code function produces code words.</description></item><item><title>Empirical Loss</title><link>https://datumorphism.leima.is/cards/machine-learning/measurement/empirical-loss/</link><pubDate>Sat, 06 Feb 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/measurement/empirical-loss/</guid><description>Given a dataset with records $\{x_i, y_i\}$ and a model $\hat y_i = f(x_i)$ the empirical loss is calculated on all the records
$$ \begin{align} \mathcal L_{E} = \frac{1}{n} \sum_i^n d(y_i, f(x_i)), \end{align} $$
where $d(y_i, f(x_i))$ is the distance defined between $y_i$ and $f(x_i)$.</description></item><item><title>Population Loss</title><link>https://datumorphism.leima.is/cards/machine-learning/measurement/population-loss/</link><pubDate>Sat, 06 Feb 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/measurement/population-loss/</guid><description>Given a dataset with records $\{x_i, y_i\}$ and a model $\hat y_i = f(x_i)$. Suppose we know the actual generating process of the dataset and the joint probability density distribution of all the data points is $p(x, y)$, the population loss is defined on the whole assumed population,
$$ \begin{align} \mathcal L_{P} = \mathop{\mathbb{E}}_{p(x,y)}[ d(y, f(x))], \end{align} $$
where $d(y, f(x))$ is the distance defined between $y$ and $f(x)$.</description></item><item><title>Akaike Information Criterion</title><link>https://datumorphism.leima.is/cards/statistics/aic/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/aic/</guid><description>Suppose we have a model that describes the data generation process behind a dataset. The distribution by the model is denoted as $\hat f$. The actual data generation process is described by a distribution $f$.
We ask the question:
How good is the approximation using $\hat f$?
To be more precise, how much information is lost if we use our model dist $\hat f$ to substitute the actual data generation distribution $f$?
AIC defines this information loss as
$$ \mathrm{AIC} = - 2 \ln p(y|\hat\theta) + 2k $$
$y$: data set $\hat\theta$: parameter of the model that is estimated by maximum-likelihood $\ln p(y|\hat\theta)$: log maximum likelihood (the goodness-of-fit) $k$: number of adjustable model params; $+2k$ is then a penalty.</description></item><item><title>Bayes Factors</title><link>https://datumorphism.leima.is/cards/statistics/bayes-factors/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/bayes-factors/</guid><description>$$ \frac{p(\mathscr M_1|y)}{ p(\mathscr M_2|y) } = \frac{p(\mathscr M_1)}{ p(\mathscr M_2) }\frac{p(y|\mathscr M_1)}{ p(y|\mathscr M_2) } $$
Bayes factor
$$ \mathrm{BF_{12}} = \frac{m(y|\mathscr M_1)}{m(y|\mathscr M_2)} $$
$\mathrm{BF_{12}}$: how many time more likely is model $\mathscr M_1$ than $\mathscr M_2$.</description></item><item><title>Bayesian Information Criterion</title><link>https://datumorphism.leima.is/cards/statistics/bic/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/bic/</guid><description>BIC is Bayesian information criterion, it replaced the $+2k$ term in [[AIC]] Akaike Information Criterion Suppose we have a model that describes the data generation process behind a dataset. The distribution by the model is denoted as $\hat f$. The actual data generation process is described by a distribution $f$. We ask the question: How good is the approximation using $\hat f$? To be more precise, how much information is lost if we use our model dist $\hat f$ to substitute the actual data generation distribution $f$? AIC defines this information loss as $$ \mathrm{AIC} = - 2 \ln p(y|\hat\theta) + … with $k\ln n$ to bring in punishment for the number of parameters of the model based on the number of data records,</description></item><item><title>Fisher Information Approximation</title><link>https://datumorphism.leima.is/cards/statistics/fia/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/fia/</guid><description>FIA is a method to describe the minimum description length ( [[MDL]] Minimum Description Length MDL is a measure of how well a model compresses data by minimizing the combined cost of the description of the model and the misfit. ) of models,
$$ \mathrm{FIA} = -\ln p(y | \hat\theta) + \frac{k}{2} \ln \frac{n}{2\pi} + \ln \int_\Theta \sqrt{ \operatorname{det}[I(\theta)] d\theta } $$
$I(\theta)$: Fisher information matrix of sample size 1. $$I_{i,j}(\theta) = E\left( \frac{\partial \ln p(y| \theta)}{\partial \theta_i}\frac{ \partial \ln p (y | \theta) }{ \partial \theta_j } \right)$$.</description></item><item><title>Kolmogorov Complexity</title><link>https://datumorphism.leima.is/cards/statistics/kolmogorov-complexity/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/kolmogorov-complexity/</guid><description>Description of Data
The measurement of complexity is based on the observation that the compressibility of data doesn&amp;rsquo;t depend on the &amp;ldquo;language&amp;rdquo; used to describe the compression process that much. This makes it possible for us to find a universal language, such as a universal computer language, to quantify the compressibility of the data.
One intuitive idea is to use a programming language to describe the data. If we have a sequence of data,
0,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,&amp;hellip;,9999
It takes a lot of space if we show the complete sequence. However, our math intuition tells us that this is nothing but a list of consecutive numbers from 0 to 9999.</description></item><item><title>Minimum Description Length</title><link>https://datumorphism.leima.is/cards/statistics/mdl/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/mdl/</guid><description>The minimum description length, aka, MDL, is based on the relations between regularity and data compression. (See [[Kolmogorov complexity]] Kolmogorov Complexity Description of Data The measurement of complexity is based on the observation that the compressibility of data doesn&amp;rsquo;t depend on the &amp;ldquo;language&amp;rdquo; used to describe the compression process that much. This makes it possible for us to find a universal language, such as a universal computer language, to quantify the compressibility of the data. One intuitive idea is to use a programming language to describe the data. If we have a sequence of data, … for more about data descriptions.</description></item><item><title>Normalized Maximum Likelihood</title><link>https://datumorphism.leima.is/cards/statistics/nml/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/nml/</guid><description>$$ \mathrm{NML} = \frac{ p(y| \hat \theta(y)) }{ \int_X p( x| \hat \theta (x) ) dx } $$</description></item></channel></rss>