<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Information Theory on Datumorphism</title><link>https://datumorphism.leima.is/tags/information-theory/</link><description>Recent content in Information Theory on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Wed, 05 May 2021 17:49:12 +0200</lastBuildDate><atom:link href="https://datumorphism.leima.is/tags/information-theory/index.xml" rel="self" type="application/rss+xml"/><item><title>Fraser Information</title><link>https://datumorphism.leima.is/cards/information/fraser-information/</link><pubDate>Wed, 05 May 2021 17:49:12 +0200</pubDate><guid>https://datumorphism.leima.is/cards/information/fraser-information/</guid><description>The Fraser information is
$$ I_F(\theta) = \int g(X) \ln f(X;\theta) , \mathrm d X. $$
When comparing two models, $\theta_0$ and $\theta_1$, the information gain is
$$ \propto (F(\theta_1) - F(\theta_0)). $$
The Fraser information is closed related to Fisher information Fisher information measures the second moment of the model sensitivity with respect to the parameters. , Shannon information, and Kullback information Kullbackâ€“Leibler divergence indicates the differences between two distributions 1.</description></item><item><title>Fisher Information</title><link>https://datumorphism.leima.is/cards/information/fisher-information/</link><pubDate>Wed, 05 May 2021 17:49:03 +0200</pubDate><guid>https://datumorphism.leima.is/cards/information/fisher-information/</guid><description>Given a probability density model $f(X; \theta)$ for a observable $X$, the amount of information that $X$ carriers regarding the model is called Fisher information.
Given ${\theta}$, the probability of observing the value $X$, i.e., the likelihood is
$$ f(X\mid\theta). $$
To describe the suitability of a model and the observables, we can use a the likelihood $f(X\mid \theta)$. One particular interesting property is the sensitivity of the likelihood in terms of the parameter $\theta$ change.</description></item><item><title>Coding Theory Concepts</title><link>https://datumorphism.leima.is/cards/information/coding-theory-concepts/</link><pubDate>Wed, 17 Feb 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/information/coding-theory-concepts/</guid><description>The code function produces code words. The expected length of the code word is limited by the entropy from the source probability $p$.
The Shannon information content, aka self-information, is described by
$$ - \log_2 p(x=a), $$
for the case that $x=a$.
The Shannon entropy is the expected information content for the whole sequence with probability distribution $p(x)$,
$$ \mathcal H = - \sum_x p(x\in X) \log_2 p(x). $$
The Shannon source coding theorem says that for $N$ samples from the source, we can roughly compress it into $N\mathcal H$.</description></item></channel></rss>