<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Basics on Datumorphism</title><link>/tags/basics/</link><description>Recent content in Basics on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Tue, 27 Oct 2020 00:00:00 +0000</lastBuildDate><atom:link href="/tags/basics/index.xml" rel="self" type="application/rss+xml"/><item><title>Receiver Operating Characteristics: ROC</title><link>/wiki/machine-learning/performance/roc/</link><pubDate>Wed, 13 May 2020 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/performance/roc/</guid><description>ROC space is the two-dimensional space spanned by True Positive Rate and False Positive Rate.
ROC Space. The color boxes are indicating the confusion matrices. Green is the fraction of true positive. Orange is the fraction of false positive. Refer to Confusion Matrix for more details.
AUC: Area under Curve TPR = TP Rate FPR = FP Rate The ROC curve is defined by the relation $f(TPR, FPR)$.</description></item><item><title>Tree-based Learning</title><link>/wiki/machine-learning/tree-based/overview/</link><pubDate>Wed, 25 Dec 2019 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/tree-based/overview/</guid><description>Decision tree is an easy-to-interpret method in supervised learning. Though simple, it is being used in some widely used algorithms such as random forest method.</description></item><item><title>Confusion Matrix (Contingency Table)</title><link>/wiki/machine-learning/basics/confusion-matrix/</link><pubDate>Fri, 31 May 2019 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/basics/confusion-matrix/</guid><description>Confusion Matrix It is much easier to understand the confusion matrix if we use a binary classification problem as an example. For example, we have a bunch of cat photos and the user labeled &amp;ldquo;cute or not&amp;rdquo; data. Now we are using the labeled data to train a cute-or-not binary classifier.
Then we apply the classifier on the test dataset and we would only find four different kinds of results.</description></item><item><title>Statistical Hypothesis Testing</title><link>/wiki/statistical-hypothesis-testing/hypothesis-testing/</link><pubDate>Sun, 20 Jan 2019 00:00:00 +0000</pubDate><guid>/wiki/statistical-hypothesis-testing/hypothesis-testing/</guid><description>When we have a sample of the population, we immediately calculate the mean using the sample, say the result is $\mu_0$. Of course, the population mean $\mu_p$ is unknown and probably can never be known.
This specific sample mean $\mu_0$ is nothing but like an advanced educated guess. Then again, how do we know if our this specific sample mean $\mu_0$ is a faithful representation of the population mean? In fact, this question is not limited to mean.</description></item><item><title>Why Estimation Theory</title><link>/wiki/statistical-estimation/why-estimation-theory/</link><pubDate>Sun, 20 Jan 2019 00:00:00 +0000</pubDate><guid>/wiki/statistical-estimation/why-estimation-theory/</guid><description>In statistics, we work with samples. For example, the sample mean is easily calculated. However, it is the population mean that is more valuable.
Suppose we have one sample $S_i$, which is used to calculate the mean of the sample $\mu_i$. We have two key problems to solve at this moment.
Can we use this sample mean $\mu_i$ to represent the population mean $\mu_p$? How good is our estimations? To answer these questions, we need to work out the properties of the samples themselves and work out a theory to instruct us to infer population statistics from sample statistics.</description></item><item><title>What is Statistics</title><link>/wiki/statistics/what-is-statistics/</link><pubDate>Fri, 18 Jan 2019 00:00:00 +0000</pubDate><guid>/wiki/statistics/what-is-statistics/</guid><description>A Case Study We have a problem.
In our lab, we found a huge amount of similar robots on a planet (physical population). To know more about the weight of these robots (statistical population), we first need to choose some of them (physical sample), then obtain the weight of them (statistical sample).
To describe the data, we could calculate the mean of the weight. We found that the mean weight is 93kg (descriptive statistics).</description></item><item><title>Association Rules</title><link>/wiki/pattern-mining/association-rules/</link><pubDate>Sun, 06 Jan 2019 00:00:00 +0000</pubDate><guid>/wiki/pattern-mining/association-rules/</guid><description>Association rule is a method for pattern mining. In this article, we perform an association rule analysis of some demo data.
The Problem Defined Suppose we own a store called KIOSK. Here at KIOSK, we sell 4 different things.
Milk Croissant Coffee Fries We need to know what items are associated with each other when the customers are buying.
We have collected the following data. Beware that this small amount of data might not be enough for a real-world problem.</description></item><item><title>Artificial Neural Networks</title><link>/wiki/machine-learning/neural-networks/artificial-neural-networks/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/neural-networks/artificial-neural-networks/</guid><description>Artificial neural networks works pretty well for solving some differential equations.
Universal Approximators Maxwell Stinchcombe and Halber White proved that no theoretical constraints for the feedforward networks to approximate any measurable function. In principle, one can use feedforward networks to approximate measurable functions to any accuracy.
However, the convergence slows down if we have a lot of hidden units. There is a balance between accuracy and convergence rate. More hidden units lead to slow convergence but more accuracy.</description></item><item><title>Basics of Computation</title><link>/wiki/computation/basics-of-computation/</link><pubDate>Thu, 13 Sep 2018 00:00:00 +0000</pubDate><guid>/wiki/computation/basics-of-computation/</guid><description>Storage, Precision, Error, etc To have some understanding of how the numbers are processed in computers, we have to understand how the numbers are stored first.
Computers stores everything in binary form 1. Suppose we randomly get some segments in the memory, we have no idea what that stands for since we do not know the type of data it represents.
Some of the most used data types in data science are</description></item><item><title>Linear Methods</title><link>/wiki/machine-learning/linear/linear-methods/</link><pubDate>Fri, 25 May 2018 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/linear/linear-methods/</guid><description>Solving Classification Problems with Linear Models One simple idea behind classification is to calculate the posterior probability of each class given the variables.
Suppose a dataset have features $F_\alpha$ where $\alpha = 1, 2, \cdots, K$, with corresponding class labels $G_\alpha$. The dataset that provides $N$ datapoints with each deoted as $X_i$. The posterior of the classification is $P(G = G_\alpha \vert X = X_i)$.
A naive idea is to classify the data into two classes $m$ and $n$ using the boundary of a linear model</description></item><item><title>Machine Learning Overview</title><link>/wiki/machine-learning/overview/</link><pubDate>Fri, 25 May 2018 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/overview/</guid><description>What is Machine Learning There are many objectives in machine learning. Two of the most applied objectives are classifications and regressions. In classifications and regression, the following four factors are relevant.
A simple framework of machine learning. The dataset $\tilde{\mathscr D}$ is first encoded by $\mathscr T$, $\mathscr D(\mathbf X, \mathbf Y) = \mathscr T(\tilde{\mathscr D})$. The dataset is feeded into the model, $\bar{\mathbf Y} = f(\mathbf X;\mathbf \theta)$.</description></item><item><title>Some Basic Ideas of Algorithms</title><link>/wiki/algorithms/algorithms-basics/</link><pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate><guid>/wiki/algorithms/algorithms-basics/</guid><description>This set of notes on algorithms is not meant to be comprehensive or complete. These notes are being used as a skeleton framework. There are many useful books to learn about algorithms from a utilitarian point of view. I have listed a few in the references section.
Numerical recipes is a very comprehensive book that I used during my PhD. It covers almost all the algorithms you need for scientific computing.</description></item><item><title>The C++ Language</title><link>/wiki/programming-languages/cpp/references/</link><pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate><guid>/wiki/programming-languages/cpp/references/</guid><description>C++!
Books The C++ Programming Language Programming Principles and Practice Using C++ The C++ Primer Lectures C++ Beginners Tutorial 1 (For Absolute Beginners) C++ Programming Introduction to C++ Coursear Course: C++ For C Programmers, Part A Top C++ Courses and Tutorials On SoloLearn: C++ Tutorial Practice SoloLearn provides this code playground that we can use to test c++ codes. There is also repl.it
Libraries For solving differential equations:</description></item><item><title>The Python Language: Basics</title><link>/wiki/programming-languages/python/basics/</link><pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate><guid>/wiki/programming-languages/python/basics/</guid><description>Numbers, Arithmetics Two types of numbers exist,
int float, 15 digits, other digits are float error It is worth noting that in Python 2, we have
print(1.0/3) # will give us float numbers # 0.333333333333 while
print(1/3) # will only give us int # 0 However, this was changed in Python 3.
Variables, Functions, Conditions A variable name should start with either a letter or an underscore.
Variables defined inside a function is local and there is no way to find it or use it outside the function.</description></item><item><title>Data Types and Level of Measurement in Machine Learning</title><link>/wiki/machine-learning/feature-engineering/data-types/</link><pubDate>Wed, 15 Jan 2020 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/feature-engineering/data-types/</guid><description>Types of Data There are several debatable categorization methods of data.
The first widely spread theory, or level of measurement, is by S. Stevens. The theory categorizes data into four types, nominal, ordinal, interval, and ratio.
Other methods are proposed for other fields of research. For example, N. R. Chrisman proposed a different method for cartography. However, these are not generic enough for data science. They are more general than a specific field of research.</description></item><item><title>Decision Tree</title><link>/wiki/machine-learning/tree-based/decision-tree/</link><pubDate>Wed, 25 Dec 2019 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/tree-based/decision-tree/</guid><description>In this article, we will explain how decision trees work and build a tree by hand.
The code used in this article can be found in this repo. Definition of the problem We will decide whether one should go to work today. In this demo project, we consider the following features.
feature possible values health 0: feeling bad, 1: feeling good weather 0: bad weather, 1: good weather holiday 1: holiday, 0: not holiday For more compact notations, we use the abstract notation $\{0,1\}^3$ to describe a set of three features each with 0 and 1 as possible values.</description></item><item><title>Bias-Variance</title><link>/wiki/machine-learning/basics/bias-variance/</link><pubDate>Fri, 07 Jun 2019 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/basics/bias-variance/</guid><description>Bias and Variance Suppose $f(X)$ is a perfect model that represents a &amp;ldquo;tight&amp;rdquo; model of the dataset $(X,Y)$ but some irredicible error $\epsilon$,
$$ \begin{equation} Y = f(X) + \epsilon. \label{dataset-using-true-model} \end{equation} $$
On the other hand, we build another model using a specific method such as k-nearest neighbors, which is denoted as $k(X)$.
Why the two models?
Why are we talking about the perfect model and a model using a specific method?</description></item><item><title>Machine Learning Basics</title><link>/wiki/machine-learning/basics/</link><pubDate>Fri, 07 Jun 2019 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/basics/</guid><description/></item><item><title>Types of Errors in Statistical Hypothesis Testing</title><link>/wiki/statistical-hypothesis-testing/type-1-error-and-type-2-error/</link><pubDate>Fri, 31 May 2019 00:00:00 +0000</pubDate><guid>/wiki/statistical-hypothesis-testing/type-1-error-and-type-2-error/</guid><description>Type I and Type II Errors In statistical hypothesis testing, we always have a null hypothesis $H_0$ which refers to the statement to be tested. We have two possible conclusions from a hypothesis testing,
to accept the hypothesis, that is concluding that $H_0$ is true, to reject the hypothesis, that is concluding that $H_0$ is false. However, it is possible that our conclusion is not correct. There are four possible results.</description></item><item><title>Confidence Interval</title><link>/wiki/statistical-estimation/confidence-interval/</link><pubDate>Sun, 20 Jan 2019 00:00:00 +0000</pubDate><guid>/wiki/statistical-estimation/confidence-interval/</guid><description>We will use upper cases for the abstract variable and lower cases for the actual numbers.
Why is Confidence Interval Needed? Suppose I sample the population multiple times, the mean value $\mu_i$ of the sample is calculated for each sample. It is a good question to ask how different these $\mu_i$ are compared to the true mean $\mu_p$ of the population.
In this article, we would need to specify several notations.</description></item><item><title>Jargons</title><link>/wiki/statistics/jargons/</link><pubDate>Sat, 24 Nov 2018 00:00:00 +0000</pubDate><guid>/wiki/statistics/jargons/</guid><description>Accuracy and Precision Accuracy: the measurement compared to the truth Precision: variability of repeated measurements; the more precise, the less variations during each measurement. Accurate Inaccurate Precise Close to true value, small variations in each measurement Far from true value, small variations in each measurement Imprecise Close to true value, large variations in each measurement Far from true value, large variations in each measurement Here is an example.</description></item><item><title>Basics of Programming</title><link>/wiki/computation/basics-of-programming/</link><pubDate>Sun, 23 Sep 2018 00:00:00 +0000</pubDate><guid>/wiki/computation/basics-of-programming/</guid><description>Recursive and Iterative Solving problems with iterative and recursive methods are two quite different approaches, somehow, to the same kind of problems.
Here we will calculate the factorial of $n$. We define two functions using the iterative method and the recursive method.
Run the program on Repl.it.
def recursiveFactorial(n): if n == 0: return 1 else: return n * recursiveFactorial(n - 1) def iterativeFactorial(n): ans = 1 i=1 while i &amp;lt;= n: ans = ans * i i=i+1 return ans print(recursiveFactorial(0)) print(iterativeFactorial(0))</description></item><item><title>Unsupervised Learning: PCA</title><link>/wiki/machine-learning/unsupervised/pca/</link><pubDate>Fri, 25 May 2018 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/unsupervised/pca/</guid><description>We use the Einstein summation notation in this article. Principal Component Analysis (PCA) is a commonly used trick for dimensionality reduction so that the new features represents most of the variances of the data.
Representations of Dataset In theory, a dataset can be represented by a matrix if we specify the basis. However, the initial given basis is not always the most convinient one. Suppose we find a new set of basis for the dataset, the matrix representation may be simpler and easier to use.</description></item><item><title>Data Structure</title><link>/wiki/algorithms/data-structure/</link><pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate><guid>/wiki/algorithms/data-structure/</guid><description>Dealing with data structure is like dealing with your clothes. Some people randomly drop their clothes somewhere without thinking. But it takes time to retrieve a specific T-shirt. Some people spend more time folding and arranging their clothes. This process makes it easy to find a specific T-shirt. Similar to retrieving clothes, there is always a balance between the computation time (retrieving clothes) and the coding time (folding clothes).
Keywords This section serves as some kind of flashcard keywords.</description></item><item><title>The C++ Language: Basics</title><link>/wiki/programming-languages/cpp/basics/</link><pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate><guid>/wiki/programming-languages/cpp/basics/</guid><description>Make it Work Apart from the traditional way of running C++ code, Jupyter notebook has a clingkernel that make it possibel to run C++ in a Jupyter notebook. Here is the post: Interactive C++ for HPC.
Concepts Namespace Operators: assignment operators (=,+=,-=,*=,/=,%=), increment/decrement operator (++x,x++,--x,x--), relational operators (&amp;gt;,&amp;lt;,&amp;gt;=,&amp;lt;=,==,!=), logicl operators (&amp;amp;&amp;amp;,||,!), left shift (&amp;lt;&amp;lt;), extration operator (&amp;gt;&amp;gt;, or right shift), understand the operator precedence Variables: variable name starts with underscore or latin letters, Pascal case (PascalCase), Camel case (pascalCase) if/else and Loops: if (condition is true ){ then something } Data Types: string (double quote), character (char, 1 byte ASCII character, using single quote), float (4 bytes, always signed), double (8 bytes, always signed), long double (8 or 16 bytes, always signed), singed or unsigned short or long int (signed long int, unsigned int) Pointers: ampersand (&amp;amp;) accesses the address, pointer is variable thus needs to be declared using asterisk (*), can be declared to be int or double or float or char (int \*pt; int\* pt; int * pt;) Functions: overload, recursion Class: identity, atrributes, method/behavior, access specifiers (private or public or protected, by default it is set to private), instantiation of object (creating object), constructor, destructor, encapsulation, scope resolution operator (TheClassYouNeed::somefunction()), selection operator (dot member selection .</description></item><item><title>The Python Language: Decorators</title><link>/wiki/programming-languages/python/decorators/</link><pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate><guid>/wiki/programming-languages/python/decorators/</guid><description>Functions: first-class objects; can be passed around as arguments.
What that tells us about is that functions can be pass into a function or even returned by a function. For example,
def a_decoration_function( yet_another_function ): def wrapper(): print(&amp;#39;Before yet_another_function&amp;#39;) yet_another_function() print(&amp;#39;After yet_another_function&amp;#39;) return wraper def yet_another_function(): print(&amp;#39;This is yet_another_function&amp;#39;) When we execute a_decoration_function, we will have
Before yet_another_function This is yet_another_function After yet_another_function So a decorator is simply a function that takes a function as an argument, adds some salt to it.</description></item><item><title>Random Forest</title><link>/wiki/machine-learning/tree-based/random-forest/</link><pubDate>Wed, 25 Dec 2019 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/tree-based/random-forest/</guid><description>Random forest is an ensemble method based on decision trees. Instead of using one decision tree and model on all the features, the decision tree method can model on a random set of features (feature subspace) using many decision trees and make decisions by democratizing the trees.
Given a proper dataset $\mathscr D(\mathbf X, \mathbf y)$, the ensemble of trees is denoted as ${f_i(\mathbf X)}$, will predict an ensemble of results.</description></item><item><title>Chi-square Correlation Test for Nominal Data</title><link>/wiki/statistics/correlation-analysis-chi-square/</link><pubDate>Sun, 18 Nov 2018 00:00:00 +0000</pubDate><guid>/wiki/statistics/correlation-analysis-chi-square/</guid><description>In this article, we will discuss the chi-square correlation test for detecting correlations between two series.
Steps Find out all the possible values of the two nominal series A and B; Count the co-occurrences of the combinations (A, B); Calculate the expected co-occurrences of the combinations (A, B); Calculate chi-square; Determine whether the hypothesis can be rejected. Define the Series Suppose we are analyzing two series A and B.</description></item><item><title>Basics of Network</title><link>/wiki/computation/basics-of-network/</link><pubDate>Sun, 23 Sep 2018 00:00:00 +0000</pubDate><guid>/wiki/computation/basics-of-network/</guid><description>HTTP Keywords Hyper Text Transfer Protocal: deliver hyper text from server to local browser etc. Based on TCP/IP Current version: HTTP/2 Server - Client Client can request through GET, HEAD, POST, PUT, DELETE, TRACE, OPTIONS, CONNECT, PATCH. Transfer anything defined by Content-Type Connectionless Protocol: doesn&amp;rsquo;t maintain the connection all the time Stateless protocal: A very nice explanation URL Keywords Uniform Resource Locator Interpret each part of this URL: http://abc.</description></item><item><title>Unsupervised Learning: SVM</title><link>/wiki/machine-learning/unsupervised/svm/</link><pubDate>Fri, 17 Aug 2018 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/unsupervised/svm/</guid><description>SVM is calculating a hyperplane to separate the data points into groups according to the label.
Hyperplane A hyperplane is defined to be of the following form
$$ \begin{equation} \boldsymbol{\beta} \cdot \mathbf x = \beta_0. \end{equation} $$
where $\boldsymbol\beta$ is the normal vector to the plane and is required to be constant.
It is straight forward to show that the distance $d$ from an arbitrary point $\mathbf x'$ to the hyperplane is</description></item><item><title>The Python Language: Multi-Processing</title><link>/wiki/programming-languages/python/multiprocessing/</link><pubDate>Thu, 10 May 2018 00:00:00 +0000</pubDate><guid>/wiki/programming-languages/python/multiprocessing/</guid><description>Python has built-in multiprocessing module in its standard library.
One simple example of using the Pool class is the following.
def myfunc(myfuncargs): &amp;#39;some thing here&amp;#39; with Pool(10) as p: records = p.map(myfunc, myfuncargs) However, there are limitations on this, especially on pickles. Another approach.
from multiprocessing import Pool from multiprocessing.dummy import Pool as ThreadPool with ThreadPool(1) as p: records = p.map(myfunc, myfuncargs) Beware that map function will feed in a list of args to the function.</description></item><item><title>Data Structure: Tree</title><link>/wiki/algorithms/data-structure-tree/</link><pubDate>Tue, 27 Mar 2018 00:00:00 +0000</pubDate><guid>/wiki/algorithms/data-structure-tree/</guid><description>mind the data structure: here comes the tree</description></item><item><title>The C++ Language: Numerical Methods</title><link>/wiki/programming-languages/cpp/numerical/</link><pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate><guid>/wiki/programming-languages/cpp/numerical/</guid><description>Modularize The code should be designed to separate physics or model from numerical methods. Speed vectors are convenient but slow. 1 Do not copy arrays if not necessary. The example would be for a function return. Most of the time, we can pass the pointer of an array to the function and update the array itself without copying anything and no return is needed at all. inline function.</description></item><item><title>Boltzmann Machine</title><link>/wiki/machine-learning/neural-networks/boltzmann-machine/</link><pubDate>Sun, 27 Aug 2017 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/neural-networks/boltzmann-machine/</guid><description>Boltzmann machine is much like a spin glass model in physics. In short words, Boltzmann machine is a machine that has nodes that can take values, and the nodes are connected through some weight. It is just like any other neural nets but with complications and theoretical implications.
Boltzmann Machine and Physics To obtain a good understanding of Boltzmann machine for a physicist, we begin with Ising model. We construct a system of neurons ${ s_i}$ which can take values of 1 or -1, where each pair of them $s_i$ and $s_j$ is connected by weight $J_{ij}$.</description></item><item><title>Statistical Sign Test</title><link>/wiki/statistical-hypothesis-testing/sign-test/</link><pubDate>Sun, 20 Jan 2019 00:00:00 +0000</pubDate><guid>/wiki/statistical-hypothesis-testing/sign-test/</guid><description>We have a small dataset, but it doesn&amp;rsquo;t satisfy the t-test conditions. Then we would use as little assumptions as possible.
Wine Taste Suppose we have two bottles of wine, one of them is 300 euros while the other is 100 euros.
Now we ask the question:
Does expensive wine taste better?
We find 10 experts and give them some experiments. The result is recorded then processed into the following table.</description></item><item><title>Correlation Coefficient and Covariance for Numeric Data</title><link>/wiki/statistics/correlation-coefficient/</link><pubDate>Sun, 18 Nov 2018 00:00:00 +0000</pubDate><guid>/wiki/statistics/correlation-coefficient/</guid><description>Covariances Correlation coefficient is also known as the Pearson&amp;rsquo;s product moment coefficient. Review of Standard Deviation For a series of data A, we have the standard deviations
$$ \sigma_A = \sqrt{ \frac{ \sum (a_i - \bar A)^2 }{ n } }, $$
where $n$ is the number of elements in series A.
The standard deviation is very easy to understand. It is basically the average Eucleadian distance between the data points and the average value.</description></item><item><title>Basics of Database</title><link>/wiki/computation/basics-of-database/</link><pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate><guid>/wiki/computation/basics-of-database/</guid><description>Basics of Database Relations Union: $A\cup B$ Intersection: $A\cap B$ $A - B$ Cartesian Product: $A \times B$ Query Union in database: will combine the data with matching common columns.
NaturalJoin and EquiJoin:
--EquiJoin where we specify what condition is used to join SELECT * FROM table1 JOIN table2 ON (table1.id = table2.id) --NaturalJoin where conditions are chosen by the database automatically SELECT * FROM table1 NATURAL JOIN table2 Procedural language: derived from structural programming, focuses on breaking tasks into variables, data structures, and subroutines.</description></item><item><title>The Python Language: Performance</title><link>/wiki/programming-languages/python/performance/</link><pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate><guid>/wiki/programming-languages/python/performance/</guid><description>Read the references for performance.
The message:
Use comprehensions Use generators</description></item><item><title>Mann-Whitney U Test</title><link>/wiki/statistical-hypothesis-testing/mann-whitney-u-test/</link><pubDate>Sun, 20 Jan 2019 00:00:00 +0000</pubDate><guid>/wiki/statistical-hypothesis-testing/mann-whitney-u-test/</guid><description>Mann-Whitney U is good at testing heavy-tailed data.</description></item><item><title>Basics of SQL</title><link>/wiki/computation/basics-of-sql/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>/wiki/computation/basics-of-sql/</guid><description>Adding a new field to data:
Relational: requires a new column Non-Relational: just add the field to one single document, thus can be easily decentralized. Basics and Background SQL: Structured Query Language
Relational Database:
usually in tables rows are called records columns are certain types of data. Data types of rows are specified: INTEGER TEXT DATE REAL, real numbers NULL &amp;hellip; RDBMS: Relational Database Management System, most RDBMS use SQL as the query language.</description></item><item><title>Normalization Methods for Numeric Data</title><link>/wiki/statistics/normalization-methods/</link><pubDate>Sun, 18 Nov 2018 00:00:00 +0000</pubDate><guid>/wiki/statistics/normalization-methods/</guid><description>Normalization of data is critical for statistical analysis and feature engineering.
Min-max Normalization This method is linear and straightforward.
Suppose we are analyzing series A, with elements $a_i$. We already know the min and max of the series, $a_{min}$ and $a_{max}$.
Now we would like to normalize the series to be within the range $[a_{min}', a_{max}']$. We simply solve the value of $a&amp;rsquo; _ i$ in $$ \frac{(a&amp;rsquo;_i - a_{min}')}{ ( a&amp;rsquo;_{max} - a&amp;rsquo;_{min} ) } = \frac{(a_i - a_{min})}{ ( a_{max} - a_{min} ) }, $$ where everything on the right hand side is known and $a_{min}&amp;lsquo;$ and $a_{max}&amp;lsquo;$ are chosen as the new min and max to be scaled to.</description></item><item><title>Linear Regression</title><link>/wiki/statistics/linear-regression/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>/wiki/statistics/linear-regression/</guid><description>In this article, we will use the Einstein summation convention. For example, $$ X_{ij}\beta_ j $$ is equivalent to $$ \sum_j X_{ij}\beta_ j $$ In statistics, we have at least three categories of quantities:
data and labels abstract theoretical quantities parameters and predictions of models The convention is that quantities with $\hat {}$ are the model quantities. Sometimes we do not distinguish the abstract theoretical quantities and model quantities.</description></item><item><title>Basics of MongoDB</title><link>/wiki/computation/basics-of-mongodb/</link><pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate><guid>/wiki/computation/basics-of-mongodb/</guid><description>This MongoDB Cheatsheet is my best friend.
MongoDB Concepts Documents Collections: just like tables in SQL. Database MongoShell Some examples:
// show the databases show dbs // show collections show collections //set any database to current database use database_name // insert entry db.database_name.insert( an_object_2_be_the_entry ) // read document db.database_name.findOne({'some_field':'value_of_field'}) db.database_name.fidn() // prettify db.database_name.find().pretty()</description></item><item><title>Describing Multi-dimensional Data</title><link>/wiki/statistics/multidimensional-data/</link><pubDate>Mon, 03 Dec 2018 00:00:00 +0000</pubDate><guid>/wiki/statistics/multidimensional-data/</guid><description>Descriptions of Multidimensional Data Dispersion Matrix As defined in Correlation Coefficient and Covariance for Numeric Data, covariance is about the variance of two series. This property makes it easy to generalize it to multidimensional data.
The generalized quantity is named as dispersion matrix. Suppose we have a $p$ dimensional dataset $X$,
index $x_1$ $x_2$ &amp;hellip; $x_p$ 1 2.3 12.3 83.2 9.3 &amp;hellip; &amp;hellip; &amp;hellip; &amp;hellip; &amp;hellip; N 3.</description></item><item><title>Signal Processing</title><link>/wiki/algorithms/singal-processing/</link><pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate><guid>/wiki/algorithms/singal-processing/</guid><description>There are many fascinating ideas in signal processing.</description></item><item><title>Signal Processing: Audio Basics</title><link>/wiki/algorithms/signal-processing-audio/</link><pubDate>Thu, 29 Mar 2018 00:00:00 +0000</pubDate><guid>/wiki/algorithms/signal-processing-audio/</guid><description>Keywords Harmonic structure of sound Parson code of music Linear time-invariant theory Autocorrelation Noise Chirps DCT compression Discrete Fourier transform filtering convolution Linear Time-Invariant System We describe the system with $Y(t) = f(X(t))$, where $X(t)$ is the input, and $Y(t)$ is the output.
Linear: $f(a X_1(t) + b X_2(t)) = a f(X_1(t)) + b f(X_2(t))$ Time-invariant: input $X(t+\Delta t)$ will produce the shifted signal $Y(t+\Delta t)$. LTI systems are memory systems, casual, real, and stable.</description></item><item><title>Basics of MapReduce</title><link>/wiki/algorithms/map-reduce/</link><pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate><guid>/wiki/algorithms/map-reduce/</guid><description>Centralized servers are not efficient for big data. Querying and processing data on centralized servers would reach bottleneck of the servers.
MapReduce is used to solve these problems of big data. The two videos are .
Map: take series of key-value pairs and divide them into groups. Reduce: recombine the key-value pairs Checkout the code challenges of MapReduce on HackerRank.</description></item><item><title>Conditional Probability Table</title><link>/cards/statistics/conditional-probability-table/</link><pubDate>Tue, 27 Oct 2020 00:00:00 +0000</pubDate><guid>/cards/statistics/conditional-probability-table/</guid><description>The conditional probability table, aka CPT, is used to calculate conditional probabilities from a dataset.
Given a dataset with features $\mathbf X$ and their corresponding classes $\mathbf Y$, the conditional probabilities of each class given a certain feature value can be calculated using a CPT which in turn can be calculated using a contigency table Detecting correlations using correlations for numeric data .</description></item><item><title>Covariance Matrix</title><link>/cards/statistics/covariance-matrix/</link><pubDate>Tue, 10 Mar 2020 00:00:00 +0000</pubDate><guid>/cards/statistics/covariance-matrix/</guid><description>We use Einstein&amp;rsquo;s summation convention. Covariance of two discrete series $A$ and $B$ is defined as
$$ \text{Cov} ({A,B}) = \sigma_{A,B}^2 = \frac{ (a_i - \bar A) (b_i - \bar B) }{ n- 1 }, $$
where $n$ is the length of the series. The normalization factor is set to $1/(n-1)$ to mitigate the bias for small $n$.
One could show that
$$ \mathrm{Cov}({A,B}) = E( A,B ) - \bar A \bar B.</description></item><item><title>Eigenvalues and Eigenvectors</title><link>/cards/math/eigendecomposition/</link><pubDate>Mon, 06 May 2019 00:00:00 +0000</pubDate><guid>/cards/math/eigendecomposition/</guid><description>To find the eigenvectors $\mathbf x$ of a matrix $\mathbf A$, we construct the eigen equation
$$ \mathbf A \mathbf x = \lambda \mathbf x, $$
where $\lambda$ is the eigenvalue.
We rewrite it in the components form,
$$ \begin{equation} A_{ij} x_j = \lambda x_i. \label{eqn-eigen-decomp-def} \end{equation} $$
Mathematically speaking, it is straightforward to find the eigenvectors and eigenvalues.
Eigenvectors are Special Directions Judging from the definition in Eq.($\ref{eqn-eigen-decomp-def}$), the eigenvectors do not change direction under the operation of the matrix $\mathbf A$.</description></item><item><title>Combinations</title><link>/cards/math/combinations/</link><pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate><guid>/cards/math/combinations/</guid><description>Choose X from N is
$$ C_N^X = \frac{N!}{ X! (N-X)! } $$</description></item><item><title>Schaum's Outline of Theories and Problems of Elements of Statistics I and II</title><link>/reading/elements-of-statistics/</link><pubDate>Thu, 01 Nov 2018 00:00:00 +0000</pubDate><guid>/reading/elements-of-statistics/</guid><description>The basics and all of modern statistics</description></item></channel></rss>