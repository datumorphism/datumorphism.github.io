<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Representation on Datumorphism</title><link>https://datumorphism.leima.is/tags/representation/</link><description>Recent content in Representation on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Mon, 08 Nov 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://datumorphism.leima.is/tags/representation/index.xml" rel="self" type="application/rss+xml"/><item><title>Centered Kernel Alignment (CKA)</title><link>https://datumorphism.leima.is/cards/machine-learning/measurement/centered-kernel-alignment/</link><pubDate>Mon, 08 Nov 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/measurement/centered-kernel-alignment/</guid><description>Centered Kernel Alignment (CKA) is a similarity metric designed to measure the similarity of between representations of features in neural networks1.
Definition of CKA CKA is based on the Hilbert-Schmidt Independence Criterion (HSIC) Hilbert-Schmidt Independence Criterion (HSIC) Given two kernels of the feature representations $K=k(x,x)$ and $L=l(y,y)$, HSIC is defined as12 $$ \operatorname{HSIC}(K, L) = \frac{1}{(n-1)^2} \operatorname{tr}( K H L H ), $$ where $x$, $y$ are the representations of features, $n$ is the dimension of the representation of the features, $H$ is the so-called centering matrix Centering Matrix Useful when centering a vector around its mean . We can choose different kernel functions $k$ and $l$.</description></item><item><title>Hilbert-Schmidt Independence Criterion (HSIC)</title><link>https://datumorphism.leima.is/cards/machine-learning/measurement/hilbert-schmidt-independence-criterion/</link><pubDate>Mon, 08 Nov 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/measurement/hilbert-schmidt-independence-criterion/</guid><description>Given two kernels of the feature representations $K=k(x,x)$ and $L=l(y,y)$, HSIC is defined as12
$$ \operatorname{HSIC}(K, L) = \frac{1}{(n-1)^2} \operatorname{tr}( K H L H ), $$
where
$x$, $y$ are the representations of features, $n$ is the dimension of the representation of the features, $H$ is the so-called centering matrix Centering Matrix Useful when centering a vector around its mean . We can choose different kernel functions $k$ and $l$. For example, if $k$ and $l$ are linear kernels, we have $k(x, y) = l(x, y) = x \cdot y$. In this linear case, HSIC is simply $\parallel\operatorname{cov}(x^T,y^T) \parallel^2_{\text{Frobenius}}$.</description></item></channel></rss>