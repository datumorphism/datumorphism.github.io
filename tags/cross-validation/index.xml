<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Cross Validation on Datumorphism</title><link>https://datumorphism.leima.is/tags/cross-validation/</link><description>Recent content in Cross Validation on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Thu, 06 May 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://datumorphism.leima.is/tags/cross-validation/index.xml" rel="self" type="application/rss+xml"/><item><title>Cross Validation</title><link>https://datumorphism.leima.is/cards/machine-learning/learning-theories/cross-validation/</link><pubDate>Thu, 06 May 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/learning-theories/cross-validation/</guid><description>Cross validation is a method to estimate the risk The Learning Problem The learning problem posed by Vapnik:1 Given a sample: $\{z_i\}$ in the probability space $Z$; Assuming a probability measure on the probability space $Z$; Assuming a set of functions $Q(z, \alpha)$ (e.g. loss functions), where $\alpha$ is a set of parameters; A risk functional to be minimized by tunning &amp;ldquo;the handles&amp;rdquo; $\alpha$, $R(\alpha)$. The risk functional is $$ R(\alpha) = \int Q(z, \alpha) \,\mathrm d F(z). $$ A learning problem is the minimization of this risk. Vapnik2000 â€¦ .
To perform cross validation, we split the train dataset $\mathcal D$ into $k$ folds, with each fold denoted as $\mathcal D_k$.</description></item></channel></rss>