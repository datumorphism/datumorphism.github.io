<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Statistics on Datumorphism</title><link>https://datumorphism.leima.is/tags/statistics/</link><description>Recent content in Statistics on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Mon, 08 Nov 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://datumorphism.leima.is/tags/statistics/index.xml" rel="self" type="application/rss+xml"/><item><title>Gibbs Sampling</title><link>https://datumorphism.leima.is/wiki/monte-carlo/gibbs-sampling/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/monte-carlo/gibbs-sampling/</guid><description/></item><item><title>Statistical Hypothesis Testing</title><link>https://datumorphism.leima.is/wiki/statistical-hypothesis-testing/hypothesis-testing/</link><pubDate>Sun, 20 Jan 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/statistical-hypothesis-testing/hypothesis-testing/</guid><description>When we have a sample of the population, we immediately calculate the mean using the sample, say the result is $\mu_0$. Of course, the population mean $\mu_p$ is unknown and probably can never be known.
This specific sample mean $\mu_0$ is nothing but like an advanced educated guess. Then again, how do we know if our this specific sample mean $\mu_0$ is a faithful representation of the population mean? In fact, this question is not limited to mean. It applies to any statistical measurement.
The Statistical Estimation Theory Solution to This Problem
In statistical estimation theory, we would tell the sample value with some indication of the degrees of faithfulness.</description></item><item><title>Why Estimation Theory</title><link>https://datumorphism.leima.is/wiki/statistical-estimation/why-estimation-theory/</link><pubDate>Sun, 20 Jan 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/statistical-estimation/why-estimation-theory/</guid><description>In statistics, we work with samples. For example, the sample mean is easily calculated. However, it is the population mean that is more valuable.
Suppose we have one sample $S_i$, which is used to calculate the mean of the sample $\mu_i$. We have two key problems to solve at this moment.
Can we use this sample mean $\mu_i$ to represent the population mean $\mu_p$? How good is our estimations? To answer these questions, we need to work out the properties of the samples themselves and work out a theory to instruct us to infer population statistics from sample statistics.</description></item><item><title>What is Statistics</title><link>https://datumorphism.leima.is/wiki/statistics/what-is-statistics/</link><pubDate>Fri, 18 Jan 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/statistics/what-is-statistics/</guid><description>A Case Study We have a problem.
In our lab, we found a huge amount of similar robots on a planet (physical population). To know more about the weight of these robots (statistical population), we first need to choose some of them (physical sample), then obtain the weight of them (statistical sample).
To describe the data, we could calculate the mean of the weight. We found that the mean weight is 93kg (descriptive statistics).
We could simply give a number to standard for the mean weight of all the robots. (point estimate) We could tell a number as the mean weight of all the robots together with a range that tells us how disperse our measurement is.</description></item><item><title>Statistics of Graphs</title><link>https://datumorphism.leima.is/wiki/graph/basics/statistics-of-graphs/</link><pubDate>Sat, 25 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/graph/basics/statistics-of-graphs/</guid><description>Local Statistics Node Degree Node Degree Node degree of a node $u$ $$ d_u = \sum_{v\in \mathcal V} A[u,v], $$ where $A$ is the adjacency matrix. Node Centrality Importance of a node on a graph:
Eigenvector Centrality of a Graph Given a graph with adjacency matrix $\mathbf A$, the eigenvector centrality is $$ \mathbf e_u = \frac{1}{\lambda} \sum_{v\in\mathcal V} \mathbf A[u,v] \mathbf e_v, \qquad \forall u \in \mathcal V. $$ Why is it called Eigenvector Centrality The definition is equivalent to $$ \lambda \mathbf e = \mathbf A\mathbf e. $$ Power Iteration The solution to $\mathbf e$ is the eigenvector that corresponds to the largest eigenvalue $\lambda_1$.</description></item><item><title>Machine Learning Basics</title><link>https://datumorphism.leima.is/wiki/machine-learning/basics/</link><pubDate>Mon, 05 Apr 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/basics/</guid><description/></item><item><title>Data Types and Level of Measurement in Machine Learning</title><link>https://datumorphism.leima.is/wiki/machine-learning/feature-engineering/data-types/</link><pubDate>Wed, 15 Jan 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/feature-engineering/data-types/</guid><description>Types of Data There are several debatable categorization methods of data.
The first widely spread theory, or level of measurement, is by S. Stevens. The theory categorizes data into four types, nominal, ordinal, interval, and ratio.
Other methods are proposed for other fields of research. For example, N. R. Chrisman proposed a different method for cartography. However, these are not generic enough for data science. They are more general than a specific field of research.
For machine learning, many statistical data types have been proposed. Some examples of data types and their relations with the level of measurement are shown in the following chart.</description></item><item><title>Types of Errors in Statistical Hypothesis Testing</title><link>https://datumorphism.leima.is/wiki/statistical-hypothesis-testing/type-1-error-and-type-2-error/</link><pubDate>Fri, 31 May 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/statistical-hypothesis-testing/type-1-error-and-type-2-error/</guid><description>Type I and Type II Errors In statistical hypothesis testing, we always have a null hypothesis $H_0$ which refers to the statement to be tested. We have two possible conclusions from a hypothesis testing,
to accept the hypothesis, that is concluding that $H_0$ is true, to reject the hypothesis, that is concluding that $H_0$ is false. However, it is possible that our conclusion is not correct. There are four possible results.
$H_0$ is True (Ground Truth) $H_0$ is False (Ground Truth) Accept $H_0$ (after hypothesis testing) Correct Type II Error Reject $H_0$ (after hypothesis testing) Type I Error Correct We could tell that there are two types of errors:</description></item><item><title>Confidence Interval</title><link>https://datumorphism.leima.is/wiki/statistical-estimation/confidence-interval/</link><pubDate>Sun, 20 Jan 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/statistical-estimation/confidence-interval/</guid><description>We will use upper cases for the abstract variable and lower cases for the actual numbers.
Why is Confidence Interval Needed? Suppose I sample the population multiple times, the mean value $\mu_i$ of the sample is calculated for each sample. It is a good question to ask how different these $\mu_i$ are compared to the true mean $\mu_p$ of the population.
In this article, we would need to specify several notations.
$X$ is the quantity we are measuring. $\bar X$ is the mean of the quantity $X$. Confidence Interval This theorem states that the probability for the true mean $\mu_p$ to fall into a specific range can be calculated using</description></item><item><title>Jargons</title><link>https://datumorphism.leima.is/wiki/statistics/jargons/</link><pubDate>Sat, 24 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/statistics/jargons/</guid><description>Accuracy and Precision Accuracy: the measurement compared to the truth Precision: variability of repeated measurements; the more precise, the less variations during each measurement. Accurate Inaccurate Precise Close to true value, small variations in each measurement Far from true value, small variations in each measurement Imprecise Close to true value, large variations in each measurement Far from true value, large variations in each measurement Here is an example. Suppose we have a huge population (with true mean $M_0$) and we draw samples from it. For the first time, we have sample $S_1$.</description></item><item><title>Graphs Spectral Methods</title><link>https://datumorphism.leima.is/wiki/graph/basics/graph-spectral-methods/</link><pubDate>Sat, 25 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/graph/basics/graph-spectral-methods/</guid><description>The Ratio Cut Graph Cuts Cut For a subset of nodes $\mathcal A\subset \mathcal V$, with the rest of nodes $\bar {\mathcal A} = \mathcal V \setminus \mathcal A$. For $k$ such subsets of nodes, $\mathcal A_1, \cdots, $\mathcal A_k$, the cut is the total number of edges between $\mathcal A$ and $\bar{\mathcal A}$ 1, $$ \operatorname{Cut} \left( \mathcal A_1, \cdots, $\mathcal A_k \right) = \frac{1}{2} \sum_{k=1}^K \lvert (u, v)\in \mathcal E: u\in \mathcal A_k, v\in \bar{\mathcal A_k} \rvert. $$ For smaller cut value, the … is closely related to the Graph Laplacians Graph Laplacians Laplacian is a useful representation of graphs.</description></item><item><title>Chi-square Correlation Test for Nominal Data</title><link>https://datumorphism.leima.is/wiki/statistics/correlation-analysis-chi-square/</link><pubDate>Sun, 18 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/statistics/correlation-analysis-chi-square/</guid><description>In this article, we will discuss the chi-square correlation test for detecting correlations between two series.
Steps Find out all the possible values of the two nominal series A and B; Count the co-occurrences of the combinations (A, B); Calculate the expected co-occurrences of the combinations (A, B); Calculate chi-square; Determine whether the hypothesis can be rejected. Define the Series Suppose we are analyzing two series A and B. Series A can take values $a_1$ and $a_2$, while series B can take values $b_1$ , $b_2$ and $b_3$.
$$ \begin{align} A &amp;:= \{a1, a2\} \\ B &amp;:= \{b1,b2,b3\} \end{align} $$</description></item><item><title>Statistical Sign Test</title><link>https://datumorphism.leima.is/wiki/statistical-hypothesis-testing/sign-test/</link><pubDate>Sun, 20 Jan 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/statistical-hypothesis-testing/sign-test/</guid><description>We have a small dataset, but it doesn&amp;rsquo;t satisfy the t-test conditions. Then we would use as little assumptions as possible.
Wine Taste Suppose we have two bottles of wine, one of them is 300 euros while the other is 100 euros.
Now we ask the question:
Does expensive wine taste better?
We find 10 experts and give them some experiments. The result is recorded then processed into the following table.
expert # expensive is better 1 yes 2 no 3 yes 4 yes 5 yes 6 no 7 yes 8 yes Naively, we could simply count the number of yes and find the probability of yes in this sample, i.</description></item><item><title>Correlation Coefficient and Covariance for Numeric Data</title><link>https://datumorphism.leima.is/wiki/statistics/correlation-coefficient/</link><pubDate>Sun, 18 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/statistics/correlation-coefficient/</guid><description>Covariances Correlation coefficient is also known as the Pearson&amp;rsquo;s product moment coefficient. Review of Standard Deviation For a series of data A, we have the standard deviations
$$ \sigma_A = \sqrt{ \frac{ \sum (a_i - \bar A)^2 }{ n } }, $$
where $n$ is the number of elements in series A.
The standard deviation is very easy to understand. It is basically the average Eucleadian distance between the data points and the average value. In this article, we will take another point of view.
Now imagine we have two series $(a_i - \bar A)$ and $(a_j - \bar A)$.</description></item><item><title>Mann-Whitney U Test</title><link>https://datumorphism.leima.is/wiki/statistical-hypothesis-testing/mann-whitney-u-test/</link><pubDate>Sun, 20 Jan 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/statistical-hypothesis-testing/mann-whitney-u-test/</guid><description>Mann-Whitney U is good at testing heavy-tailed data.</description></item><item><title>Normalization Methods for Numeric Data</title><link>https://datumorphism.leima.is/wiki/statistics/normalization-methods/</link><pubDate>Sun, 18 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/statistics/normalization-methods/</guid><description>Normalization of data is critical for statistical analysis and feature engineering.
Min-max Normalization This method is linear and straightforward.
Suppose we are analyzing series A, with elements $a_i$. We already know the min and max of the series, $a_{min}$ and $a_{max}$.
Now we would like to normalize the series to be within the range $[a_{min}', a_{max}']$. We simply solve the value of $a' _ i$ in $$ \frac{(a'i - a{min}')}{ ( a'{max} - a'{min} ) } = \frac{(a_i - a_{min})}{ ( a_{max} - a_{min} ) }, $$ where everything on the right hand side is known and $a_{min}'$ and $a_{max}'$ are chosen as the new min and max to be scaled to.</description></item><item><title>Linear Regression</title><link>https://datumorphism.leima.is/wiki/statistics/linear-regression/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/statistics/linear-regression/</guid><description>In this article, we will use the Einstein summation convention. For example, $$ X_{ij}\beta_ j $$ is equivalent to $$ \sum_j X_{ij}\beta_ j $$ In statistics, we have at least three categories of quantities:
data and labels abstract theoretical quantities parameters and predictions of models The convention is that quantities with $\hat {}$ are the model quantities. Sometimes we do not distinguish the abstract theoretical quantities and model quantities.
If it is necessary to use different notations for the abstract theoretical quantities and the model quantities, we would use bold symbols ($\mathbf Y$) or latin sub/super indices ( $Y_a$ ) for theoretical quantities and greek letters ( $Y_\alpha$ ) for model quantities.</description></item><item><title>Describing Multi-dimensional Data</title><link>https://datumorphism.leima.is/wiki/statistics/multidimensional-data/</link><pubDate>Mon, 03 Dec 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/statistics/multidimensional-data/</guid><description>Descriptions of Multidimensional Data Dispersion Matrix As defined in Correlation Coefficient and Covariance for Numeric Data, covariance is about the variance of two series. This property makes it easy to generalize it to multidimensional data.
The generalized quantity is named as dispersion matrix. Suppose we have a $p$ dimensional dataset $X$,
index $x_1$ $x_2$ &amp;hellip; $x_p$ 1 2.3 12.3 83.2 9.3 &amp;hellip; &amp;hellip; &amp;hellip; &amp;hellip; &amp;hellip; N 3.1 5.6 23.6 8.2 We could then calculate the pairwise covariance between the different dimensions.
$x_1$ $x_2$ &amp;hellip; $x_p$ $x_1$ $x_2$ &amp;hellip; $x_p$</description></item><item><title>Pattern Mining</title><link>https://datumorphism.leima.is/wiki/pattern-mining/</link><pubDate>Sun, 06 Jan 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/pattern-mining/</guid><description/></item><item><title>Statistical Estimation</title><link>https://datumorphism.leima.is/wiki/statistical-estimation/</link><pubDate>Sun, 20 Jan 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/statistical-estimation/</guid><description/></item><item><title>Statistical Hypothesis Testing</title><link>https://datumorphism.leima.is/wiki/statistical-hypothesis-testing/</link><pubDate>Fri, 31 May 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/statistical-hypothesis-testing/</guid><description/></item><item><title>Statistics</title><link>https://datumorphism.leima.is/wiki/statistics/</link><pubDate>Sun, 07 Mar 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/statistics/</guid><description/></item><item><title>Survival Analysis</title><link>https://datumorphism.leima.is/wiki/survival-analysis/</link><pubDate>Sun, 18 Oct 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/survival-analysis/</guid><description/></item><item><title>Monte Carlo</title><link>https://datumorphism.leima.is/wiki/monte-carlo/</link><pubDate>Wed, 03 Feb 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/monte-carlo/</guid><description/></item><item><title>Centering Matrix</title><link>https://datumorphism.leima.is/cards/math/statistics-centering-matrix/</link><pubDate>Mon, 08 Nov 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/statistics-centering-matrix/</guid><description>Given a vector $v$, with mean value of its elements $m$, we can center the vector by subtracting the mean $m$ from each element,
import numpy as np n = 10 v = np.random.randn(n) v_c = v - v.mean() This operation is easy and obvious. However, the formalism is not elegant. In some cases, we would like to formulate the process of centering the elements as operators,
$$ v_c = \operatorname{\hat H}v. $$
In this case, the operator $\operatorname{\hat H}$ is simply a matrix
$$ \operatorname{\hat H} \to I_n - \frac{1}{n} J_n, $$
where $n$ is the dimension of the vector $v$, $I_n$ is a identity matrix, $J_n$ is a matrix of all $1$s.</description></item><item><title>Betweenness Centrality of a Graph</title><link>https://datumorphism.leima.is/cards/graph/graph-betweenness-centrality/</link><pubDate>Sat, 25 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/graph/graph-betweenness-centrality/</guid><description>Betweenness centrality of a node $v$ is measurement of how likely the shortest path between two nodes $u_s$ and $u_t$ is gonna pass through node $v$,
$$ c(v) = \sum_{v\neq u_s\neq u_t} \frac{\sigma_{u_su_t}(v) }{\sigma_{u_su_t}}, $$
where $\sigma_{u_su_t}(v)$ is the number of shortest path between $u_s$ and $u_t$, and passing through $u$, while $\sigma_{u_su_t}$ is the number of shortest path between $u_s$ and $u_t$.
A figure from wikipedia demonstrates this idea well. The nodes on the outreach have smaller betweenness centrality, while the nodes in the core have higher betweenness centrality.
Source: Wikipedia
Outreach and Core
It is almost like cheating using the work &amp;ldquo;outreach&amp;rdquo; and &amp;ldquo;core&amp;rdquo; here.</description></item><item><title>Eigenvector Centrality of a Graph</title><link>https://datumorphism.leima.is/cards/graph/graph-eigenvector-centrality/</link><pubDate>Sat, 25 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/graph/graph-eigenvector-centrality/</guid><description>Given a graph with adjacency matrix $\mathbf A$, the eigenvector centrality is
$$ \mathbf e_u = \frac{1}{\lambda} \sum_{v\in\mathcal V} \mathbf A[u,v] \mathbf e_v, \qquad \forall u \in \mathcal V. $$
Why is it called Eigenvector Centrality
The definition is equivalent to
$$ \lambda \mathbf e = \mathbf A\mathbf e. $$
Power Iteration The solution to $\mathbf e$ is the eigenvector that corresponds to the largest eigenvalue $\lambda_1$. Power iteration method can help us get this eigenvector, i.e., the $^{(t+1)}$ iteration is related to the previous iteration $^{(t)}$, through the following relation,
$$ \mathbf e^{(t+1)} = \mathbf A \mathbf e^{(t)}.</description></item><item><title>Local Variant of Clustering Coefficient</title><link>https://datumorphism.leima.is/cards/graph/graph-local-variant-clustering-coefficient/</link><pubDate>Sat, 25 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/graph/graph-local-variant-clustering-coefficient/</guid><description>$$ c_u = \frac{ \lvert (v_1,v_2)\in \mathcal E: v_1, v_2 \in \mathcal N(u) \rvert}{ \color{red}{d_n \choose 2} }, $$
where $\color{red}{d_n \choose 2}$ means all the possible combinations of neighbor nodes, and $\mathcal N(u)$ is the set of nodes that are neighbor to $u$.
Ego Graph
If $c_u=1$, the ego graph of $u$ is fully connected; If $c_u=0$, the ego graph of $u$ is a star.</description></item><item><title>Node Degree</title><link>https://datumorphism.leima.is/cards/graph/node-degree/</link><pubDate>Sat, 25 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/graph/node-degree/</guid><description>Node degree of a node $u$
$$ d_u = \sum_{v\in \mathcal V} A[u,v], $$
where $A$ is the adjacency matrix.</description></item><item><title>Weisfeiler-Lehman Kernel</title><link>https://datumorphism.leima.is/cards/graph/graph-weisfeiler-lehman-kernel/</link><pubDate>Sat, 25 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/graph/graph-weisfeiler-lehman-kernel/</guid><description>The Weisfeiler-Lehman kernel is an iterative integration of neighborhood information.
We initialize the labels for each node using its own node degree. At each step, we take the neighboring node degrees to form a multiset. At step $K$, we have the multisets for each node. Those multisets at each node can be processed to form an representation of the graph which is in turn used to calculate statistics of the graph.
Iterate $k$ steps
This iteration can be used to test if two graphs are isomorphism1.
Shervashidze2011 Shervashidze N, Schweitzer P, van Leeuwen EJ, Mehlhorn K, Borgwardt KM.</description></item><item><title>Likelihood</title><link>https://datumorphism.leima.is/cards/statistics/likelihood/</link><pubDate>Wed, 26 May 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/likelihood/</guid><description>For some data points $\{x_i\}$ and a model $\theta$, the likelihood of our data point $x_i$ is $p(x_i\mid \theta)$. To be more specific, the likelihood of all data points is a function of the model $\theta$,
$$ L(\theta) = \Pi_i p(x_i\mid\theta). $$
It should be mentioned that this likelihood is not necessarily a pdf. As an example, we can calculate the likelihood of a Bernoulli distribution for a single event $x$,
$$ L(\theta) = \theta^x (1-\theta)^{(1-x)}. $$
If we are flipping coins, and the head $x=1$ probability is $\theta$, the likelihood for this single event $x=1$ is
$$ L(\theta)=\theta. $$</description></item><item><title>Jensen's Inequality</title><link>https://datumorphism.leima.is/cards/math/jensens-inequality/</link><pubDate>Mon, 12 Apr 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/jensens-inequality/</guid><description>Jensen&amp;rsquo;s inequality shows that
$$ f(\mathbb E(X)) \leq \mathbb E(f(X)) $$
for a concave function $f(\cdot)$.</description></item><item><title>Valid Confidence Sets in Multiclass and Multilabel Prediction</title><link>https://datumorphism.leima.is/wiki/machine-learning/classification/valid-confidence-sets-in-multiclass-multilabel-prediction/</link><pubDate>Thu, 08 Apr 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/classification/valid-confidence-sets-in-multiclass-multilabel-prediction/</guid><description>Ask for valid confidence:
&amp;ldquo;Valid&amp;rdquo;: validate for test data, train data, or the generating process? &amp;ldquo;Confidence&amp;rdquo;: $P(Y \notin C(X)) \le \alpha$ To avoid too much attention on data based validation, a framework called conformal inference was proposed by Vovk et al. in 2005,
$n$ observations, desired confidence level $1-\alpha$, construct confidence sets $C(x)$ using conform methods so that the sets capture the underlying the distribution a new pair $(X_{n+1}, Y_{n+1})$ from the same distribution, $P(Y_{n+1}\in C(X_{n+1})) \le 1-\alpha$</description></item><item><title>ANOVA</title><link>https://datumorphism.leima.is/wiki/statistics/anova/</link><pubDate>Sun, 07 Mar 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/statistics/anova/</guid><description>In many problems, we have to test if several distributions associated with several groups of experiments are the same. The null hypothesis to be used is
The distributions of several groups are the same.
ANOVA tests the null hypothesis by comparing the variability between groups and within groups. If the variability between groups are significantly larger than the variability within groups, we are more confident that the distributions of different groups are different.
We will use two-group experiments as an example. We use a fake dataset:
Group A $x^A_1$ $x^A_2$ &amp;hellip; $x^A_{N_A}$ Group B $x^B_1$ $x^B_2$ &amp;hellip; $x^B_{N_B}$ Within Group Variability The within group variability is proportional to</description></item><item><title>Conditional Probability Table</title><link>https://datumorphism.leima.is/cards/statistics/conditional-probability-table/</link><pubDate>Tue, 27 Oct 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/conditional-probability-table/</guid><description>The conditional probability table, aka CPT, is used to calculate conditional probabilities from a dataset.
Given a dataset with features $\mathbf X$ and their corresponding classes $\mathbf Y$, the conditional probabilities of each class given a certain feature value can be calculated using a CPT which in turn can be calculated using a contigency table Correlation Coefficient and Covariance for Numeric Data Detecting correlations using correlations for numeric data .</description></item><item><title>Arcsine Distribution</title><link>https://datumorphism.leima.is/cards/statistics/distributions/arcsine/</link><pubDate>Sat, 14 Mar 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/distributions/arcsine/</guid><description>Arcsine Distribution The PDF is
$$ \frac{1}{\pi\sqrt{x(1-x)}} $$
for $x\in [0,1]$.
It can also be generalized to
$$ \frac{1}{\pi\sqrt{(x-1)(b-x)}} $$
for $x\in [a,b]$.
Visualize</description></item><item><title>Bernoulli Distribution</title><link>https://datumorphism.leima.is/cards/statistics/distributions/bernoulli/</link><pubDate>Sat, 14 Mar 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/distributions/bernoulli/</guid><description>Two categories with probability $p$ and $1-p$ respectively.
For each experiment, the sample space is $\{A, B\}$. The probability for state $A$ is given by $p$ and the probability for state $B$ is given by $1-p$. The Bernoulli distribution describes the probability of $K$ results with state $s$ being $s=A$ and $N-K$ results with state $s$ being $B$ after $N$ experiments,
$$ P\left(\sum_i^N s_i = K \right) = C _ N^K p^K (1 - p)^{N-K}. $$</description></item><item><title>Beta Distribution</title><link>https://datumorphism.leima.is/cards/statistics/distributions/beta/</link><pubDate>Sat, 14 Mar 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/distributions/beta/</guid><description>Beta Distribution Interact {% include extras/vue.html %}
((makeGraph))</description></item><item><title>Binomial Distribution</title><link>https://datumorphism.leima.is/cards/statistics/distributions/binomial/</link><pubDate>Sat, 14 Mar 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/distributions/binomial/</guid><description>The number of successes in $n$ independent events where each trial has a success rate of $p$.
PMF:
$$ C_n^k p^k (1-p)^{n-k} $$</description></item><item><title>Categorical Distribution</title><link>https://datumorphism.leima.is/cards/statistics/distributions/categorical/</link><pubDate>Sat, 14 Mar 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/distributions/categorical/</guid><description>By generalizing the Bernoulli distribution to $k$ states, we get a categorical distribution. The sample space is $\{s_1, s_2, \cdots, s_k\}$. The corresponding probabilities for each state are $\{p_1, p_2, \cdots, p_k\}$ with the constraint $\sum_{i=1}^k p_i = 1$.</description></item><item><title>Cauchy-Lorentz Distribution</title><link>https://datumorphism.leima.is/cards/statistics/distributions/cauchy/</link><pubDate>Sat, 14 Mar 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/distributions/cauchy/</guid><description>Cauchy-Lorentz Distribution .. ratio of two independent normally distributed random variables with mean zero.
Source: https://en.wikipedia.org/wiki/Cauchy_distribution
Lorentz distribution is frequently used in physics.
PDF:
$$ \frac{1}{\pi\gamma} \left( \frac{\gamma^2}{ (x-x_0)^2 + \gamma^2} \right) $$
The median and mode of the Cauchy-Lorentz distribution is always $x_0$. $\gamma$ is the FWHM.
Visualize</description></item><item><title>Gamma Distribution</title><link>https://datumorphism.leima.is/cards/statistics/distributions/gamma/</link><pubDate>Sat, 14 Mar 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/distributions/gamma/</guid><description>Gamma Distribution PDF:
$$ \frac{\beta^\alpha x^{\alpha-1} e^{-\beta x}}{\Gamma(\alpha)} $$
Visualize</description></item><item><title>Covariance Matrix</title><link>https://datumorphism.leima.is/cards/statistics/covariance-matrix/</link><pubDate>Tue, 10 Mar 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/covariance-matrix/</guid><description>We use Einstein&amp;rsquo;s summation convention. Covariance of two discrete series $A$ and $B$ is defined as
$$ \text{Cov} ({A,B}) = \sigma_{A,B}^2 = \frac{ (a_i - \bar A) (b_i - \bar B) }{ n- 1 }, $$
where $n$ is the length of the series. The normalization factor is set to $1/(n-1)$ to mitigate the bias for small $n$.
One could show that
$$ \mathrm{Cov}({A,B}) = E( A,B ) - \bar A \bar B. $$
At first glance, the square in the definition seems to be only for notation purpose at this point.
Meanwhile, using this idea of the mean of geometric mean, we could easily generalize it to the covariance of three series,</description></item><item><title>Jackknife Resampling</title><link>https://datumorphism.leima.is/cards/statistics/jacknife-resampling/</link><pubDate>Sun, 26 Jan 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/jacknife-resampling/</guid><description>Jackknife resampling is a method for estimation of the mean and higher order moments.
Given a sample $\{x_i\}$ of size $n$ for the distribution $X$, the jackknife resampling estimates the mean by leaving out each data point systematically. $n$ estimations of the mean will be obtained, with each of the estimations $x_i$
$$ \bar x_i = \frac{1}{n-1} \sum_{j\neq i} x_j. $$
The mean of the sample is
$$ \bar x = \frac{1}{n}\sum_i \bar x_i = \frac{1}{n} \sum_i \left(\frac{1}{n-1} \sum_{j\neq i} x_j\right) = \frac{1}{n}\sum_i x_i. $$
The result is consistent with other sample mean methods. Jackknife estimates the variance of the sample</description></item><item><title>Cramér's V</title><link>https://datumorphism.leima.is/cards/statistics/cramers-v/</link><pubDate>Sat, 20 Jul 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/cramers-v/</guid><description/></item><item><title>Kendall Tau Correlation</title><link>https://datumorphism.leima.is/cards/statistics/kendall-correlation-coefficient/</link><pubDate>Sat, 20 Jul 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/kendall-correlation-coefficient/</guid><description>Definition two series of data: $X$ and $Y$ cooccurance of them: $(x_i, x_j)$, and we assume that $i&amp;lt;j$ concordant: $x_i &amp;lt; x_j$ and $y_i &amp;lt; y_j$; $x_i &amp;gt; x_j$ and $y_i &amp;gt; y_j$; denoted as $C$ discordant: $x_i &amp;lt; x_j$ and $y_i &amp;gt; y_j$; $x_i &amp;gt; x_j$ and $y_i &amp;lt; y_j$; denoted as $D$ neither concordant nor discordant: whenever equal sign happens Kendall&amp;rsquo;s tau is defined as
$$ \begin{equation} \tau = \frac{C- D}{\text{all possible pairs of comparison}} = \frac{C- D}{n^2/2 - n/2} \end{equation} $$</description></item><item><title>Bayes' Theorem</title><link>https://datumorphism.leima.is/cards/statistics/bayes-theorem/</link><pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/bayes-theorem/</guid><description>Bayes' Theorem is stated as
$$ P(A\mid B) = \frac{P(B \mid A) P(A)}{P(B)} $$
$P(A\mid B)$: likelihood of A given B $P(A)$: marginal probability of A There is a nice tree diagram for the Bayes' theorem on Wikipedia.
Tree diagram of Bayes' theorem</description></item><item><title>Poisson Process</title><link>https://datumorphism.leima.is/cards/statistics/poisson-process/</link><pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/poisson-process/</guid><description/></item><item><title>Controlled Experiments</title><link>https://datumorphism.leima.is/til/statistics/controlled-experiments/</link><pubDate>Tue, 04 Dec 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/statistics/controlled-experiments/</guid><description>The three levels of controlled experiments</description></item><item><title>Schaum's Outline of Theories and Problems of Elements of Statistics I and II</title><link>https://datumorphism.leima.is/reading/elements-of-statistics/</link><pubDate>Thu, 01 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/reading/elements-of-statistics/</guid><description>The basics and all of modern statistics</description></item></channel></rss>