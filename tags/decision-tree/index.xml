<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Decision tree on Datumorphism</title><link>https://datumorphism.leima.is/tags/decision-tree/</link><description>Recent content in Decision tree on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Wed, 25 Dec 2019 00:00:00 +0000</lastBuildDate><atom:link href="https://datumorphism.leima.is/tags/decision-tree/index.xml" rel="self" type="application/rss+xml"/><item><title>Tree-based Learning</title><link>https://datumorphism.leima.is/wiki/machine-learning/tree-based/overview/</link><pubDate>Wed, 25 Dec 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/tree-based/overview/</guid><description>Decision tree is an easy-to-interpret method in supervised learning. Though simple, it is being used in some widely used algorithms such as random forest method.</description></item><item><title>Decision Tree</title><link>https://datumorphism.leima.is/wiki/machine-learning/tree-based/decision-tree/</link><pubDate>Wed, 25 Dec 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/tree-based/decision-tree/</guid><description>In this article, we will explain how decision trees work and build a tree by hand.
The code used in this article can be found in this repo. Definition of the problem We will decide whether one should go to work today. In this demo project, we consider the following features.
feature possible values health 0: feeling bad, 1: feeling good weather 0: bad weather, 1: good weather holiday 1: holiday, 0: not holiday For more compact notations, we use the abstract notation $\{0,1\}^3$ to describe a set of three features each with 0 and 1 as possible values. In general, the notation $\{0,1\}^d$ indicates $d$ binary features.</description></item><item><title>Random Forest</title><link>https://datumorphism.leima.is/wiki/machine-learning/tree-based/random-forest/</link><pubDate>Wed, 25 Dec 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/tree-based/random-forest/</guid><description>Random forest is an ensemble method based on decision trees. Instead of using one decision tree and model on all the features, the decision tree method can model on a random set of features (feature subspace) using many decision trees and make decisions by democratizing the trees.
Given a proper dataset $\mathscr D(\mathbf X, \mathbf y)$, the ensemble of trees is denoted as ${f_i(\mathbf X)}$, will predict an ensemble of results. There are several key ideas in random forests.
Are the predicted results representative? How to democratize the ensemble of results? What determines the quality of the predictions? Margin, Strength, and Correlations The margin of the model, the strength of the trees, and the correlation between the trees are crucial to answer the questions.</description></item><item><title>Tree-based Methods</title><link>https://datumorphism.leima.is/wiki/machine-learning/tree-based/</link><pubDate>Wed, 25 Dec 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/tree-based/</guid><description/></item></channel></rss>