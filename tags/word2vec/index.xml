<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Word2vec on Datumorphism</title><link>https://datumorphism.leima.is/tags/word2vec/</link><description>Recent content in Word2vec on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Thu, 16 Jan 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://datumorphism.leima.is/tags/word2vec/index.xml" rel="self" type="application/rss+xml"/><item><title>Word2vec</title><link>https://datumorphism.leima.is/wiki/machine-learning/embedding/word2vec/</link><pubDate>Thu, 13 Jun 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/embedding/word2vec/</guid><description>Word2vec is a word embedding model that learns the probability of some words being neighbours in a sentence $p_{neighbours}(w_i, w_o)$.
Build a dataset of adjacent words. CBOW; skipgram; negative sampling; Encode the words using vectors. Build a model $f(\{\theta_i\})$ to calculate the probability of the words being neighours and improve the parameters $\{\theta_i\}$ using the dataset.</description></item><item><title>Embedding</title><link>https://datumorphism.leima.is/wiki/machine-learning/embedding/</link><pubDate>Sun, 13 Oct 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/embedding/</guid><description/></item><item><title>CBOW: Continuous Bag of Words</title><link>https://datumorphism.leima.is/cards/machine-learning/embedding/continuous-bag-of-words/</link><pubDate>Thu, 16 Jan 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/embedding/continuous-bag-of-words/</guid><description>Here we encode all words presented in the corpus to demostrate the idea of CBOW. In the real world, we might want to remove some certain words such as the. We use the following quote by Ford in Westworld as an example.
I read a theory once that the human intellect is like peacock feathers. Just an extravagant display intended to attract a mate, just an elaborate mating ritual. But, of course, the peacock can barely fly. It lives in the dirt, pecking insects out of the muck, consoling itself with its great beauty.
The word intended is surrunded by extravagant display in the front and to attract after it.</description></item><item><title>Negative Sampling</title><link>https://datumorphism.leima.is/cards/machine-learning/embedding/negative-sampling/</link><pubDate>Thu, 16 Jan 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/embedding/negative-sampling/</guid><description>Knowledge of CBOW or skipgram is required.
A naive model to train a model of words is to
encode input words and output words using vectors, use the input word vector to predict the output word vector, calculate the errors between predicted output word vector and real output word vector, minimize the errors. However, it is very expensive to prject out the output words and calcualte the error eveytime. A trick is to use negative sampling.
Negative sampling adds a new column to the data as the predictions.
Input (Center Word) Output (Context) Target (is Neighbour) intended extravagant 1 intended display 1 intended to 1 intended attract 1 Now we have a problem.</description></item><item><title>skipgram: Continuous skip-gram</title><link>https://datumorphism.leima.is/cards/machine-learning/embedding/continuous-skip-gram/</link><pubDate>Thu, 16 Jan 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/embedding/continuous-skip-gram/</guid><description>We use the following quote by Ford in Westworld as an example.
I read a theory once that the human intellect is like peacock feathers. Just an extravagant display intended to attract a mate, just an elaborate mating ritual. But, of course, the peacock can barely fly. It lives in the dirt, pecking insects out of the muck, consoling itself with its great beauty.
The word intended is surrunded by extravagant display in the front and to attract after it. The task is to predict the probability of words around the middle word intended, which are the &amp;lsquo;history words&amp;rsquo; extravagant, display and &amp;lsquo;future words&amp;rsquo; to, attract in our case.</description></item></channel></rss>