<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Crawler on Datumorphism</title><link>https://datumorphism.leima.is/tags/crawler/</link><description>Recent content in Crawler on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Thu, 19 Jul 2018 00:00:00 +0000</lastBuildDate><atom:link href="https://datumorphism.leima.is/tags/crawler/index.xml" rel="self" type="application/rss+xml"/><item><title>Introduction to Node Crawler Series</title><link>https://datumorphism.leima.is/wiki/nodecrawler/node-crawler-introduction/</link><pubDate>Sun, 15 Jul 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/nodecrawler/node-crawler-introduction/</guid><description>This is a set of tutorials that will help you with your very first crawler with node.js.
The plan of this tutorial is as follows. First of all, we will write a functional crawler using node.js and dump the data into files or simply print it on screen. In the following article, we will use MongoDB as our data management system and organize our data. Then we will optimize and attack some of the pitfalls.
As mentioned, Node.js and MongoDB are required for the full tutorial. Here we link to the articles that talks about the installation and configuration of them.</description></item><item><title>Basic Node Crawler</title><link>https://datumorphism.leima.is/wiki/nodecrawler/basic-crawler/</link><pubDate>Sun, 15 Jul 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/nodecrawler/basic-crawler/</guid><description>Prerequisites Nodejs &amp;gt;= 8.9 Overview A model for a crawler is as follows.
A crawler requests data from the server, while the server responds with some data. Here is a graphic illustration
+----------+ +-----------+ | | HTTP Request | | | +----------------&amp;gt; | | Nodejs | | Servers | | &amp;lt;----------------+ | | | HTTP Response | | +----------+ +-----------+ HTTP Requests For a good introduction of HTTP requests, please refer to this video on youtube: Explained HTTP, HTTPS, SSL/TLS API As for the first step, we need to find which url to request. Chrome and Firefox-based browsers come with the developer&amp;rsquo;s tool.</description></item><item><title>Manage Data Using MongoDB</title><link>https://datumorphism.leima.is/wiki/nodecrawler/manage-data-using-mongodb/</link><pubDate>Wed, 18 Jul 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/nodecrawler/manage-data-using-mongodb/</guid><description>In most cases, databases makes the management of data quite convenient. In this article, we would scrape data using the code we discussed before but write data into MongoDB.
For installation of MongoDB, please refer to the official documentation.
The Code To write data to MongoDB using Node.js, we choose the package mongojs, which provides almost exactly the standard MongoDB syntax.
To install mongojs,
npm i mongojs --save Here is a module that can write data to MongoDB. We create a file named dao.js and copy/paste the following code into it.
// use mongojs const mongojs = require(&amp;#39;mongojs&amp;#39;) // connect to the database &amp;#39;simple_spider&amp;#39; in MongoDB and use collection &amp;#39;test&amp;#39; const localdb = mongojs(&amp;#39;simple_spider&amp;#39;, [&amp;#39;test&amp;#39;]) // a function that saves data to MongoDB const saveData = (data,cb) =&amp;gt; { localdb.</description></item><item><title>Restrictions of Websites</title><link>https://datumorphism.leima.is/wiki/nodecrawler/restrictions/</link><pubDate>Thu, 19 Jul 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/nodecrawler/restrictions/</guid><description>Beware that scraping data off websites is neither always allowed nor as easy as a few lines of code. The preceding articles enable you to scrape many data, however, man websites have counter measures. In this article, we will be dealing with some of the common ones.
Request Frequency Some websites have limitations on the frequency of API requests. The solution to this is simply a brief pause after each request. In Node.js, the function setInterval enables this.
// ... require packages here // define the function fetch to get data const fetch = (aid) =&amp;gt; superagent .get(&amp;#39;https://api.bilibili.com/x/web-interface/archive/stat&amp;#39;) .query({ aid:aid }) .</description></item><item><title>Optimization</title><link>https://datumorphism.leima.is/wiki/nodecrawler/optimization/</link><pubDate>Thu, 19 Jul 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/nodecrawler/optimization/</guid><description>In this article, we will be optimizing the crawler to get better performance.
Batch Jobs In the article about using MongoDB as data storage, we write the data to database whenever we get it. In practice, this is not efficient at all. Here comes the batch jobs. It would be much better if one write to database with batch jobs.
If you recall, the code we used to write to database is
// ...other code localdb.test.save(data, (err, res)=&amp;gt;{ // do something }) The function save takes in not only one entry of document but an array of documents:
const array = [] for(let i = INI_ID ; i &amp;lt; MAX_ID; i++){ // fetch data from website const data = fetchData(i) array.</description></item></channel></rss>