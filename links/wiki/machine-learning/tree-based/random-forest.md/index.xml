<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>wiki/machine-learning/tree-based/random-forest.md on Datumorphism</title><link>https://datumorphism.leima.is/links/wiki/machine-learning/tree-based/random-forest.md/</link><description>Recent content in wiki/machine-learning/tree-based/random-forest.md on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><atom:link href="https://datumorphism.leima.is/links/wiki/machine-learning/tree-based/random-forest.md/index.xml" rel="self" type="application/rss+xml"/><item><title>Decision Tree</title><link>https://datumorphism.leima.is/wiki/machine-learning/tree-based/decision-tree/</link><pubDate>Wed, 25 Dec 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/tree-based/decision-tree/</guid><description>In this article, we will explain how decision trees work and build a tree by hand.
The code used in this article can be found in this repo. Definition of the problem We will decide whether one should go to work today. In this demo project, we consider the following features.
feature possible values health 0: feeling bad, 1: feeling good weather 0: bad weather, 1: good weather holiday 1: holiday, 0: not holiday For more compact notations, we use the abstract notation $\{0,1\}^3$ to describe a set of three features each with 0 and 1 as possible values. In general, the notation $\{0,1\}^d$ indicates $d$ binary features.</description></item></channel></rss>