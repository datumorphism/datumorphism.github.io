<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>wiki/machine-learning/basics/kl-divergence.md on Datumorphism</title><link>https://datumorphism.leima.is/links/wiki/machine-learning/basics/kl-divergence.md/</link><description>Recent content in wiki/machine-learning/basics/kl-divergence.md on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><atom:link href="https://datumorphism.leima.is/links/wiki/machine-learning/basics/kl-divergence.md/index.xml" rel="self" type="application/rss+xml"/><item><title>Fraser Information</title><link>https://datumorphism.leima.is/cards/information/fraser-information/</link><pubDate>Wed, 05 May 2021 17:49:12 +0200</pubDate><guid>https://datumorphism.leima.is/cards/information/fraser-information/</guid><description>The Fraser information is
$$ I_F(\theta) = \int g(X) \ln f(X;\theta) , \mathrm d X. $$
When comparing two models, $\theta_0$ and $\theta_1$, the information gain is
$$ \propto (F(\theta_1) - F(\theta_0)). $$
The Fraser information is closed related to Fisher information Fisher Information Fisher information measures the second moment of the model sensitivity with respect to the parameters. , Shannon information, and Kullback information KL Divergence Kullbackâ€“Leibler divergence indicates the differences between two distributions 1.</description></item><item><title>Evidence Lower Bound: ELBO</title><link>https://datumorphism.leima.is/wiki/machine-learning/bayesian/elbo/</link><pubDate>Mon, 12 Apr 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/bayesian/elbo/</guid><description>This article reuses a lot of materials from the references. Please see the references for more details on ELBO. Given a probability distribution density $p(X)$ and a latent variable $Z$, we have the marginalization of the join probability being
$$ \int dZ p(X, Z) = p(X). $$
Using Jensen&amp;rsquo;s Inequality In many models, we are interested in the log probability density $\log p(X)$ which can be decomposed using an auxillary density of the latent variable $q(Z)$,</description></item><item><title>Latent Variable Models</title><link>https://datumorphism.leima.is/wiki/machine-learning/bayesian/latent-variable-models/</link><pubDate>Wed, 27 Jan 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/bayesian/latent-variable-models/</guid><description>In the view of statistics, we know everything about a physical system if we know the probability $p(\mathbf s)$ of all possible states of the physical system $\mathbf s$. Time can also be part of the state specification.
As an example, we will classify fruits into oranges and non oranges. We will have the state vector $\mathbf s = (\text{is orange}, \text{texture } x)$. Our goal is to find the join probability $p(\text{is orange}, x)$.</description></item></channel></rss>