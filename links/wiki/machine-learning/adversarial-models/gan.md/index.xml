<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>wiki/machine-learning/adversarial-models/gan.md on Datumorphism</title><link>https://datumorphism.leima.is/links/wiki/machine-learning/adversarial-models/gan.md/</link><description>Recent content in wiki/machine-learning/adversarial-models/gan.md on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><atom:link href="https://datumorphism.leima.is/links/wiki/machine-learning/adversarial-models/gan.md/index.xml" rel="self" type="application/rss+xml"/><item><title>f-GAN</title><link>https://datumorphism.leima.is/wiki/machine-learning/adversarial-models/f-gan/</link><pubDate>Fri, 13 Aug 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/adversarial-models/f-gan/</guid><description>The essence of GAN GAN The task of GAN is to generate features $X$ from some noise $\xi$ and class labels $Y$, $$\xi, Y \to X.$$ Many different GANs are proposed. Vanilla GAN has a simple structure with a single discriminator and a single generator. It uses the minmax game setup. However, it is not stable to use minmax game to train a GAN model. WassersteinGAN was proposed to solve the stability problem during training1. More advanced GANs like BiGAN and ALI have more complex structures. Vanilla GAN Minmax Game â€¦ is comparing the generated distribution $p_G$ and the data distribution $p_\text{data}$.</description></item></channel></rss>