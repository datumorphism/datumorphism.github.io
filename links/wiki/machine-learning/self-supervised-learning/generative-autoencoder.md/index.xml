<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>wiki/machine-learning/self-supervised-learning/generative-autoencoder.md on Datumorphism</title><link>https://datumorphism.leima.is/links/wiki/machine-learning/self-supervised-learning/generative-autoencoder.md/</link><description>Recent content in wiki/machine-learning/self-supervised-learning/generative-autoencoder.md on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><atom:link href="https://datumorphism.leima.is/links/wiki/machine-learning/self-supervised-learning/generative-autoencoder.md/index.xml" rel="self" type="application/rss+xml"/><item><title>Generative Model: Variational Auto-Encoder</title><link>https://datumorphism.leima.is/wiki/machine-learning/self-supervised-learning/generative-variational-autoencoder/</link><pubDate>Fri, 13 Aug 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/self-supervised-learning/generative-variational-autoencoder/</guid><description>Variational Auto-Encoder (VAE) is very different from Generative Model: Auto-Encoder Generative Model: Auto-Encoder The simplest auto-encoder is rather simple. The loss can be chosen based on the demand, e.g., cross entropy for binary labels. . In VAE, we introduce a variational distribution $q$ to help us work out the weighted integral after introducing the latent space variable $z$,
$$ \begin{align} \ln p_\theta(x) &amp;= \ln \int p_\theta (x\mid z) p(z) \,\mathrm d z \\ &amp;= \ln \int \frac{q_{\phi}(z\mid x)}{q_{\phi}(z\mid x)} p_\theta (x\mid z) p(z) \, \mathrm d z\\ &amp; \geq - \left[ D_{\mathrm{KL}} ( q_{\phi}(z\mid x) \mathrel{\Vert} p(z) ) - \mathbb E_q ( \ln p_\theta (x\mid z) ) \right] \\ &amp;\equiv - F(x).</description></item></channel></rss>