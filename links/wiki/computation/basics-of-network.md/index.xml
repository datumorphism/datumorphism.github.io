<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>wiki/computation/basics-of-network.md on Datumorphism</title><link>https://datumorphism.leima.is/links/wiki/computation/basics-of-network.md/</link><description>Recent content in wiki/computation/basics-of-network.md on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><atom:link href="https://datumorphism.leima.is/links/wiki/computation/basics-of-network.md/index.xml" rel="self" type="application/rss+xml"/><item><title>Introduction to Node Crawler Series</title><link>https://datumorphism.leima.is/wiki/nodecrawler/node-crawler-introduction/</link><pubDate>Sun, 15 Jul 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/nodecrawler/node-crawler-introduction/</guid><description>This is a set of tutorials that will help you with your very first crawler with node.js.
The plan of this tutorial is as follows. First of all, we will write a functional crawler using node.js and dump the data into files or simply print it on screen. In the following article, we will use MongoDB as our data management system and organize our data. Then we will optimize and attack some of the pitfalls.
As mentioned, Node.js and MongoDB are required for the full tutorial. Here we link to the articles that talks about the installation and configuration of them.</description></item></channel></rss>