<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>wiki/model-selection/parsimony-of-models.md on Datumorphism</title><link>https://datumorphism.leima.is/links/wiki/model-selection/parsimony-of-models.md/</link><description>Recent content in wiki/model-selection/parsimony-of-models.md on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><atom:link href="https://datumorphism.leima.is/links/wiki/model-selection/parsimony-of-models.md/index.xml" rel="self" type="application/rss+xml"/><item><title>Goodness-of-fit</title><link>https://datumorphism.leima.is/wiki/model-selection/goodness-of-fit/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/model-selection/goodness-of-fit/</guid><description>Does the data agree with the model?
Calculate the distance between data and model predictions. Apply Bayesian methods such as likelihood estimation: likelihood of observing the data if we assume the model; the results will be a set of fitting parameters. &amp;hellip; Why don&amp;rsquo;t we always use goodness-of-fit as a measure of the goodness of a model?
We may experience overfitting. The model may not be intuitive. This is why we would like to balance it with parsimony using some measures of generalizability.
K-means and overfitting
The overfitting problem is easily demonstrated using the K-means model.
Suppose we use $k=1$, i.</description></item></channel></rss>