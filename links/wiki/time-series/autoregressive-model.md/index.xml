<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>wiki/time-series/autoregressive-model.md on Datumorphism</title><link>https://datumorphism.leima.is/links/wiki/time-series/autoregressive-model.md/</link><description>Recent content in wiki/time-series/autoregressive-model.md on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><atom:link href="https://datumorphism.leima.is/links/wiki/time-series/autoregressive-model.md/index.xml" rel="self" type="application/rss+xml"/><item><title>Generative Model: Autoregressive Model</title><link>https://datumorphism.leima.is/wiki/machine-learning/generative-models/autoregressive-model/</link><pubDate>Fri, 13 Aug 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/generative-models/autoregressive-model/</guid><description>An autoregressive (AR) model is autoregressive,
$$ \begin{equation} \log p_\theta (x) = \sum_{t=1}^T \log p_\theta ( x_{t} \mid {x_{&amp;lt;t}} ). \end{equation} $$
In the above example, the likelihood is modeled as
$$ \begin{align} p_\theta (x) &amp;= \Pi_{t=1}^T p_\theta (x_t \mid x_{1:t-1}) \\ &amp;= p_\theta(x_2 \mid x_{1:1}) p_\theta(x_3 \mid x_{1:2}) \cdots p_\theta(x_T \mid x_{1:T-1}) \end{align} $$
Taking the log of it
$$ \ln p_\theta (x) = \sum_{t=1}^T \ln p_\theta (x_t \mid x_{1:t-1}) $$
Notations and Conventions
In AR models, we have to mention the preceding nodes (${x_{&amp;lt;t}}$) of a specific node ($x_{t}$). For $t=5$, the relations between ${x_{&amp;lt;5}}$ and $x_5$ is shown in the following illustration.</description></item><item><title>Box-Cox Transformation</title><link>https://datumorphism.leima.is/cards/statistics/box-cox/</link><pubDate>Tue, 13 Jul 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/box-cox/</guid><description>Box-Cox transformation is a power transformation that involves logs and powers. It transforms data into normal distributions.
The Box-Cox transformation is defined as
$$ y_i^{(\lambda)} = \begin{cases} \lambda ^{-1} (y_i^\lambda - 1) &amp; \quad \text{if } \lambda \neq 0\\ \log(y_i) &amp; \quad \text{if } \lambda = 0. \end{cases} $$
By selecting a proper $\lambda$, we get a Guassian distributed data, with a variable mean. The transformation take $y$ to
$$ \rho(y^{(\lambda)}) =\frac{ \exp{\left( -(y^{(\lambda)} - \beta X)^{T} (y^{(\lambda)} - \beta X)/(2\sigma^2) \right) }}{(\sqrt{2\pi \sigma^2})^n} \prod_{i=1}^n \left\lvert \frac{d y_i^{(\lambda )}}{ dy_i } \right\rvert. $$
The term
$$ \prod_{i=1}^n \left\lvert \frac{d y_i^{(\lambda )}}{ dy_i } \right\rvert = \lvert J \rvert $$</description></item></channel></rss>