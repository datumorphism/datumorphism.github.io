<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>cards/information/fraser-information.md on Datumorphism</title><link>https://datumorphism.leima.is/links/cards/information/fraser-information.md/</link><description>Recent content in cards/information/fraser-information.md on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><atom:link href="https://datumorphism.leima.is/links/cards/information/fraser-information.md/index.xml" rel="self" type="application/rss+xml"/><item><title>Explained Variation</title><link>https://datumorphism.leima.is/cards/statistics/explained-variation/</link><pubDate>Wed, 05 May 2021 18:05:47 +0200</pubDate><guid>https://datumorphism.leima.is/cards/statistics/explained-variation/</guid><description>Using [[Fraser information]] Fraser Information The Fraser information is $$ I_F(\theta) = \int g(X) \ln f(X;\theta) , \mathrm d X. $$ When comparing two models, $\theta_0$ and $\theta_1$, the information gain is $$ \propto (F(\theta_1) - F(\theta_0)). $$ The Fraser information is closed related to [[Fisher information]] Fisher Information Fisher information measures the second moment of the model sensitivity with respect to the parameters. , Shannon information, and [[Kullback information]] KL Divergence Kullback–Leibler divergence indicates … , we can define a relative information gain by a model
$$ \rho_C ^2 = 1 - \frac{ \exp( - 2 F(\theta_1) ) }{ \exp( - 2 F(\theta_0) ) }, $$</description></item><item><title>Fisher Information</title><link>https://datumorphism.leima.is/cards/information/fisher-information/</link><pubDate>Wed, 05 May 2021 17:49:03 +0200</pubDate><guid>https://datumorphism.leima.is/cards/information/fisher-information/</guid><description>Given a probability density model $f(X; \theta)$ for a observable $X$, the amount of information that $X$ carriers regarding the model is called Fisher information.
Given ${\theta}$, the probability of observing the value $X$, i.e., the likelihood is
$$ f(X\mid\theta). $$
To describe the suitability of a model and the observables, we can use a the likelihood $f(X\mid \theta)$. One particular interesting property is the sensitivity of the likelihood in terms of the parameter $\theta$ change. For example, the case on the left is less compatible as we have a large variance in the parameters. The model is not very sensitive to the parameter change.</description></item></channel></rss>