<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>cards/information/shannon-entropy.md on Datumorphism</title><link>https://datumorphism.leima.is/links/cards/information/shannon-entropy.md/</link><description>Recent content in cards/information/shannon-entropy.md on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><atom:link href="https://datumorphism.leima.is/links/cards/information/shannon-entropy.md/index.xml" rel="self" type="application/rss+xml"/><item><title>Cross Entropy</title><link>https://datumorphism.leima.is/cards/information/cross-entropy/</link><pubDate>Sat, 04 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/information/cross-entropy/</guid><description>Cross entropy is1
$$ H(p, q) = \mathbb E_{p} \left[ -\log q \right]. $$
Cross entropy $H(p, q)$ can also be decomposed,
$$ H(p, q) = H(p) + \operatorname{D}_{\mathrm{KL}} \left( p \parallel q \right), $$
where $H(p)$ is the entropy of $P$ Shannon Entropy Shannon entropy $S$ is the expectation of information content $I(X)=-\log \left(p\right)$1, \begin{equation} H(p) = \mathbb E_{p}\left[ -\log \left(p\right) \right]. \end{equation} shannon_entropy_wiki Contributors to Wikimedia projects. Entropy (information theory). In: Wikipedia [Internet]. 29 Aug 2021 [cited 4 Sep 2021]. Available: https://en.wikipedia.org/wiki/Entropy_(information_theory) &amp;#160;&amp;#x21a9;&amp;#xfe0e; and $\operatorname{D}_{\mathrm{KL}}$ is the KL Divergence KL Divergence Kullbackâ€“Leibler divergence indicates the differences between two distributions .</description></item></channel></rss>