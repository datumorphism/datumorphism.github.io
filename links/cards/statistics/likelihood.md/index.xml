<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>cards/statistics/likelihood.md on Datumorphism</title><link>https://datumorphism.leima.is/links/cards/statistics/likelihood.md/</link><description>Recent content in cards/statistics/likelihood.md on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><atom:link href="https://datumorphism.leima.is/links/cards/statistics/likelihood.md/index.xml" rel="self" type="application/rss+xml"/><item><title>MaxEnt Model</title><link>https://datumorphism.leima.is/wiki/machine-learning/energy-based-model/maxent-energy-based-model/</link><pubDate>Mon, 31 May 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/energy-based-model/maxent-energy-based-model/</guid><description>The Maximum Entropy model, aka MaxEnt model, is a fascinating generative model as it is based on a very intuitive idea from statistical physics - the Principle of Maximum Entropy.
The Idea The essence of the MaxEnt model is that the underlying probability distribution $p(x)$ of the random variables $x$ should
gives the whole system the largest uncertainty, while producing reasonable observables. Uncertainty The uncertainty of the whole system is described by the Shannon entropy based on the probability distributions $p(x)$,</description></item><item><title>Naive Bayes</title><link>https://datumorphism.leima.is/wiki/machine-learning/bayesian/naive-bayes/</link><pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/bayesian/naive-bayes/</guid><description>Naive Bayesian is a classifier using Bayes&amp;#39; Theorem Bayes&amp;#39; Theorem Bayes&amp;rsquo; Theorem is stated as $$ P(A\mid B) = \frac{P(B \mid A) P(A)}{P(B)} $$ $P(A\mid B)$: likelihood of A given B $P(A)$: marginal probability of A There is a nice tree diagram for the Bayes&amp;rsquo; theorem on Wikipedia. Tree diagram of Bayes&amp;rsquo; theorem with &amp;lsquo;naive&amp;rsquo; assumptions.
Problems with Conditional Probability Calculation By definition, the conditional probability of event $\mathbf Y$ given features $\mathbf X$ is $$ \begin{equation} P(\mathbf Y\mid \mathbf X) = \frac{P(\mathbf Y, \mathbf X)}{ P(\mathbf X) }, \label{def-cp-y-given-x} \end{equation} $$</description></item><item><title>Goodness-of-fit</title><link>https://datumorphism.leima.is/wiki/model-selection/goodness-of-fit/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/model-selection/goodness-of-fit/</guid><description>Does the data agree with the model?
Calculate the distance between data and model predictions. Apply Bayesian methods such as likelihood estimation: likelihood of observing the data if we assume the model; the results will be a set of fitting parameters. &amp;hellip; Why don&amp;rsquo;t we always use goodness-of-fit as a measure of the goodness of a model?
We may experience overfitting. The model may not be intuitive. This is why we would like to balance it with parsimony using some measures of generalizability.</description></item><item><title>Bayesian Linear Regression</title><link>https://datumorphism.leima.is/wiki/machine-learning/bayesian/bayesian-linear-regression/</link><pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/bayesian/bayesian-linear-regression/</guid><description>Linear Regression and Likelihood The linear estimator $y$ is
$$ \begin{equation} y^n = \beta^m X_m^{\phantom{m}n}. \label{eq-linear-model} \end{equation} $$
As usual, we have redefined our data to get rid of the intercept $\beta^0$.
In ordinary linear models, we find the error being the difference between the target $\hat y$ and the estimator $y$
$$ \epsilon = \hat y - y, $$
which is required to have a minimum absolute value.
In linear regressions, we use least squares to solve the problem.</description></item><item><title>Logistic Regression</title><link>https://datumorphism.leima.is/wiki/machine-learning/linear/logistic-regression/</link><pubDate>Thu, 27 May 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/linear/logistic-regression/</guid><description>In a classification problem, given a list of features values $x$ and their corresponding classes $\{c_i\}$, the posterior for of the classes, aka conditional probability of the classes, is
$$ p(C=c_i\mid X=x). $$
Likelihood
The likelihood of the data is
$$ p(X=x\mid C=c_i). $$
Logistic Regression for Two Classes For two classes, the simplest model for the posterior is a linear model,
$$ \log \frac{p(C=c_1\mid X=x) }{p(C=c_2\mid X=x)} = \beta_0 + \beta_1 \cdot x, $$</description></item><item><title>Boltzmann Machine</title><link>https://datumorphism.leima.is/wiki/machine-learning/energy-based-model/boltzmann-machine/</link><pubDate>Sun, 27 Aug 2017 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/energy-based-model/boltzmann-machine/</guid><description>Boltzmann machine is much like a spin glass model in physics. In short words, Boltzmann machine is a machine that has nodes that can take values, and the nodes are connected through some weight. It is just like any other neural nets but with complications and theoretical implications.
Boltzmann machine is usually used as a generative model.
Boltzmann Machine and Physics To obtain a good understanding of Boltzmann machine for a physicist, we begin with Ising model.</description></item><item><title>Restricted Boltzmann Machine</title><link>https://datumorphism.leima.is/wiki/machine-learning/energy-based-model/restricted-boltzmann-machine/</link><pubDate>Fri, 11 Jun 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/energy-based-model/restricted-boltzmann-machine/</guid><description>Latent variables introduce extra correlations between the nodes in a network. Introducing hidden units can also help us remove the direct connection between some nodes in a Boltzmann machine and create a restricted Boltzmann machine. A restricted Boltzmann machine requires less computation while having some expressing power.
Given Ising like interactions between the nodes, flipping node V1 is likely to also flip node V2 as they are connected through hidden unit H1.</description></item><item><title>Fisher Information</title><link>https://datumorphism.leima.is/cards/information/fisher-information/</link><pubDate>Wed, 05 May 2021 17:49:03 +0200</pubDate><guid>https://datumorphism.leima.is/cards/information/fisher-information/</guid><description>Given a probability density model $f(X; \theta)$ for a observable $X$, the amount of information that $X$ carriers regarding the model is called Fisher information.
Given ${\theta}$, the probability of observing the value $X$, i.e., the likelihood is
$$ f(X\mid\theta). $$
To describe the suitability of a model and the observables, we can use a the likelihood $f(X\mid \theta)$. One particular interesting property is the sensitivity of the likelihood in terms of the parameter $\theta$ change.</description></item><item><title>ERM: Empirical Risk Minimization</title><link>https://datumorphism.leima.is/cards/machine-learning/learning-theories/empirical-risk-minimization/</link><pubDate>Thu, 18 Feb 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/learning-theories/empirical-risk-minimization/</guid><description>In a learning problem The Learning Problem The learning problem posed by Vapnik:1 Given a sample: $\{z_i\}$ in the probability space $Z$; Assuming a probability measure on the probability space $Z$; Assuming a set of functions $Q(z, \alpha)$ (e.g. loss functions), where $\alpha$ is a set of parameters; A risk functional to be minimized by tunning &amp;ldquo;the handles&amp;rdquo; $\alpha$, $R(\alpha)$. The risk functional is $$ R(\alpha) = \int Q(z, \alpha) \,\mathrm d F(z).</description></item><item><title>Latent Variable Models</title><link>https://datumorphism.leima.is/wiki/machine-learning/bayesian/latent-variable-models/</link><pubDate>Wed, 27 Jan 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/bayesian/latent-variable-models/</guid><description>In the view of statistics, we know everything about a physical system if we know the probability $p(\mathbf s)$ of all possible states of the physical system $\mathbf s$. Time can also be part of the state specification.
As an example, we will classify fruits into oranges and non oranges. We will have the state vector $\mathbf s = (\text{is orange}, \text{texture } x)$. Our goal is to find the joint probability $p(\text{is orange}, x)$.</description></item><item><title>Akaike Information Criterion</title><link>https://datumorphism.leima.is/cards/statistics/aic/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/aic/</guid><description>Suppose we have a model that describes the data generation process behind a dataset. The distribution by the model is denoted as $\hat f$. The actual data generation process is described by a distribution $f$.
We ask the question:
How good is the approximation using $\hat f$?
To be more precise, how much information is lost if we use our model dist $\hat f$ to substitute the actual data generation distribution $f$?</description></item><item><title>Minimum Description Length</title><link>https://datumorphism.leima.is/cards/statistics/mdl/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/mdl/</guid><description>The minimum description length, aka, MDL, is based on the relations between regularity and data compression. (See Kolmogorov complexity Kolmogorov Complexity Description of Data The measurement of complexity is based on the observation that the compressibility of data doesn&amp;rsquo;t depend on the &amp;ldquo;language&amp;rdquo; used to describe the compression process that much. This makes it possible for us to find a universal language, such as a universal computer language, to quantify the compressibility of the data.</description></item><item><title>Normalized Maximum Likelihood</title><link>https://datumorphism.leima.is/cards/statistics/nml/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/nml/</guid><description>$$ \mathrm{NML} = \frac{ p(y| \hat \theta(y)) }{ \int_X p( x| \hat \theta (x) ) dx } $$</description></item><item><title>Bayes' Theorem</title><link>https://datumorphism.leima.is/cards/statistics/bayes-theorem/</link><pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/bayes-theorem/</guid><description>Bayes&amp;rsquo; Theorem is stated as
$$ P(A\mid B) = \frac{P(B \mid A) P(A)}{P(B)} $$
$P(A\mid B)$: likelihood of A given B $P(A)$: marginal probability of A There is a nice tree diagram for the Bayes&amp;rsquo; theorem on Wikipedia.
Tree diagram of Bayes&amp;rsquo; theorem</description></item></channel></rss>