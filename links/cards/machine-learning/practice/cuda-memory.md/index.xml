<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>cards/machine-learning/practice/cuda-memory.md on Datumorphism</title><link>https://datumorphism.leima.is/links/cards/machine-learning/practice/cuda-memory.md/</link><description>Recent content in cards/machine-learning/practice/cuda-memory.md on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><atom:link href="https://datumorphism.leima.is/links/cards/machine-learning/practice/cuda-memory.md/index.xml" rel="self" type="application/rss+xml"/><item><title>Pytorch Data Parallelism</title><link>https://datumorphism.leima.is/cards/machine-learning/practice/pytorch-data-parallelism/</link><pubDate>Wed, 19 Oct 2022 14:54:29 +0200</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/practice/pytorch-data-parallelism/</guid><description>To train large models using PyTorch, we need to go parallel. There are two commonly used strategies123:
model parallelism, data parallelism, data-model parallelism. Model Parallelism Model parallelism splits the model on different nodes14. We will focus on data parallelism but the key idea is shown in the following illustration.
Model parallelLi X, Zhang G, Li K, Zheng W. Chapter 4 - Deep Learning and Its Parallelization. In: Buyya R, Calheiros RN, Dastjerdi AV, editors. Big Data. Morgan Kaufmann; 2016. pp. 95â€“118. doi:10.1016/B978-0-12-805394-2.00004-0
Data Parallelism Data parallelism creates replicas of the model on each device and use different subsets of training data14.</description></item></channel></rss>