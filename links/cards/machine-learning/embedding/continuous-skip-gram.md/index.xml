<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>cards/machine-learning/embedding/continuous-skip-gram.md on Datumorphism</title><link>https://datumorphism.leima.is/links/cards/machine-learning/embedding/continuous-skip-gram.md/</link><description>Recent content in cards/machine-learning/embedding/continuous-skip-gram.md on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><atom:link href="https://datumorphism.leima.is/links/cards/machine-learning/embedding/continuous-skip-gram.md/index.xml" rel="self" type="application/rss+xml"/><item><title>Word2vec</title><link>https://datumorphism.leima.is/wiki/machine-learning/embedding/word2vec/</link><pubDate>Thu, 13 Jun 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/embedding/word2vec/</guid><description>Word2vec is a word embedding model that learns the probability of some words being neighbours in a sentence $p_{neighbours}(w_i, w_o)$.
Build a dataset of adjacent words. CBOW; skipgram; negative sampling; Encode the words using vectors. Build a model $f(\{\theta_i\})$ to calculate the probability of the words being neighours and improve the parameters $\{\theta_i\}$ using the dataset.</description></item><item><title>Negative Sampling</title><link>https://datumorphism.leima.is/cards/machine-learning/embedding/negative-sampling/</link><pubDate>Thu, 16 Jan 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/embedding/negative-sampling/</guid><description>Knowledge of [[CBOW]] CBOW: Continuous Bag of Words Use the context to predict the center word or [[skipgram]] skipgram: Continuous skip-gram Use the center word to predict the context is required.
A naive model to train a model of words is to
encode input words and output words using vectors, use the input word vector to predict the output word vector, calculate the errors between predicted output word vector and real output word vector, minimize the errors. However, it is very expensive to project out the output words and calculate the error every time. A trick is to use negative sampling.</description></item></channel></rss>