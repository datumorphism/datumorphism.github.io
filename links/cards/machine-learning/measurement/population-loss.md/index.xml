<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>cards/machine-learning/measurement/population-loss.md on Datumorphism</title><link>/links/cards/machine-learning/measurement/population-loss.md/</link><description>Recent content in cards/machine-learning/measurement/population-loss.md on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><atom:link href="/links/cards/machine-learning/measurement/population-loss.md/index.xml" rel="self" type="application/rss+xml"/><item><title>Measures of Generalizability</title><link>/wiki/model-selection/measures-of-generalizability/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>/wiki/model-selection/measures-of-generalizability/</guid><description>To measure the generalization, we define a generalization error,
$$ \begin{align} \mathcal G = \mathcal L_{P}(\hat f) - \mathcal L_E(\hat f), \end{align} $$ where $\mathcal L_{P}$ is the population loss, $\mathcal L_E$ is the empirical loss, and $\hat f$ is our model by minimizing the empirical loss.
However, we do not know the actual joint probability $p(x, y)$ of our dataset $\{x_i, y_i\}$. Thus the population loss is not known. In machine learning, we usually use cross validation where we split our dataset into train and test dataset.</description></item><item><title>Empirical Loss</title><link>/cards/machine-learning/measurement/empirical-loss/</link><pubDate>Sat, 06 Feb 2021 00:00:00 +0000</pubDate><guid>/cards/machine-learning/measurement/empirical-loss/</guid><description>Given a dataset with records $\{x_i, y_i\}$ and a model $\hat y_i = f(x_i)$ the empirical loss is calculated on all the records
$$ \begin{align} \mathcal L_{E} = \frac{1}{n} \sum_i^n d(y_i, f(x_i)), \end{align} $$ where $d(y_i, f(x_i))$ is the distance defined between $y_i$ and $f(x_i)$.</description></item></channel></rss>