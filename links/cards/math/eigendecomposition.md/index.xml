<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>cards/math/eigendecomposition.md on Datumorphism</title><link>/links/cards/math/eigendecomposition.md/</link><description>Recent content in cards/math/eigendecomposition.md on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><atom:link href="/links/cards/math/eigendecomposition.md/index.xml" rel="self" type="application/rss+xml"/><item><title>NMF: Nonnegative Matrix Factorizatioin</title><link>/wiki/machine-learning/factorization/nmf/</link><pubDate>Thu, 13 Jun 2019 00:00:00 +0000</pubDate><guid>/wiki/machine-learning/factorization/nmf/</guid><description>Decomposition To make it easier to understand, we start with a data point $\mathbf P$ in a $k$-dimensional space spanned by $k$ basis vectors $\mathbf V^k$. Naturally, we could write down the component decomposition of the point using the basis vectors $\mathbf V^k$,
$$ \mathbf P = P_k \mathbf V^k. $$
This is immediately obvious to us since we have been dealing with rank 2 $(k, 1)$ basis vectors and we are talking about the $k$ coordinates for a point.</description></item><item><title>Diagnolize Matrices</title><link>/cards/math/diagonalize-matrix/</link><pubDate>Wed, 11 Mar 2020 00:00:00 +0000</pubDate><guid>/cards/math/diagonalize-matrix/</guid><description>Given a matrix $\mathbf A$, it is diagonalized using its eigenvectors.
Why are the eigenvectors needed?
Eigenvectors of a matrix $\mathbf A$ are the preferred directions. From the definition of eigenvectors,
$$ \mathbf A \mathbf x = \lambda \mathbf x, $$
we know that the matrix $\mathbf A$ only scales the eigenvectors and no rotations. These directions are special to the matrix $\mathbf A$.
Find the eigenvectors $\mathbf x_i$ of the matrix $\mathbf A$; If we find degerations, the matrix is not diagonalizable.</description></item></channel></rss>