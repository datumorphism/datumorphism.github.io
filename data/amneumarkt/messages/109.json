{
    "_": "Message",
    "id": 109,
    "peer_id": {
        "_": "PeerChannel",
        "channel_id": 1320526773
    },
    "date": "2020-12-09 12:11:33+00:00",
    "message": "#ML #paper\n\nhttps://arxiv.org/abs/2012.00152\nEvery Model Learned by Gradient Descent Is Approximately a Kernel Machine\nDeep learning's successes are often attributed to its ability to automatically discover new representations of the data, rather than relying on handcrafted features like other learning methods.",
    "out": true,
    "mentioned": false,
    "media_unread": false,
    "silent": true,
    "post": true,
    "from_scheduled": false,
    "legacy": false,
    "edit_hide": false,
    "pinned": false,
    "from_id": null,
    "fwd_from": null,
    "via_bot_id": null,
    "reply_to": null,
    "media": null,
    "reply_markup": null,
    "entities": [
        {
            "_": "MessageEntityHashtag",
            "offset": 0,
            "length": 3
        },
        {
            "_": "MessageEntityHashtag",
            "offset": 4,
            "length": 6
        },
        {
            "_": "MessageEntityUrl",
            "offset": 12,
            "length": 32
        }
    ],
    "views": 114,
    "forwards": 0,
    "replies": {
        "_": "MessageReplies",
        "replies": 0,
        "replies_pts": 1147,
        "comments": true,
        "recent_repliers": [],
        "channel_id": 1159907975,
        "max_id": null,
        "read_max_id": null
    },
    "edit_date": "2021-01-13 12:39:53+00:00",
    "post_author": "Markt Mai",
    "grouped_id": null,
    "restriction_reason": [],
    "ttl_period": null,
    "tags": [
        "ML",
        "paper"
    ]
}