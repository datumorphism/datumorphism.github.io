<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>seedling on Datumorphism</title><link>https://datumorphism.leima.is/garden/seedling/</link><description>Recent content in seedling on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Sat, 13 Aug 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://datumorphism.leima.is/garden/seedling/index.xml" rel="self" type="application/rss+xml"/><item><title>Neural ODE Basics</title><link>https://datumorphism.leima.is/wiki/machine-learning/neural-ode/neural-ode-basics/</link><pubDate>Sat, 13 Aug 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/neural-ode/neural-ode-basics/</guid><description>In [[neural networks]] Artificial Neural Networks Simple artificial neural networks using multilayer perceptron , residual connections is a popular architecture to build very deep neural networks.1 Apart from residual networks, there are many other designs for deep neural networks.23456 These methods share similar ideas that the layered structure in deep neural networks can be treated as a dynamical system and these different architectures are different numerical approaches of solving the dynamical system.
Figure taken from He K, Zhang X, Ren S, Sun J. Deep Residual Learning for Image Recognition. arXiv [cs.CV]. 2015. Available: http://arxiv.org/abs/1512.03385
The degradation problem For deep neural networks, not all deep architectures are able to produce results that are significantly better than shallow architectures.</description></item><item><title>Neyman-Pearson Theory</title><link>https://datumorphism.leima.is/wiki/statistical-hypothesis-testing/neyman-pearson-theory/</link><pubDate>Sat, 02 Apr 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/statistical-hypothesis-testing/neyman-pearson-theory/</guid><description>The Neyman-Pearson hypothesis testing tests two hypothesis, hypothesis $H$, and an alternative hypothesis $H_A$.
Neyman-Pearson Lemma The Neyman-Pearson Lemma is an very intuitive lemma to understand how to choose a hypothesis. The lecture notes from PennState is a very good read on this topic1.
An example For simplicity, we assume that there exists a test statistic $T$ and $T$ can be used to measure how likely the hypothesis $H$ is true, e.g., the hypothesis $H$ is false, corresponds to $T$ being small.
The reference from Shafer2007 assumes a random variable $T$ to be large if the hypothesis $H$ is false[^Shafer2007]. One example is the ratio of likelihood2,</description></item><item><title>Multiple Comparison Problem</title><link>https://datumorphism.leima.is/cards/statistics/multiple-comparison-problem/</link><pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/multiple-comparison-problem/</guid><description>In a multiple comparisons problem, we deal with multiple statistical tests simultaneously.
Examples We see such problems a lot in IT companies. Suppose we have a website and would like to test if a new design of a button can lead to some changes in five different KPIs (e.g., view-to-click rate, click-to-book rate, &amp;hellip;).
In multi-horizon time series forecasting, we sometimes choose to forecast multiple future data points in one shot. To properly find the confidence intervals of our predictions, one approach is the so called conformal prediction method. This becomes a multiple comparisons problem because we have to tell if we can reject at least one true null hypothesis.</description></item></channel></rss>