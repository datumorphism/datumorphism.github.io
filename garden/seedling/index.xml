<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>seedling on Datumorphism</title><link>https://datumorphism.leima.is/garden/seedling/</link><description>Recent content in seedling on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Thu, 20 Oct 2022 21:15:28 +0200</lastBuildDate><atom:link href="https://datumorphism.leima.is/garden/seedling/index.xml" rel="self" type="application/rss+xml"/><item><title>Neural ODE</title><link>https://datumorphism.leima.is/wiki/machine-learning/neural-ode/neural-ode-basics/</link><pubDate>Sat, 13 Aug 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/machine-learning/neural-ode/neural-ode-basics/</guid><description>In [[neural networks]] Artificial Neural Networks Simple artificial neural networks using multilayer perceptron , residual connections is a popular architecture to build very deep neural networks1. Apart from residual networks, there are many other designs for deep neural networks23456. These methods share similar ideas that the layered structure in deep neural networks can be treated as a dynamical system and these different architectures are different numerical approaches of solving the dynamical system.
Figure taken from He K, Zhang X, Ren S, Sun J. Deep Residual Learning for Image Recognition. arXiv [cs.CV]. 2015. Available: http://arxiv.org/abs/1512.03385
The degradation problem For deep neural networks, not all deep architectures are able to produce results that are significantly better than shallow architectures.</description></item><item><title>Neyman-Pearson Theory</title><link>https://datumorphism.leima.is/wiki/statistical-hypothesis-testing/neyman-pearson-theory/</link><pubDate>Sat, 02 Apr 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/statistical-hypothesis-testing/neyman-pearson-theory/</guid><description>The Neyman-Pearson hypothesis testing tests two hypothesis, hypothesis $H$, and an alternative hypothesis $H_A$.
Neyman-Pearson Lemma The Neyman-Pearson Lemma is an very intuitive lemma to understand how to choose a hypothesis. The lecture notes from PennState is a very good read on this topic1.
An example For simplicity, we assume that there exists a test statistic $T$ and $T$ can be used to measure how likely the hypothesis $H$ is true, e.g., the hypothesis $H$ is false, corresponds to $T$ being small.
The reference from Shafer2007 assumes a random variable $T$ to be large if the hypothesis $H$ is false[^Shafer2007]. One example is the ratio of likelihood2,</description></item><item><title>MTGNN</title><link>https://datumorphism.leima.is/wiki/forecasting/interactions/mtgnn/</link><pubDate>Thu, 20 Oct 2022 21:15:28 +0200</pubDate><guid>https://datumorphism.leima.is/wiki/forecasting/interactions/mtgnn/</guid><description/></item><item><title>Pytorch Data Parallelism</title><link>https://datumorphism.leima.is/cards/machine-learning/practice/pytorch-data-parallelism/</link><pubDate>Wed, 19 Oct 2022 14:54:29 +0200</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/practice/pytorch-data-parallelism/</guid><description>To train large models using PyTorch, we need to go parallel. There are two commonly used strategies123:
model parallelism, data parallelism, data-model parallelism. Model Parallelism Model parallelism splits the model on different nodes14. We will focus on data parallelism but the key idea is shown in the following illustration.
Model parallelLi X, Zhang G, Li K, Zheng W. Chapter 4 - Deep Learning and Its Parallelization. In: Buyya R, Calheiros RN, Dastjerdi AV, editors. Big Data. Morgan Kaufmann; 2016. pp. 95–118. doi:10.1016/B978-0-12-805394-2.00004-0
Data Parallelism Data parallelism creates replicas of the model on each device and use different subsets of training data14.</description></item><item><title>CUDA Memory</title><link>https://datumorphism.leima.is/cards/machine-learning/practice/cuda-memory/</link><pubDate>Wed, 19 Oct 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/practice/cuda-memory/</guid><description>CUDA is widely used in deep learning. Though many of deep learning professionals are not exposed to CUDA directly, most people are already using CUDA as frameworks like PyTorch are providing GPU support through CUDA.
To optimize the computational efficiency of our models, knowledge about the data transfer inside the devices is crucial. In this note, we build up the fundamentals of memory transfer for CUDA.
Segmented Memory and Paged Memory CUDA Can not Use Paged Memory A CPU host uses paged memory. However, GPU can not directly take data from paged memory on the host1. Before accessing the data, CUDA has to pin the memory so that the memory is page-locked2.</description></item><item><title>Empirical Correlation Coefficient (CORR)</title><link>https://datumorphism.leima.is/cards/time-series/ts-corr/</link><pubDate>Sun, 21 Aug 2022 20:43:36 +0200</pubDate><guid>https://datumorphism.leima.is/cards/time-series/ts-corr/</guid><description>The Empirical Correlation Coefficient (CORR) is an evaluation metric in time series forecasting,1
$$ \mathrm{CORR} = \frac{1}{N} \sum_{i=1}^N \frac{ \sum_t (y^{(i)}_t - \bar y^{(i)} ) ( \hat y^{(i)}_t -\bar{ \hat y}^{(i)} ) }{ \sqrt{ \sum_t (y^{(i)}_t - \bar y^{(i)} )^2 ( \hat y^{(i)}_t -\bar{\hat y}^{(i)} )^2 } } $$
where $y^{(i)}$ is the $i$th time series, ${} _ t$ denotes the time step $t$, and $\bar y^{(i)}$ is the mean of the $i$th forecasted series, i.e., $\bar y^{(i)} = \operatorname{mean}( y^{(i)} _ { t \in \{T _ f, T _ {f+1}, \cdots T _ {f+H}\} } )$.
Lai2017 Lai G, Chang W-C, Yang Y, Liu H.</description></item><item><title>Root Relative Squared Error (RSE)</title><link>https://datumorphism.leima.is/cards/time-series/ts-rse/</link><pubDate>Sun, 21 Aug 2022 20:43:36 +0200</pubDate><guid>https://datumorphism.leima.is/cards/time-series/ts-rse/</guid><description>The Root Relative Squared Error (RSE) is an evaluation metric in time series forecasting,1
$$ \mathrm{RSE} = \frac{ \sqrt{ \sum_{i, t} ( y^{(i)}_t - \hat y^{(i)}_t )^2 } }{ \sqrt{ \sum_{i, t} ( y^{(i)}_t - \bar y )^2 } } $$
where $y^{(i)}$ is the $i$th time series, ${} _ t$ denotes the time step $t$, and $\bar y$ is the mean of the forecasted series, i.e., $\bar y = \operatorname{mean}(y^{(i\in\{0, 1, \cdots, N\})} _ { t\in \{T _ f, T _ {f+1}, \cdots T _ {f+H}\} })$.
Lai2017 Lai G, Chang W-C, Yang Y, Liu H.</description></item><item><title>Dilated Convolution</title><link>https://datumorphism.leima.is/cards/math/convolution-dilated/</link><pubDate>Sun, 21 Aug 2022 16:03:14 +0200</pubDate><guid>https://datumorphism.leima.is/cards/math/convolution-dilated/</guid><description>For a convolution
$$ f*h(x) = \sum_{s+t=x} f(s) h(t), $$
the dilated version of it is1
$$ f*_l h(x) = \sum_{s+t*l=x} f(s) h(t), $$
where $l$ is the dilation factor.
Yu2015 Yu F, Koltun V. Multi-Scale Context Aggregation by Dilated Convolutions. arXiv [cs.CV]. 2015. Available: http://arxiv.org/abs/1511.07122 &amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Over-Smoothing in Graph Neural Networks</title><link>https://datumorphism.leima.is/cards/graph/graph-neural-networks-over-smoothing/</link><pubDate>Sun, 21 Aug 2022 13:45:33 +0200</pubDate><guid>https://datumorphism.leima.is/cards/graph/graph-neural-networks-over-smoothing/</guid><description>Over-smoothing is the problem that the representations on each node of the graph neural networks becomes way too similar to each other.1 In Chapter 7 of Hamilton2020, the author interprets this phenomenon using the lower pass filter theory in signal processing, i.e., multiplying a signal by $\mathbf A^n$ is similar to a low-pass filter when $n$ is large, with $\mathbf A$ being the adjacency matrix.
Hamilton2020 Hamilton WL. Graph Representation Learning. Morgan &amp;amp; Claypool Publishers; 2020. pp. 1–159. doi:10.2200/S01045ED1V01Y202009AIM046 &amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Multiple Comparison Problem</title><link>https://datumorphism.leima.is/cards/statistics/multiple-comparison-problem/</link><pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/multiple-comparison-problem/</guid><description>In a multiple comparisons problem, we deal with multiple statistical tests simultaneously.
Examples We see such problems a lot in IT companies. Suppose we have a website and would like to test if a new design of a button can lead to some changes in five different KPIs (e.g., view-to-click rate, click-to-book rate, &amp;hellip;).
In multi-horizon time series forecasting, we sometimes choose to forecast multiple future data points in one shot. To properly find the confidence intervals of our predictions, one approach is the so called conformal prediction method. This becomes a multiple comparisons problem because we have to tell if we can reject at least one true null hypothesis.</description></item></channel></rss>