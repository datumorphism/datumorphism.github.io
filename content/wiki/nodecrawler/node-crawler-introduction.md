---
title: "Introduction to Node Crawler Series"
description: "Installing node.js and mongodb."
date: 2018-07-15
categories:
- 'Node Crawler'
tags:
- 'Node'
- 'Crawler'
references:
- name: 01-基础环境搭建@ninthakeey
  link: 'https://nintha.github.io/2018/07/08/node_spider_compass/01-base_env/'
links:
  - wiki/computation/basics-of-mongodb.md
  - til/programming/database/mongodb-array-and-dict.md
  - wiki/computation/basics-of-network.md
notify: 'The original Chinese article is written by [ninthakeey](https://github.com/nintha). It has been translated and remixed by Datumorphism'
weight: 1
---


This is a set of tutorials that will help you with your very first crawler with node.js.

The plan of this tutorial is as follows. First of all, we will write a functional crawler using node.js and dump the data into files or simply print it on screen. In the following article, we will use MongoDB as our data management system and organize our data. Then we will optimize and attack some of the pitfalls.

As mentioned, [Node.js](https://nodejs.org/en/) and [MongoDB](https://www.mongodb.com/) are required for the full tutorial. Here we link to the articles that talks about the installation and configuration of them.

- [Node.js Download](https://nodejs.org/en/download/)
- [Documentation of MongoDB](https://docs.mongodb.com/)
