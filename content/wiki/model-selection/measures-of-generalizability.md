---
title: Measures of Generalizability
date: 2020-11-08
categories:
  - Model Selection
tags:
  - Model Selection
  - Generalizability
references:
  - link: "https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199957996.001.0001/oxfordhb-9780199957996-e-14"
    name: "Vandekerckhove, J., & Matzke, D. (2015). Model comparison and the principle of parsimony. Oxford Library of Psychology."
  - name: "Roelofs, R. (2019). Measuring Generalization and Overfitting in Machine Learning. Doctoral Dissertation, UC Berkeley, 1â€“171."
    link: "https://escholarship.org/uc/item/6j01x9mz"
links:
  - cards/machine-learning/measurement/empirical-loss.md
  - cards/machine-learning/measurement/population-loss.md
  - cards/machine-learning/learning-theories/cross-validation.md
weight: 3
---

To measure the generalization, we define a generalization error,

{{< m >}}
\begin{align}
\mathcal G = \mathcal L_{P}(\hat f) - \mathcal L_E(\hat f),
\end{align}
{{< /m >}}

where $\mathcal L_{P}$ is the population loss, $\mathcal L_E$ is the empirical loss, and $\hat f$ is our model by minimizing the empirical loss.

However, we do not know the actual joint probability $p(x, y)$ of our dataset $\\{x_i, y_i\\}$. Thus the population loss is not known. In machine learning, we usually use {{< c "cards/machine-learning/learning-theories/cross-validation.md" "cross validation">}} where we split our dataset into train and test dataset. We approximate the population loss using the test dataset.