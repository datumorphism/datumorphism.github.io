<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Time Series on Datumorphism</title><link>https://datumorphism.leima.is/categories/time-series/</link><description>Recent content in Time Series on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Sun, 21 Aug 2022 20:43:36 +0200</lastBuildDate><atom:link href="https://datumorphism.leima.is/categories/time-series/index.xml" rel="self" type="application/rss+xml"/><item><title>The Time Series Forecasting Problem</title><link>https://datumorphism.leima.is/wiki/forecasting/forecasting-problem/</link><pubDate>Sun, 24 Apr 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/forecasting/forecasting-problem/</guid><description>There are many different types of tasks on time series data:
classification, anomaly detection, forecasting. Forecasting Problem A time series forecasting problem can be formulated as the following.
Given a dataset $\mathcal D$, with
$y^{(i)}_t$, the sequential variable to be forecasted, $x^{(i)}_t$, exogenous data for the time series data, $u^{(i)}_t$, some features that can be obtained or planned in advance, where ${}^{(i)}$ indicates the $i$th variable, ${}_ t$ denotes time. In a forecasting task, we use $y^{(i)} _ {t-K:t}$, $x^{(i) _ {t-K:t}}$, and $u^{(i)} _ {t-K:t+H}$, to forecast the future $y^{(i)} _ {t+1:t+H}$.
A model $f$ will use $x^{(i)} _ {t-K:t}$ and $u^{(i)} _ {t-K:t+H}$ to forecast $y^{(i)} _ {t+1:t+H}$.</description></item><item><title>Prediction Space in Forecasting</title><link>https://datumorphism.leima.is/cards/forecasting/prediction-space/</link><pubDate>Fri, 08 Apr 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/forecasting/prediction-space/</guid><description>In a forecasting problem, we have
$\mathcal P$, the priors, e.g., price and demand is negatively correlated, $\mathcal D$, available dataset, $Y$, the observations, and $F$, the forecasts. Information Set $\mathcal A$
The priors $\mathcal D$ and the available data $\mathcal P$ can be summarized together as the information set $\mathcal A$. Under a probabilistic view, a forecaster will find out or approximate a CDF $\mathcal F$ such that1
$$ \mathcal F(Y\vert \mathcal D, \mathcal P) \to F. $$
Naively speaking, once the density $\rho(F, Y)$ is determined or estimated, a probabilistic forecaster can be formed.</description></item><item><title>Short-Time-Fourier-Transform</title><link>https://datumorphism.leima.is/wiki/time-series/short-time-fourier-transform/</link><pubDate>Wed, 20 Jun 2018 15:58:49 -0400</pubDate><guid>https://datumorphism.leima.is/wiki/time-series/short-time-fourier-transform/</guid><description>Short-Time-Fourier-Transform We Fourier transform the time series data using a Fourier transform, with some window function
\begin{equation} \tilde Y[n,k] = \sum_m Y[n+m] W[m] e^{-i \lambda_k m}, \end{equation}
where $\lambda_k=2\pi k/N$ and $W[m]$ is the window function at $m$.
References and Notes Cousera</description></item><item><title>Time Series Forecasting with Deep Learning</title><link>https://datumorphism.leima.is/wiki/forecasting/forecasting-with-deep-learning/</link><pubDate>Sun, 24 Apr 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/forecasting/forecasting-with-deep-learning/</guid><description>The Encoder-Decoder Framework Many of the models for [[time series forecasting]] The Time Series Forecasting Problem Forecasting time series using deep learning are following some sort of encoder-decoder architecture.
Encoder: $g_{\text{enc}}(x^{(i)} _ {t-K:t}, u^{(i)} _ {t-K:t}) \to z_t$, Decoder: $g_{\text{dec}}(z_t, u^{(i)} _ {t+1: t+H}) \to y_{t+1:t+H}$.</description></item><item><title>Autoregressive Model</title><link>https://datumorphism.leima.is/wiki/time-series/autoregressive-model/</link><pubDate>Wed, 20 Jun 2018 15:58:49 -0400</pubDate><guid>https://datumorphism.leima.is/wiki/time-series/autoregressive-model/</guid><description>Autoregressive Given a time series ${T^i}$, a simple predictive model can be constructed using an autoregressive model.
$$ \begin{equation} T^t = \sum_{i=1}^p \beta_i T^{t - i} + \beta^t + \beta^0. \end{equation} $$
Such a model is usually called an AR(p) model due to the fact that we are using data back in $p$ steps.
Differential Equation For simplicity we will look at a AR(1) model. Assume the time series has a step size of $dt$, our model can be rewritten as
$$ T^t = \beta_1 T^{t - 1} + \beta^t + \beta^0 $$
which can be rewritten in the following way</description></item><item><title>Evaluating Time Series Models</title><link>https://datumorphism.leima.is/wiki/forecasting/evalutate-time-series-models/</link><pubDate>Fri, 08 Apr 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/forecasting/evalutate-time-series-models/</guid><description>Evaluating time series models is usually different from most other machine learning tasks as we usually don&amp;rsquo;t have i.i.d. data.
Out-of-sample Out-of-Sample with Sliding Window
If the sliding window size is 1, then we have the simplest out-of-sample holdout scenario.
Prequential Prequential with Gap
Prequential with Growing Train
Prequential with Sliding Blocks
Cross-validation Cross-validation
Cross-validation with Neighbor removed</description></item><item><title>Predictions Using Time Series Data</title><link>https://datumorphism.leima.is/wiki/time-series/predictions-time-series-data/</link><pubDate>Fri, 21 Jun 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/time-series/predictions-time-series-data/</guid><description>General Phenological Model for Seasonality In business, time series data $f(t)$ usually carries information about trend $g(t)$ ($g$ is used since trend is usually growth), seasonalities (periodical effects) $p(t)$, holiday effects (structural effects) $s(t)$, etc. We will decompose a time series $f(t)$ into four components
$$ \begin{equation} f(t) = g(t) + p(t) + s(t) + \epsilon(t). \end{equation} $$
To train a model for the predictions, we need to write down the exact models of these three predictable components.</description></item><item><title>DeepAR</title><link>https://datumorphism.leima.is/wiki/forecasting/deepar/</link><pubDate>Sat, 09 Jul 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/forecasting/deepar/</guid><description>Focus on</description></item><item><title>State Space Models</title><link>https://datumorphism.leima.is/wiki/time-series/state-space-models/</link><pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/time-series/state-space-models/</guid><description>State space model is an important category of model for sequential data. Through simple assumptions, state space models can achieve quite complicated distributions.
To model a sequence, we can use the joint probability of all the nodes,
$$ p(x_1, x_2, \cdots, x_N), $$
where $x_i$ are the nodes in the sequence.
Orders We can introduce different order of dependencies on the past.
The simplest model for the sequence is assuming i.i.d..
Zeroth OrderEach node is independent of each other
To model the dependencies in the sequence, we can assume a node depends on the previous nodes. The first-order model assume that node $x_{i+1}$ only depends on node $x_i$.</description></item><item><title>Hidden Markov Model</title><link>https://datumorphism.leima.is/wiki/time-series/hidden-markov-model/</link><pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/time-series/hidden-markov-model/</guid><description>The hidden Markov model, HMM, is a type of [[State Space Models]] State Space Models The state space model is an important category of models for sequential data such as time series 1.
HMM Bishop2006 Christpher M. Bishop. Pattern Recognition and Machine Learning. Springer-Verlag New York; 2006. &amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Time Series Data Augmentation</title><link>https://datumorphism.leima.is/wiki/time-series/data-augmentation/</link><pubDate>Mon, 27 Jun 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/time-series/data-augmentation/</guid><description/></item><item><title>Empirical Correlation Coefficient (CORR)</title><link>https://datumorphism.leima.is/cards/time-series/ts-corr/</link><pubDate>Sun, 21 Aug 2022 20:43:36 +0200</pubDate><guid>https://datumorphism.leima.is/cards/time-series/ts-corr/</guid><description>The Empirical Correlation Coefficient (CORR) is an evaluation metric in time series forecasting,1
$$ \mathrm{CORR} = \frac{1}{N} \sum_{i=1}^N \frac{ \sum_t (y^{(i)}_t - \bar y^{(i)} ) ( \hat y^{(i)}_t -\bar{ \hat y}^{(i)} ) }{ \sqrt{ \sum_t (y^{(i)}_t - \bar y^{(i)} )^2 ( \hat y^{(i)}_t -\bar{\hat y}^{(i)} )^2 } } $$
where $y^{(i)}$ is the $i$th time series, ${} _ t$ denotes the time step $t$, and $\bar y^{(i)}$ is the mean of the $i$th forecasted series, i.e., $\bar y^{(i)} = \operatorname{mean}( y^{(i)} _ { t \in \{T _ f, T _ {f+1}, \cdots T _ {f+H}\} } )$.
Lai2017 Lai G, Chang W-C, Yang Y, Liu H.</description></item><item><title>Root Relative Squared Error (RSE)</title><link>https://datumorphism.leima.is/cards/time-series/ts-rse/</link><pubDate>Sun, 21 Aug 2022 20:43:36 +0200</pubDate><guid>https://datumorphism.leima.is/cards/time-series/ts-rse/</guid><description>The Root Relative Squared Error (RSE) is an evaluation metric in time series forecasting,1
$$ \mathrm{RSE} = \frac{ \sqrt{ \sum_{i, t} ( y^{(i)}_t - \hat y^{(i)}_t )^2 } }{ \sqrt{ \sum_{i, t} ( y^{(i)}_t - \bar y )^2 } } $$
where $y^{(i)}$ is the $i$th time series, ${} _ t$ denotes the time step $t$, and $\bar y$ is the mean of the forecasted series, i.e., $\bar y = \operatorname{mean}(y^{(i\in\{0, 1, \cdots, N\})} _ { t\in \{T _ f, T _ {f+1}, \cdots T _ {f+H}\} })$.
Lai2017 Lai G, Chang W-C, Yang Y, Liu H.</description></item><item><title>Data Generating Processes for Time Series Data</title><link>https://datumorphism.leima.is/cards/time-series/generating-process/</link><pubDate>Mon, 04 Jul 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/time-series/generating-process/</guid><description/></item><item><title>Conformal Time Series Forecasting</title><link>https://datumorphism.leima.is/wiki/forecasting/probablistic/conformal-time-series-forecasting/</link><pubDate>Tue, 19 Apr 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/forecasting/probablistic/conformal-time-series-forecasting/</guid><description>Conformal time series forecasting is a probabilistic forecasting method using [[Conformal Prediction]] Conformal Prediction Conformal prediction is a method to sequentially predict consistent confidence intervals using nonconformity measures. .
For any given model $\mathcal M$, conformal time series forecasting trains on a training dataset $\mathcal D_{\text{Train}}$ then calculates a [[Confidence Interval]] Confidence Interval Estimates from a sample can be entitled a confidence interval using a calibration dataset $\mathcal D_{\text{Calibration}}$. The confidence interval is directly used for inference. This framework is called the inductive conformal prediction (ICP).
Induction, Deduction, and Transduction How to Forecast the Confidence Interval For a dataset $\mathcal D$, we split it, e.</description></item><item><title>Continuous Ranked Probability Score - CRPS</title><link>https://datumorphism.leima.is/cards/time-series/crps/</link><pubDate>Fri, 18 Mar 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/time-series/crps/</guid><description>The Continuous Ranked Probability Score, known as CRPS, is a score to measure how a proposed distribution approximates the data, without knowledge about the true distributions of the data.
Definition CRPS is defined as1
$$ CRPS(P, x_a) = \int_{-\infty}^\infty \lVert P(x) - H(x - x_a) \rVert_2 dx, $$
where
$x_a$ is the true value of $x$, P(x) is our proposed cumulative distribution for $x$, $H(x)$ is the Heaviside step function $$ H(x) = \begin{cases} 1, &amp;\qquad x=0\\ 0, &amp;\qquad x\leq 0\\ \end{cases} $$
$\lVert \cdot \rVert_2$ is the L2 norm. Explain it The formula looks abstract on first sight, but it becomes crystal clear once we understand it.</description></item></channel></rss>