<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Machine Learning::Embedding on Datumorphism</title><link>https://datumorphism.leima.is/categories/machine-learningembedding/</link><description>Recent content in Machine Learning::Embedding on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Sat, 11 Sep 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://datumorphism.leima.is/categories/machine-learningembedding/index.xml" rel="self" type="application/rss+xml"/><item><title>Alignment and Uniformity</title><link>https://datumorphism.leima.is/cards/machine-learning/embedding/alignment-and-uniformity/</link><pubDate>Sat, 11 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/embedding/alignment-and-uniformity/</guid><description>A good representation should be able to
separate different instances, and cluster similar instances. Wang et al proposed two concepts that matches the above two ideas, alignment and uniformity, on a hypersphere1.
From Wang et al
Wang T, Isola P. Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2005.10242&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>CBOW: Continuous Bag of Words</title><link>https://datumorphism.leima.is/cards/machine-learning/embedding/continuous-bag-of-words/</link><pubDate>Thu, 16 Jan 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/embedding/continuous-bag-of-words/</guid><description>Here we encode all words presented in the corpus to demostrate the idea of CBOW. In the real world, we might want to remove some certain words such as the. We use the following quote by Ford in Westworld as an example.
I read a theory once that the human intellect is like peacock feathers. Just an extravagant display intended to attract a mate, just an elaborate mating ritual. But, of course, the peacock can barely fly. It lives in the dirt, pecking insects out of the muck, consoling itself with its great beauty.
The word intended is surrunded by extravagant display in the front and to attract after it.</description></item><item><title>Negative Sampling</title><link>https://datumorphism.leima.is/cards/machine-learning/embedding/negative-sampling/</link><pubDate>Thu, 16 Jan 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/embedding/negative-sampling/</guid><description>Knowledge of [[CBOW]] CBOW: Continuous Bag of Words Use the context to predict the center word or [[skipgram]] skipgram: Continuous skip-gram Use the center word to predict the context is required.
A naive model to train a model of words is to
encode input words and output words using vectors, use the input word vector to predict the output word vector, calculate the errors between predicted output word vector and real output word vector, minimize the errors. However, it is very expensive to project out the output words and calculate the error every time.</description></item><item><title>skipgram: Continuous skip-gram</title><link>https://datumorphism.leima.is/cards/machine-learning/embedding/continuous-skip-gram/</link><pubDate>Thu, 16 Jan 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/embedding/continuous-skip-gram/</guid><description>We use the following quote by Ford in Westworld as an example.
I read a theory once that the human intellect is like peacock feathers. Just an extravagant display intended to attract a mate, just an elaborate mating ritual. But, of course, the peacock can barely fly. It lives in the dirt, pecking insects out of the muck, consoling itself with its great beauty.
The word intended is surrunded by extravagant display in the front and to attract after it. The task is to predict the probability of words around the middle word intended, which are the &amp;lsquo;history words&amp;rsquo; extravagant, display and &amp;lsquo;future words&amp;rsquo; to, attract in our case.</description></item></channel></rss>