<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Neural Networks on Datumorphism</title><link>https://datumorphism.leima.is/categories/neural-networks/</link><description>Recent content in Neural Networks on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Mon, 01 Nov 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://datumorphism.leima.is/categories/neural-networks/index.xml" rel="self" type="application/rss+xml"/><item><title>Learning Rate</title><link>https://datumorphism.leima.is/cards/machine-learning/practice/learning-rate/</link><pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/practice/learning-rate/</guid><description>Finding a suitable learning rate for our model training is crucial.
A safe but time wasting option is to use search on a grid of parameters. However, there are smarter moves.
Karpathy’s Constant
An empirical learning rate $3^{-4}$ for Adms, aka, Karpathy's constant, was started as a tweet by Andrei Karpathy. 3e-4 is the best learning rate for Adam, hands down.
&amp;mdash; Andrej Karpathy (@karpathy) November 24, 2016 (i just wanted to make sure that people understand that this is a joke...)
&amp;mdash; Andrej Karpathy (@karpathy) November 24, 2016 Smarter Method A smarter method is to start with small learning rate and increase it on each mini-batch, then observe the loss vs learning rate (mini-batch in this case).</description></item><item><title>Initialize Artificial Neural Networks</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/neural-networks-initialization/</link><pubDate>Thu, 23 Sep 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/neural-networks-initialization/</guid><description>The weights are better if they1
are zero centered, and have similar variance across layers. Why
If we have very different variances across layers, we will need a different learning rate for each layer for our optimization. Setting the variances to be on the same scale, we can use a global learning rate for the whole network. Variance is related to the input size of the layer Suppose we are using a simple linear activation, $\sigma(x) = \alpha x$. For a series of inputs $x_j$, the outputs $y_i$ are
$$ y_i = \sum_{j} w_{ij} x_j. $$</description></item><item><title>BiPolar Sigmoid</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-bi-polar-sigmoid/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-bi-polar-sigmoid/</guid><description>A BiPolar sigmoid function is
$$ \sigma(x) = \frac{1-e^{-x}}{1+e^{-x}}. $$
Visualization Bipolar Sigmoid</description></item><item><title>Conic Section Function</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-conic-section-function/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-conic-section-function/</guid><description> TODO
Expand this article. See references 1.
Dorffner1994 Dorffner G. UNIFIED FRAMEWORK FOR MLPs AND RBFNs: INTRODUCING CONIC SECTION FUNCTION NETWORKS. Cybern Syst. 1994;25: 511–554. doi:10.1080/01969729408902340 &amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>ELU</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-elu/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-elu/</guid><description>Both ReLu and Leaky ReLu have discontinuous derivatives. ELU is smooth for first order derivative, i.e., ELU is class $C^1$.
$$ \begin{cases} x, &amp; \text{if }x=0 \\ \exp(x) - 1, &amp; \text{else.} \end{cases} $$
Visualizations ELU
Derivative of ELU
Code def elu(x, alpha): return torch.where(x &amp;gt; 0, x, torch.exp(x) -1) Full code to generate the data used in this article Full code to generate the data used in this article
from torch import nn import matplotlib.pyplot as plt import torch from typing import Union, Optional from pathlib import Path import json def visualize_activation( x: torch.Tensor, acti: torch.</description></item><item><title>Hyperbolic Tanh</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-hyperbolic-tangent/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-hyperbolic-tangent/</guid><description>$$ \tanh(x) = \frac{\sinh(x)}{\cosh(x)} = \frac{e^{x} - e^{-x}}{e^x + e^{-x}} $$
Hyperbolic tangent</description></item><item><title>Leaky ReLu</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-leaky-relu/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-leaky-relu/</guid><description>ReLu sets all negative regions to 0. Leaky ReLu sets the negative regions to a linear relation with slope $\alpha$,
$$ \begin{cases} x, &amp; \text{if }x=0 \\ \alpha x, &amp; \text{else.} \end{cases} $$
Visualizations Leaky ReLu with $\alpha=0.2$
Derivative of Leaky ReLu with $\alpha=0.2$. Notice that the derivative is $0.2$ for $x&amp;lt;0$.
Code def leaky_relu(x, alpha): return torch.where(x &amp;gt; 0, x, alpha * x) Full code to generate the data used in this article Full code to generate the data used in this article
from torch import nn import matplotlib.pyplot as plt import torch from typing import Union, Optional from pathlib import Path import json def visualize_activation( x: torch.</description></item><item><title>Radial Basis Function</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-radial-basis-function/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-radial-basis-function/</guid><description> Hyperbolic tangentTwo unnormalized Gaussian radial basis functions in one input dimension. The basis function centers are located at x1=0.75 and x2=3.25. Source Unnormalized Radial Basis Functions</description></item><item><title>ReLu</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-relu/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-relu/</guid><description>Rectified Linear Unit (ReLu) is a very popular activation function in deep learning. ReLu is defined as
$$ \begin{cases} x, &amp; \text{if }x=0 \\ 0, &amp; \text{else.} \end{cases} $$
Visualizations ReLu
Derivative of ReLu
Characteristics In trained models, ReLu doesn&amp;rsquo;t preserve the qualitative distributions of values after the activation.
Lippe P. Tutorial 3: Activation Functions — UvA DL Notebooks v1.1 documentation. In: UvA Deep Learning Tutorials [Internet].
Because of the zero values in ReLu, many neurons actually don&amp;rsquo;t participate in any of the tasks as they are just nullified to zeros and provide no gradient.</description></item><item><title>Swish</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-swish/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-swish/</guid><description>Swish is infinitely differentiable, i.e., class $C^\infty$.
$$ x \sigma(x), $$
where $\sigma$ is the [[uni-polar sigmoid]] Uni-Polar Sigmoid Uni-polar sigmoid function and its properties .
Visualizations ELU
Derivative of ELU
Code def swish(x, alpha): return x * torch.sigmoid(x) Full code to generate the data used in this article Full code to generate the data used in this article
from torch import nn import matplotlib.pyplot as plt import torch from typing import Union, Optional from pathlib import Path import json def visualize_activation( x: torch.Tensor, acti: torch.nn.Module, save_path: Optional[Union[str, Path]] = None ) -&amp;gt; dict: &amp;#34;&amp;#34;&amp;#34;Visualize activation function on the domain of x&amp;#34;&amp;#34;&amp;#34; y = acti(x) # Calculate the grad of the activation function x = x.</description></item><item><title>Uni-Polar Sigmoid</title><link>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-uni-polar-sigmoid/</link><pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-uni-polar-sigmoid/</guid><description>A uni-Polar sigmoid function is
$$ \sigma(x) = \frac{1}{1+e^{-x}}. $$
Visualization Uni-polar Sigmoid function Tricks A very useful trick: $$ 1 - \sigma(x) = \sigma(-x). $$</description></item></channel></rss>