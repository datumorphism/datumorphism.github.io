<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Math on Datumorphism</title><link>https://datumorphism.leima.is/categories/math/</link><description>Recent content in Math on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Sun, 21 Aug 2022 16:03:14 +0200</lastBuildDate><atom:link href="https://datumorphism.leima.is/categories/math/index.xml" rel="self" type="application/rss+xml"/><item><title>Wavelet Transform</title><link>https://datumorphism.leima.is/wiki/time-series/wavelets/</link><pubDate>Mon, 07 Dec 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/wiki/time-series/wavelets/</guid><description>In general, given a complete set of function $\psi(x; \tilde x)$, we can decompose a function $F(\tilde x)$
$$ F(\tilde x) = \int f(x) \psi(x;\tilde x) dx. $$
The choice of $\psi(x;\tilde x)$ gives us different properties.
Fourier Transform Fourier transform is good for stationary analysis since time is not involved in $F(\omega)$.
$$ F(\omega) = \int_{-\infty}^{\infty} f(t) e^{-i \omega t} dt $$
Short-time Fourier Transform STFT is a Fourier transform with a moving time window $\tau$,
$$ F(\tau,\omega) = \int_{-\infty}^{\infty} f(t) w(t - \tau) e^{-i\omega t} dt. $$
Moving $\tau$ gives us the ability to investigate Fourier components at different time segments (assuming the window function $w(t-\tau)$ is a step function).</description></item><item><title>Dilated Convolution</title><link>https://datumorphism.leima.is/cards/math/convolution-dilated/</link><pubDate>Sun, 21 Aug 2022 16:03:14 +0200</pubDate><guid>https://datumorphism.leima.is/cards/math/convolution-dilated/</guid><description>For a convolution
$$ f*h(x) = \sum_{s+t=x} f(s) h(t), $$
the dilated version of it is1
$$ f*_l h(x) = \sum_{s+t*l=x} f(s) h(t), $$
where $l$ is the dilation factor.
Yu2015 Yu F, Koltun V. Multi-Scale Context Aggregation by Dilated Convolutions. arXiv [cs.CV]. 2015. Available: http://arxiv.org/abs/1511.07122 &amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Convolutions Using Fourier Transform</title><link>https://datumorphism.leima.is/cards/math/convolution-and-fourier-transform/</link><pubDate>Sat, 04 Dec 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/convolution-and-fourier-transform/</guid><description>Convolution
$$ (f*h)(x) = \int \mathrm d f(y) h(x-y), $$
is equivalent to
$$ \mathcal F^{-1}\left[ \mathcal F[ f(x) ] \circ \mathcal F[h(x)] \right], $$
with $\mathcal F$ being the Fourier transform, i.e.,
$$ \mathcal F[f(x)] = \int \mathrm d x f(x) e^{-2\pi i x s}. $$
Proof
One could prove it using the Fourier integral theorem,
$$ f(x) = \iint dy d\xi f(y)e^{2\pi i (x-y)\xi}. $$
Derivation Given
$$ \begin{align} \mathcal F_s \left(f(y)\right) &amp;= \int dy f(y) e^{-2\pi i y s}, \\ \mathcal F_s \left(h(y)\right) &amp;= \int dz h(z) e^{-2\pi i z s}, \end{align} $$
we have</description></item><item><title>Centering Matrix</title><link>https://datumorphism.leima.is/cards/math/statistics-centering-matrix/</link><pubDate>Mon, 08 Nov 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/statistics-centering-matrix/</guid><description>Given a vector $v$, with mean value of its elements $m$, we can center the vector by subtracting the mean $m$ from each element,
import numpy as np n = 10 v = np.random.randn(n) v_c = v - v.mean() This operation is easy and obvious. However, the formalism is not elegant. In some cases, we would like to formulate the process of centering the elements as operators,
$$ v_c = \operatorname{\hat H}v. $$
In this case, the operator $\operatorname{\hat H}$ is simply a matrix
$$ \operatorname{\hat H} \to I_n - \frac{1}{n} J_n, $$
where $n$ is the dimension of the vector $v$, $I_n$ is a identity matrix, $J_n$ is a matrix of all $1$s.</description></item><item><title>The Hubbard-Stratonovich Identity</title><link>https://datumorphism.leima.is/cards/math/hubbard-stratonovich-identity/</link><pubDate>Thu, 17 Jun 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/hubbard-stratonovich-identity/</guid><description>The Hubbard version of the Hubbard-Stratonovich identity is1
$$ \begin{align} \exp{\left( a^2 \right)} =&amp; \frac{1}{\sqrt{\pi}} \int_{-\infty}^\infty \mathrm dx\, \exp{ \left( - x^2 - 2 a x \right)}\\ =&amp; \frac{1}{\sqrt{\pi}} \int_{\infty}^{-\infty} \mathrm dx'\, \exp{ \left( - x'^2 + 2 a x' \right)}, \end{align} $$
where we changed the sign of $x$, i.e., $x' = -x$.
In many partition functions, we have expressions like $\exp{\left( a^2/2\right)}$, using the identity, we have
$$ \begin{align} \exp{\left( \frac{a^2}{2} \right)} =&amp; \frac{1}{\sqrt{\pi}} \int_{\infty}^{-\infty} \mathrm dx\, \exp{ \left( - x^2 + \sqrt{2} a x \right)} \\ =&amp; \frac{1}{\sqrt{2\pi}} \int_{\infty}^{-\infty} \mathrm dx'\, \exp{ \left( - \frac{x'^2}{2} + a x' \right)}, \end{align} $$</description></item><item><title>Gaussian Integrals</title><link>https://datumorphism.leima.is/cards/math/gaussian-integrals/</link><pubDate>Tue, 11 May 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/gaussian-integrals/</guid><description>The diagonalized case
$$ \begin{eqnarray} Z_0 &amp;=&amp; \int d^n z \exp\left(-\frac{1}{2} z^\mathrm{T} D z\right) \\ &amp;=&amp; \prod_i \int d z_i \exp\left(-\frac{1}{2} \lambda_i z_i^2\right) \\ &amp;=&amp; \prod_i \sqrt{\frac{2\pi}{\lambda_i}} \\ &amp;=&amp; \sqrt{\frac{(2\pi)^n}{\det A}}. \end{eqnarray} $$
For an arbitrary matrix $A$,
$$ Z_J = \int d^n x \exp\left(-\frac{1}{2} x^\mathrm{T} A x + J^\mathrm{T} x\right). $$
$$ \begin{eqnarray} Z_J &amp;=&amp; \int d^n y \exp\left(-\frac{1}{2} {y}^\mathrm{T} A y + \frac{1}{2} J^\mathrm{T}A^{-1}J\right) \\ &amp;=&amp; \sqrt{\frac{(2\pi)^n}{\det A}} \exp\left(\frac{1}{2} J^\mathrm{T}A^{-1}J\right). \end{eqnarray} $$</description></item><item><title>Jensen's Inequality</title><link>https://datumorphism.leima.is/cards/math/jensens-inequality/</link><pubDate>Mon, 12 Apr 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/jensens-inequality/</guid><description>Jensen&amp;rsquo;s inequality shows that
$$ f(\mathbb E(X)) \leq \mathbb E(f(X)) $$
for a concave function $f(\cdot)$.</description></item><item><title>Multiset, mset or bag</title><link>https://datumorphism.leima.is/cards/math/multiset-mset-bag/</link><pubDate>Sun, 27 Dec 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/multiset-mset-bag/</guid><description>A bag is a set in which duplicate elements are allowed.
An ordered bag is a list that we use in programming.</description></item><item><title>Conditional Probability Table</title><link>https://datumorphism.leima.is/cards/statistics/conditional-probability-table/</link><pubDate>Tue, 27 Oct 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/conditional-probability-table/</guid><description>The conditional probability table, aka CPT, is used to calculate conditional probabilities from a dataset.
Given a dataset with features $\mathbf X$ and their corresponding classes $\mathbf Y$, the conditional probabilities of each class given a certain feature value can be calculated using a CPT which in turn can be calculated using a [[contigency table]] Correlation Coefficient and Covariance for Numeric Data Detecting correlations using correlations for numeric data .</description></item><item><title>Diagnolize Matrices</title><link>https://datumorphism.leima.is/cards/math/diagonalize-matrix/</link><pubDate>Wed, 11 Mar 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/diagonalize-matrix/</guid><description>Given a matrix $\mathbf A$, it is diagonalized using its eigenvectors.
Why are the eigenvectors needed?
Eigenvectors of a matrix $\mathbf A$ are the preferred directions. From the definition of eigenvectors,
$$ \mathbf A \mathbf x = \lambda \mathbf x, $$
we know that the matrix $\mathbf A$ only scales the eigenvectors and no rotations. These directions are special to the matrix $\mathbf A$.
Find the eigenvectors $\mathbf x_i$ of the matrix $\mathbf A$; If we find degerations, the matrix is not diagonalizable. Construct a matrix $\mathbf S = \begin{pmatrix} \mathbf x_1 &amp;amp; \mathbf x_2 &amp;amp; \cdots &amp;amp; \mathbf x_n \end{pmatrix}$; The matrix $\mathbf A$ is diagonalize using $\mathbf S^{-1} \mathbf A \mathbf S = \mathbf {A_D}$</description></item><item><title>Mahalanobis Distance</title><link>https://datumorphism.leima.is/cards/math/mahalanobis-distance/</link><pubDate>Wed, 11 Mar 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/mahalanobis-distance/</guid><description>Mahalanobis distance is a distance calculated using the inverse of the covariance matrix as the metric. For two vectors $\mathbf x$ and $\mathbf y$, the Mahalanobis distance is
$$ d^2 = (x_i - \bar x) g_{ij} (y_j - \bar y), $$
where $g_{ij} = (S^{-1})_{ij}$ and $\mathbf S$ is the covariance matrix.
The covariance is a normalization that mitigates the covariances.</description></item><item><title>Covariance Matrix</title><link>https://datumorphism.leima.is/cards/statistics/covariance-matrix/</link><pubDate>Tue, 10 Mar 2020 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/covariance-matrix/</guid><description>We use Einstein&amp;rsquo;s summation convention. Covariance of two discrete series $A$ and $B$ is defined as
$$ \text{Cov} ({A,B}) = \sigma_{A,B}^2 = \frac{ (a_i - \bar A) (b_i - \bar B) }{ n- 1 }, $$
where $n$ is the length of the series. The normalization factor is set to $1/(n-1)$ to mitigate the bias for small $n$.
One could show that
$$ \mathrm{Cov}({A,B}) = E( A,B ) - \bar A \bar B. $$
At first glance, the square in the definition seems to be only for notation purpose at this point.
Meanwhile, using this idea of the mean of geometric mean, we could easily generalize it to the covariance of three series,</description></item><item><title>Bayes' Theorem</title><link>https://datumorphism.leima.is/cards/statistics/bayes-theorem/</link><pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/statistics/bayes-theorem/</guid><description>Bayes' Theorem is stated as
$$ P(A\mid B) = \frac{P(B \mid A) P(A)}{P(B)} $$
$P(A\mid B)$: likelihood of A given B $P(A)$: marginal probability of A There is a nice tree diagram for the Bayes' theorem on Wikipedia.
Tree diagram of Bayes' theorem</description></item><item><title>Canonical Decomposition</title><link>https://datumorphism.leima.is/cards/math/canonical-decomposition/</link><pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/canonical-decomposition/</guid><description>I find this slide from Christoph Freudenthaler very useful.
Canonical decomposition visualized by Christoph Freudenthaler</description></item><item><title>Cholesky Decomposition</title><link>https://datumorphism.leima.is/cards/math/cholesky-decomposition/</link><pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/cholesky-decomposition/</guid><description>$$ A = L L^T $$</description></item><item><title>Khatri-Rao Product</title><link>https://datumorphism.leima.is/cards/math/khatri-rao/</link><pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/khatri-rao/</guid><description>$$ \mathbf{A} \ast \mathbf{B} = \left(\mathbf{A}_{ij} \otimes \mathbf{B}_{ij}\right)_{ij} $$</description></item><item><title>Modes and Slices of Tensors</title><link>https://datumorphism.leima.is/cards/math/modes-and-slices-of-tensor/</link><pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/modes-and-slices-of-tensor/</guid><description> Modes of a tensor Slices of a tensor</description></item><item><title>SVD: Singular Value Decomposition</title><link>https://datumorphism.leima.is/cards/math/svd/</link><pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/svd/</guid><description>Given a matrix $\mathbf X \to X_{m}^{\phantom{m}n}$, we can decompose it into three matrices
$$ X_{m}^{\phantom{m}n} = U_{m}^{\phantom{m}k} D_{k}^{\phantom{k}l} (V_{n}^{\phantom{n}l} )^{\mathrm T}, $$
where $D_{k}^{\phantom{k}l}$ is diagonal.
Here we have $\mathbf U$ being constructed by the eigenvectors of $\mathbf X \mathbf X^{\mathrm T}$, while $\mathbf V$ is being constructed by the eigenvectors of $\mathbf X^{\mathrm T} \mathbf X$ (which is also the reason we keep the transpose).
I find this slide from Christoph Freudenthaler very useful. The original slide has been added as a reference to this article.
SVD visualized by Christoph Freudenthaler</description></item><item><title>Tucker Decomposition</title><link>https://datumorphism.leima.is/cards/math/tucker-decomposition/</link><pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/tucker-decomposition/</guid><description>I find this slide from Christoph Freudenthaler very useful. For the definition of mode 1/2/3 unfold, please refer to Modes and Slices of Tensors.
Tucker decomposition visualized by Christoph Freudenthaler</description></item><item><title>Frobenius distance</title><link>https://datumorphism.leima.is/cards/math/frobenius-distance/</link><pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/frobenius-distance/</guid><description>Frobenius distance between the matrix $X_{n}^{\phantom{n}k}$ and $H_n^{\phantom{n}r} W_r^{\phantom{r}k}$,
$$ \lVert X_{n}^{\phantom{n}k} - H_n^{\phantom{n}r} W_r^{\phantom{r}k} \rVert^2 \equiv \sum_{n,k} (X_{n}^{\phantom{n}k} - H_n^{\phantom{n}r} W_r^{\phantom{r}k})^2. $$</description></item><item><title>Levenshtein Distance</title><link>https://datumorphism.leima.is/cards/math/levenshtein-distance/</link><pubDate>Sun, 19 May 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/levenshtein-distance/</guid><description>Levenshtein distance calculates the number of operations needed to change one word to another by applying single-character edits (insertions, deletions or substitutions).
The reference explains this concept very well. For consistency, I extracted a paragraph from it which explains the operations in Levenshtein algorithm. The source of the following paragraph is the first reference of this article.
Levenshtein Matrix
Cell (0:1) contains red number 1. It means that we need 1 operation to transform M to an empty string. And it is by deleting M. This is why this number is red. Cell (0:2) contains red number 2. It means that we need 2 operations to transform ME to an empty string.</description></item><item><title>n-gram</title><link>https://datumorphism.leima.is/cards/math/n-gram/</link><pubDate>Sun, 19 May 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/n-gram/</guid><description>n-gram is a method to split words into set of substring elements so that those can be used to match words.
Examples Use the following examples to get your first idea about it. I created two columns so that we could compare the n-grams of two different words side-by-side.
n in n-gram is Word One Clean Word: (( sentenceOneWords )) n-grams: (( sentenceOneWordsnGram )) Word Two Clean Word: (( sentenceTwoWords )) n-grams: (( sentenceTwoWordsnGram )) /*************************/ /** The function nGram is a copy of https://github.com/words/n-gram , under MIT License **/ nGram.</description></item><item><title>Cosine Similarity</title><link>https://datumorphism.leima.is/cards/math/cosine-similarity/</link><pubDate>Mon, 06 May 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/cosine-similarity/</guid><description>As simple as the inner product of two vectors
$$ d_{cos} = \frac{\vec A}{\vert \vec A \vert} \cdot \frac{\vec B }{ \vert \vec B \vert} $$
Examples To use cosine similarity, we have to vectorize the words first. There are many different methods to achieve this. For the purpose of illustrating cosine similarity, we use term frequency.
Term frequency is the occurrence of the words. We do not deal with duplications so duplicate words will have some effect on the similarity.
In principle, we could also use word set for a sentence to remove the effect of duplicate words. In most cases, if a word is repeating, it would indeed make the sentences different.</description></item><item><title>Eigenvalues and Eigenvectors</title><link>https://datumorphism.leima.is/cards/math/eigendecomposition/</link><pubDate>Mon, 06 May 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/eigendecomposition/</guid><description>To find the eigenvectors $\mathbf x$ of a matrix $\mathbf A$, we construct the eigen equation
$$ \mathbf A \mathbf x = \lambda \mathbf x, $$
where $\lambda$ is the eigenvalue.
We rewrite it in the components form,
$$ \begin{equation} A_{ij} x_j = \lambda x_i. \label{eqn-eigen-decomp-def} \end{equation} $$
Mathematically speaking, it is straightforward to find the eigenvectors and eigenvalues.
Eigenvectors are Special Directions Judging from the definition in Eq.($\ref{eqn-eigen-decomp-def}$), the eigenvectors do not change direction under the operation of the matrix $\mathbf A$.
Reconstruct $\mathbf A$ We can reconstruct $\mathbf A$ using the eigenvalues and eigenvectors.
First of all, we will construct a matrix of eigenvectors,</description></item><item><title>Jaccard Similarity</title><link>https://datumorphism.leima.is/cards/math/jaccard-similarity/</link><pubDate>Mon, 06 May 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/jaccard-similarity/</guid><description>Jaccard index is the ratio of the size of the intersect of the set and the size of the union of the set.
$$ J(A, B) = \frac{ \vert A \cap B \vert }{ \vert A \cup B \vert } $$
Jaccard distance $d_J(A,B)$ is defined as
$$ d_J(A,B) = 1 - J(A,B). $$
Properties If the two sets are the same, $A=B$, we have $J(A,B)=1$ or $d_J(A,B)=0$. We have maximum similarity.
If the two sets have nothing in common, we have $J(A,B)=0$ or $d_J(A,B)=1$. We have minimum similarity.
Examples Sentence One Word Set: (( sentenceOneWords )) Sentence Two Word Set: (( sentenceTwoWords )) Intersect: (( intersectWords )) Union: (( unionWords )) Jaccard Index: (( jaccardIndex )) Jaccard Distance: (( jaccardDistance )) Vue.</description></item><item><title>Term Frequency - Inverse Document Frequency</title><link>https://datumorphism.leima.is/cards/math/tf-idf/</link><pubDate>Mon, 06 May 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/tf-idf/</guid><description/></item><item><title>Combinations</title><link>https://datumorphism.leima.is/cards/math/combinations/</link><pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/math/combinations/</guid><description>Choose X from N is
$$ C_N^X = \frac{N!}{ X! (N-X)! } $$</description></item><item><title>Solving Equations Using Differential Transformation Method</title><link>https://datumorphism.leima.is/til/math/differential-transformation-method-solving-equations/</link><pubDate>Tue, 11 Oct 2016 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/math/differential-transformation-method-solving-equations/</guid><description>Differential transformation method can be used to solve differential equation even integro-differential equations.</description></item><item><title>Area Enclosed by a Line</title><link>https://datumorphism.leima.is/til/math/area-enclosed-in-a-line/</link><pubDate>Sun, 15 Feb 2015 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/math/area-enclosed-in-a-line/</guid><description>Calculate the area enclosed by a line</description></item><item><title>Eigensystem of A Special Matrix</title><link>https://datumorphism.leima.is/til/math/eigensystem-of-a-special-matrix/</link><pubDate>Sun, 15 Feb 2015 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/math/eigensystem-of-a-special-matrix/</guid><description>Eigenstates of a very special matrix</description></item><item><title>Feynman Trick</title><link>https://datumorphism.leima.is/til/math/feynman-tricks/</link><pubDate>Sun, 15 Feb 2015 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/math/feynman-tricks/</guid><description>An identity about integral</description></item><item><title>Symmetry of second derivatives</title><link>https://datumorphism.leima.is/til/math/symmetry-of-second-derivatives/</link><pubDate>Sun, 15 Feb 2015 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/til/math/symmetry-of-second-derivatives/</guid><description>Symmetry of second derivatives</description></item></channel></rss>