<!doctype html><html lang=en-us><head><meta name=generator content="Hugo 0.69.2"><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><title>Information | Datumorphism | Lei Ma</title><meta name=robots content="noindex"><meta name=author content="Lei Ma"><meta property="og:title" content="Information"><meta property="og:description" content="The study of the data as a means to model the world"><meta property="og:type" content="website"><meta property="og:url" content="https://datumorphism.leima.is/cards/information/"><meta property="og:updated_time" content="2021-09-04T00:00:00+00:00"><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-140452515-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><link href=https://datumorphism.leima.is/cards/information/index.xml rel=alternate type=application/rss+xml title=Datumorphism><link rel=canonical href=https://datumorphism.leima.is/cards/information/><link rel="shortcut icon" type=image/png href=/logos/logo-square.png><link rel=stylesheet href=/css/bulma.css><link rel=stylesheet href=/css/bulma-divider.min.css><link rel=stylesheet href=/assets/css/bulma-ribbon.min.css><link rel=stylesheet href=/assets/css/tooltip.css><link rel=stylesheet href=https://jenil.github.io/bulmaswatch/united/bulmaswatch.min.css><link rel=stylesheet href=/css/custom.css><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],tags:'ams',processEscapes:true,processEnvironments:true},options:{skipHtmlTags:['script','noscript','style','textarea','pre']},svg:{fontCache:'global'}};window.addEventListener('load',(event)=>{document.querySelectorAll("mjx-container").forEach(function(x){x.parentElement.classList+='has-jax'})});</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script>mermaid.initialize({startOnLoad:true});</script><script src=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/js/all.min.js integrity="sha256-KzZiKy0DWYsnwMF+X1DvQngQ2/FxF7MF3Ff72XcpuPs=" crossorigin=anonymous></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css integrity="sha256-h20CPZ0QyXlBuAw7A+KluUYx/3pK+c7lYEpqLTlxjYQ=" crossorigin=anonymous></head><body><header><nav class="navbar is-transparent"><div class=navbar-brand><a class=navbar-item href=/><img src=/site/intelligence.png alt=Datumorphism height=28 style=margin-right:.5em> Datumorphism</a><div class="navbar-burger burger" style=color:#000 data-target=navMenu><span></span><span></span><span></span></div></div><div class=navbar-menu id=navMenu><div class=navbar-start><div class="navbar-item has-dropdown is-hoverable"><a href=/projects class=navbar-link>Notebooks</a><div class=navbar-dropdown><a href=https://datumorphism.leima.is/awesome/ class=navbar-item>Awesome</a>
<a href=https://datumorphism.leima.is/blog/ class=navbar-item>Blog</a>
<a href=https://datumorphism.leima.is/cards/ class=navbar-item>Cards</a>
<a href=https://datumorphism.leima.is/hologram/ class=navbar-item>Hologram</a>
<a href=https://datumorphism.leima.is/reading/ class=navbar-item>Reading Notes</a>
<a href=https://datumorphism.leima.is/til/ class=navbar-item>TIL</a>
<a href=https://datumorphism.leima.is/wiki/ class=navbar-item>Wiki</a></div></div></div><span class=navbar-burger><span></span><span></span><span></span></span><div class=navbar-end><div class=navbar-item><a class=navbar-item href=/blog/>Blog</a>
<a class=navbar-item href=/amneumarkt/>AmNeumarkt</a>
<a class=navbar-item href=/><i class="fas fa-search"></i></a><a class=navbar-item href=/tags/><i class="fas fa-tags"></i></a><a class=navbar-item href=/graph><i class="fas fa-project-diagram"></i></a><a class=navbar-item href=https://t.me/amneumarkt><i class="fab fa-telegram"></i></a><a class=navbar-item target=blank href=https://github.com/datumorphism><span class=icon><i class="fab fa-github"></i></span></a></div></div></div></nav><script>document.addEventListener('DOMContentLoaded',()=>{const $navbarBurgers=Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'),0);if($navbarBurgers.length>0){$navbarBurgers.forEach(el=>{el.addEventListener('click',()=>{const target=el.dataset.target;const $target=document.getElementById(target);el.classList.toggle('is-active');$target.classList.toggle('is-active');});});}});</script></header><main><section><div class="hero is-dark is-medium"><div class=hero-body><div class="container has-text-left"><h1 class="title is-1">Information</h1><h2 class="subtitle is-3"></h2><h3 class="title is-5" style=margin-top:2em><a href=https://datumorphism.leima.is/projects/cards/ class=has-text-light><i class="fas fa-link"></i>Introduction: My Knowledge Cards</a></h3></div></div></div></section><div class="columns is-fullheight"><div class=column><section class="hero is-default is-bold"><div class=hero-body><div class=container><nav class="breadcrumb has-succeeds-separator is-small" aria-label=breadcrumbs><ul><li><a href=https://datumorphism.leima.is/>Datumorphism</a></li><li><a href=https://datumorphism.leima.is/cards/>Cards</a></li><li class=active><a href=https://datumorphism.leima.is/cards/information/>Information</a></li></ul></nav><div class="columns is-desktop is-multiline"><div class="columns is-multiline is-variable is-1-mobile is-0-tablet is-3-desktop is-8-widescreen is-2-fullhd is-desktop"><div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork><h1 class=blog-timestamp></h1><h1 class=title><a href=https://datumorphism.leima.is/cards/information/coding-theory-concepts/ itemprop=headline>Coding Theory Concepts</a></h1><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>Published: <time datetime=2021-02-17T00:00:00+00:00>2021-02-17</time></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-folder" aria-hidden=true></i>Category: { Information }</span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-hashtag" aria-hidden=true></i>Tags:<div class="tags is-small" style=display:inline><a href=/tags/information-theory style=margin-right:.5em><span class="tag is-warning is-small is-light">#Information Theory</span></a>
<a href=/tags/entropy style=margin-right:.5em><span class="tag is-warning is-small is-light">#Entropy</span></a></div></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-paperclip" aria-hidden=true></i>References:</span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href="https://mbernste.github.io/posts/sourcecoding/#:~:text=Shannon%27s%20Source%20Coding%20Theorem%20tells,to%20unambiguously%20communicate%20those%20samples.&text=In%20this%20post%2C%20we%20will%20walk%20through%20Shannon%27s%20theorem.">Shannon’s Source Coding Theorem (Foundations of information theory: Part 3)</a></span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=http://www.cs.cmu.edu/~aarti/Class/10704/lec8-srccodingHuffman.pdf>Lecture 8: Source Coding Theorem, Huffman coding by Aarti Singh</a></span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=https://homepages.dcc.ufmg.br/~msalvim/courses/infotheory/L03_TheSourceCodingTheorem%5Bstill%5D.pdf>The Source Coding Theorem by Mario S. Alvim</a></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-star"></i>Summary: The code function produces code words. The expected length of the code word is limited by the entropy from the source probability $p$.
The Shannon information content, aka self-information, is described by
$$ - \log_2 p(x=a), $$
for the case that $x=a$.
The Shannon entropy is the expected information content for the whole sequence with probability distribution $p(x)$,
$$ \mathcal H = - \sum_x p(x\in X) \log_2 p(x). $$
The Shannon source coding theorem says that for $N$ samples from the source, we can roughly compress it into $N\mathcal H$.</span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-list-alt"></i>Pages: 8</span></div></div><div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div><div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork><h1 class=blog-timestamp></h1><h1 class=title><a href=https://datumorphism.leima.is/cards/information/fisher-information/ itemprop=headline>Fisher Information</a></h1><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>Published: <time datetime=2021-05-05T17:49:03+02:00>2021-05-05</time></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-folder" aria-hidden=true></i>Category: { Information }</span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-hashtag" aria-hidden=true></i>Tags:<div class="tags is-small" style=display:inline><a href=/tags/information-theory style=margin-right:.5em><span class="tag is-warning is-small is-light">#Information Theory</span></a></div></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-paperclip" aria-hidden=true></i>References:</span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=http://arxiv.org/abs/1705.01064>Ly A, Marsman M, Verhagen J, Grasman R, Wagenmakers E-J. A Tutorial on Fisher Information. arXiv [math.ST]. 2017. Available: http://arxiv.org/abs/1705.01064</a></span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-36/issue-3/On-Information-in-Statistics/10.1214/aoms/1177700061.full>Fraser DAS. On Information in Statistics. aoms. 1965;36: 890–896. doi:10.1214/aoms/1177700061</a></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-star"></i>Summary: Fisher information measures the second moment of the model sensitivity with respect to the parameters.</span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-list-alt"></i>Pages: 8</span></div></div><div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div><div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork><h1 class=blog-timestamp></h1><h1 class=title><a href=https://datumorphism.leima.is/cards/information/fraser-information/ itemprop=headline>Fraser Information</a></h1><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>Published: <time datetime=2021-05-05T17:49:12+02:00>2021-05-05</time></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-folder" aria-hidden=true></i>Category: { Information }</span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-hashtag" aria-hidden=true></i>Tags:<div class="tags is-small" style=display:inline><a href=/tags/information-theory style=margin-right:.5em><span class="tag is-warning is-small is-light">#Information Theory</span></a></div></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-paperclip" aria-hidden=true></i>References:</span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-36/issue-3/On-Information-in-Statistics/10.1214/aoms/1177700061.full>Fraser DAS. On Information in Statistics. aoms. 1965;36: 890–896. doi:10.1214/aoms/1177700061</a></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-star"></i>Summary: The Fraser information is
$$ I_F(\theta) = \int g(X) \ln f(X;\theta) , \mathrm d X. $$
When comparing two models, $\theta_0$ and $\theta_1$, the information gain is
$$ \propto (F(\theta_1) - F(\theta_0)). $$
The Fraser information is closed related to Fisher information Fisher Information Fisher information measures the second moment of the model sensitivity with respect to the parameters. , Shannon information, and Kullback information KL Divergence Kullback–Leibler divergence indicates the differences between two distributions 1.</span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-list-alt"></i>Pages: 8</span></div></div><div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div><div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork><h1 class=blog-timestamp></h1><h1 class=title><a href=https://datumorphism.leima.is/cards/information/mutual-information/ itemprop=headline>Mutual Information</a></h1><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>Published: <time datetime=2021-08-13T00:00:00+00:00>2021-08-13</time></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-folder" aria-hidden=true></i>Category: { Information }</span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-hashtag" aria-hidden=true></i>Tags:<div class="tags is-small" style=display:inline><a href=/tags/information-theory style=margin-right:.5em><span class="tag is-warning is-small is-light">#Information Theory</span></a>
<a href=/tags/mutual-information style=margin-right:.5em><span class="tag is-warning is-small is-light">#Mutual Information</span></a></div></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-paperclip" aria-hidden=true></i>References:</span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=http://arxiv.org/abs/2006.08218>Liu X, Zhang F, Hou Z, Wang Z, Mian L, Zhang J, et al. Self-supervised Learning: Generative or Contrastive. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2006.08218</a></span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=http://www.scholarpedia.org/article/Mutual_information>Latham PE, Roudi Y. Mutual information. Scholarpedia. 2009;4. doi:10.4249/scholarpedia.1658</a></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-star"></i>Summary: Mutual information is defined as
$$ I(X;Y) = \mathbb E_{p_{XY}} \ln \frac{P_{XY}}{P_X P_Y}. $$
In the case that $X$ and $Y$ are independent variables, we have $P_{XY} = P_X P_Y$, thus $I(X;Y) = 0$. This makes sense as there would be no &ldquo;mutual&rdquo; information if the two variables are independent of each other.
Entropy and Cross Entropy Mutual information is closely related to entropy. A simple decomposition shows that
$$ I(X;Y) = H(X) - H(X\mid Y), $$</span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-list-alt"></i>Pages: 8</span></div></div><div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div><div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork><h1 class=blog-timestamp></h1><h1 class=title><a href=https://datumorphism.leima.is/cards/information/shannon-entropy/ itemprop=headline>Shannon Entropy</a></h1><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>Published: <time datetime=2021-09-04T00:00:00+00:00>2021-09-04</time></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-folder" aria-hidden=true></i>Category: { Information }</span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-hashtag" aria-hidden=true></i>Tags:<div class="tags is-small" style=display:inline><a href=/tags/information-theory style=margin-right:.5em><span class="tag is-warning is-small is-light">#Information Theory</span></a>
<a href=/tags/entropy style=margin-right:.5em><span class="tag is-warning is-small is-light">#Entropy</span></a></div></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-paperclip" aria-hidden=true></i>References:</span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=https://en.wikipedia.org/wiki/Entropy_%28information_theory%29>Contributors to Wikimedia projects. Entropy (information theory). In: Wikipedia [Internet]. 29 Aug 2021 [cited 4 Sep 2021]. Available: https://en.wikipedia.org/wiki/Entropy_(information_theory)</a></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-star"></i>Summary: Shannon entropy $S$ is the expectation of information content $I(X)=-\log \left(p\right)$1,
\begin{equation} H(p) = \mathbb E_{p}\left[ -\log \left(p\right) \right]. \end{equation}
shannon_entropy_wiki Contributors to Wikimedia projects. Entropy (information theory). In: Wikipedia [Internet]. 29 Aug 2021 [cited 4 Sep 2021]. Available: https://en.wikipedia.org/wiki/Entropy_(information_theory) &#8617;&#xfe0e;</span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-list-alt"></i>Pages: 8</span></div></div><div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div><div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork><h1 class=blog-timestamp></h1><h1 class=title><a href=https://datumorphism.leima.is/cards/information/jensen-shannon-divergence/ itemprop=headline>Jensen-Shannon Divergence</a></h1><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>Published: <time datetime=2021-09-04T00:00:00+00:00>2021-09-04</time></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-folder" aria-hidden=true></i>Category: { Information }</span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-hashtag" aria-hidden=true></i>Tags:<div class="tags is-small" style=display:inline><a href=/tags/information-theory style=margin-right:.5em><span class="tag is-warning is-small is-light">#Information Theory</span></a>
<a href=/tags/divergence style=margin-right:.5em><span class="tag is-warning is-small is-light">#Divergence</span></a></div></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-paperclip" aria-hidden=true></i>References:</span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence>Contributors to Wikimedia projects. Jensen–Shannon divergence. In: Wikipedia [Internet]. 15 Jun 2021 [cited 6 Sep 2021]. Available: https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence</a></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-star"></i>Summary: The Jensen-Shannon divergence is a symmetric divergence of distributions $P$ and $Q$,
$$ \operatorname{D}_{\text{JS}} = \frac{1}{2} \left[ \operatorname{D}_{\text{KL}} \left(P \bigg\Vert \frac{P+Q}{2} \right) + \operatorname{D}_{\text{KL}} \left(Q \bigg\Vert \frac{P+Q}{2}\right) \right], $$
where $\operatorname{D}_{\text{KL}}$ is the KL Divergence KL Divergence Kullback–Leibler divergence indicates the differences between two distributions .</span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-list-alt"></i>Pages: 8</span></div></div><div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div><div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork><h1 class=blog-timestamp></h1><h1 class=title><a href=https://datumorphism.leima.is/cards/information/f-divergence/ itemprop=headline>f-Divergence</a></h1><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>Published: <time datetime=2021-09-04T00:00:00+00:00>2021-09-04</time></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-folder" aria-hidden=true></i>Category: { Information }</span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-hashtag" aria-hidden=true></i>Tags:<div class="tags is-small" style=display:inline><a href=/tags/information-theory style=margin-right:.5em><span class="tag is-warning is-small is-light">#Information Theory</span></a>
<a href=/tags/divergence style=margin-right:.5em><span class="tag is-warning is-small is-light">#Divergence</span></a></div></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-paperclip" aria-hidden=true></i>References:</span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=https://en.wikipedia.org/wiki/F-divergence>Contributors to Wikimedia projects. F-divergence. In: Wikipedia [Internet]. 17 Jul 2021 [cited 4 Sep 2021]. Available: https://en.wikipedia.org/wiki/F-divergence</a></span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=http://arxiv.org/abs/1606.00709>Nowozin S, Cseke B, Tomioka R. f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization. arXiv [stat.ML]. 2016. Available: http://arxiv.org/abs/1606.00709</a></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-star"></i>Summary: The f-divergence is defined as1
$$ \operatorname{D}_f = \int f\left(\frac{p}{q}\right) q\mathrm d\mu, $$
where $p$ and $q$ are two densities and $\mu$ is a reference distribution.
Requirements on the generating function
The generating function $f$ is required to
be convex, and $f(1) =0$. For $f(x) = x \log x$ with $x=p/q$, f-divergence is reduced to the KL divergence
$$ \begin{align} &\int f\left(\frac{p}{q}\right) q\mathrm d\mu \\ =& \int \frac{p}{q} \log \left( \frac{p}{q} \right) \mathrm d\mu \\ =& \int p \log \left( \frac{p}{q} \right) \mathrm d\mu.</span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-list-alt"></i>Pages: 8</span></div></div><div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div><div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork><h1 class=blog-timestamp></h1><h1 class=title><a href=https://datumorphism.leima.is/cards/information/cross-entropy/ itemprop=headline>Cross Entropy</a></h1><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>Published: <time datetime=2021-09-04T00:00:00+00:00>2021-09-04</time></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-folder" aria-hidden=true></i>Category: { Information }</span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-hashtag" aria-hidden=true></i>Tags:<div class="tags is-small" style=display:inline><a href=/tags/information-theory style=margin-right:.5em><span class="tag is-warning is-small is-light">#Information Theory</span></a>
<a href=/tags/entropy style=margin-right:.5em><span class="tag is-warning is-small is-light">#Entropy</span></a></div></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-paperclip" aria-hidden=true></i>References:</span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=https://en.wikipedia.org/wiki/Cross_entropy>Contributors to Wikimedia projects. Cross entropy. In: Wikipedia [Internet]. 4 Jul 2021 [cited 4 Sep 2021]. Available: https://en.wikipedia.org/wiki/Cross_entropy</a></span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=https://linkinghub.elsevier.com/retrieve/pii/S0370157319300766>Mehta P, Wang C-H, Day AGR, Richardson C, Bukov M, Fisher CK, et al. A high-bias, low-variance introduction to Machine Learning for physicists. Phys Rep. 2019;810: 1–124. doi:10.1016/j.physrep.2019.03.001</a></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-star"></i>Summary: Cross entropy is1
$$ H(p, q) = \mathbb E_{p} \left[ -\log q \right]. $$
Cross entropy $H(p, q)$ can also be decomposed,
$$ H(p, q) = H(p) + \operatorname{D}_{\mathrm{KL}} \left( p \parallel q \right), $$
where $H(p)$ is the entropy of $P$ Shannon Entropy Shannon entropy $S$ is the expectation of information content $I(X)=-\log \left(p\right)$1, \begin{equation} H(p) = \mathbb E_{p}\left[ -\log \left(p\right) \right]. \end{equation} shannon_entropy_wiki Contributors to Wikimedia projects. Entropy (information theory).</span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-list-alt"></i>Pages: 8</span></div></div><div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div></div></div></div><br><div class=container></div></div></section></div></div></main><footer><footer class=footer><div class=container><div class="content has-text-centered"><p>Created and maintained by <a href=https://leima.is>Lei Ma</a>.
Acknowledgement: <a href=https://gohugo.io/>Hugo</a>,
<a href=https://themes.gohugo.io/bulma/>Bulma</a>, <a href=https://kausalflow.com>KausalFlow</a>.
<strong>love</strong>.<br><a class=tag href=/about>About</a>
<a class=tag href=/index.xml>Feed</a>
<a class=tag href=/data.json>JSON Data</a></p></div></div></footer></footer><script async type=text/javascript src=/js/bulma.js></script></body></html>