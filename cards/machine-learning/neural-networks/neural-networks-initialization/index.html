<!doctype html><html lang=en-us><head><meta name=generator content="Hugo 0.69.2"><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><title>Initialize Artificial Neural Networks | Datumorphism | Lei Ma</title><meta name=description content="Initialize a neural network is important for the training and performance. Some initializations simply don't work, some will degrade the performance of the model. We should choose wisely."><meta name=author content="Lei Ma"><meta property="og:title" content="Initialize Artificial Neural Networks"><meta property="og:description" content="Initialize a neural network is important for the training and performance. Some initializations simply don't work, some will degrade the performance of the model. We should choose wisely."><meta property="og:type" content="article"><meta property="og:url" content="https://datumorphism.leima.is/cards/machine-learning/neural-networks/neural-networks-initialization/"><meta property="article:published_time" content="2021-09-23T00:00:00+00:00"><meta property="article:modified_time" content="2021-09-23T00:00:00+00:00"><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-140452515-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><link rel=canonical href=https://datumorphism.leima.is/cards/machine-learning/neural-networks/neural-networks-initialization/><link rel="shortcut icon" type=image/png href=/logos/logo-square.png><link rel=stylesheet href=/css/bulma.css><link rel=stylesheet href=/css/bulma-divider.min.css><link rel=stylesheet href=/assets/css/bulma-ribbon.min.css><link rel=stylesheet href=/assets/css/tooltip.css><link rel=stylesheet href=https://jenil.github.io/bulmaswatch/united/bulmaswatch.min.css><link rel=stylesheet href=/css/custom.css><link rel=stylesheet href=/css/blog-post.css><link rel=stylesheet href=/css/code-highlighting/dark.css><link rel=stylesheet href=/css/custom.css><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],tags:'ams',processEscapes:true,processEnvironments:true},options:{skipHtmlTags:['script','noscript','style','textarea','pre']},svg:{fontCache:'global'}};window.addEventListener('load',(event)=>{document.querySelectorAll("mjx-container").forEach(function(x){x.parentElement.classList+='has-jax'})});</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script>mermaid.initialize({startOnLoad:true});</script><script src=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/js/all.min.js integrity="sha256-KzZiKy0DWYsnwMF+X1DvQngQ2/FxF7MF3Ff72XcpuPs=" crossorigin=anonymous></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css integrity="sha256-h20CPZ0QyXlBuAw7A+KluUYx/3pK+c7lYEpqLTlxjYQ=" crossorigin=anonymous></head><body><header><nav class="navbar is-transparent"><div class=navbar-brand><a class=navbar-item href=/><img src=/site/intelligence.png alt=Datumorphism height=28 style=margin-right:.5em> Datumorphism</a><div class="navbar-burger burger" style=color:#000 data-target=navMenu><span></span><span></span><span></span></div></div><div class=navbar-menu id=navMenu><div class=navbar-start><div class="navbar-item has-dropdown is-hoverable"><a href=/projects class=navbar-link>Notebooks</a><div class=navbar-dropdown><a href=https://datumorphism.leima.is/awesome/ class=navbar-item>Awesome</a>
<a href=https://datumorphism.leima.is/blog/ class=navbar-item>Blog</a>
<a href=https://datumorphism.leima.is/cards/ class=navbar-item>Cards</a>
<a href=https://datumorphism.leima.is/hologram/ class=navbar-item>Hologram</a>
<a href=https://datumorphism.leima.is/reading/ class=navbar-item>Reading Notes</a>
<a href=https://datumorphism.leima.is/til/ class=navbar-item>TIL</a>
<a href=https://datumorphism.leima.is/wiki/ class=navbar-item>Wiki</a></div></div><div class="navbar-item has-dropdown is-hoverable"><a class=navbar-item href=https://neuronstar.kausalflow.com/cpe-docs/>Probability Estimation</a></div></div><span class=navbar-burger><span></span><span></span><span></span></span><div class=navbar-end><div class=navbar-item><a class=navbar-item href=/blog/>Blog</a>
<a class=navbar-item href=/amneumarkt/>AmNeumarkt</a>
<a class=navbar-item href=/><i class="fas fa-search"></i></a><a class=navbar-item href=/tags/><i class="fas fa-tags"></i></a><a class=navbar-item href=/graph><i class="fas fa-project-diagram"></i></a><a class=navbar-item href=https://t.me/amneumarkt><i class="fab fa-telegram"></i></a><a class=navbar-item target=blank href=https://github.com/datumorphism><span class=icon><i class="fab fa-github"></i></span></a></div></div></div></nav><script>document.addEventListener('DOMContentLoaded',()=>{const $navbarBurgers=Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'),0);if($navbarBurgers.length>0){$navbarBurgers.forEach(el=>{el.addEventListener('click',()=>{const target=el.dataset.target;const $target=document.getElementById(target);el.classList.toggle('is-active');$target.classList.toggle('is-active');});});}});</script></header><main><div class=container itemscope itemtype=http://schema.org/BlogPosting><meta itemprop=name content="Initialize Artificial Neural Networks"><meta itemprop=description content="Initialize a neural network is important for the training and performance. Some initializations simply don't work, some will degrade the performance of the model. We should choose wisely."><meta itemprop=datePublished content="2021-09-23T00:00:00+00:00"><meta itemprop=dateModified content="2021-09-23T00:00:00+00:00"><meta itemprop=wordCount content="525"><meta itemprop=keywords content="Artificial Neuron,Neural Network,Basics,"><section class=section><div class=container><article class=post><header class=post-header><nav class="breadcrumb has-succeeds-separator is-small" aria-label=breadcrumbs><ul><li><a href=https://datumorphism.leima.is/>Datumorphism</a></li><li><a href=https://datumorphism.leima.is/cards/>Cards</a></li><li><a href=https://datumorphism.leima.is/cards/machine-learning/>Machine Learning</a></li><li><a href=https://datumorphism.leima.is/cards/machine-learning/neural-networks/>Neural Networks</a></li><li class=active><a href=https://datumorphism.leima.is/cards/machine-learning/neural-networks/neural-networks-initialization/>Initialize Artificial Neural Networks</a></li></ul></nav><h1 class="post-title has-text-centered is-size-1" itemprop="name headline">Initialize Artificial Neural Networks</h1><h2 class="title is-6 has-text-centered"><i class="fas fa-tags" style=margin-right:.5em></i><a href=/tags/artificial-neuron><span class="tag is-warning is-small is-light">#Artificial Neuron</span></a>
<a href=/tags/neural-network><span class="tag is-warning is-small is-light">#Neural Network</span></a>
<a href=/tags/basics><span class="tag is-warning is-small is-light">#Basics</span></a></h2></header><div class=columns><div class="column is-8"><div class=is-divider data-content=ARTICLE></div><div class="content blog-post section" itemprop=articleBody><p>The weights are better if they<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></p><ul><li>are zero centered, and</li><li>have similar variance across layers.</li></ul><article class="message is-info is-light"><div class=message-header><p>Why</p></div><div class=message-body>If we have very different variances across layers, we will need a different learning rate for each layer for our optimization. Setting the variances to be on the same scale, we can use a global learning rate for the whole network.</div></article><h2 id=variance-is-related-to-the-input-size-of-the-layer>Variance is related to the input size of the layer</h2><p>Suppose we are using a simple linear activation, $\sigma(x) = \alpha x$. For a series of inputs $x_j$, the outputs $y_i$ are</p><p>$$
y_i = \sum_{j} w_{ij} x_j.
$$</p><p>The variance of $y_i$ is</p><p>$$
\begin{align}
\operatorname{Var}\left[ y \right] &= \alpha^2 \operatorname{Var}\left[\sum_{j} w_{ij} x_j \right] \\
& = \alpha^2 \sum_{j}\operatorname{Var}\left[ w_{ij} x_j \right] \\
&= \alpha^2 \sum_j \left( \mathbb E\left[ (w_{ij}x_j)^2 \right] - \mathbb E^2 \left[ w_{ij} x_j \right] \right) \label{eq-var-expand-var}\\
&= \alpha^2 \sum_j \left( \mathbb E\left[ (w_{ij}x_j)^2 \right] - {\color{red}\mathbb E^2 \left[ w_{ij} \right]} \mathbb E^2 \left[ x_j \right] \right) \label{eq-var-expand-expectation-sq} \\
&= \alpha^2 \sum_j \left( \mathbb E\left[ (w_{ij}x_j)^2 \right]\right) \label{eq-var-drop-zero-exp} \\
&= \alpha^2 \sum_j \left( \mathbb E\left[ w_{ij}^2x_j^2 \right]\right) \label{eq-var-expand-sq-expectation}\\
&= \alpha^2 \sum_j \left( \mathbb E\left[ w_{ij}^2\right]\mathbb E\left[x_j^2 \right]\right) \label{eq-var-propagate-exp}\\
&= \alpha^2 \sum_j \left( \left(\mathbb E\left[ w_{ij}^2\right] - {\color{red}\mathbb E^2\left[ w_{ij} \right]}\right)\left(\mathbb E\left[x_j^2 \right] - {\color{red}\mathbb E^2\left[ x_j \right]} \right) \right) \label{eq-var-add-zero-exp-w-x}\\
&= \alpha^2 \sum_j \left( \operatorname{Var}\left[ w_{ij} \right]\operatorname{Var}\left[x_j \right]\right) \label{eq-var-form-var-w-x}\\
&= D \alpha^2 \left( \sigma_w^2\sigma_x^2\right) \label{eq-var-calculate-sum}.
\end{align}
$$</p><p>In the derivation,</p><ul><li>the red term in $\eqref{eq-var-expand-expectation-sq}$, ${\color{red}\mathbb E^2 \left[ w_{ij} \right]}$ is zero,</li><li>we assume the input has zero expectation in $\eqref{eq-var-add-zero-exp-w-x}$, i.e., ${\color{red}\mathbb E^2\left[ x_j \right]}=0$,</li><li>in the last step {eq-var-calculate-sum}, we assumed all the variances are the same, $\operatorname{Var}\left[ w_{ij} \right]=\sigma_w^2$ and $\operatorname{Var}\left[x_j \right]=\sigma_x^2$,</li><li>$D$ is the dimension of the input data $x$.</li></ul><p>If we require $\operatorname{Var}\left[ y \right] = \sigma_x^2$, the variance of the weights should be</p><p>$$
\sigma_w^2 = \frac{1}{D \alpha^2}.
$$</p><p>For linear activation function, $\alpha=1$, thus</p><p>$$
\sigma_w^2 = \frac{1}{D}.
$$</p><p>For more features we have in the input, we should choose smaller initial variance for the weights.</p><h2 id=weight-variance-is-also-related-to-the-output-size-of-the-layer>Weight Variance is also related to the output size of the layer</h2><p>Assuming $\hat D$ is the output dimension of the layer, it is also proved that we need</p><p>$$
\sigma_w^2 = \frac{1}{\hat D},
$$</p><p>to make sure the back-prop is also stable, i.e., have similar gradient variance across layers.</p><h2 id=xavier-initialization>Xavier Initialization</h2><p>The Xavier initialization is something in between the above two idea,</p><p>$$
\sigma_w^2 = \frac{2}{D + \hat D}.
$$</p><p>In Glorot2010, the authors also proposed a uniform distribution alternative to normal distribution, which they call the <strong>normalized initialization</strong><sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>. In this proposal, we uniformly sample from the range</p><p>$$
\left[ -\frac{\sqrt{6}}{D + \hat D}, \frac{\sqrt{6}}{D + \hat D} \right].
$$</p><p>This proposal is essentially the same idea as the Xavier initialization.</p><h2 id=what-are-the-differences>What are the differences</h2><p><a href=https://www.deeplearning.ai/ai-notes/initialization/>deeplearning.ai has an interactive session</a> on the effects of initializations for classification tasks on MNIST.</p><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p><a name=Lippe2020 href=#Lippe2020 style=text-decoration:none><span class="tag is-link is-light">Lippe2020</span></a>
<a href=https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial3/Activation_Functions.html style=text-decoration:none>Lippe P. Tutorial 3: Activation Functions — UvA DL Notebooks v1.1 documentation. In: UvA Deep Learning Tutorials [Internet]. [cited 23 Sep 2021]. Available: https://uvadlc-notebooks.readthedocs.io</a>
<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2 role=doc-endnote><p><a name=Glorot2010 href=#Glorot2010 style=text-decoration:none><span class="tag is-link is-light">Glorot2010</span></a>
<a href=http://proceedings.mlr.press/v9/glorot10a.html style=text-decoration:none>Glorot X, Bengio Y. Understanding the difficulty of training deep feedforward neural networks. Teh YW, Titterington M, editors. 2010;9: 249–256. Available: http://proceedings.mlr.press/v9/glorot10a.html</a>
<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></div><p><div class="has-text-right is-size-7"><span class=icon><i class="fas fa-pencil-alt"></i></span>Published: <time datetime=2021-09-23T00:00:00+00:00>2021-09-23</time>
by <span itemprop=author>Lei Ma</span>;</div></p><div class=is-divider></div><nav class="pagination is-centered" role=navigation aria-label=pagination><a class=pagination-next href="https://datumorphism.leima.is/cards/machine-learning/neural-networks/log-sum-exp-trick/?ref=footer">The log-sum-exp Trick »</a></nav></div><div class="column is-4"><div class=is-divider data-content="Cite Me"></div><div class="box is-size-7 has-text-white has-background-black"><article class=media><div class=media-content><div class=content><p>L Ma (2021). 'Initialize Artificial Neural Networks', Datumorphism, 09 April. Available at: https://datumorphism.leima.is/cards/machine-learning/neural-networks/neural-networks-initialization/.</p></div></div></article></div><style>#TableOfContents>ul{list-style-type:lower-greek;padding-left:0}#TableOfContents>ul>li ul{list-style-type:none;padding-left:1em}</style><div class=is-divider data-content=ToC></div><div class="box is-size-7"><article class=media><div class=media-content><div class=content><details><summary>Table of Contents</summary><div><div><nav id=TableOfContents><ul><li><a href=#variance-is-related-to-the-input-size-of-the-layer>Variance is related to the input size of the layer</a></li><li><a href=#weight-variance-is-also-related-to-the-output-size-of-the-layer>Weight Variance is also related to the output size of the layer</a></li><li><a href=#xavier-initialization>Xavier Initialization</a></li><li><a href=#what-are-the-differences>What are the differences</a></li></ul></nav></div></div></details></div></div></article></div><script>const el=document.querySelector('details summary')
el.onclick=()=>{(function(l,o,a,d,e,r){e=o.createElement(a),r=o.getElementsByTagName(a)[0];e.async=1;e.src=d;r.parentNode.insertBefore(e,r)})(window,document,'script','/js/smoothscroll.js');el.onclick=null}
document.querySelectorAll('#TableOfContents a').forEach(link=>{link.addEventListener('click',()=>{document.querySelector(link.href.slice(link.href.indexOf('#'))).scrollIntoView({behavior:'smooth'})})})</script><div class=is-divider data-content=REFERENCES></div><div class="box is-size-7"><article class=media><div class=media-content><div class=content><p><strong>References:</strong><br><ol><li class=has-text-weight-bold><a name=Lippe2020 href=#Lippe2020 style=text-decoration:none><span class="tag is-link is-light">Lippe2020</span></a>
<a href=https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial3/Activation_Functions.html style=text-decoration:none>Lippe P. Tutorial 3: Activation Functions — UvA DL Notebooks v1.1 documentation. In: UvA Deep Learning Tutorials [Internet]. [cited 23 Sep 2021]. Available: https://uvadlc-notebooks.readthedocs.io</a></li><li class=has-text-weight-bold><a href=https://pouannes.github.io/blog/initialization/#mjx-eqn-eqfwd_K style=text-decoration:none>Ouannes P. How to initialize deep neural networks? Xavier and Kaiming initialization. In: Title [Internet]. 22 Mar 2019 [cited 24 Sep 2021]. Available: https://pouannes.github.io/blog/initialization</a></li><li class=has-text-weight-bold><a name=Glorot2010 href=#Glorot2010 style=text-decoration:none><span class="tag is-link is-light">Glorot2010</span></a>
<a href=http://proceedings.mlr.press/v9/glorot10a.html style=text-decoration:none>Glorot X, Bengio Y. Understanding the difficulty of training deep feedforward neural networks. Teh YW, Titterington M, editors. 2010;9: 249–256. Available: http://proceedings.mlr.press/v9/glorot10a.html</a></li><li class=has-text-weight-bold><a name=Katanforoosh2019 href=#Katanforoosh2019 style=text-decoration:none><span class="tag is-link is-light">Katanforoosh2019</span></a>
<a href=https://www.deeplearning.ai/ai-notes/initialization/ style=text-decoration:none>Katanforoosh & Kunin, "Initializing neural networks", deeplearning.ai, 2019.</a></li></ol></p></div></div></article></div><div class=is-divider data-content=CONNECTUME></div><div class="box is-size-7"><article class=media><div class=media-content style=width:100%><div class=content><p><strong>Current Ref:</strong><br><ul><li style=list-style:none><div>cards/machine-learning/neural-networks/neural-networks-initialization.md</div></li></ul></p></div></div></article></div><div class="box is-size-7"><article class=media><div class=media-content style=width:100%><div class=content><p><strong>Links to:</strong><div><a class=box href=https://datumorphism.leima.is/wiki/machine-learning/neural-networks/artificial-neural-networks/><div class=media-content><div class=content><h6>Artificial Neural Networks</h6><p>Solving PDEs</p></div></div></a></div></p></div></div></article></div><div class="box is-size-7"><article class=media><div class=media-content><div class=content><p><strong>Links from:</strong><div><a class=box href=https://datumorphism.leima.is/til/machine-learning/pytorch/pytorch-initial-params/><div class=media-content><div class=content><h6>PyTorch: Initialize Parameters</h6><p>We can set the parameters in a for loop. We take some of the initialization methods from Lippe1.
To …</p></div></div></a></div></p></div></div></article></div><div id=comments class=is-divider data-content=COMMENTS></div><script src=https://giscus.app/client.js data-repo=datumorphism/comments data-repo-id="MDEwOlJlcG9zaXRvcnkxNjU5MDkyNDI=" data-category=Comments data-category-id=DIC_kwDOCeOS-s4B-Zxx data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=1 data-theme=light crossorigin=anonymous async></script></div></div></article></div></section></div><div class=navtools><a class="button is-primary is-light is-outlined" alt="Edit this page" href=https://github.com/datumorphism/datumorphism.github.io/edit/hugo/content/cards/machine-learning/neural-networks/neural-networks-initialization.md target=blank style=position:fixed;bottom:20px;right:10px;border-radius:9999px;width:35px;height:35px><i class="fas fa-pencil-alt"></i></a><a class="button is-primary is-light is-outlined" href=#comments alt=Comments style=position:fixed;bottom:60px;right:10px;border-radius:9999px;width:35px;height:35px><i class="far fa-comments"></i></a></div></main><footer><footer class=footer><div class=container><div class="content has-text-centered"><p>Created and maintained by <a href=https://leima.is>Lei Ma</a>.
Acknowledgement: <a href=https://gohugo.io/>Hugo</a>,
<a href=https://themes.gohugo.io/bulma/>Bulma</a>, <a href=https://kausalflow.com>KausalFlow</a>.
<strong>love</strong>.<br><a class=tag href=/about>About</a>
<a class=tag href=/index.xml>Feed</a>
<a class=tag href=/data.json>JSON Data</a></p></div></div></footer></footer><script async type=text/javascript src=/js/bulma.js></script></body></html>