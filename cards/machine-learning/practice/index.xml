<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Practices in Machine Learning on Datumorphism</title><link>https://datumorphism.leima.is/cards/machine-learning/practice/</link><description>Recent content in Practices in Machine Learning on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Wed, 19 Oct 2022 14:54:29 +0200</lastBuildDate><atom:link href="https://datumorphism.leima.is/cards/machine-learning/practice/index.xml" rel="self" type="application/rss+xml"/><item><title>Pytorch Data Parallelism</title><link>https://datumorphism.leima.is/cards/machine-learning/practice/pytorch-data-parallelism/</link><pubDate>Wed, 19 Oct 2022 14:54:29 +0200</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/practice/pytorch-data-parallelism/</guid><description>To train large models using PyTorch, we need to go parallel. There are two commonly used strategies123:
model parallelism, data parallelism, data-model parallelism. Model Parallelism Model parallelism splits the model on different nodes14. We will focus on data parallelism but the key idea is shown in the following illustration.
Model parallelLi X, Zhang G, Li K, Zheng W. Chapter 4 - Deep Learning and Its Parallelization. In: Buyya R, Calheiros RN, Dastjerdi AV, editors. Big Data. Morgan Kaufmann; 2016. pp. 95–118. doi:10.1016/B978-0-12-805394-2.00004-0
Data Parallelism Data parallelism creates replicas of the model on each device and use different subsets of training data14.</description></item><item><title>CUDA Memory</title><link>https://datumorphism.leima.is/cards/machine-learning/practice/cuda-memory/</link><pubDate>Wed, 19 Oct 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/practice/cuda-memory/</guid><description>CUDA is widely used in deep learning. Though many of deep learning professionals are not exposed to CUDA directly, most people are already using CUDA as frameworks like PyTorch are providing GPU support through CUDA.
To optimize the computational efficiency of our models, knowledge about the data transfer inside the devices is crucial. In this note, we build up the fundamentals of memory transfer for CUDA.
Segmented Memory and Paged Memory CUDA Can not Use Paged Memory A CPU host uses paged memory. However, GPU can not directly take data from paged memory on the host1. Before accessing the data, CUDA has to pin the memory so that the memory is page-locked2.</description></item><item><title>Learning Rate</title><link>https://datumorphism.leima.is/cards/machine-learning/practice/learning-rate/</link><pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/machine-learning/practice/learning-rate/</guid><description>Finding a suitable learning rate for our model training is crucial.
A safe but time wasting option is to use search on a grid of parameters. However, there are smarter moves.
Karpathy’s Constant
An empirical learning rate $3^{-4}$ for Adms, aka, Karpathy's constant, was started as a tweet by Andrei Karpathy. 3e-4 is the best learning rate for Adam, hands down.
&amp;mdash; Andrej Karpathy (@karpathy) November 24, 2016 (i just wanted to make sure that people understand that this is a joke...)
&amp;mdash; Andrej Karpathy (@karpathy) November 24, 2016 Smarter Method A smarter method is to start with small learning rate and increase it on each mini-batch, then observe the loss vs learning rate (mini-batch in this case).</description></item></channel></rss>