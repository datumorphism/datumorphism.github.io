<!doctype html><html lang=en-us>
<head>
<meta name=generator content="Hugo 0.89.4">
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1">
<title>Learning Theories | Datumorphism | L Ma </title>
<meta name=robots content="noindex">
<meta name=author content="L Ma">
<meta property="og:title" content="Learning Theories">
<meta property="og:description" content="The study of the data as a means to model the world">
<meta property="og:type" content="website">
<meta property="og:url" content="https://datumorphism.leima.is/cards/machine-learning/learning-theories/">
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga('create','UA-140452515-1','auto'),ga('send','pageview'))</script>
<script async src=https://www.google-analytics.com/analytics.js></script>
<link href=https://datumorphism.leima.is/cards/machine-learning/learning-theories/index.xml rel=alternate type=application/rss+xml title=Datumorphism>
<link rel=canonical href=https://datumorphism.leima.is/cards/machine-learning/learning-theories/> <link rel="shortcut icon" type=image/png href=/logos/logo-square.png>
<link rel=stylesheet href=/css/bulma.css>
<link rel=stylesheet href=/css/bulma-divider.min.css>
<link rel=stylesheet href=/assets/css/bulma-ribbon.min.css>
<link rel=stylesheet href=/assets/css/tooltip.css>
<link rel=stylesheet href=https://jenil.github.io/bulmaswatch/united/bulmaswatch.min.css>
<link rel=stylesheet href=/css/custom.css>
<script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],tags:'ams',processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:['script','noscript','style','textarea','pre']},svg:{fontCache:'global'}},window.addEventListener('load',a=>{document.querySelectorAll("mjx-container").forEach(function(a){a.parentElement.classList+='has-jax'})})</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<script src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script>
<script>mermaid.initialize({startOnLoad:!0})</script>
<link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css integrity="sha512-9usAa10IRO0HhonpyAIVpjrylPvoDwiPUiKdWk5t3PyolY1cOd4DSE0Ga+ri4AuTroPR5aQvXU9xC6qOPnzFeg==" crossorigin=anonymous referrerpolicy=no-referrer>
<script src=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/js/all.min.js integrity="sha512-yFjZbTYRCJodnuyGlsKamNE/LlEaEAxSUDe5+u61mV8zzqJVFOH7TnULE2/PP/l5vKWpUNnF4VGVkXh3MjgLsg==" crossorigin=anonymous referrerpolicy=no-referrer></script>
</head>
<body>
<header> <nav class="navbar is-transparent">
<div class=navbar-brand>
<a class=navbar-item href=/>
<img src=/site/intelligence.png alt=Datumorphism height=28 style=margin-right:.5em> Datumorphism
</a>
<div class="navbar-burger burger" style=color:#000 data-target=navMenu>
<span></span>
<span></span>
<span></span>
</div>
</div>
<div class=navbar-menu id=navMenu>
<div class=navbar-start>
<div class="navbar-item has-dropdown is-hoverable">
<a href=/projects class=navbar-link>
Notebooks
</a>
<div class=navbar-dropdown>
<a href=https://datumorphism.leima.is/awesome/ class=navbar-item>
Awesome
</a>
<a href=https://datumorphism.leima.is/blog/ class=navbar-item>
Blog
</a>
<a href=https://datumorphism.leima.is/cards/ class=navbar-item>
Cards
</a>
<a href=https://datumorphism.leima.is/hologram/ class=navbar-item>
Hologram
</a>
<a href=https://datumorphism.leima.is/reading/ class=navbar-item>
Reading Notes
</a>
<a href=https://datumorphism.leima.is/til/ class=navbar-item>
TIL
</a>
<a href=https://datumorphism.leima.is/wiki/ class=navbar-item>
Wiki
</a>
</div>
</div>
<div class="navbar-item has-dropdown is-hoverable">
<a class=navbar-item href=https://neuronstar.kausalflow.com/cpe-docs/>
Probability Estimation
</a>
</div>
</div>
<span class=navbar-burger>
<span></span>
<span></span>
<span></span>
</span>
<div class=navbar-end>
<div class=navbar-item>
<a class=navbar-item href=/blog/>
Blog
</a>
<a class=navbar-item href=/amneumarkt/>
AmNeumarkt
</a>
<a class=navbar-item href=/>
<i class="fas fa-search"></i>
</a>
<a class=navbar-item href=/tags/>
<i class="fas fa-tags"></i>
</a>
<a class=navbar-item href=/graph>
<i class="fas fa-project-diagram"></i>
</a>
<a class=navbar-item href=https://t.me/amneumarkt>
<i class="fab fa-telegram"></i>
</a>
<a class=navbar-item target=blank href=https://github.com/datumorphism>
<span class=icon>
<i class="fab fa-github"></i>
</span>
</a>
</div>
</div>
</div>
</nav>
<script>document.addEventListener('DOMContentLoaded',()=>{const a=Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'),0);a.length>0&&a.forEach(a=>{a.addEventListener('click',()=>{const b=a.dataset.target,c=document.getElementById(b);a.classList.toggle('is-active'),c.classList.toggle('is-active')})})})</script> </header>
<main>
<section>
<div class="hero is-primary is-medium">
<div class=hero-body>
<div class="container has-text-left">
<h1 class="title is-1">
Learning Theories
</h1>
<h2 class="subtitle is-3">
</h2>
<h3 class="title is-5" style=margin-top:2em>
<a href=https://datumorphism.leima.is/projects/cards/ class=has-text-light><i class="fas fa-link"></i> Introduction: My Knowledge Cards </a>
</h3>
</div>
</div>
</div>
</section>
<div class="columns is-fullheight">
<div class=column>
<section class="hero is-default is-bold">
<div class=hero-body>
<div class=container>
<nav class="breadcrumb has-succeeds-separator is-small" aria-label=breadcrumbs>
<ul>
<li>
<a href=https://datumorphism.leima.is/>Datumorphism</a>
</li>
<li>
<a href=https://datumorphism.leima.is/cards/>Cards</a>
</li>
<li>
<a href=https://datumorphism.leima.is/cards/machine-learning/>Machine Learning</a>
</li>
<li class=active>
<a href=https://datumorphism.leima.is/cards/machine-learning/learning-theories/>Learning Theories</a>
</li>
</ul>
</nav>
<div class="columns is-multiline is-variable is-1-mobile is-0-tablet is-3-desktop is-8-widescreen is-2-fullhd is-desktop">
<div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork>
<h1 class=blog-timestamp>
</h1>
<h1 class=title>
<a href=https://datumorphism.leima.is/cards/machine-learning/learning-theories/pac/ itemprop=headline> PAC: Probably Approximately Correct</a>
</h1>
<div style=margin-left:2em>
<span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>
Published: <time datetime=2020-01-16T00:00:00+00:00>2020-01-16</time>
</span>
</div>
<div style=margin-left:2em>
<span style=display:block;font-size:90%><i class="far fa-folder" aria-hidden=true></i> Category: { Machine Learning::Theories }
</span>
</div>
<div style=margin-left:2em>
<span style=display:block;font-size:90%><i class="fa fa-hashtag" aria-hidden=true></i> Tags: <div class="tags is-small" style=display:inline>
<a href=/tags/data style=margin-right:.5em><span class="tag is-warning is-small is-light">#Data</span></a>
</div>
</span>
</div>
<div style=margin-left:2em><span style=display:block;font-size:90%>
<i class="fa fa-paperclip" aria-hidden=true></i> References:</span>
<span style=display:block;font-size:90%;padding-left:1em> - <a href=https://doi.org/10.1017/CBO9781107298019>Shalev-Shwartz, S., & Ben-David, S. (2013). Understanding machine learning: From theory to algorithms. Understanding Machine Learning: From Theory to Algorithms.</a></span>
</div>
<div style=margin-left:2em>
<span style=display:block;font-size:90%>
<i class="far fa-star"></i> Summary: </span>
</div>
<div style=margin-left:2em>
<span style=display:block;font-size:90%>
<i class="far fa-list-alt"></i> Pages: 7</span>
</div>
</div>
<div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div>
<div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork>
<h1 class=blog-timestamp>
</h1>
<h1 class=title>
<a href=https://datumorphism.leima.is/cards/machine-learning/learning-theories/structural-risk-minimization/ itemprop=headline> SRM: Structural Risk Minimization</a>
</h1>
<div style=margin-left:2em>
<span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>
Published: <time datetime=2021-02-18T00:00:00+00:00>2021-02-18</time>
</span>
</div>
<div style=margin-left:2em>
<span style=display:block;font-size:90%><i class="far fa-folder" aria-hidden=true></i> Category: { Machine Learning::Theories }
</span>
</div>
<div style=margin-left:2em>
<span style=display:block;font-size:90%><i class="fa fa-hashtag" aria-hidden=true></i> Tags: <div class="tags is-small" style=display:inline>
<a href=/tags/data style=margin-right:.5em><span class="tag is-warning is-small is-light">#Data</span></a>
<a href=/tags/loss style=margin-right:.5em><span class="tag is-warning is-small is-light">#Loss</span></a>
</div>
</span>
</div>
<div style=margin-left:2em><span style=display:block;font-size:90%>
<i class="fa fa-paperclip" aria-hidden=true></i> References:</span>
<span style=display:block;font-size:90%;padding-left:1em> - <a href=https://en.wikipedia.org/wiki/Structural_risk_minimization>Structural risk minimization @ Wikipedia</a></span>
<span style=display:block;font-size:90%;padding-left:1em> - <a href=https://mitpress.mit.edu/books/machine-learning-1>Murphy, K. P. (2012). Probabilistic Machine Learning: An Introduction.</a></span>
</div>
<div style=margin-left:2em>
<span style=display:block;font-size:90%>
<i class="far fa-star"></i> Summary: [[ERM]] ERM: Empirical Risk Minimization In a [[learning problem]] The Learning Problem The learning problem posed by Vapnik:1 Given a sample: $\{z_i\}$ in the probability space $Z$; Assuming a probability measure on the probability space $Z$; Assuming a set of functions $Q(z, \alpha)$ (e.g. loss functions), where $\alpha$ is a set of parameters; A risk functional to be minimized by tunning &ldquo;the handles&rdquo; $\alpha$, $R(\alpha)$. The risk functional is $$ R(\alpha) = \int Q(z, \alpha) \,\mathrm d F(z). $$ A learning problem … may lead to overfitting since ERM only selects the model to fit the train data well.</span>
</div>
<div style=margin-left:2em>
<span style=display:block;font-size:90%>
<i class="far fa-list-alt"></i> Pages: 7</span>
</div>
</div>
<div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div>
<div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork>
<h1 class=blog-timestamp>
</h1>
<h1 class=title>
<a href=https://datumorphism.leima.is/cards/machine-learning/learning-theories/empirical-risk-minimization/ itemprop=headline> ERM: Empirical Risk Minimization</a>
</h1>
<div style=margin-left:2em>
<span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>
Published: <time datetime=2021-02-18T00:00:00+00:00>2021-02-18</time>
</span>
</div>
<div style=margin-left:2em>
<span style=display:block;font-size:90%><i class="far fa-folder" aria-hidden=true></i> Category: { Machine Learning::Theories }
</span>
</div>
<div style=margin-left:2em>
<span style=display:block;font-size:90%><i class="fa fa-hashtag" aria-hidden=true></i> Tags: <div class="tags is-small" style=display:inline>
<a href=/tags/data style=margin-right:.5em><span class="tag is-warning is-small is-light">#Data</span></a>
<a href=/tags/loss style=margin-right:.5em><span class="tag is-warning is-small is-light">#Loss</span></a>
</div>
</span>
</div>
<div style=margin-left:2em><span style=display:block;font-size:90%>
<i class="fa fa-paperclip" aria-hidden=true></i> References:</span>
<span style=display:block;font-size:90%;padding-left:1em> - <a href=https://mitpress.mit.edu/books/machine-learning-1>Murphy, K. P. (2012). Probabilistic Machine Learning: An Introduction.</a></span>
</div>
<div style=margin-left:2em>
<span style=display:block;font-size:90%>
<i class="far fa-star"></i> Summary: In a [[learning problem]] The Learning Problem The learning problem posed by Vapnik:1 Given a sample: $\{z_i\}$ in the probability space $Z$; Assuming a probability measure on the probability space $Z$; Assuming a set of functions $Q(z, \alpha)$ (e.g. loss functions), where $\alpha$ is a set of parameters; A risk functional to be minimized by tunning &ldquo;the handles&rdquo; $\alpha$, $R(\alpha)$. The risk functional is $$ R(\alpha) = \int Q(z, \alpha) \,\mathrm d F(z). $$ A learning problem is the minimization of this risk. Vapnik2000 … , empirical risk $R$ is a measurement the goodness of fit based on empirical information.</span>
</div>
<div style=margin-left:2em>
<span style=display:block;font-size:90%>
<i class="far fa-list-alt"></i> Pages: 7</span>
</div>
</div>
<div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div>
<div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork>
<h1 class=blog-timestamp>
</h1>
<h1 class=title>
<a href=https://datumorphism.leima.is/cards/machine-learning/learning-theories/learning-problem/ itemprop=headline> The Learning Problem</a>
</h1>
<div style=margin-left:2em>
<span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>
Published: <time datetime=2021-05-06T00:00:00+00:00>2021-05-06</time>
</span>
</div>
<div style=margin-left:2em>
<span style=display:block;font-size:90%><i class="far fa-folder" aria-hidden=true></i> Category: { Machine Learning::Theories }
</span>
</div>
<div style=margin-left:2em>
<span style=display:block;font-size:90%><i class="fa fa-hashtag" aria-hidden=true></i> Tags: <div class="tags is-small" style=display:inline>
<a href=/tags/learning-theory style=margin-right:.5em><span class="tag is-warning is-small is-light">#Learning Theory</span></a>
</div>
</span>
</div>
<div style=margin-left:2em><span style=display:block;font-size:90%>
<i class="fa fa-paperclip" aria-hidden=true></i> References:</span>
<span style=display:block;font-size:90%;padding-left:1em> - <a href=https://link.springer.com/book/10.1007%2F978-1-4757-3264-1>Vladimir N. Vapnik. The Nature of Statistical Learning Theory. 2000. doi:10.1007/978-1-4757-3264-1</a></span>
</div>
<div style=margin-left:2em>
<span style=display:block;font-size:90%>
<i class="far fa-star"></i> Summary: The learning problem posed by Vapnik:1
Given a sample: $\{z_i\}$ in the probability space $Z$; Assuming a probability measure on the probability space $Z$; Assuming a set of functions $Q(z, \alpha)$ (e.g. loss functions), where $\alpha$ is a set of parameters; A risk functional to be minimized by tunning &ldquo;the handles&rdquo; $\alpha$, $R(\alpha)$. The risk functional is
$$ R(\alpha) = \int Q(z, \alpha) \,\mathrm d F(z). $$
A learning problem is the minimization of this risk.
Vapnik2000 Vladimir N. Vapnik. The Nature of Statistical Learning Theory. 2000. doi:10.1007/978-1-4757-3264-1 &#160;&#8617;&#xfe0e;
</span>
</div>
<div style=margin-left:2em>
<span style=display:block;font-size:90%>
<i class="far fa-list-alt"></i> Pages: 7</span>
</div>
</div>
<div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div>
<div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork>
<h1 class=blog-timestamp>
</h1>
<h1 class=title>
<a href=https://datumorphism.leima.is/cards/machine-learning/learning-theories/cross-validation/ itemprop=headline> Cross Validation</a>
</h1>
<div style=margin-left:2em>
<span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>
Published: <time datetime=2021-05-06T00:00:00+00:00>2021-05-06</time>
</span>
</div>
<div style=margin-left:2em>
<span style=display:block;font-size:90%><i class="far fa-folder" aria-hidden=true></i> Category: { Machine Learning::Theories }
</span>
</div>
<div style=margin-left:2em>
<span style=display:block;font-size:90%><i class="fa fa-hashtag" aria-hidden=true></i> Tags: <div class="tags is-small" style=display:inline>
<a href=/tags/learning-theory style=margin-right:.5em><span class="tag is-warning is-small is-light">#Learning Theory</span></a>
<a href=/tags/cross-validation style=margin-right:.5em><span class="tag is-warning is-small is-light">#Cross Validation</span></a>
</div>
</span>
</div>
<div style=margin-left:2em><span style=display:block;font-size:90%>
<i class="fa fa-paperclip" aria-hidden=true></i> References:</span>
<span style=display:block;font-size:90%;padding-left:1em> - <a href=https://mitpress.mit.edu/books/machine-learning-1>Murphy, K. P. (2012). Probabilistic Machine Learning: An Introduction.</a></span>
</div>
<div style=margin-left:2em>
<span style=display:block;font-size:90%>
<i class="far fa-star"></i> Summary: Cross validation is a method to estimate the [[risk]] The Learning Problem The learning problem posed by Vapnik:1 Given a sample: $\{z_i\}$ in the probability space $Z$; Assuming a probability measure on the probability space $Z$; Assuming a set of functions $Q(z, \alpha)$ (e.g. loss functions), where $\alpha$ is a set of parameters; A risk functional to be minimized by tunning &ldquo;the handles&rdquo; $\alpha$, $R(\alpha)$. The risk functional is $$ R(\alpha) = \int Q(z, \alpha) \,\mathrm d F(z). $$ A learning problem is the minimization of this risk. Vapnik2000 … .
To perform cross validation, we split the train dataset $\mathcal D$ into $k$ folds, with each fold denoted as $\mathcal D_k$.</span>
</div>
<div style=margin-left:2em>
<span style=display:block;font-size:90%>
<i class="far fa-list-alt"></i> Pages: 7</span>
</div>
</div>
<div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div>
<div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork>
<h1 class=blog-timestamp>
</h1>
<h1 class=title>
<a href=https://datumorphism.leima.is/cards/machine-learning/learning-theories/noise-contrastive-estimation/ itemprop=headline> Noise Contrastive Estimation: NCE</a>
</h1>
<div style=margin-left:2em>
<span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>
Published: <time datetime=2021-08-13T00:00:00+00:00>2021-08-13</time>
</span>
</div>
<div style=margin-left:2em>
<span style=display:block;font-size:90%><i class="far fa-folder" aria-hidden=true></i> Category: { Machine Learning::Theories }
</span>
</div>
<div style=margin-left:2em>
<span style=display:block;font-size:90%><i class="fa fa-hashtag" aria-hidden=true></i> Tags: <div class="tags is-small" style=display:inline>
<a href=/tags/learning-theory style=margin-right:.5em><span class="tag is-warning is-small is-light">#Learning Theory</span></a>
</div>
</span>
</div>
<div style=margin-left:2em><span style=display:block;font-size:90%>
<i class="fa fa-paperclip" aria-hidden=true></i> References:</span>
<span style=display:block;font-size:90%;padding-left:1em> - <a href=http://arxiv.org/abs/2006.08218>Liu X, Zhang F, Hou Z, Wang Z, Mian L, Zhang J, et al. Self-supervised Learning: Generative or Contrastive. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2006.08218</a></span>
</div>
<div style=margin-left:2em>
<span style=display:block;font-size:90%>
<i class="far fa-star"></i> Summary: Noise contrastive estimation (NCE) objective function is1
$$ \mathcal L = \mathbb E_{x, x^{+}, x^{-}} \left[ - \ln \frac{ C(x, x^{+})}{ C(x,x^{+}) + C(x,x^{-}) } \right], $$
where
$x^{+}$ represents data similar to $x$, $x^{-}$ represents data dissimilar to $x$, $C(\cdot, \cdot)$ is a function to compute the similarities. For example, we can use
$$ C(x, x^{+}) = e^{ f(x)^T f(x^{+}) }, $$
so that the objective function becomes
$$ \mathcal L = \mathbb E_{x, x^{+}, x^{-}} \left[ - \ln \frac{ e^{ f(x)^T f(x^{+}) } }{ e^{ f(x)^T f(x^{+}) } + e^{ f(x)^T f(x^{-}) } } \right]. $$</span>
</div>
<div style=margin-left:2em>
<span style=display:block;font-size:90%>
<i class="far fa-list-alt"></i> Pages: 7</span>
</div>
</div>
<div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div>
<div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork>
<h1 class=blog-timestamp>
</h1>
<h1 class=title>
<a href=https://datumorphism.leima.is/cards/machine-learning/learning-theories/set-shatter/ itemprop=headline> Shatter</a>
</h1>
<div style=margin-left:2em>
<span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>
Published: <time datetime=2021-10-27T00:00:00+00:00>2021-10-27</time>
</span>
</div>
<div style=margin-left:2em>
<span style=display:block;font-size:90%><i class="far fa-folder" aria-hidden=true></i> Category: { Machine Learning::Theories }
</span>
</div>
<div style=margin-left:2em>
<span style=display:block;font-size:90%><i class="fa fa-hashtag" aria-hidden=true></i> Tags: <div class="tags is-small" style=display:inline>
<a href=/tags/learning-theory style=margin-right:.5em><span class="tag is-warning is-small is-light">#Learning Theory</span></a>
<a href=/tags/set style=margin-right:.5em><span class="tag is-warning is-small is-light">#Set</span></a>
<a href=/tags/shatter style=margin-right:.5em><span class="tag is-warning is-small is-light">#Shatter</span></a>
</div>
</span>
</div>
<div style=margin-left:2em><span style=display:block;font-size:90%>
<i class="fa fa-paperclip" aria-hidden=true></i> References:</span>
<span style=display:block;font-size:90%;padding-left:1em> - <a href=https://en.wikipedia.org/wiki/Shattered_set>Shattered Set @ Wikipedia</a></span>
</div>
<div style=margin-left:2em>
<span style=display:block;font-size:90%>
<i class="far fa-star"></i> Summary: Given a set $\mathcal S$, and a class (collection of sets) $\mathcal H$.
For any subset of $\mathcal S$, denoted as $\mathcal s$, if we have an element of class $\mathcal H$, denoted as $\mathcal h$, that leads to1
$$ \mathcal h \cap \mathcal S = \mathcal s. $$
Since the power set of $\mathcal S$ ($P(\mathcal S)$) contains all the possible subsets of $\mathcal S$, we can also rephrase the concept using power set. If we can find the power set $P(\mathcal S)$ by looking into intersections of elements $\mathcal h$ of $\mathcal H$ ($\mathcal h\in \mathcal H$), then we say $\mathcal H$ shatters $\mathcal S$ 1.</span>
</div>
<div style=margin-left:2em>
<span style=display:block;font-size:90%>
<i class="far fa-list-alt"></i> Pages: 7</span>
</div>
</div>
<div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div>
</div>
</div>
<br>
<div class=container>
</div>
</div>
</section>
</div>
</div>
</main>
<footer>
<footer class=footer>
<div class=container>
<div class="content has-text-centered">
<p>
Created and maintained by <a href=https://leima.is>L Ma</a>.
Acknowledgement: <a href=https://gohugo.io/>Hugo</a>,
<a href=https://themes.gohugo.io/bulma/>Bulma</a>, <a href=https://kausalflow.com>KausalFlow</a>.
<strong>love</strong>.
<br>
<a class=tag href=/about>About</a>
<a class=tag href=/index.xml>Feed</a>
<a class=tag href=/data.json>JSON Data</a>
</p>
</div>
</div>
</footer>
</footer>
<script async type=text/javascript src=/js/bulma.js></script>
</body>
</html>