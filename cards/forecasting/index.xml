<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Time Series Forecasting on Datumorphism</title><link>https://datumorphism.leima.is/cards/forecasting/</link><description>Recent content in Time Series Forecasting on Datumorphism</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Mon, 28 Nov 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://datumorphism.leima.is/cards/forecasting/index.xml" rel="self" type="application/rss+xml"/><item><title>Prediction Space in Forecasting</title><link>https://datumorphism.leima.is/cards/forecasting/prediction-space/</link><pubDate>Fri, 08 Apr 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/forecasting/prediction-space/</guid><description>In a forecasting problem, we have
$\mathcal P$, the priors, e.g., price and demand is negatively correlated, $\mathcal D$, available dataset, $Y$, the observations, and $F$, the forecasts. Information Set $\mathcal A$
The priors $\mathcal D$ and the available data $\mathcal P$ can be summarized together as the information set $\mathcal A$. Under a probabilistic view, a forecaster will find out or approximate a CDF $\mathcal F$ such that1
$$ \mathcal F(Y\vert \mathcal D, \mathcal P) \to F. $$
Naively speaking, once the density $\rho(F, Y)$ is determined or estimated, a probabilistic forecaster can be formed. The joint probability of $(F, Y)$ is our prediction space.</description></item><item><title>Time Convolution</title><link>https://datumorphism.leima.is/cards/forecasting/time-convolution/</link><pubDate>Mon, 28 Nov 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/forecasting/time-convolution/</guid><description>The temporal convolution is responsible for capturing temporal patterns in a sequence.
Dilated Temporal Convolution Unit8 has a nice blog about temporal convolution and dilated temporal convolution1. In this
Convolutions Using Fourier Transform Convolution and Fourier transform Dilated Convolution For a convolution $$ f*h(x) = \sum_{s+t=x} f(s) h(t), $$ the dilated version of it is1 $$ f*_l h(x) = \sum_{s+t*l=x} f(s) h(t), $$ where $l$ is the dilation factor. Yu2015 Yu F, Koltun V. Multi-Scale Context Aggregation by Dilated Convolutions. arXiv [cs.CV]. 2015. Available: http://arxiv.org/abs/1511.07122 &amp;#160;&amp;#x21a9;&amp;#xfe0e; Inception A good convolutional network should capture both short-term and long-term patterns in the time series data.</description></item><item><title>Mix-hop Propagation in GNN</title><link>https://datumorphism.leima.is/cards/forecasting/gnn-mix-hop-propagation/</link><pubDate>Mon, 28 Nov 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/forecasting/gnn-mix-hop-propagation/</guid><description>The mix-hop propagation layer has two steps1:
information propagation step: $$ \mathbf H^{(k)} = \beta \mathbf H_{in} + (1-\beta)\mathbf L \mathbf H^{(k-1)}, $$
where $\mathbf L= (1+ \operatorname{A}) (\mathbf A + \mathbf I)$. This convolution step tries to disentangle the correlation between the nodes. information selection step: $$ \mathbf H_{out} = \sum_k \mathbf H^{(k)} \mathbf W^{(k)}. $$
See Fig 4 in the paper1.
Wu2020 Wu Z, Pan S, Long G, Jiang J, Chang X, Zhang C. Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2005.11650 &amp;#160;&amp;#x21a9;&amp;#xfe0e;&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Graph Structure Learning in GNN</title><link>https://datumorphism.leima.is/cards/forecasting/gnn-graph-structure-learning/</link><pubDate>Mon, 28 Nov 2022 00:00:00 +0000</pubDate><guid>https://datumorphism.leima.is/cards/forecasting/gnn-graph-structure-learning/</guid><description>We extract the definitions in Wu et al., 20201. Given node embeddings $\mathbf E_i$1,
$$ \begin{align} \mathbf M_i &amp;= \tanh(\alpha \mathbf E_i \Theta_i) \\ \mathbf A &amp;= \operatorname{ReLU}(\tanh(\alpha (\mathbf M_1 \mathbf M_2^T - \mathbf M_2\mathbf M_1^T))), \end{align} $$
The author also proposed sparse requirement and only take the top-$k$ largest elements in $A$.
Wu2020 Wu Z, Pan S, Long G, Jiang J, Chang X, Zhang C. Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2005.11650 &amp;#160;&amp;#x21a9;&amp;#xfe0e;&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item></channel></rss>