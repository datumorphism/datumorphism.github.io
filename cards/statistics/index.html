<!doctype html><html lang=en-us><head><meta name=generator content="Hugo 0.69.2"><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><title>Statistics | Datumorphism | Lei Ma</title><meta name=description content="Knowledge cards about statistics"><meta name=robots content="noindex"><meta name=author content="Lei Ma"><meta property="og:title" content="Statistics"><meta property="og:description" content="Knowledge cards about statistics"><meta property="og:type" content="website"><meta property="og:url" content="/cards/statistics/"><meta property="og:updated_time" content="2020-10-27T00:00:00+00:00"><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-140452515-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><link href=/cards/statistics/index.xml rel=alternate type=application/rss+xml title=Datumorphism><link rel=canonical href=/cards/statistics/><link rel="shortcut icon" type=image/png href=/logos/logo-square.png><link rel=stylesheet href=/css/bulma.css><link rel=stylesheet href=/css/bulma-divider.min.css><link rel=stylesheet href=/assets/css/bulma-ribbon.min.css><link rel=stylesheet href=/assets/css/tooltip.css><link rel=stylesheet href=https://jenil.github.io/bulmaswatch/united/bulmaswatch.min.css><link rel=stylesheet href=/css/custom.css><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],tags:'ams'},svg:{fontCache:'global'}};</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js></script><script src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script>mermaid.initialize({startOnLoad:true});</script><script src=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/js/all.min.js integrity="sha256-KzZiKy0DWYsnwMF+X1DvQngQ2/FxF7MF3Ff72XcpuPs=" crossorigin=anonymous></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css integrity="sha256-h20CPZ0QyXlBuAw7A+KluUYx/3pK+c7lYEpqLTlxjYQ=" crossorigin=anonymous></head><body><header><nav class="navbar is-transparent"><div class=navbar-brand><a class=navbar-item href=/><img src=/site/intelligence.png alt=Datumorphism height=28 style=margin-right:.5em> Datumorphism</a><div class="navbar-burger burger" data-target=navMenu><span></span><span></span><span></span></div></div><div class=navbar-menu id=navMenu><div class=navbar-start><div class="navbar-item has-dropdown is-hoverable"><a href=/projects class=navbar-link>Notebooks</a><div class=navbar-dropdown><a href=/awesome/ class=navbar-item>Awesome</a>
<a href=/blog/ class=navbar-item>Blog</a>
<a href=/cards/ class=navbar-item>Cards</a>
<a href=/reading/ class=navbar-item>Reading Notes</a>
<a href=/til/ class=navbar-item>TIL</a>
<a href=/wiki/ class=navbar-item>Wiki</a></div></div></div><span class=navbar-burger><span></span><span></span><span></span></span><div class=navbar-end><div class=navbar-item><a class=navbar-item target=blank href=/blog/>Blog</a>
<a class=navbar-item target=blank href=/tags/><i class="fas fa-tags"></i></a><a class=navbar-item target=blank href=/graph><i class="fas fa-project-diagram"></i></a><a class=navbar-item target=blank href=https://github.com/datumorphism><span class=icon><i class="fab fa-github"></i></span></a></div></div></div></nav><script>document.addEventListener('DOMContentLoaded',()=>{const $navbarBurgers=Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'),0);if($navbarBurgers.length>0){$navbarBurgers.forEach(el=>{el.addEventListener('click',()=>{const target=el.dataset.target;const $target=document.getElementById(target);el.classList.toggle('is-active');$target.classList.toggle('is-active');});});}});</script></header><main><section><div class="hero is-dark is-medium"><div class=hero-body><div class="container has-text-centered"><h1 class="title is-1">Statistics</h1><h2 class="subtitle is-3">Knowledge cards about statistics</h2></div></div></div></section><div class="columns is-fullheight"><div class=column><section class="hero is-default is-bold"><div class=hero-body><div class=container><div class="columns is-desktop is-multiline"><div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork><h1 class=blog-timestamp></h1><h1 class=title><a href=/cards/statistics/conditional-probability-table/ itemprop=headline>Conditional Probability Table</a></h1><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>Published: <time datetime=2020-10-27T00:00:00+00:00>2020-10-27</time></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-folder" aria-hidden=true></i>Category: { Math }</span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-hashtag" aria-hidden=true></i>Tags:<div class="tags is-small" style=display:inline><a href=/tags/statistics style=margin-right:.5em><span class="tag is-warning is-small is-light">#Statistics</span></a>
<a href=/tags/basics style=margin-right:.5em><span class="tag is-warning is-small is-light">#Basics</span></a></div></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-paperclip" aria-hidden=true></i>References:</span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=https://en.wikipedia.org/wiki/Bayes%27_theorem#/media/File:Bayes%27_Theorem_2D.svg>Tree Diagram of Bayes' Theorem @ Wikipedia</a></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-star"></i>Summary: The conditional probability table is also called CPT</span></div></div><div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div><div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork><h1 class=blog-timestamp></h1><h1 class=title><a href=/cards/statistics/distributions/gamma/ itemprop=headline>Gamma Distribution</a></h1><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>Published: <time datetime=2020-03-14T00:00:00+00:00>2020-03-14</time></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-folder" aria-hidden=true></i>Category: { Statistics }</span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-paperclip" aria-hidden=true></i>References:</span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=https://en.wikipedia.org/wiki/Gamma_distribution>Gamma Distribution @ Wikipedia</a></span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=https://stdlib.io/docs/api/v0.0.90/@stdlib/stats/base/dists/gamma>stdlib.js documentation</a></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-star"></i>Summary: Gamma Distribution PDF:
$$ \frac{\beta^\alpha x^{\alpha-1} e^{-\beta x}}{\Gamma(\alpha)} $$
Visualize</span></div></div><div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div><div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork><h1 class=blog-timestamp></h1><h1 class=title><a href=/cards/statistics/distributions/cauchy/ itemprop=headline>Cauchy-Lorentz Distribution</a></h1><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>Published: <time datetime=2020-03-14T00:00:00+00:00>2020-03-14</time></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-folder" aria-hidden=true></i>Category: { Statistics }</span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-paperclip" aria-hidden=true></i>References:</span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=https://en.wikipedia.org/wiki/Cauchy_distribution>Cauchy Distribution @ Wikipedia</a></span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=https://stdlib.io/docs/api/v0.0.90/@stdlib/stats/base/dists/cauchy>stdlib.js documentation</a></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-star"></i>Summary: Cauchy-Lorentz Distribution .. ratio of two independent normally distributed random variables with mean zero.
Source: https://en.wikipedia.org/wiki/Cauchy_distribution
Lorentz distribution is frequently used in physics.
PDF:
$$ \frac{1}{\pi\gamma} \left( \frac{\gamma^2}{ (x-x_0)^2 + \gamma^2} \right) $$
The median and mode of the Cauchy-Lorentz distribution is always $x_0$. $\gamma$ is the FWHM.
Visualize</span></div></div><div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div><div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork><h1 class=blog-timestamp></h1><h1 class=title><a href=/cards/statistics/distributions/binomial/ itemprop=headline>Binomial Distribution</a></h1><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>Published: <time datetime=2020-03-14T00:00:00+00:00>2020-03-14</time></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-folder" aria-hidden=true></i>Category: { Statistics }</span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-paperclip" aria-hidden=true></i>References:</span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=https://en.wikipedia.org/wiki/Binomial_distribution>Binomial Distribution @ Wikipedia</a></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-star"></i>Summary: The number of successes in $n$ independent events where each trial has a success rate of $p$.
PMF:
$$ C_n^k p^k (1-p)^{n-k} $$</span></div></div><div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div><div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork><h1 class=blog-timestamp></h1><h1 class=title><a href=/cards/statistics/distributions/beta/ itemprop=headline>Beta Distribution</a></h1><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>Published: <time datetime=2020-03-14T00:00:00+00:00>2020-03-14</time></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-folder" aria-hidden=true></i>Category: { Statistics }</span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-paperclip" aria-hidden=true></i>References:</span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=https://en.wikipedia.org/wiki/Beta_distribution>Beta Distribution @ Wikipedia</a></span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=https://stdlib.io/docs/api/v0.0.90/@stdlib/stats/base/dists/beta>stdlib.js documentation</a></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-star"></i>Summary: Beta Distribution Interact {% include extras/vue.html %}
((makeGraph))</span></div></div><div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div><div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork><h1 class=blog-timestamp></h1><h1 class=title><a href=/cards/statistics/distributions/bernoulli/ itemprop=headline>Bernoulli Distribution</a></h1><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>Published: <time datetime=2020-03-14T00:00:00+00:00>2020-03-14</time></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-folder" aria-hidden=true></i>Category: { Statistics }</span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-paperclip" aria-hidden=true></i>References:</span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=https://en.wikipedia.org/wiki/Bernoulli_distribution>Bernoulli Distribution @ Wikipedia</a></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-star"></i>Summary: Two categories with probability $p$ and $1-p$ respectively.</span></div></div><div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div><div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork><h1 class=blog-timestamp></h1><h1 class=title><a href=/cards/statistics/distributions/arcsine/ itemprop=headline>Arcsine Distribution</a></h1><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>Published: <time datetime=2020-03-14T00:00:00+00:00>2020-03-14</time></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-folder" aria-hidden=true></i>Category: { Statistics }</span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-paperclip" aria-hidden=true></i>References:</span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=https://en.wikipedia.org/wiki/Arcsine_distribution>Arcsine Distribution @ Wikipedia</a></span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=https://stdlib.io/docs/api/v0.0.90/@stdlib/stats/base/dists/arcsine>stdlib.js documentation</a></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-star"></i>Summary: Arcsine Distribution The PDF is
$$ \frac{1}{\pi\sqrt{x(1-x)}} $$
for $x\in [0,1]$.
It can also be generalized to
$$ \frac{1}{\pi\sqrt{(x-1)(b-x)}} $$
for $x\in [a,b]$.
Visualize</span></div></div><div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div><div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork><h1 class=blog-timestamp></h1><h1 class=title><a href=/cards/statistics/covariance-matrix/ itemprop=headline>Covariance Matrix</a></h1><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>Published: <time datetime=2020-03-10T00:00:00+00:00>2020-03-10</time></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-folder" aria-hidden=true></i>Category: { Math }</span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-hashtag" aria-hidden=true></i>Tags:<div class="tags is-small" style=display:inline><a href=/tags/statistics style=margin-right:.5em><span class="tag is-warning is-small is-light">#Statistics</span></a>
<a href=/tags/basics style=margin-right:.5em><span class="tag is-warning is-small is-light">#Basics</span></a></div></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-paperclip" aria-hidden=true></i>References:</span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=https://en.wikipedia.org/wiki/Covariance_matrix>Covariance Matrix @ Wikipedia</a></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-star"></i>Summary: Also known as the second central moment is a measurement of the spread.</span></div></div><div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div><div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork><h1 class=blog-timestamp></h1><h1 class=title><a href=/cards/statistics/jacknife-resampling/ itemprop=headline>Jackknife Resampling</a></h1><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>Published: <time datetime=2020-01-26T00:00:00+00:00>2020-01-26</time></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-folder" aria-hidden=true></i>Category: { Statistics }</span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-hashtag" aria-hidden=true></i>Tags:<div class="tags is-small" style=display:inline><a href=/tags/statistics style=margin-right:.5em><span class="tag is-warning is-small is-light">#Statistics</span></a></div></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-paperclip" aria-hidden=true></i>References:</span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=https://en.wikipedia.org/wiki/Jackknife_resampling>Jackknife Resampling</a></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-star"></i>Summary: Jackknife resampling method</span></div></div><div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div><div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork><h1 class=blog-timestamp></h1><h1 class=title><a href=/cards/statistics/kendall-correlation-coefficient/ itemprop=headline>Kendall Tau Correlation</a></h1><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>Published: <time datetime=2019-07-20T00:00:00+00:00>2019-07-20</time></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-folder" aria-hidden=true></i>Category: { Statistics }</span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-hashtag" aria-hidden=true></i>Tags:<div class="tags is-small" style=display:inline><a href=/tags/statistics style=margin-right:.5em><span class="tag is-warning is-small is-light">#Statistics</span></a>
<a href=/tags/correlation style=margin-right:.5em><span class="tag is-warning is-small is-light">#Correlation</span></a></div></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-paperclip" aria-hidden=true></i>References:</span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href="https://academic.oup.com/biomet/article-abstract/30/1-2/81/176907?redirectedFrom=fulltext">Kendall, M. G. (1938). A new measure of correlation. Biometrika, 30(1–2), 81–93.</a></span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient>Kendall rank correlation coefficient</a></span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=https://en.wikipedia.org/wiki/Rank_correlation>Rank correlation</a></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-star"></i>Summary: Definition two series of data: $X$ and $Y$ cooccurance of them: $(x_i, x_j)$, and we assume that $i&lt;j$ concordant: $x_i &lt; x_j$ and $y_i &lt; y_j$; $x_i > x_j$ and $y_i > y_j$; denoted as $C$ discordant: $x_i &lt; x_j$ and $y_i > y_j$; $x_i > x_j$ and $y_i &lt; y_j$; denoted as $D$ neither concordant nor discordant: whenever equal sign happens Kendall&rsquo;s tau is defined as
$$ \begin{equation} \tau = \frac{C- D}{\text{all possible pairs of comparison}} = \frac{C- D}{n^2/2 - n/2} \end{equation} $$</span></div></div><div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div><div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork><h1 class=blog-timestamp></h1><h1 class=title><a href=/cards/statistics/poisson-process/ itemprop=headline>Poisson Process</a></h1><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>Published: <time datetime=2019-06-18T00:00:00+00:00>2019-06-18</time></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-folder" aria-hidden=true></i>Category: { Statistics }</span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-hashtag" aria-hidden=true></i>Tags:<div class="tags is-small" style=display:inline><a href=/tags/statistics style=margin-right:.5em><span class="tag is-warning is-small is-light">#Statistics</span></a>
<a href=/tags/poisson-process style=margin-right:.5em><span class="tag is-warning is-small is-light">#Poisson Process</span></a></div></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-paperclip" aria-hidden=true></i>References:</span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=https://github.com/axelpale/poisson-process>axelpale/poisson-process</a></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-star"></i>Summary: Poisson Process Statistics // define getUnixTime Date.prototype.getUnixTime = function () { return this.getTime() / 1000 | 0 }; if (!Date.now) Date.now = function () { return new Date(); } Date.time = function () { return Date.now().getUnixTime(); } POISSON_EVENT_RATE = 1 function get_event_time() { var time = new Date(); return time } all_event = [] all_event_diff = [] var data = [{ x: [get_event_time], y: [1], mode: 'markers', line: { color: '#80CAF6' } }] var layout = { title: { text: 'Poisson Process' }, xaxis: { title: { text: 'Event Time' }, } }; var layout_rate = { title: { text: 'Average Rate of the Poisson Process' }, xaxis: { title: { text: 'Event Time' }, }, yaxis: { title: { text: 'Average Event Rate per Second' }, rangemode: 'tozero' } }; var data_rate = [{ x: [get_event_time], y: [POISSON_EVENT_RATE], mode: 'lines+markers', line: { color: '#80CAF6' } }] Plotly.</span></div></div><div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div><div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork><h1 class=blog-timestamp></h1><h1 class=title><a href=/cards/statistics/bayes-theorem/ itemprop=headline>Bayes' Theorem</a></h1><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>Published: <time datetime=2019-06-18T00:00:00+00:00>2019-06-18</time></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-folder" aria-hidden=true></i>Category: { Math }</span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-hashtag" aria-hidden=true></i>Tags:<div class="tags is-small" style=display:inline><a href=/tags/statistics style=margin-right:.5em><span class="tag is-warning is-small is-light">#Statistics</span></a>
<a href=/tags/bayesian style=margin-right:.5em><span class="tag is-warning is-small is-light">#Bayesian</span></a></div></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-paperclip" aria-hidden=true></i>References:</span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=https://en.wikipedia.org/wiki/Bayes%27_theorem#/media/File:Bayes%27_Theorem_2D.svg>Tree Diagram of Bayes Theorem @ Wikipedia</a></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-star"></i>Summary: Bayes&rsquo; Theorem is stated as
$$ P(A\mid B) = \frac{P(B \mid A) P(A)}{P(B)} $$
$P(A\mid B)$: likelihood of A given B $P(A)$: marginal probability of A There is a nice tree diagram for the Bayes&rsquo; theorem on Wikipedia.
Tree diagram of Bayes&rsquo; theorem</span></div></div><div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div><div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork><h1 class=blog-timestamp></h1><h1 class=title><a href=/cards/statistics/nml/ itemprop=headline>Normalized Maximum Likelihood</a></h1><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>Published: <time datetime=0001-01-01T00:00:00+00:00>0001-01-01</time></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-hashtag" aria-hidden=true></i>Tags:<div class="tags is-small" style=display:inline><a href=/tags/nml style=margin-right:.5em><span class="tag is-warning is-small is-light">#NML</span></a></div></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-star"></i>Summary: $$ \mathrm{NML} = \frac{ p(y| \hat \theta(y)) }{ \int_X p( x| \hat \theta (x) ) dx } $$</span></div></div><div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div><div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork><h1 class=blog-timestamp></h1><h1 class=title><a href=/cards/statistics/mdl/ itemprop=headline>Minimum Description Length</a></h1><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>Published: <time datetime=0001-01-01T00:00:00+00:00>0001-01-01</time></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-hashtag" aria-hidden=true></i>Tags:<div class="tags is-small" style=display:inline><a href=/tags/mdl style=margin-right:.5em><span class="tag is-warning is-small is-light">#MDL</span></a></div></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-paperclip" aria-hidden=true></i>References:</span>
<span style=display:block;font-size:90%;padding-left:1em>- Vandekerckhove, J., & Matzke, D. (2015). Model comparison and the principle of parsimony. Oxford Library of Psychology.</span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-star"></i>Summary: The minimum description length ( #MDL ) is based on the idea of compression of the data.
MDL looks for the model that compresses the data well. To compress data, we need to find the regularity in the data.
There are many versions of MDL.
crude two-part code Fisher information approximation ( # FIA ) Normalized Maximum likelihood ( #NML )</span></div></div><div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div><div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork><h1 class=blog-timestamp></h1><h1 class=title><a href=/cards/statistics/kolmogorov-complexity/ itemprop=headline>Kolmogorov Complexity</a></h1><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>Published: <time datetime=0001-01-01T00:00:00+00:00>0001-01-01</time></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-paperclip" aria-hidden=true></i>References:</span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=https://people.cs.uchicago.edu/~fortnow/papers/kaikoura.pdf>Fortnow, L. (2000). Kolmogorov complexity. (January), 1–14.</a></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-star"></i>Summary: Description:
$\Sigma=\{0,1\}$, a map $f:\Sigma^* \to\Sigma^*$. To describe a string of 0 and 1 $\sigma$, the description is a map so that $f(\tau)=\sigma$.
Kolmogorov complexity $C_f$
$$ C_f(x) = \begin{cases} min\{ \vert p \vert : f(p) = x & \text{if x} \\ \infty & \text{otherwise} \} \end{cases} $$ $f$ can be a universal turing machine.</span></div></div><div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div><div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork><h1 class=blog-timestamp></h1><h1 class=title><a href=/cards/statistics/fia/ itemprop=headline>Fisher Information Approximation</a></h1><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>Published: <time datetime=0001-01-01T00:00:00+00:00>0001-01-01</time></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-hashtag" aria-hidden=true></i>Tags:<div class="tags is-small" style=display:inline><a href=/tags/fia style=margin-right:.5em><span class="tag is-warning is-small is-light">#FIA</span></a></div></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-star"></i>Summary: #FIA is a method to describe the [[minimum-description-length|minimum description length ( #MDL )]] of models,
$$ \mathrm{FIA} = -\ln p(y | \hat\theta) + \frac{k}{2} \ln \frac{n}{2\pi} + \ln \int_\Theta \sqrt{ \operatorname{det}[I(\theta)] d\theta } $$
$I(\theta)$: Fisher information matrix of sample size 1. $$I_{i,j}(\theta) = E\left( \frac{\partial \ln p(y| \theta)}{\partial \theta_i}\frac{ \partial \ln p (y | \theta) }{ \partial \theta_j } \right)$$.</span></div></div><div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div><div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork><h1 class=blog-timestamp></h1><h1 class=title><a href=/cards/statistics/bic/ itemprop=headline>Bayesian Information Criterion</a></h1><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>Published: <time datetime=0001-01-01T00:00:00+00:00>0001-01-01</time></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-paperclip" aria-hidden=true></i>References:</span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199957996.001.0001/oxfordhb-9780199957996-e-14>Vandekerckhove, J., & Matzke, D. (2015). Model comparison and the principle of parsimony. Oxford Library of Psychology.</a></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-star"></i>Summary: BIC is Bayesian information criterion, it replaced the $+2k$ term in AIC with $k\ln n$
$$ \mathrm{BIC} = -2\ln p(y|\hat\theta) + k\ln n = \ln \left(\frac{n^k}{p^2}\right) $$
$n$ is the observations. We prefer the model with a small BIC.</span></div></div><div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div><div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork><h1 class=blog-timestamp></h1><h1 class=title><a href=/cards/statistics/bayes-factors/ itemprop=headline>Bayes Factors</a></h1><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>Published: <time datetime=0001-01-01T00:00:00+00:00>0001-01-01</time></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-hashtag" aria-hidden=true></i>Tags:<div class="tags is-small" style=display:inline><a href=/tags/bayes style=margin-right:.5em><span class="tag is-warning is-small is-light">#Bayes</span></a></div></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-star"></i>Summary: $$ \frac{p(\mathscr M_1|y)}{ p(\mathscr M_2|y) } = \frac{p(\mathscr M_1)}{ p(\mathscr M_2) }\frac{p(y|\mathscr M_1)}{ p(y|\mathscr M_2) } $$
Bayes factor
$$ \mathrm{BF_{12}} = \frac{m(y|\mathscr M_1)}{m(y|\mathscr M_2)} $$
$\mathrm{BF_{12}}$: how many time more likely is model $\mathscr M_1$ than $\mathscr M_2$.</span></div></div><div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div><div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork><h1 class=blog-timestamp></h1><h1 class=title><a href=/cards/statistics/aic/ itemprop=headline>Akaike Information Criterion</a></h1><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>Published: <time datetime=0001-01-01T00:00:00+00:00>0001-01-01</time></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-paperclip" aria-hidden=true></i>References:</span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href="https://en.wikipedia.org/wiki/Akaike_information_criterion#:~:text=The%20Akaike%20information%20criterion%20%28AIC,response%20to%20a%20training%20sample.">Akaike Information Criterion @ Wikipedia</a></span>
<span style=display:block;font-size:90%;padding-left:1em>- Vandekerckhove, J., & Matzke, D. (2015). Model comparison and the principle of parsimony. Oxford Library of Psychology.</span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-star"></i>Summary: Suppose we have a model that describes the data generation process behind a dataset. The distribution by the model is denoted as $\hat f$. The actual data generation process is described by a distribution $f$.
We ask the question:
How good is the approximation using $\hat f$?
To be more precise, how much information is lost if we use our model dist $\hat f$ to substitute the actual data generation distribution $f$?</span></div></div><div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div></div></div><br><div class=container></div></div></section></div></div></main><footer><footer class=footer><div class=container><div class="content has-text-centered"><p>Created and maintained by <a href=/>Lei Ma</a>.
Acknowledgement: <a href=https://gohugo.io/>Hugo</a>,
<a href=https://themes.gohugo.io/bulma/>Bulma</a>, <a href=https://kausalflow.com>KausalFlow</a>.
<strong>love</strong>.<br><a class=tag href=/about>About</a>
<a class=tag href=/index.xml>Feed</a>
<a class=tag href=/data.json>JSON Data</a></p></div></div></footer></footer><script async type=text/javascript src=/js/bulma.js></script></body></html>