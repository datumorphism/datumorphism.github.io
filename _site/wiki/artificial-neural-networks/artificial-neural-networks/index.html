<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.4.0 -->
<title>Artificial Neural Networks | Datumorphism</title>
<meta name="generator" content="Jekyll v3.7.3" />
<meta property="og:title" content="Artificial Neural Networks" />
<meta name="author" content="Datumorphism Collective" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Solving PDEs" />
<meta property="og:description" content="Solving PDEs" />
<link rel="canonical" href="http://localhost:4000/wiki/artificial-neural-networks/artificial-neural-networks/" />
<meta property="og:url" content="http://localhost:4000/wiki/artificial-neural-networks/artificial-neural-networks/" />
<meta property="og:site_name" content="Datumorphism" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-11-19T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"Solving PDEs","author":{"@type":"Person","name":"Datumorphism Collective"},"@type":"BlogPosting","url":"http://localhost:4000/wiki/artificial-neural-networks/artificial-neural-networks/","headline":"Artificial Neural Networks","dateModified":"2018-11-19T00:00:00-05:00","datePublished":"2018-11-19T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/wiki/artificial-neural-networks/artificial-neural-networks/"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css">
  <link rel="stylesheet" href="/assets/bulma/bulma.css">
  <link rel="stylesheet" href="/assets/bulma/bulma-divider.min.css">
  <link rel="stylesheet" href="/assets/bulmawatch/united/bulmaswatch.min.css">
  <link rel="stylesheet" href="/assets/fontawesome/web-fonts-with-css/css/fontawesome-all.min.css">
  <link rel="stylesheet" href="/assets/code/base16-solarized.light.css">


  <link rel="apple-touch-icon" sizes="57x57" href="/assets/favicon/apple-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/assets/favicon/apple-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/assets/favicon/apple-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/assets/favicon/apple-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/assets/favicon/apple-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/assets/favicon/apple-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/assets/favicon/apple-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/assets/favicon/apple-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/favicon/apple-icon-180x180.png">
  <link rel="icon" type="image/png" sizes="192x192"  href="/assets/favicon/android-icon-192x192.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="96x96" href="/assets/favicon/favicon-96x96.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon/favicon-16x16.png">
  <link rel="manifest" href="/assets/favicon/manifest.json">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="msapplication-TileImage" content="/assets/favicon/ms-icon-144x144.png">
  <meta name="theme-color" content="#ffffff">



  

  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      TeX: { extensions: ["color.js","cancel.js", "AMSmath.js", "AMSsymbols.js"] },
      messageStyle: "none"
   });
  </script>
  <script type="text/x-mathjax-config">
  MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
    MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
      cancel: ["Extension","cancel"]
      });
   });
   </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
        inlineMath: [['$','$'], ['\\(','\\)']]
      }
    });
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      processEscapes: true
    }
  });
</script>
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      Macros: {
        overlr: ['\\overset\\leftrightarrow{\#1}',1],
        overl: ['\\overset\leftarrow{\#1}',1],
        overr: ['\\overset\rightarrow{\#1}',1],
        bra: ['\\left\\langle \#1\\right|',1],
        ket: ['\\left| \#1\\right\\rangle',1],
        braket: ['\\langle \#1 \\mid \#2 \\rangle',2],
        avg: ['\\left< \#1 \\right>',1],
        slashed: ['\\cancel{\#1}',1],
        bold: ['\\boldsymbol{\#1}',1],
        sech: ['\\operatorname{sech}{\#1}',1],
        csch: ['\\operatorname{csch}{\#1}',1]
      },
      equationNumbers: { autoNumber: "AMS" }
    }
  });
  </script>
  <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>


  <!-- <link rel="stylesheet" href="//cdn.jsdelivr.net/chartist.js/latest/chartist.min.css"> --><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Datumorphism" /></head>



  <body><header class="site-header" role="banner"><section class="hero is-primary is-medium">
    <!-- Hero head: will stick at the top -->
    <!-- <div class="hero-head"> -->
      <nav class="navbar" role="navigation" aria-label="main navigation">
        <div class="container">
          <div class="navbar-brand">
            <a class="navbar-item has-text-weight-bold" rel="author" href="/">Datumorphism</a>
            <!-- <img src="https://bulma.io/images/bulma-type-white.png" alt="Logo"> -->
            <span class="navbar-burger burger" data-target="navbarMenuHeroA">
            <span></span>
            <span></span>
            <span></span>
            </span>
          </div>
          <div id="navbarMenuHeroA" class="navbar-menu">
            <div class="navbar-end">

              <a class="navbar-item has-text-weight-bold" href="/"><i class="fas fa-home"></i></a>

              
              
              <a class="navbar-item has-text-weight-bold" href="http://localhost:4000/til/" >TIL</a>
              
              
              <a class="navbar-item has-text-weight-bold" href="http://localhost:4000/wiki/" >Wiki</a>
              
              
              <a class="navbar-item has-text-weight-bold" href="http://localhost:4000/reading/" >Reading</a>
              
              
              <a class="navbar-item has-text-weight-bold" href="http://localhost:4000/awesome/" >Awesome</a>
              
              
              <a class="navbar-item has-text-weight-bold" href="http://localhost:4000/blog/" >Blog</a>
              

              

               
              
              
              
              
              
              
              
              <a class="navbar-item has-text-weight-bold" href="/about/">About</a>
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              

            </div>
          </div>
        </div>
      </nav>
    <!-- </div> -->
  </section>
  <script>
  document.addEventListener('DOMContentLoaded', function () {

  // Get all "navbar-burger" elements
  var $navbarBurgers = Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'), 0);

  // Check if there are any navbar burgers
  if ($navbarBurgers.length > 0) {

    // Add a click event on each of them
    $navbarBurgers.forEach(function ($el) {
      $el.addEventListener('click', function () {

        // Get the target from the "data-target" attribute
        var target = $el.dataset.target;
        var $target = document.getElementById(target);

        // Toggle the class on both the "navbar-burger" and the "navbar-menu"
        $el.classList.toggle('is-active');
        $target.classList.toggle('is-active');

      });
    });
  }

});
  </script>

</header>
<main class="main--content" aria-label="Content">
    <div class="columns">
      <div class="column is-12">
        <div class="content is-medium">
          <article class="post">

  <header class="post-header">
    <h1 class="post-title has-text-centered is-size-1" itemprop="name headline">Artificial Neural Networks</h1>
    <p class="post-meta has-text-centered">
      <time class="dt-published" datetime="2018-11-19T00:00:00-05:00" itemprop="datePublished">Nov 19, 2018
      </time>
       • <span style="font-style: regular;">{</span>
       <span style="padding-right: 0.2em;padding-left: 0.2em;"><a href="http://localhost:4000/wiki/#artificial-neural-networks">⸢Artificial Neural Networks⸥</a></span>
       
       }
      
      
       • <span style="font-style: regular;">[</span>
       <span style="padding-right: 0.2em;padding-left: 0.2em;"><a href="http://localhost:4000/wiki/?keywords=Machine Learning">#Machine Learning</a></span>
       
       <span style="padding-right: 0.2em;padding-left: 0.2em;"><a href="http://localhost:4000/wiki/?keywords=Artificial Neural Networks">#Artificial Neural Networks</a></span>
       
       <span style="padding-right: 0.2em;padding-left: 0.2em;"><a href="http://localhost:4000/wiki/?keywords=Basics">#Basics</a></span>
       
       ]
      
      
      
      <div class="notification is-primary">
         Machine Learning!
       </div>
      
    </p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>Artificial neural networks works pretty well for some equation solving.</p>

<h2 id="universal-approximators">Universal Approximators</h2>

<p>Mawell Stinchcombe and Halber White proved that no theoretical constraints for the feedforward networks to approximate any measureable function. In principle one can use feedforward networks to approximate measurable functions to any accuracy.</p>

<p>However the convergence slows done if we have a lot of hidden units. There is a balance between accuracy and convergence rate. More hidden units means slow convergence but more accuracy.</p>

<p>Here is a quick review of the history of this topic.</p>

<div class="notes--info">
  <p><strong>Kolmogorov’s Theorem</strong></p>

  <p>Kolmogorov’s theorem shows that one can use finite number of carefully chosen continuous functions to exactly mix up by sums and multiplication with weights to a continuous multivariable fnction on a copact set.</p>

  <p><a href="http://neuron.eng.wayne.edu/tarek/MITbook/chap2/2_3.html">Here is the exact math.</a></p>
</div>

<ol>
  <li>
    <p>Cybenko 1989</p>

    <p>Cybenko proved that</p>

    <script type="math/tex; mode=display">\sum_k v_k \sigma(w_k x + u_k)</script>

    <p>is a good approximation of continuous functions because it is dense in continous function space. In this result, $\sigma$ is a continuous sigmoidal function and the parameters are real.</p>
  </li>
  <li>
    <p>Hornik 1989</p>

    <p>“Single hidden layer feedforward networks can approximate any measurable functions arbitrarily well regardless of the activation function, the dimension of the input and the input space environment.”</p>

    <p>Reference: http://deeplearning.cs.cmu.edu/notes/Sonia_Hornik.pdf</p>
  </li>
</ol>

<div class="notes--info">
  <p><strong>Dense</strong></p>

  <p>Set A is dense in set X means that we can use A to arbitarily approximate X. Mathematically for any given element in X, the neighbour of x always has nonzero intersection.</p>
</div>

<div class="notes--info">
  <p><strong>Measurable Function</strong></p>

  <p>Basically it means continuous.</p>
</div>

<h2 id="activation-functions">Activation Functions</h2>

<ol>
  <li>
    <p>Uni-Polar Sigmoid Function</p>

    <script type="math/tex; mode=display">\frac{1}{1+e^{-x}}</script>

    <figure>
      <p><img src="../assets/artificial-neural-networks/sigmoidFunction.png" alt="" /></p>
      <figcaption>
        <p>Sigmoid function</p>
      </figcaption>
    </figure>
  </li>
  <li>
    <p>Bipolar Sigmoid Function</p>

    <script type="math/tex; mode=display">\frac{1-e^{-x}}{1+e^{-x}}</script>

    <figure>
      <p><img src="../assets/artificial-neural-networks/bipolarSigmoid.png" alt="" /></p>
      <figcaption>
        <p>Bipolar Sigmoid</p>
      </figcaption>
    </figure>
  </li>
  <li>
    <p>Hyperbolic Tangent</p>

    <script type="math/tex; mode=display">\tanh(x) = \frac{\sinh(x)}{\cosh(x)} = \frac{e^{x} - e^{-x}}{e^x + e^{-x}}</script>

    <figure>
      <p><img src="../assets/artificial-neural-networks/tanh.png" alt="" /></p>
      <figcaption>
        <p>Hyperbolic tangent</p>
      </figcaption>
    </figure>
  </li>
  <li>
    <p>Radial Basis Function</p>

    <figure>
      <p><img src="../assets/artificial-neural-networks/unnormalized_radial_basis_functions.svg.png" alt="" /></p>
      <figcaption>
        <p>Two unnormalized Gaussian radial basis functions in one input dimension. The basis function centers are located at x1=0.75 and x2=3.25. Source <a href="https://en.wikipedia.org/wiki/Radial_basis_function#/media/File:Unnormalized_radial_basis_functions.svg">Unnormalized Radial Basis Functions</a></p>
      </figcaption>
    </figure>
  </li>
  <li>
    <p>Conic Section Function</p>
  </li>
</ol>

<h2 id="solving-differential-equations">Solving Differential Equations</h2>

<p>The problem here to solve is</p>

<script type="math/tex; mode=display">\frac{d}{dt}y(t)= - y(t),</script>

<p>with initial condition $y(0)=1$.</p>

<p>To construct a single layered neural network, the function is decomposed using</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
y(t_i) & = y(t_0) + t_i v_k f(t_i w_k+u_k) \\
 &= 1+t_i v_k f(t_i w_k+u_k) ,
\end{align} %]]></script>

<p>where $y(t_0)$ is the initial condition and $k$ is summed over.</p>

<div class="notes--info">
  <p><strong>Articifial Neural Network</strong></p>

</div>

<p>Presumably this should be the gate controlling trigering of the neuron or not. Therefore the following expit function serves this purpose well,</p>

<script type="math/tex; mode=display">f(x) = \frac{1}{1+\exp(-x)}.</script>

<p>One important reason for chosing this is that a lot of expressions can be calculated analytically and easily.</p>

<div class="notes--info">
  <p><strong>Fermi-Dirac Distribution</strong></p>

  <p>Aha, the Fermi-Dirac distribution.</p>
</div>

<p>With the form of the function to be solved, we can define a cost</p>

<script type="math/tex; mode=display">I=\sum_i\left( \frac{dy}{dt}(t_i)+y(t_i) \right)^2,</script>

<p>which should be minimized to 0 if our struture of networks is optimized for this problem.</p>

<p>Now the task becomes clear:</p>

<ol>
  <li>Write down the cost analytically;</li>
  <li>Minimized cost to find structure;</li>
  <li>Substitute back to the function and we are done.</li>
</ol>

<h2 id="overfitting">Overfitting</h2>

<p>It is possible that we could over fit a network so that it works only for the training data. To avoid that, people use several strategies.</p>

<ol>
  <li>Split data into two parts, one for training and one for testing. <a href="https://www.youtube.com/watch?v=S4ZUwgesjS8">A youtube video</a></li>
  <li>Throw more data in. At least 10 times as many as examples as the DoFs of the model.  <a href="https://www.youtube.com/watch?v=S4ZUwgesjS8">A youtube video</a></li>
  <li>Regularization by plugin a artifical term to the cost function, as an example we could add the . <a href="https://www.youtube.com/watch?v=S4ZUwgesjS8">A youtube video</a></li>
</ol>

<h2 id="neural-network-and-finite-element-method">Neural Network and Finite Element Method</h2>

<p>We consider the solution to a differential equation</p>

<script type="math/tex; mode=display">\mathcal L \psi - f = 0.</script>

<p>Neural network is quite similar to finite element method. In terms of finite element method, we can write down a neural network structured form of a function <sup id="fnref:Freitag2007"><a href="#fn:Freitag2007" class="footnote">1</a></sup></p>

<script type="math/tex; mode=display">\psi(x_i) = A(x_i) + F(x_i, \mathcal N_i),</script>

<p>where $\mathcal N$ is the neural network structure. Specifically,</p>

<script type="math/tex; mode=display">\mathcal N_i = \sigma( w_{ij} x_j + u_i ).</script>

<p>The function is parameterized using the network. Such parameterization is similar to collocation method in finite element method, where multiple basis is used for each location.</p>

<p>One of the choice of the function $F$ is a linear combination,</p>

<script type="math/tex; mode=display">F(x_i, \mathcal N_i) = x_i \mathcal N_i,</script>

<p>and $A(x_i)$ should take care of the boundary condition.</p>

<div class="notes--info">
  <p><strong>Relation to finite element method</strong></p>

  <p>This function is similar to the finite element function basis approximation. The goal in finite element method is to find the coefficients of each basis functions to achieve a good approximation. In ANN method, each sigmoid is the analogy to the basis functions, where we are looking for both the coefficients of sigmoids and the parameters of them. These sigmoid functions are some kind of adaptive basis functions.</p>
</div>

<p>With such parameterization, the differential equation itself is parameterized such that</p>

<script type="math/tex; mode=display">\mathcal L \psi - f = 0,</script>

<p>such that the minimization should be</p>

<script type="math/tex; mode=display">\lvert \mathcal L \psi - f \rvert^2 \to 0</script>

<p>at each point.</p>

<h2 id="references-and-notes">References and Notes</h2>

<ol>
  <li><a href="https://www.youtube.com/watch?v=vq2nnJ4g6N0&amp;t=663s">Tensorflow and deep learning - without a PhD by Martin Görner</a>.</li>
  <li>Kolmogorov, A. N. (1957). “On the Representation of Continuous Functions of Several Variables by Superposition of Continuous Functions of one Variable and Addition,” Doklady Akademii. Nauk USSR, 114, 679-681.</li>
  <li>Maxwell Stinchcombe, Halbert White (1989). <a href="http://www.sciencedirect.com/science/article/pii/0893608089900208">“Multilayer feedforward networks are universal approximators”</a>. Neural Networks, Vol 2, 5, 359-366.</li>
  <li><a href="http://www.cscjournals.org/manuscript/Journals/IJAE/volume1/Issue4/IJAE-26.pdf">Performance Analysis of Various Activation Functions in Generalized MLP Architectures of Neural Networks</a></li>
</ol>

<div class="footnotes">
  <ol>
    <li id="fn:Freitag2007">
      <p>Freitag, K. J. (2007). Neural networks and differential equations. <a href="#fnref:Freitag2007" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div>

  <div class="is-divider" data-content="END"></div>

  
      <article class="message">
        <div class="message-header">
          <p>References:</p>
        </div>
        <div class="message-body">
          <ul>
          
            <li><a href="" style="text-decoration:none;"></a></li>
          
          </ul>
        </div>
      </article>
      


  <a class="u-url" href="/wiki/artificial-neural-networks/artificial-neural-networks/" hidden></a>





<div class="is-clearfix"></div>

<nav class="pagination is-centered" role="navigation" aria-label="pagination">
  <ul class="pagination-list">
    
    













    
    
    <li style="list-style:none;">
      <a class="pagination-link is-current" aria-label="Artificial Neural Networks" aria-current="page" href="/wiki/artificial-neural-networks/artificial-neural-networks/">1</a>
    </li>
<!--
<div class="columns">

<div class="column">

</div>


<div class="column">
  
</div>
 -->
































</ul>
</nav>


</article>

        </div>
      </div>
    </div>
    </main><footer class="footer">
  <data class="u-url" href="/"></data>
  <div class="container">
    <div class="content has-text-centered">
      <p>
        <strong>Datumorphism</strong> by <a>Datumorphism Collective</a>.
       <span class="icon">
         <a href="https://github.com/datumorphism"><i class="fab fa-github"></i></a>
       </span>
       <span class="icon">
      <a href="/feed.xml"><i class="fas fa-rss"></i></a>
       </span>
      </p>

    </div>
  </div>
</footer>
<!-- <script src="//cdn.jsdelivr.net/chartist.js/latest/chartist.min.js"></script>
<script src="/assets/js/home-chart.js"></script> -->

<script src="/assets/js/collapse.js"></script>



  </body>

</html>
