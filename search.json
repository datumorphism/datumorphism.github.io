[{"type":"wiki","title":"Neural ODE","url":"https://datumorphism.leima.is/wiki/machine-learning/neural-ode/neural-ode-basics/","category":["Machine Learning"],"tags":["Deep Learning","Neural ODE"],"description":"The concepts and ideas of neural ODE","authors":null,"summary":null,"date":"2022-08-13T00:00:00Z","references":[{"key":"He2015","link":"http://arxiv.org/abs/1512.03385","name":"He K, Zhang X, Ren S, Sun J. Deep Residual Learning for Image Recognition. arXiv [cs.CV]. 2015. Available: http://arxiv.org/abs/1512.03385"},{"key":"Srivastava2015","link":"http://arxiv.org/abs/1505.00387","name":"Srivastava RK, Greff K, Schmidhuber J. Highway Networks. arXiv [cs.LG]. 2015. Available: http://arxiv.org/abs/1505.00387"},{"key":"Zhang2016","link":"http://arxiv.org/abs/1611.05725","name":"Zhang X, Li Z, Loy CC, Lin D. PolyNet: A Pursuit of Structural Diversity in Very Deep Networks. arXiv [cs.CV]. 2016. Available: http://arxiv.org/abs/1611.05725"},{"key":"Larsson2016","link":"http://arxiv.org/abs/1605.07648","name":"Larsson G, Maire M, Shakhnarovich G. FractalNet: Ultra-Deep Neural Networks without Residuals. arXiv [cs.CV]. 2016. Available: http://arxiv.org/abs/1605.07648"},{"key":"Gomez2017","link":"http://arxiv.org/abs/1707.04585","name":"Gomez AN, Ren M, Urtasun R, Grosse RB. The Reversible Residual Network: Backpropagation Without Storing Activations. arXiv [cs.CV]. 2017. Available: http://arxiv.org/abs/1707.04585"},{"key":"Lu2017","link":"http://arxiv.org/abs/1710.10121","name":"Lu Y, Zhong A, Li Q, Dong B. Beyond Finite Layer Neural Networks: Bridging Deep Architectures and Numerical Differential Equations. arXiv [cs.CV]. 2017. Available: http://arxiv.org/abs/1710.10121"},{"key":"msurtsukov2019","link":"https://github.com/msurtsukov/neural-ode","name":"msurtsukov. msurtsukov/neural-ode: Jupyter notebook with Pytorch implementation of Neural Ordinary Differential Equations. In: GitHub [Internet]. [cited 21 Aug 2022]. Available: https://github.com/msurtsukov/neural-ode"},{"key":"Chen2018","link":"http://arxiv.org/abs/1806.07366","name":"Chen RTQ, Rubanova Y, Bettencourt J, Duvenaud D. Neural Ordinary Differential Equations. arXiv [cs.LG]. 2018. Available: http://arxiv.org/abs/1806.07366"}]},{"type":"wiki","title":"The Time Series Forecasting Problem","url":"https://datumorphism.leima.is/wiki/forecasting/forecasting-problem/","category":["Time Series"],"tags":["Machine Learning","Time Series","Deep Learning"],"description":"Forecasting time series","authors":null,"summary":null,"date":"2022-04-24T00:00:00Z","references":[{"link":"http://arxiv.org/abs/2004.13408","name":"Lim B, Zohren S. Time Series Forecasting With Deep Learning: A Survey. arXiv [stat.ML]. 2020. Available: http://arxiv.org/abs/2004.13408"},{"key":"Hewamalage2022","link":"http://arxiv.org/abs/2203.10716","name":"Hewamalage H, Ackermann K, Bergmeir C. Forecast Evaluation for Data Scientists: Common Pitfalls and Best Practices. arXiv [cs.LG]. 2022. Available: http://arxiv.org/abs/2203.10716"},{"key":"Januschowski2019","link":"https://www.sciencedirect.com/science/article/pii/S0169207019301529","name":"Januschowski T, Gasthaus J, Wang Y, Salinas D, Flunkert V, Bohlke-Schneider M, et al. Criteria for classifying forecasting methods. Int J Forecast. 2020;36: 167–177. doi:10.1016/j.ijforecast.2019.05.008"}]},{"type":"cards","title":"Prediction Space in Forecasting","url":"https://datumorphism.leima.is/cards/forecasting/prediction-space/","category":["Time Series"],"tags":["Time Series","Forecasting"],"description":"In a forecasting problem, we have\n $\\mathcal P$, the priors, e.g., price and demand is negatively correlated, $\\mathcal D$, available dataset, $Y$, the observations, and $F$, the forecasts.  Information Set $\\mathcal A$\n The priors $\\mathcal D$ and the available data $\\mathcal P$ can be summarized together as the information set $\\mathcal A$.   Under a probabilistic view, a forecaster will find out or approximate a CDF $\\mathcal F$ such that1\n$$ \\mathcal F(Y\\vert \\mathcal D, \\mathcal P) \\to F. $$\nNaively speaking, once the density $\\rho(F, Y)$ is determined or estimated, a probabilistic forecaster can be formed.","authors":null,"summary":null,"date":"2022-04-08T00:00:00Z","references":[{"key":"Gneiting2014","link":"https://www.annualreviews.org/doi/10.1146/annurev-statistics-062713-085831","name":"Gneiting T, Katzfuss M. Probabilistic Forecasting. Annu Rev Stat Appl. 2014;1: 125–151. doi:10.1146/annurev-statistics-062713-085831"}]},{"type":"wiki","title":"Feasibility of Learning","url":"https://datumorphism.leima.is/wiki/learning-theory/feasibility-of-learning/","category":["Learning Theory"],"tags":["Learning Theory","Basics"],"description":"Why is learning from data even possible? To discuss this problem, we need a framework for learning. Operationally, we can think of learning as the following framework1.\n  Abu-Mostafa2012\n  Naive View Naively speaking, a model should have two key properties,\n enough capacity to hold the necessary information embedded in the data, and a method to find the combination of parameters so that the model can generate/complete new data.  Most neural networks have enough capacity to hold the necessary information in the data2. The problem is, the capacity is so large. Why does backprop even work? How did backprop find a suitable set of parameters that can generalize?","authors":null,"summary":null,"date":"2021-10-17T00:00:00Z","references":[{"key":"Abu-Mostafa2012","link":"https://www.semanticscholar.org/paper/Learning-From-Data-Abu-Mostafa-Magdon-Ismail/1c0ed9ed3201ef381cc392fc3ca91cae6ecfc698","name":"Abu-Mostafa, Yaser S and Magdon-Ismail, Malik and Lin, Hsuan-Tien. Learning from Data. AMLBook; 2012. Available: https://www.semanticscholar.org/paper/Learning-From-Data-Abu-Mostafa-Magdon-Ismail/1c0ed9ed3201ef381cc392fc3ca91cae6ecfc698"},{"key":"Bernstein2021","link":"https://jeremybernste.in/writing/ml-is-just-statistics","name":"Bernstein J. Machine learning is just statistics + quantifier reversal. In: jeremybernste [Internet]. [cited 1 Nov 2021]. Available: https://jeremybernste.in/writing/ml-is-just-statistics"},{"key":"Guedj2019","link":"http://arxiv.org/abs/1901.05353","name":"Guedj B. A Primer on PAC-Bayesian Learning. arXiv [stat.ML]. 2019. Available: http://arxiv.org/abs/1901.05353"},{"key":"Zhang2016","link":"http://arxiv.org/abs/1611.03530","name":"Zhang C, Bengio S, Hardt M, Recht B, Vinyals O. Understanding deep learning requires rethinking generalization. arXiv [cs.LG]. 2016. Available: http://arxiv.org/abs/1611.03530"}]},{"type":"wiki","title":"What is Graph","url":"https://datumorphism.leima.is/wiki/graph/basics/what-is-graph/","category":["Graph"],"tags":["Graph","Basics"],"description":"Graph A graph $\\mathcal G$ has nodes $\\mathcal V$ and edges $\\mathcal E$,\n$$ \\mathcal G = ( \\mathcal V, \\mathcal E). $$\nEdges\n Edges are relations between nodes. For $u\\in \\mathcal V$ and $v\\in \\mathcal V$, if there is an edge between them, then $(u, v)\\in \\mathcal E$.     Representations of Graph There are different representations of a graph.\nAdjacency Matrix A adjacency matrix of a graph represents the nodes using row and column indices and edges using elements of the matrix.\nFor simple graph, the adjacency matrix is rank two and dimension $\\lvert \\mathcal V \\rvert \\times \\lvert \\mathcal V \\rvert$.","authors":null,"summary":null,"date":"2021-09-25T00:00:00Z","references":[{"key":"Hamilton2020","link":"https://www.morganclaypool.com/doi/10.2200/S01045ED1V01Y202009AIM046","name":"Hamilton WL. Graph Representation Learning. Morgan \u0026 Claypool Publishers; 2020. pp. 1–159. doi:10.2200/S01045ED1V01Y202009AIM046"},{"key":"Sanchez-Lengeling2021","link":"https://distill.pub/2021/gnn-intro/","name":"Sanchez-Lengeling B, Reif E, Pearce A, Wiltschko AB. A Gentle Introduction to Graph Neural Networks. Distill. 2021;6. doi:10.23915/distill.00033"}]},{"type":"wiki","title":"An Introduction to Generative Models","url":"https://datumorphism.leima.is/wiki/machine-learning/generative-models/generative/","category":["Machine Learning"],"tags":["Self-supervised Learning","Generative Model","Basics"],"description":"Discriminative model:\n The conditional probability of class label on data (posterior) $p(C_k\\mid x)$  Generative models:\n Likelihood $p(x\\mid C_k)$ Sample from the likelihood to generate data With latent variables $z$ and some neural network parameters $\\theta$: $P(x,z\\mid \\theta) = p(x\\mid z, \\theta)p(z)$  ","authors":null,"summary":null,"date":"2021-08-13T00:00:00Z","references":[{"link":"http://arxiv.org/abs/2006.08218","name":"Liu X, Zhang F, Hou Z, Wang Z, Mian L, Zhang J, et al. Self-supervised Learning: Generative or Contrastive. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2006.08218"}]},{"type":"wiki","title":"Contrastive Model","url":"https://datumorphism.leima.is/wiki/machine-learning/contrastive-models/contrastive/","category":["Machine Learning"],"tags":["Self-supervised Learning","Contrastive Model","Basics"],"description":"Contrastive models learn to compare1. Contrastive use special objective functions such as [[NCE]]  Noise Contrastive Estimation: NCE Noise contrastive estimation (NCE) objective function is1 $$ \\mathcal L = \\mathbb E_{x, x^{+}, x^{-}} \\left[ - \\ln \\frac{ C(x, x^{+})}{ C(x,x^{+}) + C(x,x^{-}) } \\right], $$ where $x^{+}$ represents data similar to $x$, $x^{-}$ represents data dissimilar to $x$, $C(\\cdot, \\cdot)$ is a function to compute the similarities. For example, we can use $$ C(x, x^{+}) = e^{ f(x)^T f(x^{+}) }, $$ so that the objective function becomes $$ \\mathcal L = \\mathbb E_{x, x^{+}, x^{-}} \\left[ - \\ln \\frac{ e^{ …   and [[Mutual Information]]  Mutual Information Mutual information is defined as $$ I(X;Y) = \\mathbb E_{p_{XY}} \\ln \\frac{P_{XY}}{P_X P_Y}.","authors":null,"summary":null,"date":"2021-08-13T00:00:00Z","references":[{"key":"Liu2020","link":"http://arxiv.org/abs/2006.08218","name":"Liu X, Zhang F, Hou Z, Wang Z, Mian L, Zhang J, et al. Self-supervised Learning: Generative or Contrastive. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2006.08218"}]},{"type":"wiki","title":"GAN","url":"https://datumorphism.leima.is/wiki/machine-learning/adversarial-models/gan/","category":["Machine Learning"],"tags":["Self-supervised Learning","Adversarial Model","GAN","Basics"],"description":"The task of GAN is to generate features $X$ from some noise $\\xi$ and class labels $Y$,\n$$\\xi, Y \\to X.$$\nMany different GANs are proposed. Vanilla GAN has a simple structure with a single discriminator and a single generator. It uses the minmax game setup. However, it is not stable to use minmax game to train a GAN model. WassersteinGAN was proposed to solve the stability problem during training1. More advanced GANs like BiGAN and ALI have more complex structures.\nVanilla GAN  Minmax Game  Suppose we have two players $G$ and $D$, and a utility $v(D, G)$, a minmax game is maximizing the utility $v(D, G)$ for the worst case of $G=\\hat G$ that minimizes $v$ then we have to find $D=\\hat D$ that maximizes $v$, i.","authors":null,"summary":null,"date":"2021-08-13T00:00:00Z","references":[{"key":"Liu2020","link":"http://arxiv.org/abs/2006.08218","name":"Liu X, Zhang F, Hou Z, Wang Z, Mian L, Zhang J, et al. Self-supervised Learning: Generative or Contrastive. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2006.08218"},{"key":"Arjovsky2017","link":"http://arxiv.org/abs/1701.07875","name":"Arjovsky M, Chintala S, Bottou L. Wasserstein GAN. arXiv [stat.ML]. 2017. Available: http://arxiv.org/abs/1701.07875"},{"link":"https://neuronstar.github.io/cpe-docs","name":"Probability Estimation"}]},{"type":"wiki","title":"MaxEnt Model","url":"https://datumorphism.leima.is/wiki/machine-learning/energy-based-model/maxent-energy-based-model/","category":["Energy-based Model"],"tags":["Neural Network","Energy-based Model","Entropy"],"description":"Maximum Entropy models makes least assumption about the data","authors":null,"summary":null,"date":"2021-05-31T00:00:00Z","references":[{"key":"Mehta2018","link":"https://arxiv.org/abs/1803.08823","name":"Mehta P, Bukov M, Wang C-HH, Day AGRR, Richardson C, Fisher CK, et al. A high-bias, low-variance introduction to Machine Learning for physicists. Phys Rep. 2018;810: 122. doi:10.1016/j.physrep.2019.03.001"}]},{"type":"wiki","title":"Data Engineering for Data Scientists: Checklist","url":"https://datumorphism.leima.is/wiki/data-engeering-for-data-scientist/checklist/","category":["Data Engineering"],"tags":["Data Engineering"],"description":"A checklist to get a shallow understanding of the basics and the ecosystem","authors":null,"summary":null,"date":"2021-05-05T00:00:00Z","references":[{"key":"Kretz2019","link":"https://github.com/andkret/Cookbook","name":"The Data Engineering Cookbook"}]},{"type":"wiki","title":"Gibbs Sampling","url":"https://datumorphism.leima.is/wiki/monte-carlo/gibbs-sampling/","category":["Monte Carlo"],"tags":["Statistics","Basics"],"description":"Gibbs sampling is a sampling method for Bayesian inference and MCMC","authors":null,"summary":null,"date":"2021-01-01T00:00:00Z","references":[{"link":"https://www.youtube.com/watch?v=ER3DDBFzH2g","name":"An introduction to Gibbs sampling"},{"link":"http://www.mit.edu/~ilkery/papers/GibbsSampling.pdf","name":"Bayesian Inference: Gibbs Sampling"},{"link":"https://ermongroup.github.io/cs323-notes/probabilistic/gibbs/","name":"Gibbs sampling | Stanford CS323"}]},{"type":"wiki","title":"Principles of Design","url":"https://datumorphism.leima.is/wiki/data-visualization/design/","category":["Data Visualization"],"tags":["Data Visualization","Design"],"description":"There are many principles of designing a visual representation of data. However, before we understand how data is represented visually, it would benefit us a lot if we understand the basic principles of designing on 2D surface.\nRobin\u0026rsquo;s CRAP Robin Williams proposed the four elements of design:\n Contrast Repetition Alignment Proximity  Contrast Use some contrast to distinguish the elements of different contents.\nRepetition Repeat the design of similar elements on the same page and across pages to make sure the readers learn the meaning of the design quickly.\nAlignment  Find a strong line and stick to it.","authors":null,"summary":null,"date":"2020-11-20T00:00:00Z","references":[{"link":"https://www.goodreads.com/book/show/41597.The_Non_Designer_s_Design_Book","name":"The Non-Designer's Design Book by Robin P. Williams"}]},{"type":"wiki","title":"Model Selection","url":"https://datumorphism.leima.is/wiki/model-selection/model-selection/","category":["Model Selection"],"tags":["Model Selection","AIC","BIC","MDL"],"description":"Suppose we have a generating process that generates some numbers based on a distribution. Based on a data sample, we could reconstruct some sort of theoretical models to represent the actual generating process.\n  Which is a Good Model? (1)The black curve represent the generating process. The red rectangle is a very simple model that captures some major samples. The blue step-wise model is capturing more sample data but with more parameters.\n  In the above example, the red model on the left is not that good in most cases while the blue model seems to be better. In reality, the choice depends on the usage of the model.","authors":null,"summary":null,"date":"2020-11-08T00:00:00Z","references":[{"link":"https://www.coursera.org/lecture/linear-regression-model/collinearity-and-parsimony-ukePA","name":"Collinearity and Parsimony from Linear Regression and Modeling on Coursear"},{"link":"https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199957996.001.0001/oxfordhb-9780199957996-e-14","name":"Vandekerckhove, J., \u0026 Matzke, D. (2015). Model comparison and the principle of parsimony. Oxford Library of Psychology."}]},{"type":"wiki","title":"Receiver Operating Characteristics: ROC","url":"https://datumorphism.leima.is/wiki/machine-learning/performance/roc/","category":["Machine Learning"],"tags":["Machine Learning","Performance","Basics"],"description":"ROC is used to judging the performance of classifiers","authors":null,"summary":null,"date":"2020-05-13T00:00:00Z","references":[{"link":"https://doi.org/10.1016/j.patrec.2005.10.010","name":"Fawcett, T. (2006). An introduction to ROC analysis. Pattern Recognition Letters, 27(8), 861–874."}]},{"type":"wiki","title":"Tree-based Learning","url":"https://datumorphism.leima.is/wiki/machine-learning/tree-based/overview/","category":["Machine Learning"],"tags":["Classification","Supervised Learning","Statistical Learning","Basics","Decision Tree"],"description":"Tree-based learning","authors":null,"summary":null,"date":"2019-12-25T00:00:00Z","references":null},{"type":"wiki","title":"Embedding","url":"https://datumorphism.leima.is/wiki/machine-learning/embedding/overview/","category":["Machine Learning"],"tags":["Machine Learning","Embedding"],"description":"","authors":null,"summary":null,"date":"2019-10-13T00:00:00Z","references":null},{"type":"wiki","title":"Factorization","url":"https://datumorphism.leima.is/wiki/machine-learning/factorization/overview/","category":["Machine Learning"],"tags":["Machine Learning","Factorization","Tensor"],"description":"","authors":null,"summary":null,"date":"2019-06-17T00:00:00Z","references":null},{"type":"wiki","title":"Feature Engineering","url":"https://datumorphism.leima.is/wiki/machine-learning/feature-engineering/overview/","category":["Machine Learning"],"tags":["Machine Learning","Feature Engineering"],"description":"Feature engineering is crucial to the performance of our mdoel.","authors":null,"summary":null,"date":"2019-06-17T00:00:00Z","references":null},{"type":"wiki","title":"Naive Bayes","url":"https://datumorphism.leima.is/wiki/machine-learning/bayesian/naive-bayes/","category":["Machine Learning"],"tags":["Machine Learning","Classification","Bayesian"],"description":"Naive Bayes","authors":null,"summary":null,"date":"2019-06-17T00:00:00Z","references":[{"link":"https://www.saedsayad.com/naive_bayesian.htm","name":"Naive Bayesian"},{"link":"https://en.wikipedia.org/wiki/Naive_Bayes_classifier","name":"Naive Bayes classifier @ Wikipedia"},{"link":"","name":"Ross, S. M. (2021). Introduction to Probability and Statistics for Engineers and Scientists (6th ed.). Academic Press."}]},{"type":"wiki","title":"Confusion Matrix (Contingency Table)","url":"https://datumorphism.leima.is/wiki/machine-learning/basics/confusion-matrix/","category":["Machine Learning"],"tags":["Statistical Learning","Basics","Learning Metrics","Classification"],"description":"Confusion Matrix It is much easier to understand the confusion matrix if we use a binary classification problem as an example. For example, we have a bunch of cat photos and the user labeled \u0026ldquo;cute or not\u0026rdquo; data. Now we are using the labeled data to train a cute-or-not binary classifier.\nThen we apply the classifier on the test dataset and we would only find four different kinds of results.\n   Labeled as Cute Labeled as Not Cute     Classifier Predicted to be Cute True Positive (TP) False Positive (FP)   Classifier Predicted to be Not Cute False Negative (FN) True Negative (TN)    This table is easy enough to comprehend.","authors":null,"summary":null,"date":"2019-05-31T00:00:00Z","references":[{"link":"https://doi.org/10.1016/j.patrec.2005.10.010","name":"Fawcett, T. (2006). An introduction to ROC analysis. Pattern Recognition Letters, 27(8), 861–874."},{"link":"https://en.wikipedia.org/wiki/Confusion_matrix#Table_of_confusion","name":"Confusion_matrix#Table_of_confusion @ Wikipedia"},{"link":"https://en.wikipedia.org/wiki/F1_score","name":"F1 Score"}]},{"type":"wiki","title":"Normal Distribution","url":"https://datumorphism.leima.is/wiki/distributions/normal-distribution/","category":["Distributions"],"tags":["Distributions","Normal Distribution","Gaussian Distribution"],"description":"Gaussian distribution","authors":null,"summary":null,"date":"2019-01-22T00:00:00Z","references":null},{"type":"wiki","title":"Statistical Hypothesis Testing","url":"https://datumorphism.leima.is/wiki/statistical-hypothesis-testing/hypothesis-testing/","category":["Statistical Hypothesis Testing"],"tags":["Statistics","Basics","Hypothesis Testing"],"description":"hypothesis testing is about the probability of alternative hypothesis if the null hypothesis is true, or even more general","authors":null,"summary":null,"date":"2019-01-20T00:00:00Z","references":[{"name":"Schaum's Outline of Theories and Problems of Elements of Statistics II, by Ruth Bernstein and Stephen Bernstein, Chapter 16","title":""}]},{"type":"wiki","title":"Why Estimation Theory","url":"https://datumorphism.leima.is/wiki/statistical-estimation/why-estimation-theory/","category":["Statistical Estimation"],"tags":["Statistics","Basics","Estimation Theory"],"description":"In statistics, we work with samples. For example, the sample mean is easily calculated. However, it is the population mean that is more valuable.\nSuppose we have one sample $S_i$, which is used to calculate the mean of the sample $\\mu_i$. We have two key problems to solve at this moment.\n Can we use this sample mean $\\mu_i$ to represent the population mean $\\mu_p$? How good is our estimations?  To answer these questions, we need to work out the properties of the samples themselves and work out a theory to instruct us to infer population statistics from sample statistics.","authors":null,"summary":null,"date":"2019-01-20T00:00:00Z","references":[{"name":"Schaum's Outline of Theories and Problems of Elements of Statistics II, by Ruth Bernstein and Stephen Bernstein","title":""}]},{"type":"wiki","title":"What is Statistics","url":"https://datumorphism.leima.is/wiki/statistics/what-is-statistics/","category":["Statistics"],"tags":["Statistics","Basics"],"description":"An overview of statistics","authors":null,"summary":null,"date":"2019-01-18T00:00:00Z","references":null},{"type":"wiki","title":"Association Rules","url":"https://datumorphism.leima.is/wiki/pattern-mining/association-rules/","category":["Pattern Mining"],"tags":["Basics","Pattern"],"description":"Frequent patterns using association rules","authors":null,"summary":null,"date":"2019-01-06T00:00:00Z","references":[{"link":"","name":"Data Mining by Jiawei Han, Micheline Kamber, Jian Pei"}]},{"type":"wiki","title":"Some Concepts about Data Warehouse","url":"https://datumorphism.leima.is/wiki/data-warehouse/data-warehouse-concepts/","category":["Data Warehouse"],"tags":["Data Warehouse"],"description":"The Three Key Ideas about Warehouse The purpose of the data warehouse should be clear. In most cases, it is for the analysis of data, not for data production.1\n Subject-oriented: since data warehouses are for decision-makers, arrange them into subjects makes it much easier to access. Integrated: many sources are integrated for easy analysis Time-variant: observation time should be recorded since the data is also used to analyze the time evolution Nonvolatile: simply for analysis  OLTP and OLAP  OLTP: online transaction processing OLAP: online analytical processing      OLTP OLAP     user customer data scientist, managers   purpose production analysis   content everything cleaner data   database entity relation model, application-oriented star/snowflake model, subject-oriented   history usually no need to record the history history is crucial   query short and frequent read and write read-only and but complicated analysis    Scope of Data Warehouse  Enterprise warehouse: targeting the whole organization Data mart: for a specific group of people Virtual warehouse: views not tables  Fact and Dimension Fact is the value of something specified by the dimension.","authors":null,"summary":null,"date":"2018-11-23T00:00:00Z","references":[{"link":"","name":"Data Mining by Jiawei Han, Micheline Kamber, Jian Pei"}]},{"type":"wiki","title":"Artificial Neural Networks","url":"https://datumorphism.leima.is/wiki/machine-learning/neural-networks/artificial-neural-networks/","category":["Artificial Neural Networks"],"tags":["Machine Learning","Artificial Neural Networks","Basics"],"description":"Simple artificial neural networks using multilayer perceptron","authors":null,"summary":null,"date":"2018-11-19T00:00:00Z","references":[{"link":"https://mitpress.mit.edu/books/fundamentals-artificial-neural-networks","name":"Hassoun MH, Assistant Professor of Computer Engineering Mohamad H Hassoun. Fundamentals of Artificial Neural Networks. MIT Press; 1995. Available: https://mitpress.mit.edu/books/fundamentals-artificial-neural-networks"},{"link":"https://link.springer.com/chapter/10.1007%2F11759966_125","name":"Shenouda EAMA. A Quantitative Comparison of Different MLP Activation Functions in Classification. Advances in Neural Networks - ISNN 2006. Springer Berlin Heidelberg; 2006. pp. 849–857. doi:10.1007/11759966_125"},{"link":"https://doi.org/10.1016/0893-6080(89)90020-8","name":"Hornik, K., Stinchcombe, M., \u0026 White, H. (1989). Multilayer feedforward networks are universal approximators. Neural Networks, 2(5), 359–366."},{"link":"https://doi.org/10.1007/BF02551274","name":"Cybenko, G. (1989). Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals, and Systems, 2(4), 303–314."},{"key":"Freitag2007","link":"http://dx.doi.org/10.31979/etd.h2n8-mb9r","name":"Freitag KJ. Neural networks and differential equations. San Jose State University. 2007. doi:10.31979/etd.h2n8-mb9r"},{"link":"https://www.youtube.com/watch?v=vq2nnJ4g6N0\u0026t=663s","name":"Tensorflow and deep learning - without a PhD by Martin Görner"},{"name":"Kolmogorov, A. N. (1957). On the Representation of Continuous Functions of Several Variables by Superposition of Continuous Functions of one Variable and Addition, Doklady Akademii. Nauk USSR, 114, 679-681."},{"link":"http://www.sciencedirect.com/science/article/pii/0893608089900208","name":"Maxwell Stinchcombe, Halbert White (1989). Multilayer feedforward networks are universal approximators. Neural Networks, Vol 2, 5, 359-366."},{"link":"http://www.cscjournals.org/manuscript/Journals/IJAE/volume1/Issue4/IJAE-26.pdf","name":"Performance Analysis of Various Activation Functions in Generalized MLP Architectures of Neural Networks"},{"key":"Lippe","link":"https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial3/Activation_Functions.html","name":"Lippe P. Tutorial 3: Activation Functions — UvA DL Notebooks v1.1 documentation. In: UvA Deep Learning Tutorials [Internet]. [cited 23 Sep 2021]. Available: https://uvadlc-notebooks.readthedocs.io"},{"key":"Srivastava2015","link":"http://arxiv.org/abs/1505.00387","name":"Srivastava RK, Greff K, Schmidhuber J. Highway Networks. arXiv [cs.LG]. 2015. Available: http://arxiv.org/abs/1505.00387"}]},{"type":"wiki","title":"Ordinary Differential Equations","url":"https://datumorphism.leima.is/wiki/dynamical-system/ordinary-differential-method/","category":["Dynamical System"],"tags":["Dynamical System","ODE","Finite Difference Method","scipy"],"description":"Solving ODEs Using Finite Difference Methods","authors":null,"summary":null,"date":"2018-11-19T00:00:00Z","references":[{"link":"http://web.mit.edu/10.001/Web/Course_Notes/Differential_Equations_Notes/node6.html","name":"Adams Methods @ MIT Web Course"},{"link":"http://mathworld.wolfram.com/AdamsMethod.html","name":"Adams' Method @ Wolfram MathWorld"}]},{"type":"wiki","title":"Basics of Computation","url":"https://datumorphism.leima.is/wiki/computation/basics-of-computation/","category":["Computation"],"tags":["Computation","Basics"],"description":"Essential knowledge of computations","authors":null,"summary":null,"date":"2018-09-13T00:00:00Z","references":[{"link":"https://medium.com/@LeeJulija/how-integers-are-stored-in-memory-using-twos-complement-5ba04d61a56c","name":"How integers are stored in memory using two’s complement"}]},{"type":"wiki","title":"Introduction to Node Crawler Series","url":"https://datumorphism.leima.is/wiki/nodecrawler/node-crawler-introduction/","category":["Node Crawler"],"tags":["Node","Crawler"],"description":"Installing node.js and mongodb.","authors":null,"summary":null,"date":"2018-07-15T00:00:00Z","references":[{"link":"https://nintha.github.io/2018/07/08/node_spider_compass/01-base_env/","name":"01-基础环境搭建@ninthakeey"}]},{"type":"wiki","title":"Jupyter Notebook","url":"https://datumorphism.leima.is/wiki/tools/jupyter/","category":["Tools"],"tags":["Tools","Jupyter"],"description":"Jupyter Notebook is a useful tool for data scientists","authors":null,"summary":null,"date":"2018-06-20T15:58:49-04:00","references":[{"link":"https://ipython.readthedocs.io/en/stable/interactive/magics.html","name":"Built-in magic command @ ipython"}]},{"type":"wiki","title":"Regular Expression Basics","url":"https://datumorphism.leima.is/wiki/sugar/regular-experssions/","category":["Basics"],"tags":["Regular Expression","RegEx"],"description":"Some quick start material on regular expression.","authors":null,"summary":null,"date":"2018-06-20T15:58:49-04:00","references":[{"link":"https://docs.python.org/3/library/re.html#module-contents","name":"Module Contents@Python3 Documentation"},{"link":"https://medium.com/factory-mind/regex-tutorial-a-simple-cheatsheet-by-examples-649dc1c3f285","name":"Regex tutorial — A quick cheatsheet by examples by Jonny Fox"},{"link":"https://regex101.com/","name":"regex101"},{"link":"https://extendsclass.com/regex-tester.html","name":"extendsclass"}]},{"type":"wiki","title":"Short-Time-Fourier-Transform","url":"https://datumorphism.leima.is/wiki/time-series/short-time-fourier-transform/","category":["Time Series"],"tags":["Short-Time-Fourier-Transform","STFT"],"description":"Some quick start material on regular expression.","authors":null,"summary":null,"date":"2018-06-20T15:58:49-04:00","references":[{"link":"https://www.coursera.org/learn/practical-time-series-analysis/lecture/pPtHq/course-introduction","name":"Practical Time Series Analysis @ Coursera"}]},{"type":"wiki","title":"Linear Methods","url":"https://datumorphism.leima.is/wiki/machine-learning/linear/linear-methods/","category":["Machine Learning"],"tags":["Classification","Supervised Learning","Statistical Learning","Basics","Linear Models"],"description":"linear methods","authors":null,"summary":null,"date":"2018-05-25T00:00:00Z","references":null},{"type":"wiki","title":"Machine Learning Overview","url":"https://datumorphism.leima.is/wiki/machine-learning/overview/","category":["Machine Learning"],"tags":["Statistical Learning","Machine Learning","Basics"],"description":"A brief overview of machine learning","authors":null,"summary":null,"date":"2018-05-25T00:00:00Z","references":[{"link":"https://doi.org/10.1016/j.physrep.2019.03.001","name":"Mehta, P., Bukov, M., Wang, C. H., Day, A. G. R., Richardson, C., Fisher, C. K., \u0026 Schwab, D. J. (2019). A high-bias, low-variance introduction to Machine Learning for physicists. Physics Reports, 810, 1–124."},{"link":"https://doi.org/10.1017/CBO9781107298019","name":"Shalev-Shwartz, S., \u0026 Ben-David, S. (2013). Understanding machine learning: From theory to algorithms. Understanding Machine Learning: From Theory to Algorithms"},{"key":"Domingos2012","link":"https://doi.org/10.1145/2347736.2347755","name":"Domingos, P. (2012). A few useful things to know about machine learning. Communications of the ACM, 55 (10), 78–87."},{"key":"Abu-Mostafa2012","link":"https://www.semanticscholar.org/paper/Learning-From-Data-Abu-Mostafa-Magdon-Ismail/1c0ed9ed3201ef381cc392fc3ca91cae6ecfc698","name":"Abu-Mostafa, Yaser S and Magdon-Ismail, Malik and Lin, Hsuan-Tien. Learning from Data. 2012. Available: https://www.semanticscholar.org/paper/Learning-From-Data-Abu-Mostafa-Magdon-Ismail/1c0ed9ed3201ef381cc392fc3ca91cae6ecfc698"},{"key":"Deckert2017","link":"https://www.mathematik.uni-muenchen.de/~deckert/teaching/SS17/ATML/","name":"Deckert D-A. Advanced Topics in Machine Learning. In: Advanced Topics in Machine Learning [Internet]. Apr 2017 [cited 17 Oct 2021]. Available: https://www.mathematik.uni-muenchen.de/~deckert/teaching/SS17/ATML/"}]},{"type":"wiki","title":"Unsupervised Learning","url":"https://datumorphism.leima.is/wiki/machine-learning/unsupervised/overview/","category":["Machine Learnin"],"tags":["Unsupervised Learning","Statistical Learning","Basics"],"description":"Unsupervised Learning!\nPrinciple components analysis Clustering K-means Clustering Algorithm:\n Assign data points to a group Iterate through until no change:  Find centroid Find the point that is closest to the centroids. Assign that data point to the corresponding group of the centroids.    How Many Groups\n The art of chosing K.   Hierarchical Clustering Bottom-up hierarchical groups can be read out from the dendrogram.","authors":null,"summary":null,"date":"2018-05-25T00:00:00Z","references":null},{"type":"wiki","title":"Some Basic Ideas of Algorithms","url":"https://datumorphism.leima.is/wiki/algorithms/algorithms-basics/","category":["Algorithms"],"tags":["Algorithms","Basics"],"description":"Algorithms is pretty","authors":null,"summary":null,"date":"2018-03-20T00:00:00Z","references":[{"key":"numrec","link":"http://numerical.recipes/","name":"Press, W. H., Teukolsky, S. A., Vetterling, W. T., \u0026 Flannery, B. P. (2544). Numerical Recipes (3rd Editio). Cambridge University Press."},{"key":"Aditya2016","link":"https://www.manning.com/books/grokking-algorithms","name":"Aditya Bhargava. (2016). Grokking Algorithms: An Illustrated Guide for Programmers and Other Curious People. Manning Publications."}]},{"type":"wiki","title":"The C++ Language","url":"https://datumorphism.leima.is/wiki/programming-languages/cpp/references/","category":["Programming Language"],"tags":["Programming Language","Basics","C","C++"],"description":"C++ as a programming language","authors":null,"summary":null,"date":"2018-03-20T00:00:00Z","references":null},{"type":"wiki","title":"The Python Language: Basics","url":"https://datumorphism.leima.is/wiki/programming-languages/python/basics/","category":["Programming Language"],"tags":["Python","Programming Language","Basics"],"description":"Python as a programming language","authors":null,"summary":null,"date":"2018-03-20T00:00:00Z","references":[{"link":"http://www.pythonforbeginners.com/super/working-python-super-function","name":"Superfunction"},{"link":"https://www.python.org/doc/essays/list2str/","name":"Python Patterns - An Optimization Anecdote"}]},{"type":"awesome","title":"Curriculum","url":"https://datumorphism.leima.is/awesome/curriculum/","category":["Curriculum"],"tags":["roadmap","career"],"description":"The path that I follow","authors":null,"summary":null,"date":null,"references":[{"key":"Kretz2019","link":"https://github.com/andkret/Cookbook","name":"The Data Engineering Cookbook"}]},{"type":"wiki","title":"Time Series Forecasting with Deep Learning","url":"https://datumorphism.leima.is/wiki/forecasting/forecasting-with-deep-learning/","category":["Time Series"],"tags":["Machine Learning","Time Series","Deep Learning"],"description":"Evaluate time series models","authors":null,"summary":null,"date":"2022-04-24T00:00:00Z","references":[{"key":"Lim2020","link":"http://arxiv.org/abs/2004.13408","name":"Lim B, Zohren S. Time Series Forecasting With Deep Learning: A Survey. arXiv [stat.ML]. 2020. Available: http://arxiv.org/abs/2004.13408"}]},{"type":"wiki","title":"Statistics of Graphs","url":"https://datumorphism.leima.is/wiki/graph/basics/statistics-of-graphs/","category":["Graph"],"tags":["Graph","Basics","Statistics"],"description":"Local Statistics Node Degree Node Degree   Node degree of a node $u$ $$ d_u = \\sum_{v\\in \\mathcal V} A[u,v], $$ where $A$ is the adjacency matrix.     Node Centrality Importance of a node on a graph:\n  Eigenvector Centrality of a Graph   Given a graph with adjacency matrix $\\mathbf A$, the eigenvector centrality is $$ \\mathbf e_u = \\frac{1}{\\lambda} \\sum_{v\\in\\mathcal V} \\mathbf A[u,v] \\mathbf e_v, \\qquad \\forall u \\in \\mathcal V. $$ Why is it called Eigenvector Centrality The definition is equivalent to $$ \\lambda \\mathbf e = \\mathbf A\\mathbf e. $$ Power Iteration The solution to $\\mathbf e$ is the eigenvector that corresponds to the largest eigenvalue $\\lambda_1$.","authors":null,"summary":null,"date":"2021-09-25T00:00:00Z","references":[{"key":"Hamilton2020","link":"https://www.morganclaypool.com/doi/10.2200/S01045ED1V01Y202009AIM046","name":"Hamilton WL. Graph Representation Learning. Morgan \u0026 Claypool Publishers; 2020. pp. 1–159. doi:10.2200/S01045ED1V01Y202009AIM046"}]},{"type":"wiki","title":"Contrastive Model: Context-Instance","url":"https://datumorphism.leima.is/wiki/machine-learning/contrastive-models/context-instance/","category":["Machine Learning"],"tags":["Self-supervised Learning","Contrastive Model","Mutual Information","Basics"],"description":"In contrastive methods, we can manipulate the data to create data entries and infer the changes using a model. These methods are models that \u0026ldquo;predict relative position\u0026rdquo;1. Common tricks are\n shuffling image sections like jigsaw, and rotate the image.  We can also adjust the model to discriminate the similarities and differences. For example, to generate contrast, we can also use [[Mutual Information]]  Mutual Information Mutual information is defined as $$ I(X;Y) = \\mathbb E_{p_{XY}} \\ln \\frac{P_{XY}}{P_X P_Y}. $$ In the case that $X$ and $Y$ are independent variables, we have $P_{XY} = P_X P_Y$, thus $I(X;Y) = 0$.","authors":null,"summary":null,"date":"2021-08-13T00:00:00Z","references":[{"key":"Liu2020","link":"http://arxiv.org/abs/2006.08218","name":"Liu X, Zhang F, Hou Z, Wang Z, Mian L, Zhang J, et al. Self-supervised Learning: Generative or Contrastive. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2006.08218"}]},{"type":"wiki","title":"f-GAN","url":"https://datumorphism.leima.is/wiki/machine-learning/adversarial-models/f-gan/","category":["Machine Learning"],"tags":["Self-supervised Learning","Adversarial Model","GAN","Basics"],"description":"The essence of [[GAN]]  GAN The task of GAN is to generate features $X$ from some noise $\\xi$ and class labels $Y$, $$\\xi, Y \\to X.$$ Many different GANs are proposed. Vanilla GAN has a simple structure with a single discriminator and a single generator. It uses the minmax game setup. However, it is not stable to use minmax game to train a GAN model. WassersteinGAN was proposed to solve the stability problem during training1. More advanced GANs like BiGAN and ALI have more complex structures. Vanilla GAN Minmax Game …   is comparing the generated distribution $p_G$ and the data distribution $p_\\text{data}$.","authors":null,"summary":null,"date":"2021-08-13T00:00:00Z","references":[{"key":"Liu2020","link":"http://arxiv.org/abs/2006.08218","name":"Liu X, Zhang F, Hou Z, Wang Z, Mian L, Zhang J, et al. Self-supervised Learning: Generative or Contrastive. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2006.08218"},{"key":"Nowozin2016","link":"http://arxiv.org/abs/1606.00709","name":"Nowozin S, Cseke B, Tomioka R. f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization. arXiv [stat.ML]. 2016. Available: http://arxiv.org/abs/1606.00709"},{"key":"f-divergence_wiki","link":"https://en.wikipedia.org/wiki/F-divergence#Instances_of_f-divergences","name":"Contributors to Wikimedia projects. F-divergence. In: Wikipedia [Internet]. 17 Jul 2021 [cited 6 Sep 2021]. Available: https://en.wikipedia.org/wiki/F-divergence#Instances_of_f-divergences"},{"key":"convex_conjugate_wiki","link":"https://en.wikipedia.org/wiki/Convex_conjugate","name":"Contributors to Wikimedia projects. Convex conjugate. In: Wikipedia [Internet]. 20 Feb 2021 [cited 7 Sep 2021]. Available: https://en.wikipedia.org/wiki/Convex_conjugate"}]},{"type":"wiki","title":"Generative Model: Autoregressive Model","url":"https://datumorphism.leima.is/wiki/machine-learning/generative-models/autoregressive-model/","category":["Machine Learning"],"tags":["Self-supervised Learning","Generative Model","Autoregressive Model","Basics"],"description":"An autoregressive (AR) model is autoregressive,\n$$ \\begin{equation} \\log p_\\theta (x) = \\sum_{t=1}^T \\log p_\\theta ( x_{t} \\mid {x_{\u0026lt;t}} ). \\end{equation} $$\n  In the above example, the likelihood is modeled as\n$$ \\begin{align} p_\\theta (x) \u0026= \\Pi_{t=1}^T p_\\theta (x_t \\mid x_{1:t-1}) \\\\ \u0026= p_\\theta(x_2 \\mid x_{1:1}) p_\\theta(x_3 \\mid x_{1:2}) \\cdots p_\\theta(x_T \\mid x_{1:T-1}) \\end{align} $$\nTaking the log of it\n$$ \\ln p_\\theta (x) = \\sum_{t=1}^T \\ln p_\\theta (x_t \\mid x_{1:t-1}) $$\nNotations and Conventions\n In AR models, we have to mention the preceding nodes (${x_{\u0026lt;t}}$) of a specific node ($x_{t}$). For $t=5$, the relations between ${x_{\u0026lt;5}}$ and $x_5$ is shown in the following illustration.","authors":null,"summary":null,"date":"2021-08-13T00:00:00Z","references":[{"key":"Uria2016","link":"http://arxiv.org/abs/1605.02226","name":"Uria B, Côté M-A, Gregor K, Murray I, Larochelle H. Neural Autoregressive Distribution Estimation. arXiv [cs.LG]. 2016. Available: http://arxiv.org/abs/1605.02226"},{"key":"Triebe2019","link":"http://arxiv.org/abs/1911.12436","name":"Triebe O, Laptev N, Rajagopal R. AR-Net: A simple Auto-Regressive Neural Network for time-series. arXiv [cs.LG]. 2019. Available: http://arxiv.org/abs/1911.12436"},{"key":"Ho2019","link":"https://www.eigenfoo.xyz/deep-autoregressive-models","name":"Ho G. George Ho. In: Eigenfoo [Internet]. 9 Mar 2019 [cited 19 Sep 2021]. Available: https://www.eigenfoo.xyz/deep-autoregressive-models/"},{"key":"Papamakarios2017","link":"http://arxiv.org/abs/1705.07057","name":"Papamakarios G, Pavlakou T, Murray I. Masked Autoregressive Flow for Density Estimation. arXiv [stat.ML]. 2017. Available: http://arxiv.org/abs/1705.07057"},{"key":"Germain2015","link":"http://arxiv.org/abs/1502.03509","name":"Germain M, Gregor K, Murray I, Larochelle H. MADE: Masked autoencoder for distribution estimation. 32nd International Conference on Machine Learning, ICML 2015. 2015;2: 881–889. Available: http://arxiv.org/abs/1502.03509"},{"key":"Liu2020","link":"http://arxiv.org/abs/2006.08218","name":"Liu X, Zhang F, Hou Z, Wang Z, Mian L, Zhang J, et al. Self-supervised Learning: Generative or Contrastive. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2006.08218"},{"key":"Lippe","link":"https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial12/Autoregressive_Image_Modeling.html","name":"Lippe P. Tutorial 12: Autoregressive Image Modeling — UvA DL Notebooks v1.1 documentation. In: UvA Deep Learning Tutorials [Internet]. [cited 20 Sep 2021]. Available: https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial12/Autoregressive_Image_Modeling.html"},{"key":"rogen","link":"https://github.com/rogen-george/Deep-Autoregressive-Model","name":"rogen-george. rogen-george/Deep-Autoregressive-Model. In: GitHub [Internet]. [cited 20 Sep 2021]. Available: https://github.com/rogen-george/Deep-Autoregressive-Model"}]},{"type":"wiki","title":"Poisson Regression","url":"https://datumorphism.leima.is/wiki/machine-learning/linear/poisson-regression/","category":["Machine Learning"],"tags":["Basics","Linear Models"],"description":"Poisson regression is a generalized linear model for count data.\nTo model a dataset that is generated from a [[Poisson distribution]]  Poisson Process    , we only need to model the mean $\\mu$ as it is the only parameters. The simplest model we can have for some given features $X$ is a linear model. However, for count data, the effects of the predictors are often multiplicative. The next simplest model we can have is\n$$ \\mu = \\exp\\left(\\beta X\\right). $$\nThe $\\exp$ makes sure that the mean is positive as this is required for count data.","authors":null,"summary":null,"date":"2021-05-07T00:00:00Z","references":[{"link":"https://play.google.com/store/books/details?id=cjB3BwAAQBAJ","name":"Fox J. Applied Regression Analysis and Generalized Linear Models. SAGE Publications; 2015. Available: https://play.google.com/store/books/details?id=cjB3BwAAQBAJ"},{"link":"https://data.princeton.edu/wws509/notes","name":"Rodríguez G, editor. Poisson Models for CountData. Generalized Linear Models. Available: https://data.princeton.edu/wws509/notes"},{"link":"https://ademos.people.uic.edu/Chapter19.html","name":"Chapter 19: Logistic and Poisson Regression by Marie Chesaniuk"},{"link":"https://scikit-learn.org/stable/auto_examples/linear_model/plot_poisson_regression_non_normal_loss.html","name":"Poisson regression and non-normal loss (sklearn documentation)"},{"link":"https://bookdown.org/roback/bookdown-BeyondMLR/ch-poissonreg.html","name":"Beyond Multiple Linear Regression, Chapter 4 Poisson Regression"}]},{"type":"wiki","title":"Principles of Colors","url":"https://datumorphism.leima.is/wiki/data-visualization/colors/","category":["Data Visualization"],"tags":["Data Visualization","Design","Color"],"description":"ColorTeller\n I wrote a python package called colorteller to help us manage and benchmark colors.   Basic Concepts of Colors Color Wheel and Color Sphere There are two dimensions in the color wheel:\n Hue Saturation  When we add another dimension, lightness, to the wheel, we have a color sphere (1, 2).\nMany color systems have been invented. Color wheel and color sphere are two examples of them.","authors":null,"summary":null,"date":"2020-11-20T00:00:00Z","references":[{"link":"https://www.goodreads.com/book/show/41597.The_Non_Designer_s_Design_Book","name":"The Non-Designer's Design Book by Robin P. Williams"},{"link":"https://en.wikipedia.org/wiki/Color_wheel","name":"Color Wheel @ Wikipedia"},{"link":"https://en.wikipedia.org/wiki/Tints_and_shades","name":"Tints and Shades @ Wikipedia"},{"link":"https://en.wikipedia.org/wiki/Colorfulness#Saturation","name":"Saturation @ Wikipedia"}]},{"type":"wiki","title":"Goodness-of-fit","url":"https://datumorphism.leima.is/wiki/model-selection/goodness-of-fit/","category":["Model Selection"],"tags":["Model Selection"],"description":"Does the data agree with the model?\n Calculate the distance between data and model predictions. Apply Bayesian methods such as likelihood estimation: likelihood of observing the data if we assume the model; the results will be a set of fitting parameters. \u0026hellip;  Why don\u0026rsquo;t we always use goodness-of-fit as a measure of the goodness of a model?\n We may experience overfitting. The model may not be intuitive.  This is why we would like to balance it with parsimony using some measures of generalizability.\nK-means and overfitting\n The overfitting problem is easily demonstrated using the K-means model.","authors":null,"summary":null,"date":"2020-11-08T00:00:00Z","references":[{"link":"https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199957996.001.0001/oxfordhb-9780199957996-e-14","name":"Vandekerckhove, J., \u0026 Matzke, D. (2015). Model comparison and the principle of parsimony. Oxford Library of Psychology."}]},{"type":"wiki","title":"Data Types and Level of Measurement in Machine Learning","url":"https://datumorphism.leima.is/wiki/machine-learning/feature-engineering/data-types/","category":["Machine Learning"],"tags":["Machine Learning","Feature Engineering","Statistics","Data Types","Basics"],"description":"identifying the data types and level of measurement is important in data science","authors":null,"summary":null,"date":"2020-01-15T00:00:00Z","references":[{"link":"https://en.wikipedia.org/wiki/Level_of_measurement","name":"Level of Measurement"},{"link":"https://www.semanticscholar.org/paper/On-the-Theory-of-Scales-of-Measurement.-Stevens/25cb1c43983d7b0bdb2263472973dc008da135b7","name":"Stevens, S. S. (1946). On the theory of scales of measurement. Science, 103(2684), 677."},{"link":"https://doi.org/10.1559/152304098782383043","name":"Chrisman, N. R. (1998). Rethinking Levels of Measurement for Cartography. Cartography and Geographic Information Science, 25(4), 231–242."},{"link":"https://en.wikipedia.org/wiki/Statistical_data_type","name":"Statistical data type"}]},{"type":"wiki","title":"Decision Tree","url":"https://datumorphism.leima.is/wiki/machine-learning/tree-based/decision-tree/","category":["Machine Learning"],"tags":["Decision Tree","Supervised Learning","Statistical Learning","Basics"],"description":"In this article, we will explain how decision trees work and build a tree by hand.\nThe code used in this article can be found in this repo.   Definition of the problem We will decide whether one should go to work today. In this demo project, we consider the following features.\n   feature possible values     health 0: feeling bad, 1: feeling good   weather 0: bad weather, 1: good weather   holiday 1: holiday, 0: not holiday    For more compact notations, we use the abstract notation $\\{0,1\\}^3$ to describe a set of three features each with 0 and 1 as possible values.","authors":null,"summary":null,"date":"2019-12-25T00:00:00Z","references":[{"link":"https://doi.org/10.1017/CBO9781107298019","name":"Shalev-Shwartz, S., \u0026 Ben-David, S. (2013). Understanding machine learning: From theory to algorithms. Understanding Machine Learning: From Theory to Algorithms."}]},{"type":"wiki","title":"Bayesian Linear Regression","url":"https://datumorphism.leima.is/wiki/machine-learning/bayesian/bayesian-linear-regression/","category":["Machine Learning"],"tags":["Machine Learning","Regression","Bayesian"],"description":"Bayesian Linear Regression","authors":null,"summary":null,"date":"2019-06-18T00:00:00Z","references":[{"link":"https://towardsdatascience.com/introduction-to-bayesian-linear-regression-e66e60791ea7","name":"Introduction to Bayesian Linear Regression"},{"link":"https://wiseodd.github.io/techblog/2017/01/05/bayesian-regression/","name":"Linear Regression: A Bayesian Point of View"},{"link":"https://github.com/zjost/bayesian-linear-regression","name":"zjost/bayesian-linear-regression"}]},{"type":"wiki","title":"NMF: Nonnegative Matrix Factorizatioin","url":"https://datumorphism.leima.is/wiki/machine-learning/factorization/nmf/","category":["Machine Learning"],"tags":["Machine Learning","Factorization"],"description":"Nonnegative Matrix Factorizatioin has a bright future","authors":null,"summary":null,"date":"2019-06-13T00:00:00Z","references":[{"link":"http://arxiv.org/abs/1401.5226","name":"Gillis, N. (2014). The Why and How of Nonnegative Matrix Factorization."},{"link":"https://en.wikipedia.org/wiki/Spherical_harmonics#Connection_with_representation_theory","name":"Spherical Harmonics"},{"link":"https://blog.acolyer.org/2019/02/18/the-why-and-how-of-nonnegative-matrix-factorization/","name":"The why and how of nonnegative matrix factorization"}]},{"type":"wiki","title":"Word2vec","url":"https://datumorphism.leima.is/wiki/machine-learning/embedding/word2vec/","category":["Machine Learning"],"tags":["Machine Learning","Embedding","Word2vec"],"description":"Single-layer neural network creates embedding space","authors":null,"summary":null,"date":"2019-06-13T00:00:00Z","references":[{"link":"http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/","name":"Word2Vec Tutorial - The Skip-Gram Model"},{"link":"http://jalammar.github.io/illustrated-word2vec/","name":"The Illustrated Word2vec"},{"link":"https://ruder.io/word-embeddings-1/index.html","name":"On word embeddings - Part 1"}]},{"type":"wiki","title":"Bias-Variance","url":"https://datumorphism.leima.is/wiki/machine-learning/basics/bias-variance/","category":["Machine Learning","Basics"],"tags":["Statistical Learning","Basics"],"description":"Bias Variance Trade off is a key concept in statistical learning","authors":null,"summary":null,"date":"2019-06-07T00:00:00Z","references":[{"link":"http://scott.fortmann-roe.com/docs/BiasVariance.html","name":"Understanding the Bias-Variance Tradeoff"},{"link":"https://towardsdatascience.com/the-bias-variance-trade-off-explanation-and-demo-8f462f8d6326","name":"The Bias-Variance trade-off : Explanation and Demo"},{"link":"https://web.stanford.edu/~hastie/ElemStatLearn/","name":"The Element of Statistical Learning"},{"link":"https://books.google.de/books?id=qcI_AAAAQBAJ\u0026source=gbs_navlinks_s","name":"James, G., Witten, D., Hastie, T., \u0026 Tibshirani, R. (2013). An Introduction to Statistical Learning. In Springer Texts in Statistics. Springer Science \u0026 Business Media."}]},{"type":"wiki","title":"Types of Errors in Statistical Hypothesis Testing","url":"https://datumorphism.leima.is/wiki/statistical-hypothesis-testing/type-1-error-and-type-2-error/","category":["Statistical Hypothesis Testing"],"tags":["Statistics","Basics","Hypothesis Testing"],"description":"We all make mistakes. The question is, what kind of mistakes.","authors":null,"summary":null,"date":"2019-05-31T00:00:00Z","references":[{"link":"https://books.google.de/books/about/Schaum_s_Outline_of_Elements_of_Statisti.html?id=3LhPwUhrVIcC","name":"Elements of Statistics II by Stephen Bernstein and Ruth Bernstein"}]},{"type":"wiki","title":"Amazon CloudWatch Logs","url":"https://datumorphism.leima.is/wiki/tools/awslogs/","category":["Tools"],"tags":["Tools","AWS"],"description":"CloudWatch logs as a tool for pipeline logs","authors":null,"summary":null,"date":"2019-03-11T00:00:00Z","references":[{"link":"https://ipython.readthedocs.io/en/stable/interactive/magics.html","name":"Built-in magic commands for Jupyter"},{"link":"https://aws.amazon.com/cloudwatch/","name":"AWS Cloudwatch"}]},{"type":"wiki","title":"Confidence Interval","url":"https://datumorphism.leima.is/wiki/statistical-estimation/confidence-interval/","category":["Statistical Estimation"],"tags":["Statistics","Basics","Confidence Interval"],"description":"Estimates from a sample can be entitled a confidence interval","authors":null,"summary":null,"date":"2019-01-20T00:00:00Z","references":[{"link":"","name":"Schaum's Outline of Theories and Problems of Elements of Statistics II, by Ruth Bernstein and Stephen Bernstein. Chapter 13, 14"}]},{"type":"wiki","title":"Jargons","url":"https://datumorphism.leima.is/wiki/statistics/jargons/","category":["Statistics"],"tags":["Statistics","Basics"],"description":"Jargons in statistics, accuracy, precision, population, sample, etl","authors":null,"summary":null,"date":"2018-11-24T00:00:00Z","references":[{"name":"Elements of Statistics by Stephen Bernstein and Ruth Bernstein","title":""}]},{"type":"wiki","title":"Extract, Transform and Load","url":"https://datumorphism.leima.is/wiki/data-warehouse/extract-transform-load/","category":["Data Warehouse"],"tags":["Data Warehouse","ETL"],"description":"ETL Process ETL\n ETL\n Extract: extract data from sources Transform: transform it to proper format Load: load it to data storage infrastructure    E for Extract  Should not affect the source system.  T for Transform  Cleaning Filtering Enriching Splitting Joining  L for Load Deal with sync and waiting","authors":null,"summary":null,"date":"2018-11-23T00:00:00Z","references":[{"link":"","name":"Data Mining by Jiawei Han, Micheline Kamber, Jian Pei"},{"link":"http://blog.appliedinformaticsinc.com/etl-extract-transform-and-load-process-concept/","name":"ETL (Extract, Transform, and Load) Process \u0026 Concept"},{"link":"https://book.douban.com/subject/26197294/","name":"Designing Data-Intensive Applications"},{"link":"https://www.youtube.com/playlist?list=PL73oFZbnYuix7Xi5C3oFjGlZsnMjisZ-y","name":"Design and Build a Data Warehouse for Business"}]},{"type":"wiki","title":"Partial Differential Equations","url":"https://datumorphism.leima.is/wiki/dynamical-system/partial-difference-method/","category":["Dynamical System"],"tags":["Dynamical System","PDE","Finite Difference Method"],"description":"Solving PDEs","authors":null,"summary":null,"date":"2018-11-19T00:00:00Z","references":null},{"type":"wiki","title":"Basics of Programming","url":"https://datumorphism.leima.is/wiki/computation/basics-of-programming/","category":["Computation"],"tags":["Programming","Basics"],"description":"Essential knowledge of programming","authors":null,"summary":null,"date":"2018-09-23T00:00:00Z","references":null},{"type":"wiki","title":"Basic Node Crawler","url":"https://datumorphism.leima.is/wiki/nodecrawler/basic-crawler/","category":["Node Crawler"],"tags":["Node","Crawler"],"description":"Prerequisites  Nodejs \u0026gt;= 8.9  Overview A model for a crawler is as follows.\nA crawler requests data from the server, while the server responds with some data. Here is a graphic illustration\n+----------+ +-----------+ | | HTTP Request | | | +----------------\u0026gt; | | Nodejs | | Servers | | \u0026lt;----------------+ | | | HTTP Response | | +----------+ +-----------+  HTTP Requests  For a good introduction of HTTP requests, please refer to this video on youtube: Explained HTTP, HTTPS, SSL/TLS     API As for the first step, we need to find which url to request.","authors":null,"summary":null,"date":"2018-07-15T00:00:00Z","references":[{"link":"https://nintha.github.io/2018/07/08/node_spider_compass/02-impl_a_simple_spider/","name":"02-实现一个简单的爬虫@ninthakeey"}]},{"type":"wiki","title":"Autoregressive Model","url":"https://datumorphism.leima.is/wiki/time-series/autoregressive-model/","category":["Time Series"],"tags":["ARMA"],"description":"Time series modeling","authors":null,"summary":null,"date":"2018-06-20T15:58:49-04:00","references":null},{"type":"wiki","title":"Unsupervised Learning: PCA","url":"https://datumorphism.leima.is/wiki/machine-learning/unsupervised/pca/","category":["Machine Learning"],"tags":["PCA","Unsupervised Learning","Statistical Learning","Basics"],"description":"Principal component analysis is a method to remove redundancies of the features by looking into the variances.","authors":null,"summary":null,"date":"2018-05-25T00:00:00Z","references":[{"link":"https://www.cs.princeton.edu/picasso/mats/PCA-Tutorial-Intuition_jp.pdf","name":"Shlens, J. (2003). A Tutorial on Principal Component Analysis"}]},{"type":"wiki","title":"Data Structure","url":"https://datumorphism.leima.is/wiki/algorithms/data-structure/","category":["Algorithms"],"tags":["Data Structure","Basics"],"description":"mind the data structure","authors":null,"summary":null,"date":"2018-03-20T00:00:00Z","references":null},{"type":"wiki","title":"The C++ Language: Basics","url":"https://datumorphism.leima.is/wiki/programming-languages/cpp/basics/","category":["Programming Language"],"tags":["Programming Language","Basics","C","C++"],"description":"C++ as a programming language","authors":null,"summary":null,"date":"2018-03-20T00:00:00Z","references":null},{"type":"wiki","title":"The Python Language: Decorators","url":"https://datumorphism.leima.is/wiki/programming-languages/python/decorators/","category":["Programming Language"],"tags":["Python","Programming Language","Basics"],"description":"Python as a programming language","authors":null,"summary":null,"date":"2018-03-20T00:00:00Z","references":null},{"type":"wiki","title":"A Physicist's Crash Course on Artificial Neural Network","url":"https://datumorphism.leima.is/wiki/machine-learning/neural-networks/physicists-crash-course-neural-network/","category":["Artificial Neural Networks"],"tags":["Machine Learning","Artificial Neural Networks","Basics"],"description":"A very very brief introduction to neural network for physicists","authors":null,"summary":null,"date":"2015-05-02T00:00:00Z","references":[{"link":"","name":""}]},{"type":"awesome","title":"Workflows","url":"https://datumorphism.leima.is/awesome/workflows/","category":null,"tags":["machine learning","workflow","EDA"],"description":"Workflow for data","authors":null,"summary":null,"date":null,"references":[{"link":"https://rstudio-pubs-static.s3.amazonaws.com/182250_19977d0c5c06403fbad1e653850fc7c6.html","name":"EDA Checklist by Matthew R. Versaggi"}]},{"type":"wiki","title":"Evaluating Time Series Models","url":"https://datumorphism.leima.is/wiki/forecasting/evalutate-time-series-models/","category":["Time Series"],"tags":["Machine Learning","Time Series"],"description":"Evaluate time series models","authors":null,"summary":null,"date":"2022-04-08T00:00:00Z","references":[{"link":"http://arxiv.org/abs/1905.11744","name":"Cerqueira V, Torgo L, Mozetic I. Evaluating time series forecasting models: An empirical study on performance estimation methods. arXiv [cs.LG]. 2019. Available: http://arxiv.org/abs/1905.11744"}]},{"type":"wiki","title":"Conformal Prediction","url":"https://datumorphism.leima.is/wiki/statistical-estimation/conformal-prediction/","category":["Statistical Estimation"],"tags":["Statistics","Basics","Confidence Interval","Conformal Prediction"],"description":"Conformal prediction is a method to sequentially predict consistent confidence intervals using nonconformity measures.","authors":null,"summary":null,"date":"2022-04-01T00:00:00Z","references":[{"key":"Shafer2007","link":"http://arxiv.org/abs/0706.3188","name":"Shafer G, Vovk V. A tutorial on conformal prediction. arXiv [cs.LG]. 2007. Available: http://arxiv.org/abs/0706.3188"},{"key":"Zeni2020","link":"http://arxiv.org/abs/2005.07972","name":"Zeni G, Fontana M, Vantini S. Conformal Prediction: a Unified Review of Theory and New Challenges. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2005.07972"}]},{"type":"wiki","title":"Graphs Spectral Methods","url":"https://datumorphism.leima.is/wiki/graph/basics/graph-spectral-methods/","category":["Graph"],"tags":["Graph","Basics","Statistics"],"description":"The [[Ratio Cut]]  Graph Cuts Cut For a subset of nodes $\\mathcal A\\subset \\mathcal V$, the rest of nodes can be denoted as $\\bar {\\mathcal A} = \\mathcal V \\setminus \\mathcal A$. In other words, $\\mathcal A \\cup \\bar {\\mathcal A} = \\mathcal V$ and $\\mathcal A \\cap \\bar {\\mathcal A} = \\emptyset$. That being said, the nodes can be partitioned into two subsets, $\\mathcal A$ and $\\bar {\\mathcal A}$. The cut of this partition is defined as the total number of edges between them, $$ \\operatorname{Cut} \\left( \\mathcal A, …   is closely related to the [[Graph Laplacians]]  Graph Laplacians Laplacian is a useful representation of graphs.","authors":null,"summary":null,"date":"2021-09-25T00:00:00Z","references":[{"key":"Hamilton2020","link":"https://www.morganclaypool.com/doi/10.2200/S01045ED1V01Y202009AIM046","name":"Hamilton WL. Graph Representation Learning. Morgan \u0026 Claypool Publishers; 2020. pp. 1–159. doi:10.2200/S01045ED1V01Y202009AIM046"},{"key":"Luxburg2007","link":"https://link.springer.com/article/10.1007/s11222-007-9033-z","name":"von Luxburg U. A tutorial on spectral clustering. Stat Comput. 2007;17: 395–416. doi:10.1007/s11222-007-9033-z"}]},{"type":"wiki","title":"Contrastive Model: Instance-Instance","url":"https://datumorphism.leima.is/wiki/machine-learning/contrastive-models/instance-instance/","category":["Machine Learning"],"tags":["Self-supervised Learning","Contrastive Model","Basics"],"description":"It was discovered that the success of [[mutual information based contrastive learning]]  Contrastive Model: Context-Instance In contrastive methods, we can manipulate the data to create data entries and infer the changes using a model. These methods are models that \u0026ldquo;predict relative position\u0026rdquo;1. Common tricks are shuffling image sections like jigsaw, and rotate the image. We can also adjust the model to discriminate the similarities and differences. For example, to generate contrast, we can also use [[Mutual Information]] Mutual Information Mutual information is defined as $$ I(X;Y) = \\mathbb E_{p_{XY}} …   is more related to the encoder architecture and the negative sampling strategy1.","authors":null,"summary":null,"date":"2021-08-13T00:00:00Z","references":[{"key":"Liu2020","link":"http://arxiv.org/abs/2006.08218","name":"Liu X, Zhang F, Hou Z, Wang Z, Mian L, Zhang J, et al. Self-supervised Learning: Generative or Contrastive. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2006.08218"}]},{"type":"wiki","title":"Generative Model: Normalizing Flow","url":"https://datumorphism.leima.is/wiki/machine-learning/generative-models/flow/","category":["Machine Learning"],"tags":["Self-supervised Learning","Generative Model","Normalizing Flow","Basics"],"description":"Normalizing flow is a method to convert a complicated distribution $p(x)$ to a simpler distribution $\\tilde p(z)$ by building up a map $z=f(y)$ for the variable $x$ to $z$. The relations between the two distributions is established using the conservation law for distributions, $\\int p(x) \\mathrm d x = \\int \\tilde p (z) \\mathrm d z = 1$. One could imagine that changing the variable also brings in the Jacobian.\n  Liu X, Zhang F, Hou Z, Wang Z, Mian L, Zhang J, et al. Self-supervised Learning: Generative or Contrastive. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2006.08218\n  Normalizing Flows: An Introduction and Review of Current Methods   To generate complicated distributions step by step from a simple and interpretable distribution.","authors":null,"summary":null,"date":"2021-08-13T00:00:00Z","references":[{"key":"Liu2020","link":"http://arxiv.org/abs/2006.08218","name":"Liu X, Zhang F, Hou Z, Wang Z, Mian L, Zhang J, et al. Self-supervised Learning: Generative or Contrastive. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2006.08218"}]},{"type":"wiki","title":"infoGAN","url":"https://datumorphism.leima.is/wiki/machine-learning/adversarial-models/infogan/","category":["Machine Learning"],"tags":["Self-supervised Learning","Adversarial Model","GAN","Basics"],"description":"In GAN, the latent space input is usually random noise, e.g., Gaussian noise. The objective of [[GAN]]  GAN The task of GAN is to generate features $X$ from some noise $\\xi$ and class labels $Y$, $$\\xi, Y \\to X.$$ Many different GANs are proposed. Vanilla GAN has a simple structure with a single discriminator and a single generator. It uses the minmax game setup. However, it is not stable to use minmax game to train a GAN model. WassersteinGAN was proposed to solve the stability problem during training1. More advanced GANs like BiGAN and ALI have more complex structures. Vanilla GAN Minmax Game …   is a very generic one.","authors":null,"summary":null,"date":"2021-08-13T00:00:00Z","references":[{"key":"Chen2016","link":"http://arxiv.org/abs/1606.03657","name":"Chen X, Duan Y, Houthooft R, Schulman J, Sutskever I, Abbeel P. InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets. arXiv [cs.LG]. 2016. Available: http://arxiv.org/abs/1606.03657"},{"key":"Agakov2004","link":"https://books.google.com/books?hl=en\u0026lr=\u0026id=0F-9C7K8fQ8C\u0026oi=fnd\u0026pg=PA201\u0026dq=Algorithm+variational+approach+Information+Maximization+Barber+Agakov\u0026ots=TJGrkVS610\u0026sig=yTKM2ZdcZQBTY4e5Vqk42ayUDxo","name":"Agakov DBF. The im algorithm: a variational approach to information maximization. Adv Neural Inf Process Syst. 2004. Available: https://books.google.com/books?hl=en\u0026lr=\u0026id=0F-9C7K8fQ8C\u0026oi=fnd\u0026pg=PA201\u0026dq=Algorithm+variational+approach+Information+Maximization+Barber+Agakov\u0026ots=TJGrkVS610\u0026sig=yTKM2ZdcZQBTY4e5Vqk42ayUDxo"}]},{"type":"wiki","title":"Logistic Regression","url":"https://datumorphism.leima.is/wiki/machine-learning/linear/logistic-regression/","category":["Machine Learning"],"tags":["Unsupervised Learning","Statistical Learning","Basics","Linear Models","Supervised Learning","Classification"],"description":"logistics regression is a simple model for classification","authors":null,"summary":null,"date":"2021-05-27T00:00:00Z","references":[{"link":"https://play.google.com/store/books/details?id=yPfZBwAAQBAJ","name":"Hastie T, Tibshirani R, Friedman J. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer Science \u0026 Business Media; 2013. pp. 567–567. Available: https://play.google.com/store/books/details?id=yPfZBwAAQBAJ"},{"key":"friedman2000","link":"https://projecteuclid.org/journals/annals-of-statistics/volume-28/issue-2/Additive-logistic-regression--a-statistical-view-of-boosting-With/10.1214/aos/1016218223.full","name":"Friedman J, Hastie T, Tibshirani R. Additive Logistic Regression. The Annals of Statistics. 2000. pp. 337–374. doi:10.1214/aos/1016218223"},{"key":"Mehta2019","link":"https://linkinghub.elsevier.com/retrieve/pii/S0370157319300766","name":"Mehta P, Wang C-H, Day AGR, Richardson C, Bukov M, Fisher CK, et al. A high-bias, low-variance introduction to Machine Learning for physicists. Phys Rep. 2019;810: 1–124. doi:10.1016/j.physrep.2019.03.001"}]},{"type":"wiki","title":"VC Dimension","url":"https://datumorphism.leima.is/wiki/learning-theory/vc-dimension/","category":["Learning Theory"],"tags":["Learning Theory","Basics"],"description":"Two of the key elements in a learning problem are:\n a set of hypothesis $\\mathcal H$, and a set of data samples $\\mathcal S$.  $\\mathcal H$\n Inside $\\mathcal H$, we have a lot of hypotheses, for example, $\\mathcal h$. Given some input, e.g., $x_1$ and $x_2$, we can produce some outputs, e.g., $h(x_1)$ and $h(x_2)$.   $\\mathcal S$\n A sample $\\mathcal S$ is a fair sample drawn from all the possible inputs $\\mathcal X$, where $\\mathcal X$ is called the input space.   A dataset $\\mathcal S$ can be used as a probe of the hypothesis set $\\mathcal H$.","authors":null,"summary":null,"date":"2021-02-21T00:00:00Z","references":[{"key":"Vapnik2000","link":"https://doi.org/10.1007/978-1-4757-3264-1","name":"Vapnik, V. N. (2000). The Nature of Statistical Learning Theory. Springer New York. "},{"key":"Deckert2017","link":"https://www.mathematik.uni-muenchen.de/~deckert/teaching/SS17/ATML/","name":"Deckert D-A. Advanced Topics in Machine Learning. In: Advanced Topics in Machine Learning [Internet]. Apr 2017 [cited 17 Oct 2021]. Available: https://www.mathematik.uni-muenchen.de/~deckert/teaching/SS17/ATML/"},{"key":"Zhang2016","link":"http://arxiv.org/abs/1611.03530","name":"Zhang C, Bengio S, Hardt M, Recht B, Vinyals O. Understanding deep learning requires rethinking generalization. arXiv [cs.LG]. 2016. Available: http://arxiv.org/abs/1611.03530"},{"key":"Bernstein2021","link":"https://jeremybernste.in/writing/ml-is-just-statistics","name":"Bernstein J. Machine learning is just statistics + quantifier reversal. In: jeremybernste [Internet]. [cited 1 Nov 2021]. Available: https://jeremybernste.in/writing/ml-is-just-statistics"},{"key":"Guedj2019","link":"http://arxiv.org/abs/1901.05353","name":"Guedj B. A Primer on PAC-Bayesian Learning. arXiv [stat.ML]. 2019. Available: http://arxiv.org/abs/1901.05353"},{"key":"Abu-Mostafa2012","link":"https://www.semanticscholar.org/paper/Learning-From-Data-Abu-Mostafa-Magdon-Ismail/1c0ed9ed3201ef381cc392fc3ca91cae6ecfc698","name":"Abu-Mostafa, Yaser S and Magdon-Ismail, Malik and Lin, Hsuan-Tien. Learning from Data. AMLBook; 2012. Available: https://www.semanticscholar.org/paper/Learning-From-Data-Abu-Mostafa-Magdon-Ismail/1c0ed9ed3201ef381cc392fc3ca91cae6ecfc698"}]},{"type":"wiki","title":"Measures of Generalizability","url":"https://datumorphism.leima.is/wiki/model-selection/measures-of-generalizability/","category":["Model Selection"],"tags":["Model Selection","Generalizability"],"description":"To measure the generalization, we define a generalization error,\n$$ \\begin{align} \\mathcal G = \\mathcal L_{P}(\\hat f) - \\mathcal L_E(\\hat f), \\end{align} $$\nwhere $\\mathcal L_{P}$ is the population loss, $\\mathcal L_E$ is the empirical loss, and $\\hat f$ is our model by minimizing the empirical loss.\nHowever, we do not know the actual joint probability $p(x, y)$ of our dataset $\\{x_i, y_i\\}$. Thus the population loss is not known. In machine learning, we usually use [[cross validation]]  Cross Validation Cross validation is a method to estimate the [[risk]] The Learning Problem The learning problem posed by Vapnik:1 Given a sample: $\\{z_i\\}$ in the probability space $Z$; Assuming a probability measure on the probability space $Z$; Assuming a set of functions $Q(z, \\alpha)$ (e.","authors":null,"summary":null,"date":"2020-11-08T00:00:00Z","references":[{"link":"https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199957996.001.0001/oxfordhb-9780199957996-e-14","name":"Vandekerckhove, J., \u0026 Matzke, D. (2015). Model comparison and the principle of parsimony. Oxford Library of Psychology."},{"link":"https://escholarship.org/uc/item/6j01x9mz","name":"Roelofs, R. (2019). Measuring Generalization and Overfitting in Machine Learning. Doctoral Dissertation, UC Berkeley, 1–171."}]},{"type":"wiki","title":"Random Forest","url":"https://datumorphism.leima.is/wiki/machine-learning/tree-based/random-forest/","category":["Machine Learning"],"tags":["Decision Tree","Ensemble","Supervised Learning","Statistical Learning","Basics"],"description":"random forest in machine learning","authors":null,"summary":null,"date":"2019-12-25T00:00:00Z","references":[{"link":"https://doi.org/10.1023/A:101093340","name":"Breiman, L. (2001). Random Forests. Machine Learning, 45(1), 5–32."},{"link":"https://doi.org/10.5555/2188385.2343682","name":"Biau, G. (2012). Analysis of a Random Forests Model. J. Mach. Learn. Res., 13, 1063–1095."}]},{"type":"wiki","title":"Predictions Using Time Series Data","url":"https://datumorphism.leima.is/wiki/time-series/predictions-time-series-data/","category":["Time Series"],"tags":["Seasonality"],"description":"Seasonalities etc","authors":null,"summary":null,"date":"2019-06-21T00:00:00Z","references":[{"link":"https://www.ritchievink.com/blog/2018/10/09/build-facebooks-prophet-in-pymc3-bayesian-time-series-analyis-with-generalized-additive-models/","name":"Build Facebook's Prophet in PyMC3; Bayesian time series analyis with Generalized Additive Models"}]},{"type":"wiki","title":"Tensor Factorization","url":"https://datumorphism.leima.is/wiki/machine-learning/factorization/tensor-factorization/","category":["Machine Learning"],"tags":["Machine Learning","Factorization","Tensor"],"description":"A generalization of matrix factorization","authors":null,"summary":null,"date":"2019-06-17T00:00:00Z","references":[{"link":"https://doi.org/10.1007/978-3-319-24486-0_2","name":"Anandkumar, A., Ge, R., Hsu, D., Kakade, S. M., \u0026 Telgarsky, M. (2012). Tensor decompositions for learning latent variable models. Journal of Machine Learning Research, 15(1), 2773–2832."},{"link":"https://www.offconvex.org/2015/12/17/tensor-decompositions/","name":"Tensor Methods in Machine Learning"},{"link":"https://en.wikipedia.org/wiki/Penrose_graphical_notation","name":"Penrose graphical notation"},{"link":"https://math.stackexchange.com/questions/455478/what-is-the-practical-difference-between-abstract-index-notation-and-ordinary","name":"What is the practical difference between abstract index notation and “ordinary” index notation"},{"link":"https://medium.com/@keremturgutlu/tensor-decomposition-fast-cnn-in-your-pocket-f03e9b2a6788","name":"Tensor Decomposition: Fast CNN in your pocket"}]},{"type":"wiki","title":"Anscombe's quartet","url":"https://datumorphism.leima.is/wiki/data-visualization/anscombes-quartet/","category":["Data Visualization"],"tags":["Data Visualization"],"description":"Anscombe\u0026rsquo;s Quartet Anscombe\u0026rsquo;s quartet is a brilliant idea that shows the importance and convenience of visual representation of data.\nAnscombe\u0026rsquo;s quartet has four datasets. The values of each dataset are shown below.\nx1 = [10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5] y1 = [8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68] x2 = [10.0, 8.0, 13.0, 9.0, 11.0, 14.0, 6.0, 4.0, 12.0, 7.0, 5.0] y2 = [9.14, 8.14, 8.74, 8.77, 9.26, 8.1, 6.13, 3.1, 9.13, 7.26, 4.74] x3 = [10.0, 8.0, 13.0, 9.0, 11.0, 14.0, 6.0, 4.0, 12.0, 7.0, 5.0], y3 = [7.46, 6.","authors":null,"summary":null,"date":"2019-03-18T00:00:00Z","references":[{"link":"https://en.wikipedia.org/wiki/Anscombe's_quartet","name":"Anscombe's quartet @ Wikipedia"}]},{"type":"wiki","title":"OLAP Operations","url":"https://datumorphism.leima.is/wiki/data-warehouse/olap-operations/","category":["Data Warehouse"],"tags":["OLAP"],"description":"Some useful OLAP operations","authors":null,"summary":null,"date":"2018-11-23T00:00:00Z","references":[{"link":"","name":"Data Mining, Section 4.2.5, by Jiawei Han, Micheline Kamber, Jian Pei"}]},{"type":"wiki","title":"Finite Element Method","url":"https://datumorphism.leima.is/wiki/dynamical-system/finite-element-method/","category":["Dynamical System"],"tags":["Dynamical System","PDE","Finite Element Method"],"description":"Solving PDEs","authors":null,"summary":null,"date":"2018-11-19T00:00:00Z","references":[{"link":null,"name":"Freitag, K. J. (2007). Neural networks and differential equations."},{"link":"https://www.comsol.com/multiphysics/finite-element-method","name":"COMSOL Multiphysics has a nice article"}]},{"type":"wiki","title":"Chi-square Correlation Test for Nominal Data","url":"https://datumorphism.leima.is/wiki/statistics/correlation-analysis-chi-square/","category":["Statistics"],"tags":["Statistics","Basics","Correlation"],"description":"Detecting correlations using Pearson's chi square correlation test","authors":null,"summary":null,"date":"2018-11-18T00:00:00Z","references":[{"link":"","name":"Data Mining by Jiawei Han, Micheline Kamber, Jian Pei"},{"link":"https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient","name":"Kendall rank correlation coefficient"},{"link":"https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient","name":"Spearman's rank correlation coefficient"}]},{"type":"wiki","title":"Basics of Network","url":"https://datumorphism.leima.is/wiki/computation/basics-of-network/","category":["Computation"],"tags":["Internet","Basics"],"description":"Essential knowledge of internet","authors":null,"summary":null,"date":"2018-09-23T00:00:00Z","references":null},{"type":"wiki","title":"Unsupervised Learning: SVM","url":"https://datumorphism.leima.is/wiki/machine-learning/unsupervised/svm/","category":["Machine Learning"],"tags":["SVM","Unsupervised Learning","Statistical Learning","Basics"],"description":"unsupervised learning: support vector machine","authors":null,"summary":null,"date":"2018-08-17T00:00:00Z","references":[{"link":"https://jeremykun.com/2017/06/05/formulating-the-support-vector-machine-optimization-problem/","name":"Formulating the Support Vector Machine Optimization Problem"},{"link":"http://j2kun.github.io/svm-primal/index.html","name":"Interactive demo of SVM"},{"link":"https://www.svm-tutorial.com/2014/11/svm-understanding-math-part-1/","name":"SVM - Understanding the math - Part 1 - The margin"}]},{"type":"wiki","title":"Manage Data Using MongoDB","url":"https://datumorphism.leima.is/wiki/nodecrawler/manage-data-using-mongodb/","category":["Node Crawler"],"tags":["Node","Crawler"],"description":"In most cases, databases makes the management of data quite convenient. In this article, we would scrape data using the code we discussed before but write data into MongoDB.\nFor installation of MongoDB, please refer to the official documentation.\nThe Code To write data to MongoDB using Node.js, we choose the package mongojs, which provides almost exactly the standard MongoDB syntax.\nTo install mongojs,\nnpm i mongojs --save Here is a module that can write data to MongoDB. We create a file named dao.js and copy/paste the following code into it.\n// use mongojs const mongojs = require(\u0026#39;mongojs\u0026#39;) // connect to the database \u0026#39;simple_spider\u0026#39; in MongoDB and use collection \u0026#39;test\u0026#39; const localdb = mongojs(\u0026#39;simple_spider\u0026#39;, [\u0026#39;test\u0026#39;]) // a function that saves data to MongoDB const saveData = (data,cb) =\u0026gt; { localdb.","authors":null,"summary":null,"date":"2018-07-18T00:00:00Z","references":[{"link":"https://nintha.github.io/2018/07/08/node_spider_compass/03-save_into_db/","name":"03-保存数据到数据库@ninthakeey"}]},{"type":"wiki","title":"The Python Language: Multi-Processing","url":"https://datumorphism.leima.is/wiki/programming-languages/python/multiprocessing/","category":["Programming Language"],"tags":["Python","Programming Language","Basics"],"description":"Python as a programming language","authors":null,"summary":null,"date":"2018-05-10T00:00:00Z","references":[{"link":"http://sebastianraschka.com/Articles/2014_multiprocessing.html","name":"An introduction to parallel programming using Python's multiprocessing module"},{"link":"http://chriskiehl.com/article/parallelism-in-one-line/","name":"Parallelism in one line A Better Model for Day to Day Threading Tasks"}]},{"type":"wiki","title":"Data Structure: Tree","url":"https://datumorphism.leima.is/wiki/algorithms/data-structure-tree/","category":["Algorithms"],"tags":["Tree","Data Structure","Basics"],"description":"mind the data structure: here comes the tree","authors":null,"summary":"mind the data structure: here comes the tree","date":"2018-03-27T00:00:00Z","references":null},{"type":"wiki","title":"The C++ Language: Numerical Methods","url":"https://datumorphism.leima.is/wiki/programming-languages/cpp/numerical/","category":["Programming Language"],"tags":["Numerical Methods","Programming Language","C","C++","Basics"],"description":"C++ as a programming language","authors":null,"summary":null,"date":"2018-03-20T00:00:00Z","references":[{"link":"http://en.cppreference.com/w/cpp/container/vector","name":"std::vector"}]},{"type":"wiki","title":"GNUPlot","url":"https://datumorphism.leima.is/wiki/tools/gnuplot/","category":["Tools"],"tags":["Tools","Command Line","Bash"],"description":"quickly make a graph in your command line","authors":null,"summary":null,"date":"2017-09-04T00:00:00Z","references":[{"link":"http://www.gnuplot.info/","name":"GNUPLOT"}]},{"type":"wiki","title":"Boltzmann Machine","url":"https://datumorphism.leima.is/wiki/machine-learning/energy-based-model/boltzmann-machine/","category":["Energy-based Model"],"tags":["Machine Learning","Artificial Neural Networks","Basics"],"description":"Boltzmann machine is much like a spin glass model in physics. In short words, Boltzmann machine is a machine that has nodes that can take values, and the nodes are connected through some weight. It is just like any other neual nets but with complications and theoretical implications.","authors":null,"summary":null,"date":"2017-08-27T00:00:00Z","references":[{"link":"https://arxiv.org/abs/1803.08823","name":"Mehta P, Bukov M, Wang C-HH, Day AGRR, Richardson C, Fisher CK, et al. A high-bias, low-variance introduction to Machine Learning for physicists. Phys Rep. 2018;810: 122. doi:10.1016/j.physrep.2019.03.001"},{"link":"https://www.cs.toronto.edu/~hinton/csc321/readings/boltz321.pdf","name":"Hinton GE. Boltzmann Machines. 2007."}]},{"type":"wiki","title":"Neyman-Pearson Theory","url":"https://datumorphism.leima.is/wiki/statistical-hypothesis-testing/neyman-pearson-theory/","category":["Statistics Hypothesis Testing"],"tags":["Statistics","Basics","Hypothesis Testing"],"description":"The Neyman-Pearson hypothesis testing tests two hypothesis, hypothesis $H$, and an alternative hypothesis $H_A$.\nNeyman-Pearson Lemma The Neyman-Pearson Lemma is an very intuitive lemma to understand how to choose a hypothesis. The lecture notes from PennState is a very good read on this topic1.\nAn example For simplicity, we assume that there exists a test statistic $T$ and $T$ can be used to measure how likely the hypothesis $H$ is true, e.g., the hypothesis $H$ is false, corresponds to $T$ being small.\nThe reference from Shafer2007 assumes a random variable $T$ to be large if the hypothesis $H$ is false[^Shafer2007].   One example is the ratio of likelihood2,","authors":null,"summary":null,"date":"2022-04-02T00:00:00Z","references":[{"key":"Shafer2007","link":"http://arxiv.org/abs/0706.3188","name":"Shafer G, Vovk V. A tutorial on conformal prediction. arXiv [cs.LG]. 2007. Available: http://arxiv.org/abs/0706.3188"},{"key":"Perezgonzalez2015","link":"https://www.frontiersin.org/articles/10.3389/fpsyg.2015.00223/full","name":"Perezgonzalez JD. Fisher, Neyman-Pearson or NHST? A tutorial for teaching data testing. Front Psychol. 2015;6: 223. doi:10.3389/fpsyg.2015.00223"},{"key":"pennstate_stats","link":"https://online.stat.psu.edu/stat415/lesson/26/26.1","name":"26.1 - Neyman-Pearson Lemma. In: PennState: Statistics Online Courses [Internet]. [cited 2 Apr 2022]. Available: https://online.stat.psu.edu/stat415/lesson/26/26.1"},{"key":"wiki_likelihood-ratio_test","link":"https://en.wikipedia.org/wiki/Likelihood-ratio_test","name":"Contributors to Wikimedia projects. Likelihood-ratio test. In: Wikipedia [Internet]. 10 Jun 2021 [cited 2 Apr 2022]. Available: https://en.wikipedia.org/wiki/Likelihood-ratio_test"}]},{"type":"wiki","title":"Solving Problems on Graph","url":"https://datumorphism.leima.is/wiki/graph/basics/ml-problems-on-graph/","category":["Graph"],"tags":["Graph","Basics"],"description":"Graphs can be used in many problem and there are many possible problems on graphs. We will mention a few popular problems on graphs12.\nNode Classification   Is the user in black a bot or a normal user?Created based on the text in Hamilton2020\n  Given graph that has incomplete attribute labeling of the nodes, predict the attributes on the nodes.\nThe following concepts can be used to classify nodes.\n  [[Homophily]]  Homophily on Graph Homophily is the principle that a contact between similar people occurs at ahigher rate than among dissimilar people \u0026ndash; McPherson20011 McPherson2001 McPherson M, Smith-Lovin L, Cook JM.","authors":null,"summary":null,"date":"2021-09-25T00:00:00Z","references":[{"key":"Hamilton2020","link":"https://www.morganclaypool.com/doi/10.2200/S01045ED1V01Y202009AIM046","name":"Hamilton WL. Graph Representation Learning. Morgan \u0026 Claypool Publishers; 2020. pp. 1–159. doi:10.2200/S01045ED1V01Y202009AIM046"},{"key":"Sanchez-Lengeling2021","link":"https://distill.pub/2021/gnn-intro/","name":"Sanchez-Lengeling B, Reif E, Pearce A, Wiltschko AB. A Gentle Introduction to Graph Neural Networks. Distill. 2021;6. doi:10.23915/distill.00033"},{"key":"McPherson2001","link":"https://www.annualreviews.org/doi/10.1146/annurev.soc.27.1.415","name":"McPherson M, Smith-Lovin L, Cook JM. Birds of a Feather: Homophily in Social Networks. Annu Rev Sociol. 2001;27: 415–444. doi:10.1146/annurev.soc.27.1.415"}]},{"type":"wiki","title":"Contrastive Predictive Coding","url":"https://datumorphism.leima.is/wiki/machine-learning/contrastive-models/contrastive-predictive-codeing/","category":["Machine Learning"],"tags":["Self-supervised Learning","Contrastive Model","Mutual Information","Contrastive","Predictive Coding","Basics"],"description":"Contrastive Predictive Coding, aka CPC, is an autoregressive model combined with InfoNCE loss1.\nThere are two key ideas in CPC:\n Autoregressive models in latent space, and InfoNCE loss that combines mutual information and [[NCE]]  Noise Contrastive Estimation: NCE Noise contrastive estimation (NCE) objective function is1 $$ \\mathcal L = \\mathbb E_{x, x^{+}, x^{-}} \\left[ - \\ln \\frac{ C(x, x^{+})}{ C(x,x^{+}) + C(x,x^{-}) } \\right], $$ where $x^{+}$ represents data similar to $x$, $x^{-}$ represents data dissimilar to $x$, $C(\\cdot, \\cdot)$ is a function to compute the similarities. For example, we can use $$ C(x, x^{+}) = e^{ f(x)^T f(x^{+}) }, $$ so that the objective function becomes $$ \\mathcal L = \\mathbb E_{x, x^{+}, x^{-}} \\left[ - \\ln \\frac{ e^{ …   .","authors":null,"summary":null,"date":"2021-09-08T00:00:00Z","references":[{"key":"Oord2018","link":"http://arxiv.org/abs/1807.03748","name":"van den Oord A, Li Y, Vinyals O. Representation learning with Contrastive Predictive Coding. arXiv [cs.LG]. 2018. Available: http://arxiv.org/abs/1807.03748"}]},{"type":"wiki","title":"Deep Infomax","url":"https://datumorphism.leima.is/wiki/machine-learning/contrastive-models/deep-infomax/","category":["Machine Learning"],"tags":["Self-supervised Learning","Contrastive Model","Mutual Information","Deep Infomax","Basics"],"description":"Max Global Mutual Information\n Why not just use the global mutual information of the input and encoder output as the objective?\n \u0026hellip; maximizing MI between the complete input and the encoder output (i.e.,globalMI) is ofteninsufficient for learning useful representations.\n\u0026ndash; Devon et al[^Devon2018]\n    [[Mutual information]]  Mutual Information Mutual information is defined as $$ I(X;Y) = \\mathbb E_{p_{XY}} \\ln \\frac{P_{XY}}{P_X P_Y}. $$ In the case that $X$ and $Y$ are independent variables, we have $P_{XY} = P_X P_Y$, thus $I(X;Y) = 0$. This makes sense as there would be no \u0026ldquo;mutual\u0026rdquo; information if the two variables are independent of each other.","authors":null,"summary":null,"date":"2021-09-08T00:00:00Z","references":[{"key":"Devon2018","link":"http://arxiv.org/abs/1808.06670","name":"Devon Hjelm R, Fedorov A, Lavoie-Marchildon S, Grewal K, Bachman P, Trischler A, et al. Learning deep representations by mutual information estimation and maximization. arXiv [stat.ML]. 2018. Available: http://arxiv.org/abs/1808.06670"},{"key":"Newell2020","link":"http://arxiv.org/abs/2003.14323","name":"Newell A, Deng J. How Useful is Self-Supervised Pretraining for Visual Tasks? arXiv [cs.CV]. 2020. Available: http://arxiv.org/abs/2003.14323"}]},{"type":"wiki","title":"Generative Model: Auto-Encoder","url":"https://datumorphism.leima.is/wiki/machine-learning/generative-models/autoencoder/","category":["Machine Learning"],"tags":["Self-supervised Learning","Generative Model","Auto-Encoder","Basics"],"description":"Autoencoders (AE) are machines that encodes inputs into a compact latent space.\n  The simplest auto-encoder is rather easy to understand.\n  The loss can be chosen based on the demand, e.g., cross entropy for binary labels.\nNotation: dot ($\\cdot$)\n We use a single vertically centered dot, i.e., $\\cdot$, to indicate that the function or machine can take in arguments.   A simple autoencoder can be achieved using two neural nets, e.g.,\n$$ \\begin{align} {\\color{green}h} \u0026amp;= {\\color{blue}g}{\\color{blue}(}{\\color{blue}b} + {\\color{blue}w} x{\\color{blue})} \\ \\hat x \u0026amp;= {\\color{red}\\sigma}{\\color{red}(c} + {\\color{red}v} {\\color{green}h}{\\color{red})}, \\end{align} $$\nwhere in this simple example,\n ${\\color{blue}g(b + w \\cdot )}$ is the encoder, and ${\\color{red}\\sigma(c + v \\cdot )}$ is the decoder.","authors":null,"summary":null,"date":"2021-08-13T00:00:00Z","references":[{"key":"Lippe","link":"https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial9/AE_CIFAR10.html","name":"Lippe P. Tutorial 9: Deep Autoencoders — UvA DL Notebooks v1.1 documentation. In: UvA Deep Learning Tutorials [Internet]. [cited 20 Sep 2021]. Available: https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial9/AE_CIFAR10.html"}]},{"type":"wiki","title":"Restricted Boltzmann Machine","url":"https://datumorphism.leima.is/wiki/machine-learning/energy-based-model/restricted-boltzmann-machine/","category":["Energy-based Model"],"tags":["Neural Network","Energy-based Model","Entropy","Restricted Boltzmann Machine"],"description":"Introducing latent variables to Boltzmann machine and restrict the connections within groups.","authors":null,"summary":null,"date":"2021-06-11T00:00:00Z","references":[{"key":"Mehta2018","link":"https://arxiv.org/abs/1803.08823","name":"Mehta P, Bukov M, Wang C-HH, Day AGRR, Richardson C, Fisher CK, et al. A high-bias, low-variance introduction to Machine Learning for physicists. Phys Rep. 2018;810: 122. doi:10.1016/j.physrep.2019.03.001"},{"key":"Hubbard1959","link":"http://dx.doi.org/10.1103/physrevlett.3.77","name":"Hubbard J. Calculation of Partition Functions. Physical Review Letters. 1959. pp. 77–78. doi:10.1103/physrevlett.3.77"},{"key":"Cumulant","link":"https://en.wikipedia.org/wiki/Cumulant","name":"Contributors to Wikimedia projects. Cumulant. In: Wikipedia [Internet]. 7 May 2021 [cited 18 Jun 2021]. Available: https://en.wikipedia.org/wiki/Cumulant"}]},{"type":"wiki","title":"Deep Autoregressive Network","url":"https://datumorphism.leima.is/wiki/machine-learning/neural-networks/deep-autoregressive-networks/","category":["Machine Learning"],"tags":["Machine Learning","Autoregressive Network"],"description":"DARN","authors":null,"summary":null,"date":"2021-02-15T00:00:00Z","references":[{"link":"http://arxiv.org/abs/1310.8499","name":"Gregor, K., Danihelka, I., Mnih, A., Blundell, C., \u0026 Wierstra, D. (2013). Deep AutoRegressive Networks. 31st International Conference on Machine Learning, ICML 2014, 4, 2991–3000."}]},{"type":"wiki","title":"Wavelet Transform","url":"https://datumorphism.leima.is/wiki/time-series/wavelets/","category":["Math"],"tags":["Transform","Data","Data Science","Time Series"],"description":"Transforms that captures the local patterns","authors":["LM"],"summary":null,"date":"2020-12-07T00:00:00Z","references":[{"link":"https://www.youtube.com/watch?v=kuuUaqAjeoA","name":"The Wavelet Transform for Beginners"},{"link":"https://www.youtube.com/watch?v=LMqTM7EYlqY","name":"Parameters of Morlet wavelet (time-frequency trade-off)"},{"link":"http://gwyddion.net/documentation/user-guide-en/wavelet-transform.html","name":"Wavelet Transform from Gwyddion Documentation"}]},{"type":"wiki","title":"Parsimony of Models","url":"https://datumorphism.leima.is/wiki/model-selection/parsimony-of-models/","category":null,"tags":null,"description":"For models with a lot of parameters, the goodness-of-fit is very likely to be very high. However, it is also likely to generalize bad. So we need measure of generalizability\nHere parsinomy gives us a few advantages.\n easy to perceive better generalizations  ","authors":null,"summary":null,"date":"2020-11-08T00:00:00Z","references":[{"link":null,"name":"Vandekerckhove, J., \u0026 Matzke, D. (2015). Model comparison and the principle of parsimony. Oxford Library of Psychology."}]},{"type":"wiki","title":"Terminal","url":"https://datumorphism.leima.is/wiki/tools/terminal/","category":["Tools"],"tags":["Tools","Command Line","Terminal"],"description":"work more efficiently","authors":null,"summary":null,"date":"2019-12-31T00:00:00Z","references":[{"link":"https://app.enkipro.com/#/public/insight/557a202244b51154005882a2?signedUp=1","name":"Rapidly invoke an editor to write a long, complex, or tricky command"}]},{"type":"wiki","title":"Data Storage","url":"https://datumorphism.leima.is/wiki/data-warehouse/data-storage/","category":["Data Warehouse"],"tags":["Data Warehouse"],"description":"tl;dr: Use type safe formats such as HDF5 or parquet\n HDF5 BCOLZ \u0026lt;http://bcolz.blosc.org/en/latest/\u0026gt;_ : not designed for multidimentional data. Zarr \u0026lt;https://github.com/alimanfoo/zarr\u0026gt;_ : works with multidimensional data and also parallel computating. Blaze ecosystem \u0026lt;http://blaze.pydata.org/\u0026gt;_  A article that compares HDF5, BCOLZ, and Zarr: To HDF5 and beyond\nI also recommend pandas. It is a python module that works very well with data. It even loads HDF5 out of box.","authors":null,"summary":null,"date":"2018-11-23T00:00:00Z","references":null},{"type":"wiki","title":"Bin Size of Histogram","url":"https://datumorphism.leima.is/wiki/data-visualization/histogram-bin-size/","category":["Data Visualization"],"tags":["Data Visualization"],"description":"[[Histograms]]  Histogram Suppose we check out the burger prices at the stores of Han im Glück, we get a list of numbers. We can arrange the numbers into bins of prices. For example, we can count the number stores that have a price between 10 to 11 euros.    are good for understanding the distribution of your data.\nThe Bin Size Problem As an example, we will use the following series as an example.\n[1.45,2.20,0.75,1.23,1.25,1.25,3.09,1.99,2.00,0.78,1.32,2.25,3.15,3.85,0.52,0.99,1.38,1.75,1.21,1.75] If we use bin size 1, we get this spiky chart and it is not so informing.\n We could also set bin size to 2.","authors":null,"summary":null,"date":"2018-11-22T00:00:00Z","references":[{"link":"https://en.wikipedia.org/wiki/Histogram","name":"Histogram @ Wikipedia"},{"link":"https://www.statisticshowto.datasciencecentral.com/choose-bin-sizes-statistics/","name":"Choose Bin Sizes for Histograms in Easy Steps + Sturge’s Rule"}]},{"type":"wiki","title":"Correlation Coefficient and Covariance for Numeric Data","url":"https://datumorphism.leima.is/wiki/statistics/correlation-coefficient/","category":["Statistics"],"tags":["Statistics","Basics","Correlation"],"description":"Detecting correlations using correlations for numeric data","authors":null,"summary":null,"date":"2018-11-18T00:00:00Z","references":[{"link":"","name":"Data Mining by Jiawei Han, Micheline Kamber, Jian Pei"}]},{"type":"wiki","title":"Basics of Database","url":"https://datumorphism.leima.is/wiki/computation/basics-of-database/","category":["Computation"],"tags":["Database","Basics"],"description":"Essential knowledge of database","authors":null,"summary":null,"date":"2018-10-03T00:00:00Z","references":[{"link":"https://www.red-gate.com/simple-talk/sql/learn-sql-server/sql-server-index-basics/","name":"SQL Server Index Basics"},{"link":"https://apandre.wordpress.com/data/datacube/","name":"OLAP Cubes"},{"link":"https://app.enkipro.com/#/insight/5daefca67cef0ef7a1970975","name":"What is NoSQL"},{"link":"http://blog.knuthaugen.no/2010/03/the-nosql-landscape.html","name":"Analysis of the NoSQL Landscape"}]},{"type":"wiki","title":"Restrictions of Websites","url":"https://datumorphism.leima.is/wiki/nodecrawler/restrictions/","category":["Node Crawler"],"tags":["Node","Crawler"],"description":"Beware that scraping data off websites is neither always allowed nor as easy as a few lines of code. The preceding articles enable you to scrape many data, however, man websites have counter measures. In this article, we will be dealing with some of the common ones.\nRequest Frequency Some websites have limitations on the frequency of API requests. The solution to this is simply a brief pause after each request. In Node.js, the function setInterval enables this.\n// ... require packages here  // define the function fetch to get data const fetch = (aid) =\u0026gt; superagent .get(\u0026#39;https://api.bilibili.com/x/web-interface/archive/stat\u0026#39;) .query({ aid:aid }) .","authors":null,"summary":null,"date":"2018-07-19T00:00:00Z","references":[{"link":"https://nintha.github.io/2018/07/08/node_spider_compass/04-against_website_limitation/","name":"04-应对网站的限制@ninthakeey"},{"link":"https://medium.com/platformer-blog/node-js-concurrency-with-async-await-and-promises-b4c4ae8f4510","name":"Node.JS Concurrency with Async/Await and Promises!"}]},{"type":"wiki","title":"Data Structure: Graph","url":"https://datumorphism.leima.is/wiki/algorithms/data-structure-graph/","category":["Algorithms"],"tags":["Graph","Data Structure","Basics"],"description":"mind the data structure: here comes the graph","authors":null,"summary":"mind the data structure: here comes the graph","date":"2018-03-27T00:00:00Z","references":null},{"type":"wiki","title":"The Python Language: Performance","url":"https://datumorphism.leima.is/wiki/programming-languages/python/performance/","category":["Programming Language"],"tags":["Python","Programming Language","Basics"],"description":"Python as a programming language","authors":null,"summary":null,"date":"2018-03-20T00:00:00Z","references":[{"link":"https://www.ibm.com/developerworks/community/blogs/jfp?lang=en","name":"IT Best Kept Secret Is Optimization"},{"link":"https://www.ibm.com/developerworks/community/blogs/jfp/entry/Python_Meets_Julia_Micro_Performance?lang=en","name":"How To Make Python Run As Fast As Julia"},{"link":"https://realpython.com/introduction-to-python-generators/","name":"How to Use Generators and yield in Python"}]},{"type":"wiki","title":"DeepAR","url":"https://datumorphism.leima.is/wiki/forecasting/deepar/","category":["Time Series"],"tags":["Machine Learning","Time Series","Deep Learning"],"description":"Forecasting time series","authors":null,"summary":null,"date":"2022-07-09T00:00:00Z","references":[{"key":"Salinas2017","link":"http://arxiv.org/abs/1704.04110","name":"Salinas D, Flunkert V, Gasthaus J. DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks. arXiv [cs.AI]. 2017. Available: http://arxiv.org/abs/1704.04110"}]},{"type":"wiki","title":"Information Bottleneck","url":"https://datumorphism.leima.is/wiki/learning-theory/information-bottleneck/","category":["Learning Theory"],"tags":["Learning Theory","Basics","Information Bottleneck"],"description":"Information Bottleneck In a [[induction-deduction framework]]  Induction, Deduction, and Transduction    , for a given training dataset\n$$ \\{X, Y\\}, $$\na prediction Markov chain1\n$$ X \\to \\hat X \\to Y, $$\nwhere $\\hat X$ is supposed to be the minimal sufficient statistics of $X$. $\\hat X$ is the minimal data that can still represent the relation between $X$ and $Y$, i.e., $I(X;Y)$, the [[mutual information]]  Mutual Information Mutual information is defined as $$ I(X;Y) = \\mathbb E_{p_{XY}} \\ln \\frac{P_{XY}}{P_X P_Y}. $$ In the case that $X$ and $Y$ are independent variables, we have $P_{XY} = P_X P_Y$, thus $I(X;Y) = 0$.","authors":null,"summary":null,"date":"2022-04-30T00:00:00Z","references":[{"key":"Tishby2015","link":"http://arxiv.org/abs/1503.02406","name":"Tishby N, Zaslavsky N. Deep Learning and the Information Bottleneck Principle. arXiv [cs.LG]. 2015. Available: http://arxiv.org/abs/1503.02406"}]},{"type":"wiki","title":"State Space Models","url":"https://datumorphism.leima.is/wiki/time-series/state-space-models/","category":["Time Series"],"tags":["Machine Learning","Time Series","Sequential Data","HMM"],"description":"The state space model is an important category of models for sequential data such as time series","authors":["LM"],"summary":null,"date":"2022-02-27T00:00:00Z","references":[{"key":"Bishop2006","link":"https://link.springer.com/gp/book/9780387310732","name":"Christpher M. Bishop. Pattern Recognition and Machine Learning. Springer-Verlag New York; 2006."}]},{"type":"wiki","title":"The Python Language: Packaging","url":"https://datumorphism.leima.is/wiki/programming-languages/python/packaging/","category":["Programming Language"],"tags":["Python","Programming Language","Basics"],"description":"Build a python package","authors":null,"summary":null,"date":"2021-08-27T00:00:00Z","references":[{"key":"Torborg2016","link":"https://python-packaging.readthedocs.io/en/latest/index.html","name":"Torborg S. How To Package Your Python Code — Python Packaging Tutorial."},{"key":"documentation","link":"https://packaging.python.org/","name":"Python Packaging User Guide — Python Packaging User Guide."}]},{"type":"wiki","title":"Variational Auto-Encoder","url":"https://datumorphism.leima.is/wiki/machine-learning/generative-models/variational-autoencoder/","category":["Machine Learning"],"tags":["Self-supervised Learning","Generative Model","VAE","Basics"],"description":"In an inference problem, $p(z\\vert x)$, which is used to infer $z$ from $x$.\n$$ p(z\\vert x) = \\frac{p(x, z)}{p(x)}. $$\nFor example, we have an observable $x$ and a latent space $z$, we would like to find a good latent space for the observable $x$. However, $p(x)$ is something we don\u0026rsquo;t really know. We would like to use some simpler quantities to help us inferring $z$ from $x$ or generating $x$ from $z$.\nNow we introduce a simple distribution $q(z\\vert x)$. We want to make sure this $q(z\\vert x)$ is doing a good job of replacing $p(z\\vert x)$, i.e., minimizing the [[KL divergence]]  KL Divergence Kullback–Leibler divergence indicates the differences between two distributions   ,","authors":null,"summary":null,"date":"2021-08-13T00:00:00Z","references":[{"key":"Liu2020","link":"http://arxiv.org/abs/2006.08218","name":"Liu X, Zhang F, Hou Z, Wang Z, Mian L, Zhang J, et al. Self-supervised Learning: Generative or Contrastive. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2006.08218"},{"key":"Doersch2016","link":"http://arxiv.org/abs/1606.05908","name":"Doersch C. Tutorial on Variational Autoencoders. arXiv [stat.ML]. 2016. Available: http://arxiv.org/abs/1606.05908"},{"key":"Kingma2019","link":"http://arxiv.org/abs/1906.02691","name":"Kingma DP, Welling M. An Introduction to Variational Autoencoders. arXiv [cs.LG]. 2019. Available: http://arxiv.org/abs/1906.02691"},{"link":"https://www.jeremyjordan.me/variational-autoencoders/","name":"Jordan J. Variational autoencoders. Jeremy Jordan. 19 Mar 2018. Available: https://www.jeremyjordan.me/variational-autoencoders/. Accessed 22 Aug 2021."},{"link":"https://youtu.be/uaaqyVS9-rM?t=19m42s","name":"Courses DS. Ali Ghodsi, Lec : Deep Learning, Variational Autoencoder, Oct 12 2017 [Lect 6.2]. YouTube. 2017. Available: https://youtu.be/uaaqyVS9-rM?t=19m42s"}]},{"type":"wiki","title":"MDL and Neural Networks","url":"https://datumorphism.leima.is/wiki/model-selection/mdl-and-neural-networks/","category":["Model Selection"],"tags":["Model Selection","Neural Network","MDL"],"description":"Minimum Description Length ( [[MDL]]  Minimum Description Length MDL is a measure of how well a model compresses data by minimizing the combined cost of the description of the model and the misfit.   ) can be used to construct a concise network. A fully connected network has great expressing power but it is easily overfitting.\nOne strategy is to apply constraints to the networks:\n Limit the connections; Shared weights in subgroups of the network; Constrain the weights using some probability distributions.  By minimizing the MDL of the network and the misfits on the data, we can build a concise network.","authors":null,"summary":null,"date":"2021-02-14T00:00:00Z","references":[{"key":"Hinton1993","link":"https://doi.org/10.1145/168304.168306","name":"Hinton, G. E., \u0026 van Camp, D. (1993). Keeping the neural networks simple by minimizing the description length of the weights. Proceedings of the Sixth Annual Conference on Computational Learning Theory - COLT 93, 5–13."},{"link":"https://mbernste.github.io/posts/sourcecoding/#:~:text=Shannon's%20Source%20Coding%20Theorem%20tells,to%20unambiguously%20communicate%20those%20samples.\u0026text=In%20this%20post%2C%20we%20will%20walk%20through%20Shannon's%20theorem.","name":"Shannon’s Source Coding Theorem (Foundations of information theory: Part 3)"}]},{"type":"wiki","title":"Histogram","url":"https://datumorphism.leima.is/wiki/data-visualization/histogram/","category":["Data Visualization"],"tags":["Data Visualization","Histogram"],"description":"Suppose we check out the burger prices at the stores of Han im Glück, we get a list of numbers. We can arrange the numbers into bins of prices. For example, we can count the number stores that have a price between 10 to 11 euros.\n  ","authors":null,"summary":null,"date":"2019-08-20T00:00:00Z","references":null},{"type":"wiki","title":"Basics of SQL","url":"https://datumorphism.leima.is/wiki/computation/basics-of-sql/","category":["Computation"],"tags":["Programming","Basics"],"description":"Essential knowledge of programming","authors":null,"summary":null,"date":"2018-11-19T00:00:00Z","references":[{"link":"https://www.codecademy.com/articles/what-is-rdbms-sql","name":"What is a Relational Database Management System (RDBMS)?"},{"link":"https://www.codecademy.com/articles/sql-commands","name":"List of SQL commands"}]},{"type":"wiki","title":"Normalization Methods for Numeric Data","url":"https://datumorphism.leima.is/wiki/statistics/normalization-methods/","category":["Statistics"],"tags":["Statistics","Basics","Normalization"],"description":"Detecting correlations using correlations for numerical data","authors":null,"summary":null,"date":"2018-11-18T00:00:00Z","references":[{"name":"Data Mining by Jiawei Han, Micheline Kamber, Jian Pei","title":""}]},{"type":"wiki","title":"Optimization","url":"https://datumorphism.leima.is/wiki/nodecrawler/optimization/","category":["Node Crawler"],"tags":["Node","Crawler"],"description":"In this article, we will be optimizing the crawler to get better performance.\nBatch Jobs In the article about using MongoDB as data storage, we write the data to database whenever we get it. In practice, this is not efficient at all. Here comes the batch jobs. It would be much better if one write to database with batch jobs.\nIf you recall, the code we used to write to database is\n// ...other code localdb.test.save(data, (err, res)=\u0026gt;{ // do something }) The function save takes in not only one entry of document but an array of documents:\nconst array = [] for(let i = INI_ID ; i \u0026lt; MAX_ID; i++){ // fetch data from website \tconst data = fetchData(i) array.","authors":null,"summary":null,"date":"2018-07-19T00:00:00Z","references":[{"link":"https://nintha.github.io/2018/07/10/node_spider_compass/05-lite_optimization/","name":"05-微小的优化@ninthakeey"}]},{"type":"wiki","title":"Git","url":"https://datumorphism.leima.is/wiki/tools/git/","category":["Tools"],"tags":["Tools","Command Line","Git"],"description":"git the tool you need for your everyday work","authors":null,"summary":null,"date":"2016-06-22T00:00:00Z","references":[{"link":"http://durdn.com/blog/2012/11/22/must-have-git-aliases-advanced-examples/","name":"Must Have Git Aliases: Advanced Examples"},{"link":"https://www.atlassian.com/git/tutorials/setting-up-a-repository/git-config","name":"Setting up a repository @ atlassian.com"}]},{"type":"wiki","title":"Hidden Markov Model","url":"https://datumorphism.leima.is/wiki/time-series/hidden-markov-model/","category":["Time Series"],"tags":["Machine Learning","Time Series","Sequential Data","HMM"],"description":"The hidden Markov model, HMM, is a type of [[State Space Models]]  State Space Models The state space model is an important category of models for sequential data such as time series   1.\n  HMM      Bishop2006  Christpher M. Bishop. Pattern Recognition and Machine Learning. Springer-Verlag New York; 2006. \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","authors":["LM"],"summary":null,"date":"2022-02-27T00:00:00Z","references":[{"key":"Bishop2006","link":"https://link.springer.com/gp/book/9780387310732","name":"Christpher M. Bishop. Pattern Recognition and Machine Learning. Springer-Verlag New York; 2006."}]},{"type":"wiki","title":"Some ML Workflow Frameworks","url":"https://datumorphism.leima.is/wiki/tools/ml-flow-frameworks/","category":["Tools"],"tags":["Machine Learning","Workflow"],"description":"Managing workflows in machine learning projects is not trivial.","authors":null,"summary":null,"date":"2021-01-13T00:00:00Z","references":[{"link":"https://ipython.readthedocs.io/en/stable/interactive/magics.html","name":"Built-in magic commands for Jupyter"},{"link":"https://aws.amazon.com/cloudwatch/","name":"AWS Cloudwatch"}]},{"type":"wiki","title":"Boxplot","url":"https://datumorphism.leima.is/wiki/data-visualization/boxplots/","category":["Data Visualization"],"tags":["Data Visualization","Boxplot","Outlier"],"description":"Example   The Whiskers in Boxplot They are the outlier data points.\nOutliers are determined using the interquatile range (IQR, i.e., 25 percentile to 75 percentile.). We usually the lowest data point within 1.5 IQR range below the 25 percentile or the data point within 1.5 IQR range above the 75 percentile.","authors":null,"summary":null,"date":"2019-08-20T00:00:00Z","references":null},{"type":"wiki","title":"Linear Regression","url":"https://datumorphism.leima.is/wiki/statistics/linear-regression/","category":["Statistics"],"tags":["Statistics","Basics","Linear Regression"],"description":"Linear regression of multidimensional data","authors":null,"summary":null,"date":"2019-01-01T00:00:00Z","references":[{"link":"","name":"The Elements of Statistical Learning by Jerome H. Friedman, Robert Tibshirani, and Trevor Hastie"},{"key":"Shafer2007","link":"http://arxiv.org/abs/0706.3188","name":"Shafer G, Vovk V. A tutorial on conformal prediction. arXiv [cs.LG]. 2007. Available: http://arxiv.org/abs/0706.3188"}]},{"type":"wiki","title":"Basics of MongoDB","url":"https://datumorphism.leima.is/wiki/computation/basics-of-mongodb/","category":["Computation"],"tags":["Database","Basics"],"description":"MongoDB is a document based database","authors":null,"summary":null,"date":"2018-10-03T00:00:00Z","references":[{"link":"https://gist.github.com/emptymalei/8c6b4fbbf0d92ba0ffb856439ec9cc64","name":"MongoDB Cheatsheet"}]},{"type":"wiki","title":"Time Series Data Augmentation","url":"https://datumorphism.leima.is/wiki/time-series/data-augmentation/","category":["Time Series"],"tags":["Machine Learning","Time Series","Sequential Data","Data Augmentation"],"description":"","authors":["LM"],"summary":null,"date":"2022-06-27T00:00:00Z","references":[{"key":"Wen2020","link":"http://arxiv.org/abs/2002.12478","name":"Wen Q, Sun L, Yang F, Song X, Gao J, Wang X, et al. Time Series Data Augmentation for Deep Learning: A Survey. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2002.12478"}]},{"type":"wiki","title":"Cookiecutter","url":"https://datumorphism.leima.is/wiki/tools/cookiecutter/","category":["Tools"],"tags":["Tools","Python","Data Science"],"description":"Use cookiecutter to initialize a project","authors":null,"summary":null,"date":"2021-08-27T00:00:00Z","references":[{"link":"https://github.com/drivendata/cookiecutter-data-science","name":"drivendata. drivendata/cookiecutter-data-science: A logical, reasonably standardized, but flexible project structure for doing and sharing data science work. In: GitHub [Internet]. [cited 27 Aug 2021]. Available: https://github.com/drivendata/cookiecutter-data-science"},{"link":"https://github.com/cookiecutter/cookiecutter","name":"cookiecutter. cookiecutter/cookiecutter: A command-line utility that creates projects from cookiecutters (project templates), e.g. Python package projects, VueJS projects. In: GitHub [Internet]. [cited 27 Aug 2021]. Available: https://github.com/cookiecutter/cookiecutter"}]},{"type":"wiki","title":"Statistical Sign Test","url":"https://datumorphism.leima.is/wiki/statistical-hypothesis-testing/sign-test/","category":["Statistical Hypothesis Testing"],"tags":["Statistics","Basics","Hypothesis Testing"],"description":"Statistical test without assuming models","authors":null,"summary":null,"date":"2019-01-20T00:00:00Z","references":[{"link":"https://www.coursera.org/learn/inferential-statistics/lecture/UvETp/6-02-the-sign-test","name":"The sign test @ inferential statistics"}]},{"type":"wiki","title":"Documentation","url":"https://datumorphism.leima.is/wiki/tools/documentation/","category":["Tools"],"tags":["Tools","Python","Documentation"],"description":"Documenting my data science project using sphinx or mkdocs-material","authors":null,"summary":null,"date":"2021-08-28T00:03:10+02:00","references":[{"key":"sphinx","link":"https://github.com/sphinx-doc/sphinx","name":"sphinx-doc. sphinx-doc/sphinx: Main repository for the Sphinx documentation builder. In: GitHub [Internet]. [cited 28 Aug 2021]. Available: https://github.com/sphinx-doc/sphinx"},{"key":"rtd","link":"https://readthedocs.org/","name":"Read the Docs"},{"key":"mdm","link":"https://squidfunk.github.io/mkdocs-material/","name":"squidfunk. squidfunk/mkdocs-material: Technical documentation that just works. In: GitHub [Internet]. [cited 28 Aug 2021]. Available: https://github.com/squidfunk/mkdocs-material"}]},{"type":"wiki","title":"Dashboards","url":"https://datumorphism.leima.is/wiki/data-visualization/dashboards/","category":["Data Visualization"],"tags":["Data Visualization","Dashboard"],"description":"Building interactive dashboards is not easy task. However, with the right tool, we can build a prototype fast.\nTheories Dashboard building seems to be a task to build whatever charts the business would like to add.\nHowever, theories are required to build quality dashboards1.\nAmNeumarkt/253\n I wrote a comment about this: AmNeumarkt/253.\nCreating visualizations seems to be a creative task. At least for entry-level visualization tasks, we follow our hearts and build whatever is needed. However, visualizations are made for different purposes. Some visualizations are simply explorations and for us to get some feelings on the data. Some others are built for the validation of hypotheses.","authors":null,"summary":null,"date":"2021-08-27T00:00:00Z","references":[{"key":"Hullman2021","link":"https://hdsr.mitpress.mit.edu/pub/w075glo6/release/2","name":"Hullman J, Gelman A. Designing for interactive exploratory data analysis requires theories of graphical inference. Harvard Data Science Review. 2021. doi:10.1162/99608f92.3ab8a587"},{"link":"https://streamlit.io/","name":"streamlit"},{"link":"https://plotly.com/dash/","name":"plotly dash"}]},{"type":"wiki","title":"Mann-Whitney U Test","url":"https://datumorphism.leima.is/wiki/statistical-hypothesis-testing/mann-whitney-u-test/","category":["Statistical Hypothesis Testing"],"tags":["Statistics","Basics","Hypothesis Testing","Non-parametric Test"],"description":"Non-parametric test using ranking","authors":null,"summary":null,"date":"2019-01-20T00:00:00Z","references":[{"link":"https://miningthedetails.com/blog/r/non-parametric-tests","name":"What's the difference An Overview of Non-Parametric Tests"},{"link":"https://stats.stackexchange.com/questions/121852/how-to-choose-between-t-test-or-non-parametric-test-e-g-wilcoxon-in-small-sampl","name":"How to choose between t-test or non-parametric test e.g. Wilcoxon in small samples"}]},{"type":"wiki","title":"Describing Multi-dimensional Data","url":"https://datumorphism.leima.is/wiki/statistics/multidimensional-data/","category":["Statistics"],"tags":["Statistics","Basics","Covariance"],"description":"Describing multi-dimensional data","authors":null,"summary":null,"date":"2018-12-03T00:00:00Z","references":null},{"type":"wiki","title":"Data Storage","url":"https://datumorphism.leima.is/wiki/data-engeering-for-data-scientist/data-storage/","category":["Data Engineering"],"tags":["Data Engineering","Data Storage"],"description":"Storing big data","authors":null,"summary":null,"date":"2021-05-05T00:00:00Z","references":[{"key":"Kretz2019","link":"https://github.com/andkret/Cookbook","name":"The Data Engineering Cookbook"}]},{"type":"wiki","title":"Comparison of MLOps Frameworks","url":"https://datumorphism.leima.is/wiki/mlops/comparison-of-frameworks/","category":["MLOps"],"tags":["MLOps"],"description":"Preliminary Investigation","authors":null,"summary":null,"date":"2021-05-05T00:00:00Z","references":[{"link":"https://docs.metaflow.org/metaflow/client","name":"Metaflow"},{"link":"","name":""}]},{"type":"wiki","title":"Data Processing","url":"https://datumorphism.leima.is/wiki/data-engeering-for-data-scientist/data-processing/","category":["Data Engineering"],"tags":["Data Engineering"],"description":"Processing Data is essential.","authors":null,"summary":null,"date":"2021-05-05T00:00:00Z","references":[{"key":"Kretz2019","link":"https://github.com/andkret/Cookbook","name":"The Data Engineering Cookbook"},{"key":"databricksblog2016","link":"https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html","name":"Damji JJD. A Tale of Three Apache Spark APIs: RDDs vs DataFrames and Datasets. In: Databricks [Internet]. 14 Jul 2016 [cited 1 Feb 2022]. Available: https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html"}]},{"type":"wiki","title":"Signal Processing","url":"https://datumorphism.leima.is/wiki/algorithms/singal-processing/","category":["Algorithms"],"tags":["Signal Processing","Basics"],"description":"signal processing","authors":null,"summary":null,"date":"2018-03-20T00:00:00Z","references":null},{"type":"wiki","title":"Data Processing - (Py)Spark","url":"https://datumorphism.leima.is/wiki/tools/data-processing-spark/","category":["Tools"],"tags":["Tools","Data Engineering","Spark","PySpark"],"description":"Processing Data using (Py)Spark","authors":null,"summary":null,"date":"2022-01-31T00:00:00Z","references":[{"key":"DataCampPySparkIntro","link":"https://www.datacamp.com/courses/introduction-to-pyspark","name":"Introduction to PySpark on DataCamp"},{"key":"dc-clean-spark-perform","link":"https://campus.datacamp.com/courses/cleaning-data-with-pyspark/improving-performance?ex=7","name":"Cluster configurations - Cleaning Data with PySpark"}]},{"type":"wiki","title":"Signal Processing: Audio Basics","url":"https://datumorphism.leima.is/wiki/algorithms/signal-processing-audio/","category":["Algorithms"],"tags":["Audio","Signal Processing","Basics","scipy"],"description":"basics about audio","authors":null,"summary":null,"date":"2018-03-29T00:00:00Z","references":[{"link":"https://www.youtube.com/watch?v=0ALKGR0I5MA","name":"Basic Sound Processing in Python | SciPy 2015 | Allen Downey"},{"link":"https://github.com/AllenDowney/ThinkDSP","name":"AllenDowney/ThinkDSP"},{"link":"http://greenteapress.com/wp/think-dsp/","name":"Think DSP"},{"link":"https://freesound.org","name":"freesound.org"},{"link":"https://brilliant.org/wiki/linear-time-invariant-systems/","name":"LTI Systems"}]},{"type":"wiki","title":"Basics of MapReduce","url":"https://datumorphism.leima.is/wiki/algorithms/map-reduce/","category":["Algorithms"],"tags":["Algorithms","Basics"],"description":"mapreduce","authors":null,"summary":null,"date":"2018-10-03T00:00:00Z","references":[{"link":"https://www.tutorialspoint.com/map_reduce/map_reduce_introduction.htm","name":"MapReduce Introduction"},{"link":"https://www.youtube.com/watch?v=gI4HN0JhPmo","name":"Introduction to MapReduce"},{"link":"https://www.youtube.com/watch?v=bcjSe0xCHbE","name":"Learn MapReduce with Playing Cards"}]},{"type":"wiki","title":"Scale Up","url":"https://datumorphism.leima.is/wiki/data-engeering-for-data-scientist/scale-up/","category":["Data Engineering"],"tags":["Data Engineering","Data Storage"],"description":"scale up your services","authors":null,"summary":null,"date":"2021-05-05T00:00:00Z","references":[{"key":"Kretz2019","link":"https://github.com/andkret/Cookbook","name":"The Data Engineering Cookbook"}]},{"type":"cards","title":"Empirical Correlation Coefficient (CORR)","url":"https://datumorphism.leima.is/cards/time-series/ts-corr/","category":["Time Series"],"tags":["Time Series","Forecasting"],"description":"The Empirical Correlation Coefficient (CORR) is an evaluation metric in time series forecasting,1\n$$ \\mathrm{CORR} = \\frac{1}{N} \\sum_{i=1}^N \\frac{ \\sum_t (y^{(i)}_t - \\bar y^{(i)} ) ( \\hat y^{(i)}_t -\\bar{ \\hat y}^{(i)} ) }{ \\sqrt{ \\sum_t (y^{(i)}_t - \\bar y^{(i)} )^2 ( \\hat y^{(i)}_t -\\bar{\\hat y}^{(i)} )^2 } } $$\nwhere $y^{(i)}$ is the $i$th time series, ${} _ t$ denotes the time step $t$, and $\\bar y^{(i)}$ is the mean of the $i$th forecasted series, i.e., $\\bar y^{(i)} = \\operatorname{mean}( y^{(i)} _ { t \\in \\{T _ f, T _ {f+1}, \\cdots T _ {f+H}\\} } )$.\n   Lai2017  Lai G, Chang W-C, Yang Y, Liu H.","authors":["Lei Ma"],"summary":null,"date":"2022-08-21T20:43:36+02:00","references":[{"key":"Lai2017","link":"http://arxiv.org/abs/1703.07015","name":"Lai G, Chang W-C, Yang Y, Liu H. Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks. arXiv [cs.LG]. 2017. Available: http://arxiv.org/abs/1703.07015"}]},{"type":"cards","title":"Root Relative Squared Error (RSE)","url":"https://datumorphism.leima.is/cards/time-series/ts-rse/","category":["Time Series"],"tags":["Time Series","Forecasting"],"description":"The Root Relative Squared Error (RSE) is an evaluation metric in time series forecasting,1\n$$ \\mathrm{RSE} = \\frac{ \\sqrt{ \\sum_{i, t} ( y^{(i)}_t - \\hat y^{(i)}_t )^2 } }{ \\sqrt{ \\sum_{i, t} ( y^{(i)}_t - \\bar y )^2 } } $$\nwhere $y^{(i)}$ is the $i$th time series, ${} _ t$ denotes the time step $t$, and $\\bar y$ is the mean of the forecasted series, i.e., $\\bar y = \\operatorname{mean}(y^{(i\\in\\{0, 1, \\cdots, N\\})} _ { t\\in \\{T _ f, T _ {f+1}, \\cdots T _ {f+H}\\} })$.\n   Lai2017  Lai G, Chang W-C, Yang Y, Liu H.","authors":["Lei Ma"],"summary":null,"date":"2022-08-21T20:43:36+02:00","references":[{"key":"Lai2017","link":"http://arxiv.org/abs/1703.07015","name":"Lai G, Chang W-C, Yang Y, Liu H. Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks. arXiv [cs.LG]. 2017. Available: http://arxiv.org/abs/1703.07015"}]},{"type":"cards","title":"Dilated Convolution","url":"https://datumorphism.leima.is/cards/math/convolution-dilated/","category":["Math"],"tags":["Convolution"],"description":"For a convolution\n$$ f*h(x) = \\sum_{s+t=x} f(s) h(t), $$\nthe dilated version of it is1\n$$ f*_l h(x) = \\sum_{s+t*l=x} f(s) h(t), $$\nwhere $l$ is the dilation factor.\n   Yu2015  Yu F, Koltun V. Multi-Scale Context Aggregation by Dilated Convolutions. arXiv [cs.CV]. 2015. Available: http://arxiv.org/abs/1511.07122 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","authors":["Lei Ma"],"summary":null,"date":"2022-08-21T16:03:14+02:00","references":[{"key":"Yu2015","link":"http://arxiv.org/abs/1511.07122","name":"Yu F, Koltun V. Multi-Scale Context Aggregation by Dilated Convolutions. arXiv [cs.CV]. 2015. Available: http://arxiv.org/abs/1511.07122"}]},{"type":"cards","title":"Over-Smoothing in Graph Neural Networks","url":"https://datumorphism.leima.is/cards/graph/graph-neural-networks-over-smoothing/","category":["Graph"],"tags":["graph","graph neural networks","GNN"],"description":"Over-smoothing is the problem that the representations on each node of the graph neural networks becomes way too similar to each other.1 In Chapter 7 of Hamilton2020, the author interprets this phenomenon using the lower pass filter theory in signal processing, i.e., multiplying a signal by $\\mathbf A^n$ is similar to a low-pass filter when $n$ is large, with $\\mathfb A$ being the adjacency matrix.\n   Hamilton2020  Hamilton WL. Graph Representation Learning. Morgan \u0026amp; Claypool Publishers; 2020. pp. 1–159. doi:10.2200/S01045ED1V01Y202009AIM046 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","authors":["Lei Ma"],"summary":null,"date":"2022-08-21T13:45:33+02:00","references":[{"key":"Hamilton2020","link":"https://link.springer.com/book/10.1007/978-3-031-01588-5","name":"Hamilton WL. Graph Representation Learning. Morgan \u0026 Claypool Publishers; 2020. pp. 1–159. doi:10.2200/S01045ED1V01Y202009AIM046"}]},{"type":"cards","title":"Data Generating Processes for Time Series Data","url":"https://datumorphism.leima.is/cards/time-series/generating-process/","category":["Time Series"],"tags":["Time Series","Generating Process"],"description":"","authors":null,"summary":null,"date":"2022-07-04T00:00:00Z","references":[{"key":"Das2019","link":"https://link.springer.com/chapter/10.1007/978-981-32-9019-8_9","name":"Das P. Time Series: Data Generating Process. In: Das P, editor. Econometrics in Theory and Practice: Analysis of Cross Section, Time Series and Panel Data with Stata 151. Singapore: Springer Singapore; 2019. pp. 247–259. doi:10.1007/978-981-32-9019-8_9"}]},{"type":"til","title":"PySpark: Beware of Python Mutable Objects","url":"https://datumorphism.leima.is/til/data/pyspark.beware-of-mutable-objects/","category":["data"],"tags":["PySpark","Python"],"description":"We should be careful when dealing with python mutable objects. For example, make copies of python mutable objects in pyspark udfs.","authors":["Lei Ma"],"summary":null,"date":"2022-04-20T00:00:00Z","references":null},{"type":"wiki","title":"Conformal Time Series Forecasting","url":"https://datumorphism.leima.is/wiki/forecasting/probablistic/conformal-time-series-forecasting/","category":["Time Series"],"tags":["Machine Learning","Time Series","Conformal Prediction","Probabilistic"],"description":"Conformal time series forecasting is a probabilistic forecasting method using [[Conformal Prediction]]  Conformal Prediction Conformal prediction is a method to sequentially predict consistent confidence intervals using nonconformity measures.   .\nFor any given model $\\mathcal M$, conformal time series forecasting trains on a training dataset $\\mathcal D_{\\text{Train}}$ then calculates a [[Confidence Interval]]  Confidence Interval Estimates from a sample can be entitled a confidence interval   using a calibration dataset $\\mathcal D_{\\text{Calibration}}$. The confidence interval is directly used for inference. This framework is called the inductive conformal prediction (ICP).\nInduction, Deduction, and Transduction       How to Forecast the Confidence Interval For a dataset $\\mathcal D$, we split it, e.","authors":null,"summary":null,"date":"2022-04-19T00:00:00Z","references":[{"key":"Stankevičiūtė2022","link":"[Vapnik, V. N. (2000). The Nature of Statistical Learning Theory. Springer New York. ](https://proceedings.neurips.cc/paper/2021/hash/312f1ba2a72318edaaa995a67835fad5-Abstract.html)","name":"Stankevičiūtė K, Alaa AM, Van Der Schaar M. Conformal Time-Series Forecasting. [cited 9 Feb 2022]. Available: https://proceedings.neurips.cc/paper/2021/hash/312f1ba2a72318edaaa995a67835fad5-Abstract.html"}]},{"type":"cards","title":"Induction, Deduction, and Transduction","url":"https://datumorphism.leima.is/cards/machine-learning/learning-theories/induction-deduction-transduction/","category":["Machine Learning::Theories"],"tags":["Learning Theory","Induction","Deduction","Transduction"],"description":"  ","authors":null,"summary":null,"date":"2022-04-19T00:00:00Z","references":null},{"type":"cards","title":"Graph Edge Sampling","url":"https://datumorphism.leima.is/cards/graph/graph-edge-sampling/","category":["Graph"],"tags":["Graph"],"description":"Edge sampling is a technique to deal with weighted edges in large [[graph]]  What is Graph Graph A graph $\\mathcal G$ has nodes $\\mathcal V$ and edges $\\mathcal E$, $$ \\mathcal G = ( \\mathcal V, \\mathcal E). $$ Edges Edges are relations between nodes. For $u\\in \\mathcal V$ and $v\\in \\mathcal V$, if there is an edge between them, then $(u, v)\\in \\mathcal E$. Representations of Graph There are different representations of a graph. Adjacency Matrix A adjacency matrix of a graph represents the nodes using row and column indices and edges using elements of the matrix. For simple …   .","authors":null,"summary":null,"date":"2022-03-20T00:00:00Z","references":[{"key":"Tang2015","link":"http://arxiv.org/abs/1503.03578","name":"Tang J, Qu M, Wang M, Zhang M, Yan J, Mei Q. LINE: Large-scale Information Network Embedding. arXiv [cs.LG]. 2015. Available: http://arxiv.org/abs/1503.03578"}]},{"type":"cards","title":"Continuous Ranked Probability Score - CRPS","url":"https://datumorphism.leima.is/cards/time-series/crps/","category":["Time Series"],"tags":["Time Series","Forecasting"],"description":"The Continuous Ranked Probability Score, known as CRPS, is a score to measure how a proposed distribution approximates the data, without knowledge about the true distributions of the data.\nDefinition CRPS is defined as1\n$$ CRPS(P, x_a) = \\int_{-\\infty}^\\infty \\lVert P(x) - H(x - x_a) \\rVert_2 dx, $$\nwhere\n $x_a$ is the true value of $x$, P(x) is our proposed cumulative distribution for $x$, $H(x)$ is the Heaviside step function $$ H(x) = \\begin{cases} 1, \u0026\\qquad x=0\\\\ 0, \u0026\\qquad x\\leq 0\\\\ \\end{cases} $$\n $\\lVert \\cdot \\rVert_2$ is the L2 norm.  Explain it The formula looks abstract on first sight, but it becomes crystal clear once we understand it.","authors":null,"summary":null,"date":"2022-03-18T00:00:00Z","references":[{"key":"Hersbach2000","link":"http://dx.doi.org/10.1175/1520-0434(2000)015%3C0559:DOTCRP%3E2.0.CO;2","name":"Hersbach H. Decomposition of the Continuous Ranked Probability Score for Ensemble Prediction Systems. Weather Forecast. 2000;15: 559–570. doi:10.1175/1520-0434(2000)015\u003c0559:DOTCRP\u003e2.0.CO;2"},{"key":"Gouttes2021","link":"https://arxiv.org/abs/2107.03743","name":"Gouttes A, Rasul K, Koren M, Stephan J, Naghibi T. Probabilistic Time Series Forecasting with Implicit Quantile Networks. arXiv [cs.LG]. 2021. doi:10.1109/icdmw.2017.19"},{"key":"Gebetsberger2018","link":"https://journals.ametsoc.org/view/journals/mwre/146/12/mwr-d-17-0364.1.xml","name":"Gebetsberger M, Messner JW, Mayr GJ, Zeileis A. Estimation Methods for Nonhomogeneous Regression Models: Minimum Continuous Ranked Probability Score versus Maximum Likelihood. Mon Weather Rev. 2018;146: 4323–4338. doi:10.1175/MWR-D-17-0364.1"}]},{"type":"til","title":"PySpark: Compare Two Schemas","url":"https://datumorphism.leima.is/til/data/pyspark-schema-comparison/","category":["data"],"tags":["PySpark","Python"],"description":"To compare two dataframe schemas in [[PySpark]]  Data Processing - (Py)Spark Processing Data using (Py)Spark   , we can utilize the set operations in python.\ndef schema_diff(schema1, schema2): return { \u0026#39;fields_in_1_not_2\u0026#39;: set(schema1) - set(schema2), \u0026#39;fields_in_2_not_1\u0026#39;: set(schema2) - set(schema1) } ","authors":["Lei Ma"],"summary":null,"date":"2022-02-14T00:00:00Z","references":null},{"type":"reading","title":"StemGNN","url":"https://datumorphism.leima.is/reading/stemgnn/","category":["graph neural network"],"tags":["graph neural network","graph","forecasting","time series"],"description":"What problem is StemGNN solving:  intra-series temporal pattern: DFT  Each series   inter-series correlations  At each step, the interactions between nodes reversible operator   Example problem  Covid cases: DE, AT, NL, \u0026hellip; Predicting each country without considering the interactions between them Or introduce the people flow between them     GFT:  Completes DFT as it takes care of the inter-series correlations one extra slide for this topic   Convolutions on Graphs  one extra slide for this topic    Graph Basics   How to build the graph  \u0026ldquo;self-attention\u0026rdquo;:  outer product of key, query, as the adjacency matrix  key, query are of length # of ndoes      Weights and Biases  LC 1DConv GLU FC  Experiments  Traffic adjacency matrix  neighbouring sensors have higher correlations   Covid  Neighbouring countries have higher correlation Spetral analysis:  Some eigenvectors have clear meanings      Week spots  Paper and code are not consistent  https://github.","authors":["Lei Ma"],"summary":"","date":"2022-01-01T00:00:00Z","references":[{"link":"http://arxiv.org/abs/2103.07719","name":"Cao D, Wang Y, Duan J, Zhang C, Zhu X, Huang C, et al. Spectral Temporal Graph Neural Network for multivariate time-series forecasting. arXiv [cs.LG]. 2021. Available: http://arxiv.org/abs/2103.07719"}]},{"type":"cards","title":"Convolutions Using Fourier Transform","url":"https://datumorphism.leima.is/cards/math/convolution-and-fourier-transform/","category":["Math"],"tags":["Fourier Transform","Convolution"],"description":"Convolution and Fourier transform","authors":null,"summary":null,"date":"2021-12-04T00:00:00Z","references":[{"link":"https://www.morganclaypool.com/doi/10.2200/S01045ED1V01Y202009AIM046","name":"Hamilton WL. Graph Representation Learning. Morgan \u0026 Claypool Publishers; 2020. pp. 1–159. doi:10.2200/S01045ED1V01Y202009AIM046"}]},{"type":"cards","title":"Graph Convolution Operator","url":"https://datumorphism.leima.is/cards/graph/graph-convolution-operator/","category":["Graph"],"tags":["Graph","Convolution"],"description":"For a given graph $\\mathcal G$, we have an attribute on each node, denoted as $f_v$. All the node attributes put together can be written as a list $\\mathbf f\\to (f_{v_1}, f_{v_2}, \\cdots, f_{v_N})$.\nConvolution on graph is combining attributes on nodes with their neighbors'. The [[adjacency matrix]]  Graph Adjacency Matrix A graph $\\mathcal G$ can be represented with an adjacency matrix $\\mathbf A$. There are some nice and clear examples on wikipedia1, for example, $$ \\begin{pmatrix} 2 \u0026 1 \u0026 0 \u0026 0 \u0026 1 \u0026 0\\\\ 1 \u0026 0 \u0026 1 \u0026 0 \u0026 1 \u0026 0\\\\ 0 \u0026 1 \u0026 0 \u0026 1 \u0026 0 \u0026 0\\\\ 0 \u0026 0 \u0026 1 \u0026 0 \u0026 1 \u0026 1\\\\ 1 \u0026 1 \u0026 0 \u0026 1 \u0026 0 \u0026 0\\\\ 0 \u0026 0 \u0026 0 \u0026 1 \u0026 0 \u0026 0 \\end{pmatrix} $$ for the graph Public Domain, Link   $\\mathbf A$ applied on all node attributes $\\mathbf f$ is such an operation, i.","authors":null,"summary":null,"date":"2021-11-25T00:00:00Z","references":null},{"type":"cards","title":"Centered Kernel Alignment (CKA)","url":"https://datumorphism.leima.is/cards/machine-learning/measurement/centered-kernel-alignment/","category":["Machine Learning"],"tags":["Data","Representation","Similarity"],"description":"Centered Kernel Alignment (CKA) is a similarity metric designed to measure the similarity of between representations of features in neural networks.","authors":null,"summary":null,"date":"2021-11-08T00:00:00Z","references":null},{"type":"cards","title":"Centering Matrix","url":"https://datumorphism.leima.is/cards/math/statistics-centering-matrix/","category":["Math"],"tags":["Matrix","Statistics","numpy"],"description":"Useful when centering a vector around its mean","authors":null,"summary":null,"date":"2021-11-08T00:00:00Z","references":[{"link":"https://en.wikipedia.org/wiki/Centering_matrix","name":"Contributors to Wikimedia projects. Centering matrix. In: Wikipedia [Internet]. 9 Jun 2021 [cited 8 Nov 2021]. Available: https://en.wikipedia.org/wiki/Centering_matrix"}]},{"type":"cards","title":"Hilbert-Schmidt Independence Criterion (HSIC)","url":"https://datumorphism.leima.is/cards/machine-learning/measurement/hilbert-schmidt-independence-criterion/","category":["Machine Learning"],"tags":["Data","Representation","Similarity"],"description":"Given two kernels of the feature representations $K=k(x,x)$ and $L=l(y,y)$, HSIC is defined as12\n$$ \\operatorname{HSIC}(K, L) = \\frac{1}{(n-1)^2} \\operatorname{tr}( K H L H ), $$\nwhere\n $x$, $y$ are the representations of features, $n$ is the dimension of the representation of the features, $H$ is the so-called [[centering matrix]]  Centering Matrix Useful when centering a vector around its mean   .  We can choose different kernel functions $k$ and $l$. For example, if $k$ and $l$ are linear kernels, we have $k(x, y) = l(x, y) = x \\cdot y$. In this linear case, HSIC is simply $\\parallel\\operatorname{cov}(x^T,y^T) \\parallel^2_{\\text{Frobenius}}$.","authors":null,"summary":null,"date":"2021-11-08T00:00:00Z","references":null},{"type":"til","title":"Differential Learning Rates in PyTorch","url":"https://datumorphism.leima.is/til/machine-learning/pytorch/pytorch-differential-learning-rates/","category":["machine-learning"],"tags":["Python","PyTorch","Learning Rate"],"description":"Using different learning rates in different layers of our artificial neural network.","authors":["L Ma"],"summary":"Using different learning rates in different layers of our artificial neural network.","date":"2021-11-01T00:00:00Z","references":[{"key":"Pointer2019","link":"https://www.oreilly.com/library/view/programming-pytorch-for/9781492045342/","name":"Pointer I. Programming PyTorch for deep learning: Creating and deploying deep learning applications. Sebastopol, CA: O’Reilly Media; 2019."},{"key":"PyTorchDocs","link":"https://pytorch.org/docs/stable/optim.html","name":"torch.optim — PyTorch 1.10.0 documentation. [cited 30 Nov 2021]. Available: https://pytorch.org/docs/stable/optim.html"}]},{"type":"cards","title":"Learning Rate","url":"https://datumorphism.leima.is/cards/machine-learning/practice/learning-rate/","category":["Neural Networks"],"tags":["Machine Learning","Deep Learning","Basics","Learning Rate"],"description":"Find a good learning rate","authors":["L Ma"],"summary":null,"date":"2021-11-01T00:00:00Z","references":[{"key":"Pointer2019","link":"https://www.oreilly.com/library/view/programming-pytorch-for/9781492045342/","name":"Pointer I. Programming PyTorch for deep learning: Creating and deploying deep learning applications. Sebastopol, CA: O’Reilly Media; 2019."},{"key":"fastaidocs","link":"https://docs.fast.ai/callback.schedule.html#Learner.lr_find","name":"Howard J, Thomas R. Hyperparam schedule. In: fastai [Internet]. [cited 30 Nov 2021]. Available: https://docs.fast.ai/callback.schedule.html#Learner.lr_find"},{"key":"Howard\u0026Gugger2020","link":"https://www.oreilly.com/library/view/deep-learning-for/9781492045519/","name":"Howard J, Gugger S. Deep Learning for Coders with Fastai and PyTorch: AI Applications Without a PhD. O’Reilly Media, Incorporated; 2020. Available: https://www.oreilly.com/library/view/deep-learning-for/9781492045519/"}]},{"type":"cards","title":"Shatter","url":"https://datumorphism.leima.is/cards/machine-learning/learning-theories/set-shatter/","category":["Machine Learning::Theories"],"tags":["Learning Theory","Set","Shatter"],"description":"Given a set $\\mathcal S$, and a class (collection of sets) $\\mathcal H$.\nFor any subset of $\\mathcal S$, denoted as $\\mathcal s$, if we have an element of class $\\mathcal H$, denoted as $\\mathcal h$, that leads to1\n$$ \\mathcal h \\cap \\mathcal S = \\mathcal s. $$\nSince the power set of $\\mathcal S$ ($P(\\mathcal S)$) contains all the possible subsets of $\\mathcal S$, we can also rephrase the concept using power set. If we can find the power set $P(\\mathcal S)$ by looking into intersections of elements $\\mathcal h$ of $\\mathcal H$ ($\\mathcal h\\in \\mathcal H$), then we say $\\mathcal H$ shatters $\\mathcal S$ 1.","authors":null,"summary":null,"date":"2021-10-27T00:00:00Z","references":[{"key":"shattered_set_wikipedia","link":"https://en.wikipedia.org/wiki/Shattered_set","name":"Shattered Set @ Wikipedia"}]},{"type":"cards","title":"Graph Global Overlap Measure: Katz Index","url":"https://datumorphism.leima.is/cards/graph/graph-global-overlap-katz-index/","category":["Graph"],"tags":["Graph","Basics","Clustering"],"description":"The Katz index is\n$$ \\mathbf S_{\\text{Katz}}[u,v] = \\sum_{i=1}^\\infty \\beta^i \\mathbf A^i[u, v], $$\nwhere $\\mathbf A^i[u, v]$ is the matrix $\\mathbf A$ to the $i$th power. Some for $\\beta^i$. The Katz index describes the similarity between of node $u$ and node $v$.\nDo not confuse power with contravariant indices\n For readers familiar with tensor notations, it might be confusing. We some times use contravariant indices on the top right of the tensor notation.\nBut here ${}^{i}$ means to the $i$th power.\n  The index is proved to be the following\n$$ \\mathbf S_{\\text{Katz}} = (\\mathbf I - \\beta \\mathbf A)^{-1} - \\mathbf I.","authors":null,"summary":null,"date":"2021-09-26T00:00:00Z","references":[{"key":"Katz1953","link":"https://link.springer.com/article/10.1007%2FBF02289026","name":"Katz L. A new status index derived from sociometric analysis. Psychometrika. 1953;18: 39–43. doi:10.1007/BF02289026"},{"key":"Hamilton2020","link":"https://www.morganclaypool.com/doi/10.2200/S01045ED1V01Y202009AIM046","name":"Hamilton WL. Graph Representation Learning. Morgan \u0026 Claypool Publishers; 2020. pp. 1–159. doi:10.2200/S01045ED1V01Y202009AIM046"}]},{"type":"cards","title":"Graph Global Overlap Measure: Leicht-Holme-Newman Index","url":"https://datumorphism.leima.is/cards/graph/graph-global-overlap-leicht-holme-newman-similarity/","category":["Graph"],"tags":["Graph","Basics","Clustering"],"description":"The LHN index is a normalized similarity index.\nFrom Katz Index to LHN Index  [[Katz Index]]  Graph Global Overlap Measure: Katz Index The Katz index is $$ \\mathbf S_{\\text{Katz}}[u,v] = \\sum_{i=1}^\\infty \\beta^i \\mathbf A^i[u, v], $$ where $\\mathbf A^i[u, v]$ is the matrix $\\mathbf A$ to the $i$th power. Some for $\\beta^i$. The Katz index describes the similarity between of node $u$ and node $v$. Do not confuse power with contravariant indices For readers familiar with tensor notations, it might be confusing. We some times use contravariant indices on the top right of the tensor notation. But here ${}^{i}$ means to the $i$th …   has a knob to tune the punishment towards longer paths.","authors":null,"summary":null,"date":"2021-09-26T00:00:00Z","references":[{"key":"Lue2010","link":"http://arxiv.org/abs/1010.0725","name":"Lu L, Zhou T. Link Prediction in Complex Networks: A Survey. arXiv [physics.soc-ph]. 2010. Available: http://arxiv.org/abs/1010.0725"},{"key":"Hamilton2020","link":"https://www.morganclaypool.com/doi/10.2200/S01045ED1V01Y202009AIM046","name":"Hamilton WL. Graph Representation Learning. Morgan \u0026 Claypool Publishers; 2020. pp. 1–159. doi:10.2200/S01045ED1V01Y202009AIM046"}]},{"type":"cards","title":"Graph Global Overlap Measure: Random Walk Similarity","url":"https://datumorphism.leima.is/cards/graph/graph-global-overlap-random-walk-similarity/","category":["Graph"],"tags":["Graph","Basics","Clustering"],"description":"Random Walk Construct a stochastic transfer matrix $P$ by normalizing the adjacency matrix $\\mathbf A$ using the node degrees of the target nodes,\n$$ \\mathbf P = \\mathbf A \\mathbf D^{-1}, $$\nwhere $\\mathbf A$ is the [[adjacency matrix]]  Graph Adjacency Matrix A graph $\\mathcal G$ can be represented with an adjacency matrix $\\mathbf A$. There are some nice and clear examples on wikipedia1, for example, $$ \\begin{pmatrix} 2 \u0026 1 \u0026 0 \u0026 0 \u0026 1 \u0026 0\\\\ 1 \u0026 0 \u0026 1 \u0026 0 \u0026 1 \u0026 0\\\\ 0 \u0026 1 \u0026 0 \u0026 1 \u0026 0 \u0026 0\\\\ 0 \u0026 0 \u0026 1 \u0026 0 \u0026 1 \u0026 1\\\\ 1 \u0026 1 \u0026 0 \u0026 1 \u0026 0 \u0026 0\\\\ 0 \u0026 0 \u0026 0 \u0026 1 \u0026 0 \u0026 0 \\end{pmatrix} $$ for the graph Public Domain, Link   and $\\mathbf D$ is a diagonalized matrix with the diagonal elements being the degrees.","authors":null,"summary":null,"date":"2021-09-26T00:00:00Z","references":[{"key":"Hamilton2020","link":"https://www.morganclaypool.com/doi/10.2200/S01045ED1V01Y202009AIM046","name":"Hamilton WL. Graph Representation Learning. Morgan \u0026 Claypool Publishers; 2020. pp. 1–159. doi:10.2200/S01045ED1V01Y202009AIM046"}]},{"type":"cards","title":"Graph Isomorphism","url":"https://datumorphism.leima.is/cards/graph/graph-isomorphism/","category":["Graph"],"tags":["Graph","Basics"],"description":"For two graphs, $\\mathcal G$ and $\\mathcal H$, the two graphs are isomorphism on the following condition\n$$ u, v \\text{ adjacent in } G \\iff u, v \\text{ adjacent in } H. $$\nAn algorithm to find approximate isomorphism is the [[Weisfeiler Lehman Method]]  Weisfeiler-Lehman Kernel The Weisfeiler-Lehman kernel is an iterative integration of neighborhood information. We initialize the labels for each node using its own node degree. At each step, we take the neighboring node degrees to form a [[multiset]] Multiset, mset or bag A bag is a set in which duplicate elements are allowed. An ordered bag is a list that we use in programming.","authors":null,"summary":null,"date":"2021-09-26T00:00:00Z","references":[{"key":"Hamilton2020","link":"https://www.morganclaypool.com/doi/10.2200/S01045ED1V01Y202009AIM046","name":"Hamilton WL. Graph Representation Learning. Morgan \u0026 Claypool Publishers; 2020. pp. 1–159. doi:10.2200/S01045ED1V01Y202009AIM046"}]},{"type":"cards","title":"Graph Local Overlap Measure: Adamic Adar Index","url":"https://datumorphism.leima.is/cards/graph/graph-local-overlap-adamic-adar-index/","category":["Graph"],"tags":["Graph","Basics","Clustering"],"description":"The Adamic Adar (AA) index is1\n$$ \\mathbf S_{\\text{AA}}[v_1,v_2] = \\sum_{u\\in\\mathcal N(u) \\cap \\mathcal N(v)} \\frac{1}{\\log d_u}, $$\nwhere $d_u$ is the node degree of node $u$ and $\\mathcal N(u)$ is the neighbor nodes of $u$.\nIf two nodes have shared neighbor, the degree of the neighbors will be at least 2. So it is safe to use $1/\\log d_u$.\n   Adamic2003  Adamic LA, Adar E. Friends and neighbors on the Web. Soc Networks. 2003;25: 211–230. doi:10.1016/S0378-8733(03)00009-1 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","authors":null,"summary":null,"date":"2021-09-26T00:00:00Z","references":[{"key":"Adamic2003","link":"https://www.sciencedirect.com/science/article/abs/pii/S0378873303000091","name":"Adamic LA, Adar E. Friends and neighbors on the Web. Soc Networks. 2003;25: 211–230. doi:10.1016/S0378-8733(03)00009-1"},{"key":"Hamilton2020","link":"https://www.morganclaypool.com/doi/10.2200/S01045ED1V01Y202009AIM046","name":"Hamilton WL. Graph Representation Learning. Morgan \u0026 Claypool Publishers; 2020. pp. 1–159. doi:10.2200/S01045ED1V01Y202009AIM046"}]},{"type":"cards","title":"Graph Local Overlap Measure: Resource Allocation Index","url":"https://datumorphism.leima.is/cards/graph/graph-local-overlap-resource-allocation-index/","category":["Graph"],"tags":["Graph","Basics","Clustering"],"description":"The Resource Allocation (RA) index is\n$$ \\mathbf S_{\\text{RA}}[v_1,v_2] = \\sum_{u\\in\\mathcal N(u) \\cap \\mathcal N(v)} \\frac{1}{d_u}, $$\nwhere $d_u$ is the node degree of node $u$ and $\\mathcal N(u)$ is the neighbor nodes of $u$.","authors":null,"summary":null,"date":"2021-09-26T00:00:00Z","references":[{"key":"Hamilton2020","link":"https://www.morganclaypool.com/doi/10.2200/S01045ED1V01Y202009AIM046","name":"Hamilton WL. Graph Representation Learning. Morgan \u0026 Claypool Publishers; 2020. pp. 1–159. doi:10.2200/S01045ED1V01Y202009AIM046"}]},{"type":"cards","title":"Graph Local Overlap Measure: Salton Index","url":"https://datumorphism.leima.is/cards/graph/graph-local-overlap-salton-index/","category":["Graph"],"tags":["Graph","Basics","Clustering"],"description":"The Salton index is\n$$ \\mathbf S_{\\text{Salton}}[u,v] = \\frac{ 2\\lvert \\mathcal N (u) \\cap \\mathcal N(v) \\rvert }{ \\sqrt{d_u d_v}}, $$\nwhere $d_u$ is the node degree of node $u$ and $\\mathcal N(u)$ is the neighbor nodes of $u$.","authors":null,"summary":null,"date":"2021-09-26T00:00:00Z","references":[{"key":"Hamilton2020","link":"https://www.morganclaypool.com/doi/10.2200/S01045ED1V01Y202009AIM046","name":"Hamilton WL. Graph Representation Learning. Morgan \u0026 Claypool Publishers; 2020. pp. 1–159. doi:10.2200/S01045ED1V01Y202009AIM046"}]},{"type":"cards","title":"Graph Local Overlap Measure: Sorensen Index","url":"https://datumorphism.leima.is/cards/graph/graph-local-overlap-sorensen-index/","category":["Graph"],"tags":["Graph","Basics","Clustering"],"description":"The Sorensen index is\n$$ \\mathbf S_{\\text{Sorensen}}[u,v] = \\frac{ 2\\lvert \\mathcal N (u) \\cap \\mathcal N(v) \\rvert }{ d_u + d_v}, $$\nwhere $d_u$ is the node degree of node $u$ and $\\mathcal N(u)$ is the neighbor nodes of $u$.","authors":null,"summary":null,"date":"2021-09-26T00:00:00Z","references":[{"key":"Hamilton2020","link":"https://www.morganclaypool.com/doi/10.2200/S01045ED1V01Y202009AIM046","name":"Hamilton WL. Graph Representation Learning. Morgan \u0026 Claypool Publishers; 2020. pp. 1–159. doi:10.2200/S01045ED1V01Y202009AIM046"}]},{"type":"cards","title":"Betweenness Centrality of a Graph","url":"https://datumorphism.leima.is/cards/graph/graph-betweenness-centrality/","category":["Graph"],"tags":["Graph","Statistics"],"description":"Betweenness centrality of a node $v$ is measurement of how likely the shortest path between two nodes $u_s$ and $u_t$ is gonna pass through node $v$,\n$$ c(v) = \\sum_{v\\neq u_s\\neq u_t} \\frac{\\sigma_{u_su_t}(v) }{\\sigma_{u_su_t}}, $$\nwhere $\\sigma_{u_su_t}(v)$ is the number of shortest path between $u_s$ and $u_t$, and passing through $u$, while $\\sigma_{u_su_t}$ is the number of shortest path between $u_s$ and $u_t$.\nA figure from wikipedia demonstrates this idea well. The nodes on the outreach have smaller betweenness centrality, while the nodes in the core have higher betweenness centrality.\n  Source: Wikipedia\n  Outreach and Core\n It is almost like cheating using the work \u0026ldquo;outreach\u0026rdquo; and \u0026ldquo;core\u0026rdquo; here.","authors":null,"summary":null,"date":"2021-09-25T00:00:00Z","references":[{"key":"BetweennessCentrality","link":"https://en.wikipedia.org/wiki/Betweenness_centrality","name":"Contributors to Wikimedia projects. Betweenness centrality. In: Wikipedia [Internet]. 21 Aug 2021 [cited 26 Sep 2021]. Available: https://en.wikipedia.org/wiki/Betweenness_centrality"}]},{"type":"cards","title":"Eigenvector Centrality of a Graph","url":"https://datumorphism.leima.is/cards/graph/graph-eigenvector-centrality/","category":["Graph"],"tags":["Graph","Statistics"],"description":"Given a graph with adjacency matrix $\\mathbf A$, the eigenvector centrality is\n$$ \\mathbf e_u = \\frac{1}{\\lambda} \\sum_{v\\in\\mathcal V} \\mathbf A[u,v] \\mathbf e_v, \\qquad \\forall u \\in \\mathcal V. $$\nWhy is it called Eigenvector Centrality\n The definition is equivalent to\n$$ \\lambda \\mathbf e = \\mathbf A\\mathbf e. $$\n  Power Iteration The solution to $\\mathbf e$ is the eigenvector that corresponds to the largest eigenvalue $\\lambda_1$. Power iteration method can help us get this eigenvector, i.e., the $^{(t+1)}$ iteration is related to the previous iteration $^{(t)}$, through the following relation,\n$$ \\mathbf e^{(t+1)} = \\mathbf A \\mathbf e^{(t)}.","authors":null,"summary":null,"date":"2021-09-25T00:00:00Z","references":[{"key":"Hamilton2020","link":"https://www.morganclaypool.com/doi/10.2200/S01045ED1V01Y202009AIM046","name":"Hamilton WL. Graph Representation Learning. Morgan \u0026 Claypool Publishers; 2020. pp. 1–159. doi:10.2200/S01045ED1V01Y202009AIM046"}]},{"type":"cards","title":"Graph Adjacency Matrix","url":"https://datumorphism.leima.is/cards/graph/graph-adjacency-matrix/","category":["Graph"],"tags":["Graph"],"description":"A graph $\\mathcal G$ can be represented with an adjacency matrix $\\mathbf A$. There are some nice and clear examples on wikipedia1, for example,\n$$ \\begin{pmatrix} 2 \u0026 1 \u0026 0 \u0026 0 \u0026 1 \u0026 0\\\\ 1 \u0026 0 \u0026 1 \u0026 0 \u0026 1 \u0026 0\\\\ 0 \u0026 1 \u0026 0 \u0026 1 \u0026 0 \u0026 0\\\\ 0 \u0026 0 \u0026 1 \u0026 0 \u0026 1 \u0026 1\\\\ 1 \u0026 1 \u0026 0 \u0026 1 \u0026 0 \u0026 0\\\\ 0 \u0026 0 \u0026 0 \u0026 1 \u0026 0 \u0026 0 \\end{pmatrix} $$\nfor the graph\n  Public Domain, Link","authors":["Lei Ma"],"summary":null,"date":"2021-09-25T00:00:00Z","references":[{"key":"wiki_adjacency_matrix","link":"https://en.wikipedia.org/wiki/Adjacency_matrix#Undirected_graphs","name":"Contributors to Wikimedia projects. Adjacency matrix. In: Wikipedia [Internet]. 4 Oct 2021 [cited 4 Dec 2021]. Available: https://en.wikipedia.org/wiki/Adjacency_matrix#Undirected_graphs"}]},{"type":"cards","title":"Graph Clustering Coefficient","url":"https://datumorphism.leima.is/cards/graph/graph-local-clustering-coefficient/","category":["Graph"],"tags":["Graph","Statistics"],"description":"Local Clustering Coefficient $$ c_u = \\frac{ \\lvert (v_1,v_2)\\in \\mathcal E: v_1, v_2 \\in \\mathcal N(u) \\rvert}{ \\color{red}{d_n \\choose 2} }, $$\nwhere $\\color{red}{d_n \\choose 2}$ means all the possible combinations of neighbor nodes, and $\\mathcal N(u)$ is the set of nodes that are neighbor to $u$.\nClosed Triangles Ego Graph\n   Counting the closed triangles of the ego graph of a node and normalize it by the total possible number of triangles is also a measure of clustering coefficients.\n If the ego graph of $u$ is fully connected, we have $c_u=1$; If the ego graph of $u$ is a star, we have $c_u=0$.","authors":null,"summary":null,"date":"2021-09-25T00:00:00Z","references":[{"key":"Hamilton2020","link":"https://www.morganclaypool.com/doi/10.2200/S01045ED1V01Y202009AIM046","name":"Hamilton WL. Graph Representation Learning. Morgan \u0026 Claypool Publishers; 2020. pp. 1–159. doi:10.2200/S01045ED1V01Y202009AIM046"}]},{"type":"cards","title":"Graph Cuts","url":"https://datumorphism.leima.is/cards/graph/graph-cuts/","category":["Graph"],"tags":["Graph","Heterophily"],"description":"Cut For a subset of nodes $\\mathcal A\\subset \\mathcal V$, the rest of nodes can be denoted as $\\bar {\\mathcal A} = \\mathcal V \\setminus \\mathcal A$. In other words, $\\mathcal A \\cup \\bar {\\mathcal A} = \\mathcal V$ and $\\mathcal A \\cap \\bar {\\mathcal A} = \\emptyset$. That being said, the nodes can be partitioned into two subsets, $\\mathcal A$ and $\\bar {\\mathcal A}$. The cut of this partition is defined as the total number of edges between them,\n$$ \\operatorname{Cut} \\left( \\mathcal A, \\bar{\\mathcal A} \\right) = \\frac{1}{2} \\left( \\lvert (u, v)\\in \\mathcal E: u\\in \\mathcal A, v\\in \\bar{\\mathcal A} \\rvert + \\lvert (u, v)\\in \\mathcal E: u\\in \\bar{\\mathcal A}, v\\in {\\mathcal A} \\rvert \\right).","authors":null,"summary":null,"date":"2021-09-25T00:00:00Z","references":[{"key":"Hamilton2020","link":"https://www.morganclaypool.com/doi/10.2200/S01045ED1V01Y202009AIM046","name":"Hamilton WL. Graph Representation Learning. Morgan \u0026 Claypool Publishers; 2020. pp. 1–159. doi:10.2200/S01045ED1V01Y202009AIM046"}]},{"type":"cards","title":"Graph Laplacians","url":"https://datumorphism.leima.is/cards/graph/graph-laplacians/","category":["Graph"],"tags":["Graph","Heterophily"],"description":"Laplacian is a useful representation of graphs. The unnormalized Laplacian is\n$$ \\mathbf L = \\mathbf D - \\mathbf A, $$\nwhere $\\mathbf A$ is the [[adjacency matrix]]  Graph Adjacency Matrix A graph $\\mathcal G$ can be represented with an adjacency matrix $\\mathbf A$. There are some nice and clear examples on wikipedia1, for example, $$ \\begin{pmatrix} 2 \u0026 1 \u0026 0 \u0026 0 \u0026 1 \u0026 0\\\\ 1 \u0026 0 \u0026 1 \u0026 0 \u0026 1 \u0026 0\\\\ 0 \u0026 1 \u0026 0 \u0026 1 \u0026 0 \u0026 0\\\\ 0 \u0026 0 \u0026 1 \u0026 0 \u0026 1 \u0026 1\\\\ 1 \u0026 1 \u0026 0 \u0026 1 \u0026 0 \u0026 0\\\\ 0 \u0026 0 \u0026 0 \u0026 1 \u0026 0 \u0026 0 \\end{pmatrix} $$ for the graph Public Domain, Link   and $\\mathbf D$ is the degree matrix, i.","authors":null,"summary":null,"date":"2021-09-25T00:00:00Z","references":[{"key":"Hamilton2020","link":"https://www.morganclaypool.com/doi/10.2200/S01045ED1V01Y202009AIM046","name":"Hamilton WL. Graph Representation Learning. Morgan \u0026 Claypool Publishers; 2020. pp. 1–159. doi:10.2200/S01045ED1V01Y202009AIM046"},{"key":"Li2014","link":"http://dx.doi.org/10.1186/1029-242X-2014-316","name":"Li J, Guo J-M, Shiu WC. Bounds on normalized Laplacian eigenvalues of graphs. J Inequal Appl. 2014;2014: 1–8. doi:10.1186/1029-242X-2014-316"}]},{"type":"cards","title":"Heterophily on Graph","url":"https://datumorphism.leima.is/cards/graph/heterophily/","category":["Graph"],"tags":["Graph","Heterophily"],"description":"Heterophily is the tendency to differ from others. Heterophily on a graph is the tendency to connect to nodes that are different from itself, e.g., nodes with different attributes have higher probability of edge.","authors":null,"summary":null,"date":"2021-09-25T00:00:00Z","references":[{"key":"Hamilton2020","link":"https://www.morganclaypool.com/doi/10.2200/S01045ED1V01Y202009AIM046","name":"Hamilton WL. Graph Representation Learning. Morgan \u0026 Claypool Publishers; 2020. pp. 1–159. doi:10.2200/S01045ED1V01Y202009AIM046"}]},{"type":"cards","title":"Homophily on Graph","url":"https://datumorphism.leima.is/cards/graph/homophily/","category":["Graph"],"tags":["Graph","Homophily"],"description":" Homophily is the principle that a contact between similar people occurs at ahigher rate than among dissimilar people \u0026ndash; McPherson20011\n    McPherson2001  McPherson M, Smith-Lovin L, Cook JM. Birds of a Feather: Homophily in Social Networks. Annu Rev Sociol. 2001;27: 415–444. doi:10.1146/annurev.soc.27.1.415 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","authors":null,"summary":null,"date":"2021-09-25T00:00:00Z","references":[{"key":"McPherson2001","link":"https://www.annualreviews.org/doi/10.1146/annurev.soc.27.1.415","name":"McPherson M, Smith-Lovin L, Cook JM. Birds of a Feather: Homophily in Social Networks. Annu Rev Sociol. 2001;27: 415–444. doi:10.1146/annurev.soc.27.1.415"}]},{"type":"cards","title":"Node Degree","url":"https://datumorphism.leima.is/cards/graph/node-degree/","category":["Graph"],"tags":["Graph","Statistics"],"description":"Node degree of a node $u$\n$$ d_u = \\sum_{v\\in \\mathcal V} A[u,v], $$\nwhere $A$ is the adjacency matrix.","authors":null,"summary":null,"date":"2021-09-25T00:00:00Z","references":[{"key":"Hamilton2020","link":"https://www.morganclaypool.com/doi/10.2200/S01045ED1V01Y202009AIM046","name":"Hamilton WL. Graph Representation Learning. Morgan \u0026 Claypool Publishers; 2020. pp. 1–159. doi:10.2200/S01045ED1V01Y202009AIM046"}]},{"type":"cards","title":"Structural Equivalence on Graph","url":"https://datumorphism.leima.is/cards/graph/structural-equivalence/","category":["Graph"],"tags":["Graph"],"description":"Structural Equivalence means that nodes with similar neighborhood structures will share similar attributes.","authors":null,"summary":null,"date":"2021-09-25T00:00:00Z","references":[{"key":"Hamilton2020","link":"https://www.morganclaypool.com/doi/10.2200/S01045ED1V01Y202009AIM046","name":"Hamilton WL. Graph Representation Learning. Morgan \u0026 Claypool Publishers; 2020. pp. 1–159. doi:10.2200/S01045ED1V01Y202009AIM046"}]},{"type":"cards","title":"Weisfeiler-Lehman Kernel","url":"https://datumorphism.leima.is/cards/graph/graph-weisfeiler-lehman-kernel/","category":["Graph"],"tags":["Graph","Statistics"],"description":"The Weisfeiler-Lehman kernel is an iterative integration of neighborhood information.\nWe initialize the labels for each node using its own node degree. At each step, we take the neighboring node degrees to form a [[multiset]]  Multiset, mset or bag A bag is a set in which duplicate elements are allowed. An ordered bag is a list that we use in programming.   . At step $K$, we have the multisets for each node. Those multisets at each node can be processed to form an representation of the graph which is in turn used to calculate statistics of the graph.","authors":null,"summary":null,"date":"2021-09-25T00:00:00Z","references":[{"key":"Shervashidze2011","link":"https://dl.acm.org/doi/10.5555/1953048.2078187","name":"Shervashidze N, Schweitzer P, van Leeuwen EJ, Mehlhorn K, Borgwardt KM. Weisfeiler-Lehman Graph Kernels. J Mach Learn Res. 2011;12: 2539–2561. Available: https://dl.acm.org/doi/10.5555/1953048.2078187"},{"key":"Hamilton2020","link":"https://www.morganclaypool.com/doi/10.2200/S01045ED1V01Y202009AIM046","name":"Hamilton WL. Graph Representation Learning. Morgan \u0026 Claypool Publishers; 2020. pp. 1–159. doi:10.2200/S01045ED1V01Y202009AIM046"}]},{"type":"cards","title":"Initialize Artificial Neural Networks","url":"https://datumorphism.leima.is/cards/machine-learning/neural-networks/neural-networks-initialization/","category":["Neural Networks"],"tags":["Artificial Neuron","Neural Network","Basics"],"description":"Initialize a neural network is important for the training and performance. Some initializations simply don't work, some will degrade the performance of the model. We should choose wisely.","authors":["L Ma"],"summary":null,"date":"2021-09-23T00:00:00Z","references":[{"key":"Lippe2020","link":"https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial3/Activation_Functions.html","name":"Lippe P. Tutorial 3: Activation Functions — UvA DL Notebooks v1.1 documentation. In: UvA Deep Learning Tutorials [Internet]. [cited 23 Sep 2021]. Available: https://uvadlc-notebooks.readthedocs.io"},{"link":"https://pouannes.github.io/blog/initialization/#mjx-eqn-eqfwd_K","name":"Ouannes P. How to initialize deep neural networks? Xavier and Kaiming initialization. In: Title [Internet]. 22 Mar 2019 [cited 24 Sep 2021]. Available: https://pouannes.github.io/blog/initialization"},{"key":"Glorot2010","link":"http://proceedings.mlr.press/v9/glorot10a.html","name":"Glorot X, Bengio Y. Understanding the difficulty of training deep feedforward neural networks. Teh YW, Titterington M, editors. 2010;9: 249–256. Available: http://proceedings.mlr.press/v9/glorot10a.html"},{"key":"Katanforoosh2019","link":"https://www.deeplearning.ai/ai-notes/initialization/","name":"Katanforoosh \u0026 Kunin, \"Initializing neural networks\", deeplearning.ai, 2019."}]},{"type":"cards","title":"Alignment and Uniformity","url":"https://datumorphism.leima.is/cards/machine-learning/embedding/alignment-and-uniformity/","category":["Machine Learning::Embedding"],"tags":["Representation","Machine Learning","Contrastive Representation Learning"],"description":"A good representation should be able to separate different instances and cluster similar instances.","authors":null,"summary":null,"date":"2021-09-11T00:00:00Z","references":[{"key":"Wang2005","link":"http://arxiv.org/abs/2005.10242","name":"Wang T, Isola P. Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2005.10242"}]},{"type":"cards","title":"Cross Entropy","url":"https://datumorphism.leima.is/cards/information/cross-entropy/","category":["Information"],"tags":["Information Theory","Entropy"],"description":"Cross entropy is1\n$$ H(p, q) = \\mathbb E_{p} \\left[ -\\log q \\right]. $$\nCross entropy $H(p, q)$ can also be decomposed,\n$$ H(p, q) = H(p) + \\operatorname{D}_{\\mathrm{KL}} \\left( p \\parallel q \\right), $$\nwhere $H(p)$ is the [[entropy of $P$]]  Shannon Entropy Shannon entropy $S$ is the expectation of information content $I(X)=-\\log \\left(p\\right)$1, \\begin{equation} H(p) = \\mathbb E_{p}\\left[ -\\log \\left(p\\right) \\right]. \\end{equation} shannon_entropy_wiki Contributors to Wikimedia projects. Entropy (information theory). In: Wikipedia [Internet]. 29 Aug 2021 [cited 4 Sep 2021]. Available: https://en.wikipedia.org/wiki/Entropy_(information_theory) \u0026#160;\u0026#x21a9;\u0026#xfe0e;    and $\\operatorname{D}_{\\mathrm{KL}}$ is the [[KL Divergence]]  KL Divergence Kullback–Leibler divergence indicates the differences between two distributions   .","authors":["Lei Ma"],"summary":null,"date":"2021-09-04T00:00:00Z","references":[{"key":"cross_entropy_wiki","link":"https://en.wikipedia.org/wiki/Cross_entropy","name":"Contributors to Wikimedia projects. Cross entropy. In: Wikipedia [Internet]. 4 Jul 2021 [cited 4 Sep 2021]. Available: https://en.wikipedia.org/wiki/Cross_entropy"},{"key":"Mehta2019","link":"https://linkinghub.elsevier.com/retrieve/pii/S0370157319300766","name":"Mehta P, Wang C-H, Day AGR, Richardson C, Bukov M, Fisher CK, et al. A high-bias, low-variance introduction to Machine Learning for physicists. Phys Rep. 2019;810: 1–124. doi:10.1016/j.physrep.2019.03.001"}]},{"type":"cards","title":"f-Divergence","url":"https://datumorphism.leima.is/cards/information/f-divergence/","category":["Information"],"tags":["Information Theory","Divergence"],"description":"The f-divergence is defined as1\n$$ \\operatorname{D}_f = \\int f\\left(\\frac{p}{q}\\right) q\\mathrm d\\mu, $$\nwhere $p$ and $q$ are two densities and $\\mu$ is a reference distribution.\nRequirements on the generating function\n The generating function $f$ is required to\n be convex, and $f(1) =0$.    For $f(x) = x \\log x$ with $x=p/q$, f-divergence is reduced to the KL divergence\n$$ \\begin{align} \u0026\\int f\\left(\\frac{p}{q}\\right) q\\mathrm d\\mu \\\\ =\u0026 \\int \\frac{p}{q} \\log \\left( \\frac{p}{q} \\right) \\mathrm d\\mu \\\\ =\u0026 \\int p \\log \\left( \\frac{p}{q} \\right) \\mathrm d\\mu. \\end{align} $$\nFor more special cases of f-divergence, please refer to wikipedia1.","authors":["Lei Ma"],"summary":null,"date":"2021-09-04T00:00:00Z","references":[{"key":"f-divergence_wiki","link":"https://en.wikipedia.org/wiki/F-divergence","name":"Contributors to Wikimedia projects. F-divergence. In: Wikipedia [Internet]. 17 Jul 2021 [cited 4 Sep 2021]. Available: https://en.wikipedia.org/wiki/F-divergence"},{"key":"Nowozin2016","link":"http://arxiv.org/abs/1606.00709","name":"Nowozin S, Cseke B, Tomioka R. f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization. arXiv [stat.ML]. 2016. Available: http://arxiv.org/abs/1606.00709"}]},{"type":"cards","title":"Jensen-Shannon Divergence","url":"https://datumorphism.leima.is/cards/information/jensen-shannon-divergence/","category":["Information"],"tags":["Information Theory","Divergence"],"description":"The Jensen-Shannon divergence is a symmetric divergence of distributions $P$ and $Q$,\n$$ \\operatorname{D}_{\\text{JS}} = \\frac{1}{2} \\left[ \\operatorname{D}_{\\text{KL}} \\left(P \\bigg\\Vert \\frac{P+Q}{2} \\right) + \\operatorname{D}_{\\text{KL}} \\left(Q \\bigg\\Vert \\frac{P+Q}{2}\\right) \\right], $$\nwhere $\\operatorname{D}_{\\text{KL}}$ is the [[KL Divergence]]  KL Divergence Kullback–Leibler divergence indicates the differences between two distributions   .","authors":["Lei Ma"],"summary":null,"date":"2021-09-04T00:00:00Z","references":[{"key":"jensen-shannon_divergence_wiki","link":"https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence","name":"Contributors to Wikimedia projects. Jensen–Shannon divergence. In: Wikipedia [Internet]. 15 Jun 2021 [cited 6 Sep 2021]. Available: https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence"}]},{"type":"cards","title":"Shannon Entropy","url":"https://datumorphism.leima.is/cards/information/shannon-entropy/","category":["Information"],"tags":["Information Theory","Entropy"],"description":"Shannon entropy $S$ is the expectation of information content $I(X)=-\\log \\left(p\\right)$1,\n\\begin{equation} H(p) = \\mathbb E_{p}\\left[ -\\log \\left(p\\right) \\right]. \\end{equation}\n   shannon_entropy_wiki  Contributors to Wikimedia projects. Entropy (information theory). In: Wikipedia [Internet]. 29 Aug 2021 [cited 4 Sep 2021]. Available: https://en.wikipedia.org/wiki/Entropy_(information_theory) \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","authors":["Lei Ma"],"summary":null,"date":"2021-09-04T00:00:00Z","references":[{"key":"shannon_entropy_wiki","link":"https://en.wikipedia.org/wiki/Entropy_(information_theory)","name":"Contributors to Wikimedia projects. Entropy (information theory). In: Wikipedia [Internet]. 29 Aug 2021 [cited 4 Sep 2021]. Available: https://en.wikipedia.org/wiki/Entropy_(information_theory)"}]},{"type":"til","title":"VSCode Setup Tests when Module is in a Different Folder","url":"https://datumorphism.leima.is/til/misc/vscode/vscode-setup-python-tests-with-module-in-src-folder/","category":["misc"],"tags":["VSCode"],"description":"Use .env file","authors":null,"summary":"Use .env file","date":"2021-08-31T00:00:00Z","references":[{"link":"https://code.visualstudio.com/docs/python/environments#_use-of-the-pythonpath-variable","name":"Using Python environments in VS Code"}]},{"type":"til","title":"VSCode Terminal Python Can Not Activate Conda on Mac","url":"https://datumorphism.leima.is/til/misc/vscode/vscode-terminal-python-not-conda-mac/","category":["misc"],"tags":["VSCode"],"description":"Enable your key repeat in vscode on mac","authors":null,"summary":"Enable your key repeat in vscode on mac","date":"2021-08-31T00:00:00Z","references":null},{"type":"til","title":"Postgres Timezone Conversions","url":"https://datumorphism.leima.is/til/data/postgres.timezone-conversion/","category":["data"],"tags":["Postgres","Warehouse"],"description":"Pitfals of timezone conversion in Postgres","authors":["Lei Ma"],"summary":"Pitfals of timezone conversion in Postgres","date":"2021-08-27T00:00:00Z","references":[{"link":"https://stackoverflow.com/questions/27938857/postgres-at-time-zone-function-shows-wrong-time","name":"kadkaz. Postgres AT TIME ZONE function shows wrong time? In: Stack Overflow [Internet]. [cited 27 Aug 2021]. Available: https://stackoverflow.com/questions/27938857/postgres-at-time-zone-function-shows-wrong-time"},{"link":"https://www.postgresql.org/docs/current/datatype-datetime.html","name":"8.5. Date/Time Types. In: PostgreSQL Documentation [Internet]. 12 Aug 2021 [cited 27 Aug 2021]. Available: https://www.postgresql.org/docs/current/datatype-datetime.html"},{"link":"https://www.postgresql.org/docs/7.2/timezones.html","name":"Time Zones. (2012, January 1). PostgreSQL Documentation. https://www.postgresql.org/docs/7.2/timezones.html"},{"link":"https://www.enterprisedb.com/postgres-tutorials/postgres-time-zone-explained","name":"Momjian B. Postgres AT TIME ZONE Explained. In: EDB [Internet]. 7 Nov 2019 [cited 27 Aug 2021]. Available: https://www.enterprisedb.com/postgres-tutorials/postgres-time-zone-explained"}]},{"type":"cards","title":"Mutual Information","url":"https://datumorphism.leima.is/cards/information/mutual-information/","category":["Information"],"tags":["Information Theory","Mutual Information"],"description":"Mutual information is defined as\n$$ I(X;Y) = \\mathbb E_{p_{XY}} \\ln \\frac{P_{XY}}{P_X P_Y}. $$\nIn the case that $X$ and $Y$ are independent variables, we have $P_{XY} = P_X P_Y$, thus $I(X;Y) = 0$. This makes sense as there would be no \u0026ldquo;mutual\u0026rdquo; information if the two variables are independent of each other.\nEntropy and Cross Entropy Mutual information is closely related to entropy. A simple decomposition shows that\n$$ I(X;Y) = H(X) - H(X\\mid Y), $$\nwhich is the reduction of uncertainty in $X$ after observing $Y$.\nKL Divergence This definition of mutual information is equivalent to the following [[KL Divergence]]  KL Divergence Kullback–Leibler divergence indicates the differences between two distributions   ,","authors":["Lei Ma"],"summary":null,"date":"2021-08-13T00:00:00Z","references":[{"key":"Liu2020","link":"http://arxiv.org/abs/2006.08218","name":"Liu X, Zhang F, Hou Z, Wang Z, Mian L, Zhang J, et al. Self-supervised Learning: Generative or Contrastive. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2006.08218"},{"key":"Latham2009","link":"http://www.scholarpedia.org/article/Mutual_information","name":"Latham PE, Roudi Y. Mutual information. Scholarpedia. 2009;4. doi:10.4249/scholarpedia.1658"}]},{"type":"cards","title":"Noise Contrastive Estimation: NCE","url":"https://datumorphism.leima.is/cards/machine-learning/learning-theories/noise-contrastive-estimation/","category":["Machine Learning::Theories"],"tags":["Learning Theory"],"description":"Noise contrastive estimation (NCE) objective function is1\n$$ \\mathcal L = \\mathbb E_{x, x^{+}, x^{-}} \\left[ - \\ln \\frac{ C(x, x^{+})}{ C(x,x^{+}) + C(x,x^{-}) } \\right], $$\nwhere\n $x^{+}$ represents data similar to $x$, $x^{-}$ represents data dissimilar to $x$, $C(\\cdot, \\cdot)$ is a function to compute the similarities.  For example, we can use\n$$ C(x, x^{+}) = e^{ f(x)^T f(x^{+}) }, $$\nso that the objective function becomes\n$$ \\mathcal L = \\mathbb E_{x, x^{+}, x^{-}} \\left[ - \\ln \\frac{ e^{ f(x)^T f(x^{+}) } }{ e^{ f(x)^T f(x^{+}) } + e^{ f(x)^T f(x^{-}) } } \\right]. $$","authors":null,"summary":null,"date":"2021-08-13T00:00:00Z","references":[{"key":"Liu2020","link":"http://arxiv.org/abs/2006.08218","name":"Liu X, Zhang F, Hou Z, Wang Z, Mian L, Zhang J, et al. Self-supervised Learning: Generative or Contrastive. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2006.08218"}]},{"type":"reading","title":"Self-supervised Learning: Generative or Constrastive","url":"https://datumorphism.leima.is/reading/self-supervised-learning-generative-or-contrastive-2006.08218/","category":["theory"],"tags":["machine learning","self-supervised learning"],"description":"Review of self-supervised learning.","authors":null,"summary":"Review of self-supervised learning.","date":"2021-08-13T00:00:00Z","references":[{"link":"http://arxiv.org/abs/2006.08218","name":"Liu X, Zhang F, Hou Z, Wang Z, Mian L, Zhang J, et al. Self-supervised Learning: Generative or Contrastive. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2006.08218"}]},{"type":"cards","title":"The log-sum-exp Trick","url":"https://datumorphism.leima.is/cards/machine-learning/neural-networks/log-sum-exp-trick/","category":["Machine Learning"],"tags":["Numerical","Neural Network","Basics"],"description":"For numerical stability we can use the log-sum-exp trick to calculate some loss such as cross entropy","authors":["LM"],"summary":null,"date":"2021-07-28T00:00:00Z","references":[{"link":"https://www.xarg.org/2016/06/the-log-sum-exp-trick-in-machine-learning/","name":"Eisele R. The log-sum-exp trick in Machine Learning • Computer Science and Machine Learning. In: Robert Eisele [Internet]. 22 Jun 2016 [cited 28 Jul 2021]. Available: https://www.xarg.org/2016/06/the-log-sum-exp-trick-in-machine-learning/"},{"link":"http://tagkopouloslab.ucdavis.edu/?p=2197","name":"Wang X. Numerical stability of binary cross entropy loss and the log-sum-exp trick – Integrative Biology and Predictive Analytics. In: Integrative Biology and Predictive Analytics [Internet]. 26 Sep 2018 [cited 28 Jul 2021]. Available: http://tagkopouloslab.ucdavis.edu/?p=2197"}]},{"type":"til","title":"Managing path using pathlib in Python","url":"https://datumorphism.leima.is/til/programming/python/python-managing-paths-using-pathlib-is-easier/","category":["programming"],"tags":["Python","os"],"description":"It is a convinient package to manage path and files","authors":["LM"],"summary":null,"date":"2021-07-15T00:00:00Z","references":[{"link":"https://docs.python.org/3/library/pathlib.html","name":"pathlib — Object-oriented filesystem paths"},{"link":"https://treyhunner.com/2018/12/why-you-should-be-using-pathlib","name":"# Why you should be using pathlib"}]},{"type":"cards","title":"Box-Cox Transformation","url":"https://datumorphism.leima.is/cards/statistics/box-cox/","category":["statistics"],"tags":["data","transformation","time series"],"description":"The Box-Cox transformation transforms data into Gaussian data, which is especially useful in feature engineering, e.g., fixing irregularities in variances of a time series.","authors":["LM"],"summary":null,"date":"2021-07-13T00:00:00Z","references":[{"link":"http://dx.doi.org/10.1111/j.2517-6161.1964.tb00553.x","name":"Box GEP, Cox DR. An analysis of transformations. J R Stat Soc. 1964;26: 211–243. doi:10.1111/j.2517-6161.1964.tb00553.x"},{"link":"https://www.frontiersin.org/articles/10.3389/fams.2015.00012/full","name":"Vélez JI, Correa JC, Marmolejo-Ramos F. A new approach to the Box–Cox transformation. Frontiers in Applied Mathematics and Statistics. 2015;1: 12. doi:10.3389/fams.2015.00012"},{"link":"https://machinelearningmastery.com/power-transform-time-series-forecast-data-python/","name":"How to Use Power Transforms for Time Series Forecast Data with Python"}]},{"type":"cards","title":"The Hubbard-Stratonovich Identity","url":"https://datumorphism.leima.is/cards/math/hubbard-stratonovich-identity/","category":["Math"],"tags":["Linear Algebra","Calculus"],"description":"Very useful in calculating the partition function","authors":null,"summary":null,"date":"2021-06-17T00:00:00Z","references":[{"key":"Hubbard1959","link":"http://dx.doi.org/10.1103/physrevlett.3.77","name":"Hubbard J. Calculation of Partition Functions. Physical Review Letters. 1959. pp. 77–78. doi:10.1103/physrevlett.3.77"}]},{"type":"cards","title":"Likelihood","url":"https://datumorphism.leima.is/cards/statistics/likelihood/","category":["Statistics"],"tags":["Statistics","Likelihood","Bayes"],"description":"Likelihood is not necessarily a pdf","authors":null,"summary":null,"date":"2021-05-26T00:00:00Z","references":[{"key":"jaynes2003","link":"https://www.cambridge.org/core/books/probability-theory/9CA08E224FF30123304E6D8935CF1A99","name":"Jaynes ET. Probability Theory: The Logic of Science. Cambridge University Press; 2003. doi:10.1017/CBO9780511790423"},{"key":"cs109","link":"https://web.stanford.edu/class/archive/cs/cs109/cs109.1192/reader/11%20Parameter%20Estimation.pdf","name":"Parameter Estimation | CS109: Probability for Computer Scientists"}]},{"type":"cards","title":"Gaussian Integrals","url":"https://datumorphism.leima.is/cards/math/gaussian-integrals/","category":["Math"],"tags":["Integral"],"description":"Gaussian integral is one of the most useful things if one could write it down.","authors":null,"summary":null,"date":"2021-05-11T00:00:00Z","references":[{"link":"https://math.stackexchange.com/questions/126227/reference-for-multidimensional-gaussian-integral","name":"reference for multidimensional gaussian integral"}]},{"type":"cards","title":"Cross Validation","url":"https://datumorphism.leima.is/cards/machine-learning/learning-theories/cross-validation/","category":["Machine Learning::Theories"],"tags":["Learning Theory","Cross Validation"],"description":"Cross validation is a method to estimate the [[risk]]  The Learning Problem The learning problem posed by Vapnik:1 Given a sample: $\\{z_i\\}$ in the probability space $Z$; Assuming a probability measure on the probability space $Z$; Assuming a set of functions $Q(z, \\alpha)$ (e.g. loss functions), where $\\alpha$ is a set of parameters; A risk functional to be minimized by tunning \u0026ldquo;the handles\u0026rdquo; $\\alpha$, $R(\\alpha)$. The risk functional is $$ R(\\alpha) = \\int Q(z, \\alpha) \\,\\mathrm d F(z). $$ A learning problem is the minimization of this risk. Vapnik2000 …   .\nTo perform cross validation, we split the train dataset $\\mathcal D$ into $k$ folds, with each fold denoted as $\\mathcal D_k$.","authors":null,"summary":null,"date":"2021-05-06T00:00:00Z","references":[{"key":"Murphy2012","link":"https://mitpress.mit.edu/books/machine-learning-1","name":"Murphy, K. P. (2012). Probabilistic Machine Learning: An Introduction."}]},{"type":"cards","title":"The Learning Problem","url":"https://datumorphism.leima.is/cards/machine-learning/learning-theories/learning-problem/","category":["Machine Learning::Theories"],"tags":["Learning Theory"],"description":"The learning problem posed by Vapnik:1\n Given a sample: $\\{z_i\\}$ in the probability space $Z$; Assuming a probability measure on the probability space $Z$; Assuming a set of functions $Q(z, \\alpha)$ (e.g. loss functions), where $\\alpha$ is a set of parameters; A risk functional to be minimized by tunning \u0026ldquo;the handles\u0026rdquo; $\\alpha$, $R(\\alpha)$.  The risk functional is\n$$ R(\\alpha) = \\int Q(z, \\alpha) \\,\\mathrm d F(z). $$\nA learning problem is the minimization of this risk.\n   Vapnik2000  Vladimir N. Vapnik. The Nature of Statistical Learning Theory. 2000. doi:10.1007/978-1-4757-3264-1 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","authors":null,"summary":null,"date":"2021-05-06T00:00:00Z","references":[{"key":"Vapnik2000","link":"https://link.springer.com/book/10.1007%2F978-1-4757-3264-1","name":"Vladimir N. Vapnik. The Nature of Statistical Learning Theory. 2000. doi:10.1007/978-1-4757-3264-1"}]},{"type":"cards","title":"Explained Variation","url":"https://datumorphism.leima.is/cards/statistics/explained-variation/","category":["statistics"],"tags":["analysis of variance"],"description":"Using [[Fraser information]]  Fraser Information The Fraser information is $$ I_F(\\theta) = \\int g(X) \\ln f(X;\\theta) , \\mathrm d X. $$ When comparing two models, $\\theta_0$ and $\\theta_1$, the information gain is $$ \\propto (F(\\theta_1) - F(\\theta_0)). $$ The Fraser information is closed related to [[Fisher information]] Fisher Information Fisher information measures the second moment of the model sensitivity with respect to the parameters. , Shannon information, and [[Kullback information]] KL Divergence Kullback–Leibler divergence …   , we can define a relative information gain by a model\n$$ \\rho_C ^2 = 1 - \\frac{ \\exp( - 2 F(\\theta_1) ) }{ \\exp( - 2 F(\\theta_0) ) }, $$","authors":["Lei Ma"],"summary":null,"date":"2021-05-05T18:05:47+02:00","references":[{"link":"https://en.m.wikipedia.org/wiki/Explained_variation","name":"Explained variation"}]},{"type":"cards","title":"Fraser Information","url":"https://datumorphism.leima.is/cards/information/fraser-information/","category":["Information"],"tags":["Information Theory"],"description":"The Fraser information is\n$$ I_F(\\theta) = \\int g(X) \\ln f(X;\\theta) , \\mathrm d X. $$\nWhen comparing two models, $\\theta_0$ and $\\theta_1$, the information gain is\n$$ \\propto (F(\\theta_1) - F(\\theta_0)). $$\nThe Fraser information is closed related to [[Fisher information]]  Fisher Information Fisher information measures the second moment of the model sensitivity with respect to the parameters.   , Shannon information, and [[Kullback information]]  KL Divergence Kullback–Leibler divergence indicates the differences between two distributions   1.\n  Fraser DAS. On Information in Statistics. aoms. 1965;36: 890–896. doi:10.1214/aoms/1177700061\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","authors":["Lei Ma"],"summary":null,"date":"2021-05-05T17:49:12+02:00","references":[{"link":"https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-36/issue-3/On-Information-in-Statistics/10.1214/aoms/1177700061.full","name":"Fraser DAS. On Information in Statistics. aoms. 1965;36: 890–896. doi:10.1214/aoms/1177700061"}]},{"type":"cards","title":"Fisher Information","url":"https://datumorphism.leima.is/cards/information/fisher-information/","category":["Information"],"tags":["Information Theory"],"description":"Fisher information measures the second moment of the model sensitivity with respect to the parameters.","authors":["Lei Ma"],"summary":null,"date":"2021-05-05T17:49:03+02:00","references":[{"link":"http://arxiv.org/abs/1705.01064","name":"Ly A, Marsman M, Verhagen J, Grasman R, Wagenmakers E-J. A Tutorial on Fisher Information. arXiv [math.ST]. 2017. Available: http://arxiv.org/abs/1705.01064"},{"link":"https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-36/issue-3/On-Information-in-Statistics/10.1214/aoms/1177700061.full","name":"Fraser DAS. On Information in Statistics. aoms. 1965;36: 890–896. doi:10.1214/aoms/1177700061"}]},{"type":"wiki","title":"Evidence Lower Bound: ELBO","url":"https://datumorphism.leima.is/wiki/machine-learning/bayesian/elbo/","category":["machine learning"],"tags":["variational method","probabilistic"],"description":"ELBO is an very important concept in variational methods","authors":["LM"],"summary":null,"date":"2021-04-12T00:00:00Z","references":[{"link":"http://legacydirs.umiacs.umd.edu/~xyang35/files/understanding-variational-lower.pdf","name":"Yang X. Understanding the Variational Lower Bound. Available: http://legacydirs.umiacs.umd.edu/~xyang35/files/understanding-variational-lower.pdf"}]},{"type":"cards","title":"Jensen's Inequality","url":"https://datumorphism.leima.is/cards/math/jensens-inequality/","category":["math"],"tags":["statistics"],"description":"Jensen\u0026rsquo;s inequality shows that\n$$ f(\\mathbb E(X)) \\leq \\mathbb E(f(X)) $$\nfor a concave function $f(\\cdot)$.","authors":["LM"],"summary":null,"date":"2021-04-12T00:00:00Z","references":[{"link":"https://en.wikipedia.org/wiki/Jensen%27s_inequality","name":"Jensen's Inequality @ Wikipedia"}]},{"type":"wiki","title":"Valid Confidence Sets in Multiclass and Multilabel Prediction","url":"https://datumorphism.leima.is/wiki/machine-learning/classification/valid-confidence-sets-in-multiclass-multilabel-prediction/","category":["machine learning"],"tags":["statistics","classification","multilabel","multiclass","conformal inference"],"description":"Ask for valid confidence:\n \u0026ldquo;Valid\u0026rdquo;: validate for test data, train data, or the generating process? \u0026ldquo;Confidence\u0026rdquo;: $P(Y \\notin C(X)) \\le \\alpha$  To avoid too much attention on data based validation, a framework called conformal inference was proposed by Vovk et al. in 2005,\n $n$ observations, desired confidence level $1-\\alpha$, construct confidence sets $C(x)$ using conform methods so that the sets capture the underlying the distribution  a new pair $(X_{n+1}, Y_{n+1})$ from the same distribution, $P(Y_{n+1}\\in C(X_{n+1})) \\le 1-\\alpha$    ","authors":["Lei Ma"],"summary":null,"date":"2021-04-08T00:00:00Z","references":[{"link":"https://jmlr.org/papers/v22/20-753.html","name":"Cauchois M, Gupta S, Duchi JC. Knowing what You Know: valid and validated confidence sets in multiclass and multilabel prediction. J Mach Learn Res. 2021;22: 1–42. Available: http://jmlr.org/papers/v22/20-753.html"}]},{"type":"wiki","title":"KL Divergence","url":"https://datumorphism.leima.is/wiki/machine-learning/basics/kl-divergence/","category":["statistics"],"tags":["probability"],"description":"Kullback–Leibler divergence indicates the differences between two distributions","authors":["LM"],"summary":null,"date":"2021-04-05T00:00:00Z","references":[{"link":"https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence","name":"Kullback–Leibler divergence @ Wikipedia"},{"key":"Kurt2017","link":"https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained","name":"Kullback-Leibler Divergence Explained @ COUNT BAYESIE by Will Kurt"}]},{"type":"wiki","title":"Hierarchical Classification","url":"https://datumorphism.leima.is/wiki/machine-learning/classification/hierarchical-classification/","category":["machine learning"],"tags":["machine learning","supervised learning","classification","multilabel","multiclass"],"description":"Hierarchical Classification Problem Hierarchical classification labels involves hierarchical class labels. The hierarchical class labels maybe predefined or inferred. 1\nClass Taxonomy A hierarchical classification problem comes with a class taxonomy.\n \u0026ldquo;IS-A\u0026rdquo; operator: $\\prec$, \u0026ldquo;IS-NOT-A\u0026rdquo; operator: $\\nprec$  A IS-A relationship of the labels $c_a$ class set $C$ is\n one root $R$ in the tree, asymmetric, i.e., $c_i \\prec c_j$ and $c_j\\prec c_i$ can not be both true, anti-reflexive, i.e., $c_i \\nprec c_i$, transitive, i.e., $c_i \\prec c_j$ and $c_j\\prec c_k$ $\\Rightarrow$ $c_i \\prec c_k$.  There are different representations of the hierarchical taxonomies.\n  Figure 2 in Silla2011, showing the difference between tree taxonomy and DAG taxonomy.","authors":["LM"],"summary":null,"date":"2021-03-30T00:00:00Z","references":[{"key":"Silla2011","link":"https://link.springer.com/article/10.1007%2Fs10618-010-0175-9","name":"Silla CN, Freitas AA. A survey of hierarchical classification across different application domains. Data Min Knowl Discov. 2011;22: 31–72. doi:10.1007/s10618-010-0175-9"},{"key":"Read2009","link":"https://doi.org/10.1007/978-3-642-04174-7_17","name":"Read J, Pfahringer B, Holmes G, Frank E. Classifier Chains for Multi-label Classification. 2009. pp. 254–269. doi:10.1007/978-3-642-04174-7_17"},{"link":"https://towardsdatascience.com/https-medium-com-noa-weiss-the-hitchhikers-guide-to-hierarchical-classification-f8428ea1e076","name":"The Hitchhiker’s Guide to Hierarchical Classification"}]},{"type":"wiki","title":"Classifier Chains for Multilabel Classification","url":"https://datumorphism.leima.is/wiki/machine-learning/classification/classifier-chains/","category":["machine learning"],"tags":["machine learning","supervised learning","classification","multilabel"],"description":"Classifier chains is a method to predict hierarchical class labels","authors":["LM"],"summary":null,"date":"2021-03-24T00:00:00Z","references":[{"link":"https://doi.org/10.1007/978-3-642-04174-7_17","name":"Read J, Pfahringer B, Holmes G, Frank E. Classifier Chains for Multi-label Classification. 2009. pp. 254–269."}]},{"type":"til","title":"Binning Data Values using Pandas","url":"https://datumorphism.leima.is/til/programming/pandas/pandas-binning-values/","category":["programming","basics"],"tags":["Python","Pandas","numpy"],"description":"Convert continuous values into bins in pandas","authors":["LM"],"summary":null,"date":"2021-03-10T00:00:00Z","references":[{"link":"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html","name":"pandas.cut"}]},{"type":"til","title":"Deal with Rare Categories Using Pandas","url":"https://datumorphism.leima.is/til/data/deal-with-rare-categories-using-pandas/","category":["data"],"tags":["feature","feature engineering","pandas"],"description":"Deal with rare categories using pandas","authors":["LM"],"summary":null,"date":"2021-03-10T00:00:00Z","references":[{"link":"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mask.html","name":"pandas.DataFrame.mask"}]},{"type":"wiki","title":"ANOVA","url":"https://datumorphism.leima.is/wiki/statistics/anova/","category":["statistics"],"tags":["statistics","variacne","ANOVA","numpy","matplotlib","scipy"],"description":"analysis of variance","authors":["LM"],"summary":null,"date":"2021-03-07T00:00:00Z","references":[{"link":"","name":"Fox, J. Applied Regression Analysis and Generalized Linear Models. (SAGE Publications, 2015)."},{"link":"https://www.analyticsvidhya.com/blog/2018/01/anova-analysis-of-variance","name":"A Simple Introduction to ANOVA (with applications in Excel)"}]},{"type":"cards","title":"McCulloch-Pitts Model","url":"https://datumorphism.leima.is/cards/machine-learning/neural-networks/mcculloch-pitts-model/","category":["Machine Learning"],"tags":["Artificial Neuron","Neural Network","Basics"],"description":"Artificial neuron that separates the state space","authors":["LM"],"summary":null,"date":"2021-02-25T00:00:00Z","references":[{"link":"https://doi.org/10.1007/978-1-4757-3264-1","name":"Vladimir N. Vapnik. 2000. The Nature of Statistical Learning Theory."}]},{"type":"cards","title":"Rosenblatt's Perceptron","url":"https://datumorphism.leima.is/cards/machine-learning/neural-networks/rosenblatt-perceptron/","category":["Machine Learning"],"tags":["Artificial Neuron","Neural Network","Perceptron","Basics"],"description":"Connected perceptrons","authors":["LM"],"summary":null,"date":"2021-02-25T00:00:00Z","references":[{"link":"https://doi.org/10.1007/978-1-4757-3264-1","name":"Vladimir N. Vapnik. 2000. The Nature of Statistical Learning Theory."},{"link":"http://classes.engr.oregonstate.edu/eecs/fall2017/cs534/extra/novikoff-1963.pdf.","name":"Novikoff, Albert B. 1963. On Convergence Proofs for Perceptrons. STANFORD RESEARCH INST MENLO PARK CA."}]},{"type":"cards","title":"ERM: Empirical Risk Minimization","url":"https://datumorphism.leima.is/cards/machine-learning/learning-theories/empirical-risk-minimization/","category":["Machine Learning::Theories"],"tags":["Data","Loss"],"description":"In a [[learning problem]]  The Learning Problem The learning problem posed by Vapnik:1 Given a sample: $\\{z_i\\}$ in the probability space $Z$; Assuming a probability measure on the probability space $Z$; Assuming a set of functions $Q(z, \\alpha)$ (e.g. loss functions), where $\\alpha$ is a set of parameters; A risk functional to be minimized by tunning \u0026ldquo;the handles\u0026rdquo; $\\alpha$, $R(\\alpha)$. The risk functional is $$ R(\\alpha) = \\int Q(z, \\alpha) \\,\\mathrm d F(z). $$ A learning problem is the minimization of this risk. Vapnik2000 …   , empirical risk $R$ is a measurement the goodness of fit based on empirical information.","authors":null,"summary":null,"date":"2021-02-18T00:00:00Z","references":[{"link":"https://mitpress.mit.edu/books/machine-learning-1","name":"Murphy, K. P. (2012). Probabilistic Machine Learning: An Introduction."}]},{"type":"cards","title":"SRM: Structural Risk Minimization","url":"https://datumorphism.leima.is/cards/machine-learning/learning-theories/structural-risk-minimization/","category":["Machine Learning::Theories"],"tags":["Data","Loss"],"description":"[[ERM]]  ERM: Empirical Risk Minimization In a [[learning problem]] The Learning Problem The learning problem posed by Vapnik:1 Given a sample: $\\{z_i\\}$ in the probability space $Z$; Assuming a probability measure on the probability space $Z$; Assuming a set of functions $Q(z, \\alpha)$ (e.g. loss functions), where $\\alpha$ is a set of parameters; A risk functional to be minimized by tunning \u0026ldquo;the handles\u0026rdquo; $\\alpha$, $R(\\alpha)$. The risk functional is $$ R(\\alpha) = \\int Q(z, \\alpha) \\,\\mathrm d F(z). $$ A learning problem …   may lead to overfitting since ERM only selects the model to fit the train data well.","authors":null,"summary":null,"date":"2021-02-18T00:00:00Z","references":[{"link":"https://en.wikipedia.org/wiki/Structural_risk_minimization","name":"Structural risk minimization @ Wikipedia"},{"link":"https://mitpress.mit.edu/books/machine-learning-1","name":"Murphy, K. P. (2012). Probabilistic Machine Learning: An Introduction."}]},{"type":"cards","title":"Coding Theory Concepts","url":"https://datumorphism.leima.is/cards/information/coding-theory-concepts/","category":["Information"],"tags":["Information Theory","Entropy"],"description":"The code function produces code words. The expected length of the code word is limited by the entropy from the source probability $p$.\nThe Shannon information content, aka self-information, is described by\n$$ - \\log_2 p(x=a), $$\nfor the case that $x=a$.\nThe Shannon entropy is the expected information content for the whole sequence with probability distribution $p(x)$,\n$$ \\mathcal H = - \\sum_x p(x\\in X) \\log_2 p(x). $$\nThe Shannon source coding theorem says that for $N$ samples from the source, we can roughly compress it into $N\\mathcal H$.","authors":null,"summary":null,"date":"2021-02-17T00:00:00Z","references":[{"link":"https://mbernste.github.io/posts/sourcecoding/#:~:text=Shannon's%20Source%20Coding%20Theorem%20tells,to%20unambiguously%20communicate%20those%20samples.\u0026text=In%20this%20post%2C%20we%20will%20walk%20through%20Shannon's%20theorem.","name":"Shannon’s Source Coding Theorem (Foundations of information theory: Part 3)"},{"link":"http://www.cs.cmu.edu/~aarti/Class/10704/lec8-srccodingHuffman.pdf","name":"Lecture 8: Source Coding Theorem, Huffman coding by Aarti Singh"},{"link":"https://homepages.dcc.ufmg.br/~msalvim/courses/infotheory/L03_TheSourceCodingTheorem%5Bstill%5D.pdf","name":"The Source Coding Theorem by Mario S. Alvim"}]},{"type":"cards","title":"Empirical Loss","url":"https://datumorphism.leima.is/cards/machine-learning/measurement/empirical-loss/","category":["Machine Learning","Measurement"],"tags":["Data","Model Selection"],"description":"The loss calculated on all the data points","authors":null,"summary":null,"date":"2021-02-06T00:00:00Z","references":null},{"type":"cards","title":"Population Loss","url":"https://datumorphism.leima.is/cards/machine-learning/measurement/population-loss/","category":["Machine Learning","Measurement"],"tags":["Data","Model Selection"],"description":"The loss calculated on all the whole population","authors":null,"summary":null,"date":"2021-02-06T00:00:00Z","references":null},{"type":"cards","title":"Data File Formats","url":"https://datumorphism.leima.is/cards/machine-learning/datatypes/data-file-formats/","category":["data science"],"tags":["data","data-engineering","data-science"],"description":"Data storage is diverse. For data on smaller scales, we are mostly dealing with some data files.\nwork_with_data_files\nEfficiencies and Compressions Parquet Parquet is fast. But\n Don\u0026rsquo;t use json or list of json as columns. Convert them to strings or binary objects if it is really needed.  ","authors":["LM"],"summary":null,"date":"2021-02-02T00:00:00Z","references":[{"link":"https://github.com/apache/parquet-format/blob/master/LogicalTypes.md","name":"Parquet Logical Type Definitions"},{"link":"https://databricks.com/blog/2017/02/23/working-complex-data-formats-structured-streaming-apache-spark-2-1.html","name":"Working with Complex Data Formats with Structured Streaming in Apache Spark 2.1 Part 2 of Scalable Data @ Databricks"}]},{"type":"projects","title":"Machine as a Hologram","url":"https://datumorphism.leima.is/projects/hologram/","category":["Machine Learning"],"tags":["Machine Learning","Data Science","Tutorial"],"description":"Tutorials on machine learning and data science productivity articles","authors":null,"summary":"Tutorials on machine learning and data science productivity articles","date":"2021-01-31T00:00:00Z","references":null},{"type":"wiki","title":"Latent Variable Models","url":"https://datumorphism.leima.is/wiki/machine-learning/bayesian/latent-variable-models/","category":["bayesian","machine learning"],"tags":["latent variable model","variational autoencoder","normalizing flow"],"description":"Latent variable models brings us new insights on identifying the patterns of some sample data.","authors":["LM"],"summary":null,"date":"2021-01-27T00:00:00Z","references":[{"link":"","name":"Trevor Hastie, Robert Tibshirani, J. F. (2004). The Elements of Statistical Learning. Springer Science \u0026 Business Media."},{"link":"","name":"Christpher M. Bishop. (2006). Pattern Recognition and Machine Learning. Springer-Verlag New York."},{"link":"http://akosiorek.github.io/ml/2018/03/14/what_is_wrong_with_vaes.html","name":"What is wrong with VAEs?"}]},{"type":"cards","title":"Reparametrization in Expectation Sampling","url":"https://datumorphism.leima.is/cards/statistics/reparametrization-expectation-sampling/","category":["statistics"],"tags":["normalizing flow"],"description":"Reparametrize the sampling distribution to simplify the sampling","authors":["LM"],"summary":null,"date":"2021-01-20T00:00:00Z","references":null},{"type":"reading","title":"Normalizing Flows: An Introduction and Review of Current Methods","url":"https://datumorphism.leima.is/reading/normalizing-flow-introduction-1908.09257/","category":["theory"],"tags":["machine learning","normalizing flow"],"description":"To generate complicated distributions step by step from a simple and interpretable distribution.","authors":null,"summary":"To generate complicated distributions step by step from a simple and interpretable distribution.","date":"2021-01-17T00:00:00Z","references":[{"link":"https://doi.org/10.1109/TPAMI.2020.2992934","name":"Kobyzev, I., Prince, S., \u0026 Brubaker, M. (2020). Normalizing Flows: An Introduction and Review of Current Methods. IEEE Transactions on Pattern Analysis and Machine Intelligence, 1–1."}]},{"type":"blog","title":"A Simple Machine Learning Project Framework","url":"https://datumorphism.leima.is/blog/data-science/a-simple-machine-learning-framework/","category":["blog"],"tags":["Data Science"],"description":"The HaferML package is developed following this naive framework.\n  A simple almost stateless machine learning framework\n  ","authors":null,"summary":null,"date":"2021-01-12T00:00:00Z","references":null},{"type":"wiki","title":"Basics of Redis","url":"https://datumorphism.leima.is/wiki/computation/basics-of-redis/","category":["Computation"],"tags":["dev","Database","cache","Basics"],"description":"Redis is an in-memory nosql data structure server","authors":["LM"],"summary":null,"date":"2021-01-08T00:00:00Z","references":[{"link":"https://www.youtube.com/watch?v=qr4FVhBTq0I","name":"Redis Tutorial - A Brief Introduction to Redis"},{"link":"https://www.youtube.com/watch?v=Koh6piVaYh0","name":"Redis with Python for Data Science"}]},{"type":"blog","title":"Audiolization of Covid 19 Data in Europe","url":"https://datumorphism.leima.is/blog/ruthless/audiolization-of-covid19-in-eu/","category":["blog"],"tags":["Ruthless Data Scientist"],"description":"Here is an audiolization sound track using a sample of covid19 data in Europe. The audio is the result of the audiorepr Python package I wrote.\n  ","authors":null,"summary":null,"date":"2021-01-03T00:00:00Z","references":[{"link":"https://github.com/emptymalei/audiorepr","name":"audiorepr Package"},{"link":"https://www.youtube.com/watch?v=0fvycIvVnoc","name":"Covid19 Audiolization for FR, IT, ES, DE, PL on Youtube"},{"link":"https://www.bilibili.com/video/BV1Tv411t7cu","name":"Audiolization：欧盟新冠最严重五国每日新增 on Bilibili"}]},{"type":"cards","title":"PREP","url":"https://datumorphism.leima.is/cards/communication/prep/","category":["Communication"],"tags":["Communication","Soft Skills"],"description":"PREP is a framework for making your point.","authors":null,"summary":null,"date":"2021-01-03T00:00:00Z","references":null},{"type":"cards","title":"SCQ-A","url":"https://datumorphism.leima.is/cards/communication/scq-a/","category":["Communication"],"tags":["Communication","Problem Solving","Soft Skills"],"description":"SCQ-A is a framework for problem solving.","authors":null,"summary":null,"date":"2021-01-03T00:00:00Z","references":null},{"type":"cards","title":"WWH","url":"https://datumorphism.leima.is/cards/communication/wwh/","category":["Communication"],"tags":["Communication","Problem Solving","Soft Skills"],"description":"WWH WWH: What (happened) + Why (this happened) + How (to improve) ","authors":null,"summary":null,"date":"2021-01-03T00:00:00Z","references":null},{"type":"til","title":"PyTorch: Initialize Parameters","url":"https://datumorphism.leima.is/til/machine-learning/pytorch/pytorch-initial-params/","category":["machine-learning"],"tags":["Python","PyTorch"],"description":"We can set the parameters in a for loop. We take some of the initialization methods from Lippe1.\nTo set based on the input dimension of the layer ( [[Initialize Artificial Neural Networks]]  Initialize Artificial Neural Networks Initialize a neural network is important for the training and performance. Some initializations simply don\u0026#39;t work, some will degrade the performance of the model. We should choose wisely.   ) (normalized initialization),\nfor name, param in model.named_parameters(): if name.endswith(\u0026#34;.bias\u0026#34;): param.data.fill_(0) else: bound = math.sqrt(6)/math.sqrt(param.shape[0]+param.shape[1]) param.data.uniform_(-bound, bound) or set the parameters based on the input size of each layer\nfor name, param in model.","authors":["Lei Ma"],"summary":null,"date":"2021-01-01T00:00:00Z","references":[{"key":"Lippe","link":"https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial3/Activation_Functions.html","name":"Lippe P. Tutorial 3: Activation Functions — UvA DL Notebooks v1.1 documentation. In: UvA Deep Learning Tutorials [Internet]. [cited 23 Sep 2021]. Available: https://uvadlc-notebooks.readthedocs.io"}]},{"type":"reading","title":"Graph Creation","url":"https://datumorphism.leima.is/reading/grammar-of-graphics/graph-creation/","category":["theory"],"tags":["data visualization"],"description":"Stages Three stages of making a graph:\n Specification Assembly Display  Specification Statistical graphic specifications are expressed in six statements\n DATA: a set of data operations that create variables from datasets TRANS: variable transformations (e.g., rank) SCALE: scale transformations (e.g., log) COORD: a coordinate system (e.g., polar) ELEMENT: graphs (e.g., points) and their aesthetic attributes (e.g., color) GUIDE: one or more guides (axes, legends, etc.)  Assembly Assembling a scene from a specification requires a variety of structures in order to index and link components with each other. One of the structures we can use is a network or a tree.","authors":null,"summary":"","date":"2020-12-29T00:00:00Z","references":[{"link":"https://doi.org/10.1007/0-387-28695-0","name":"Wilkinson, L. (2005). The Grammar of Graphics. Springer-Verlag."}]},{"type":"cards","title":"Multiset, mset or bag","url":"https://datumorphism.leima.is/cards/math/multiset-mset-bag/","category":["Math"],"tags":["Sets","Basics"],"description":"A bag is a set in which duplicate elements are allowed.\nAn ordered bag is a list that we use in programming.","authors":["LM"],"summary":null,"date":"2020-12-27T00:00:00Z","references":[{"link":"https://en.wikipedia.org/wiki/Multiset","name":"Multiset @ Wikipedia"}]},{"type":"til","title":"Python Class Sequential Inheritance","url":"https://datumorphism.leima.is/til/programming/python/python-class-inheritance-sequential/","category":["programming","basics"],"tags":["Python","OOP"],"description":"Sequentially inherit python classes","authors":["LM"],"summary":null,"date":"2020-12-03T00:00:00Z","references":null},{"type":"til","title":"Three dots in Python","url":"https://datumorphism.leima.is/til/programming/python/python-three-dots/","category":["programming","basics"],"tags":["Python"],"description":"Use three dots as placeholder for python empty function","authors":["LM"],"summary":null,"date":"2020-12-03T00:00:00Z","references":null},{"type":"til","title":"Ordered Member Functions of a Class in Python","url":"https://datumorphism.leima.is/til/programming/python/python-class-methods-ordered/","category":["programming","basics"],"tags":["Python","OOP"],"description":"Build an ordered list of methods in a python class by adding attributes to member functions","authors":["LM"],"summary":null,"date":"2020-12-02T00:00:00Z","references":[{"link":"https://stackoverflow.com/a/48146924/1477359","name":"Can I add attributes to class methods in Python?"}]},{"type":"til","title":"Postgres Optimization in JOIN","url":"https://datumorphism.leima.is/til/data/postgres.join-begin-with-smallest-cardinality/","category":["data"],"tags":["Postgres","Warehouse"],"description":"Join tables together starting with the smallest table (table with less cardinality) speeds things up.","authors":["Lei Ma"],"summary":"Join tables together starting with the smallest table (table with less cardinality) speeds things up.","date":"2020-11-28T11:39:21+01:00","references":[{"link":"https://explain.dalibo.com/","name":"Postgres Explain Visualizer 2"},{"link":"https://www.postgresql.org/docs/9.4/using-explain.html","name":"Using EXPLAIN @ Postgres documentation"}]},{"type":"til","title":"Deal with NULL in Postgres","url":"https://datumorphism.leima.is/til/data/postgres.deal-with-null/","category":["data"],"tags":["Data Warehouse","Postgres","PGSQL"],"description":"Please deal with null carefully.","authors":["Lei Ma"],"summary":"Please deal with null carefully.","date":"2020-11-26T00:00:00Z","references":null},{"type":"cards","title":"Akaike Information Criterion","url":"https://datumorphism.leima.is/cards/statistics/aic/","category":null,"tags":["Bayesian","Model Selection"],"description":"Suppose we have a model that describes the data generation process behind a dataset. The distribution by the model is denoted as $\\hat f$. The actual data generation process is described by a distribution $f$.\nWe ask the question:\nHow good is the approximation using $\\hat f$?\nTo be more precise, how much information is lost if we use our model dist $\\hat f$ to substitute the actual data generation distribution $f$?\nAIC defines this information loss as\n$$ \\mathrm{AIC} = - 2 \\ln p(y|\\hat\\theta) + 2k $$\n $y$: data set $\\hat\\theta$: parameter of the model that is estimated by maximum-likelihood $\\ln p(y|\\hat\\theta)$: log maximum likelihood (the goodness-of-fit) $k$: number of adjustable model params; $+2k$ is then a penalty.","authors":null,"summary":null,"date":"2020-11-08T00:00:00Z","references":[{"link":"https://en.wikipedia.org/wiki/Akaike_information_criterion#:~:text=The%20Akaike%20information%20criterion%20(AIC,response%20to%20a%20training%20sample.","name":"Akaike Information Criterion @ Wikipedia"},{"link":null,"name":"Vandekerckhove, J., \u0026 Matzke, D. (2015). Model comparison and the principle of parsimony. Oxford Library of Psychology."}]},{"type":"cards","title":"Bayes Factors","url":"https://datumorphism.leima.is/cards/statistics/bayes-factors/","category":null,"tags":["Bayesian","Model Selection"],"description":"$$ \\frac{p(\\mathscr M_1|y)}{ p(\\mathscr M_2|y) } = \\frac{p(\\mathscr M_1)}{ p(\\mathscr M_2) }\\frac{p(y|\\mathscr M_1)}{ p(y|\\mathscr M_2) } $$\nBayes factor\n$$ \\mathrm{BF_{12}} = \\frac{m(y|\\mathscr M_1)}{m(y|\\mathscr M_2)} $$\n $\\mathrm{BF_{12}}$: how many time more likely is model $\\mathscr M_1$ than $\\mathscr M_2$.  ","authors":null,"summary":null,"date":"2020-11-08T00:00:00Z","references":null},{"type":"cards","title":"Bayesian Information Criterion","url":"https://datumorphism.leima.is/cards/statistics/bic/","category":null,"tags":["Bayesian","Model Selection"],"description":"BIC considers the number of parameters and the total number of data records.","authors":null,"summary":null,"date":"2020-11-08T00:00:00Z","references":[{"link":"https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199957996.001.0001/oxfordhb-9780199957996-e-14","name":"Vandekerckhove, J., \u0026 Matzke, D. (2015). Model comparison and the principle of parsimony. Oxford Library of Psychology."}]},{"type":"cards","title":"Fisher Information Approximation","url":"https://datumorphism.leima.is/cards/statistics/fia/","category":null,"tags":["Bayesian","Model Selection","FIA"],"description":"FIA is a method to describe the minimum description length ( [[MDL]]  Minimum Description Length MDL is a measure of how well a model compresses data by minimizing the combined cost of the description of the model and the misfit.   ) of models,\n$$ \\mathrm{FIA} = -\\ln p(y | \\hat\\theta) + \\frac{k}{2} \\ln \\frac{n}{2\\pi} + \\ln \\int_\\Theta \\sqrt{ \\operatorname{det}[I(\\theta)] d\\theta } $$\n $I(\\theta)$: Fisher information matrix of sample size 1. $$I_{i,j}(\\theta) = E\\left( \\frac{\\partial \\ln p(y| \\theta)}{\\partial \\theta_i}\\frac{ \\partial \\ln p (y | \\theta) }{ \\partial \\theta_j } \\right)$$.  ","authors":null,"summary":null,"date":"2020-11-08T00:00:00Z","references":null},{"type":"cards","title":"Kolmogorov Complexity","url":"https://datumorphism.leima.is/cards/statistics/kolmogorov-complexity/","category":null,"tags":["Model Selection"],"description":"Description of Data\n The measurement of complexity is based on the observation that the compressibility of data doesn\u0026rsquo;t depend on the \u0026ldquo;language\u0026rdquo; used to describe the compression process that much. This makes it possible for us to find a universal language, such as a universal computer language, to quantify the compressibility of the data.\nOne intuitive idea is to use a programming language to describe the data. If we have a sequence of data,\n 0,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,\u0026hellip;,9999\n It takes a lot of space if we show the complete sequence. However, our math intuition tells us that this is nothing but a list of consecutive numbers from 0 to 9999.","authors":null,"summary":null,"date":"2020-11-08T00:00:00Z","references":[{"key":"Fortnow2000","link":"https://people.cs.uchicago.edu/~fortnow/papers/kaikoura.pdf","name":"Fortnow, L. (2000). Kolmogorov complexity. (January), 1–14."},{"key":"Grünwald2007","link":"https://ieeexplore.ieee.org/book/6267274","name":"Grünwald, P. D. (2007). The Minimum Description Length Principle. MIT Press."}]},{"type":"cards","title":"Minimum Description Length","url":"https://datumorphism.leima.is/cards/statistics/mdl/","category":["Statistics"],"tags":["MDL","Model Selection"],"description":"MDL is a measure of how well a model compresses data by minimizing the combined cost of the description of the model and the misfit.","authors":null,"summary":null,"date":"2020-11-08T00:00:00Z","references":[{"link":"https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199957996.001.0001/oxfordhb-9780199957996-e-14","name":"Vandekerckhove, J., \u0026 Matzke, D. (2015). Model comparison and the principle of parsimony. Oxford Library of Psychology."},{"key":"Grünwald2007","link":"https://ieeexplore.ieee.org/book/6267274","name":"Grünwald, P. D. (2007). The Minimum Description Length Principle. MIT Press."}]},{"type":"cards","title":"Normalized Maximum Likelihood","url":"https://datumorphism.leima.is/cards/statistics/nml/","category":null,"tags":["Model Selection","NML"],"description":"$$ \\mathrm{NML} = \\frac{ p(y| \\hat \\theta(y)) }{ \\int_X p( x| \\hat \\theta (x) ) dx } $$","authors":null,"summary":null,"date":"2020-11-08T00:00:00Z","references":null},{"type":"blog","title":"Experiments in Biology","url":"https://datumorphism.leima.is/blog/ruthless/experiments-in-biology/","category":["blog"],"tags":["Ruthless Data Scientist"],"description":"  Inspired by @hanlu.io\n  ","authors":null,"summary":null,"date":"2020-11-01T00:00:00Z","references":null},{"type":"blog","title":"The Science Part in Data Science","url":"https://datumorphism.leima.is/blog/ruthless/science-part-in-data-science/","category":["blog"],"tags":["Ruthless Data Scientist"],"description":"graph TD; s1(An Idea)--d1{Is this idea in the current literature?}; d1{Is this idea in the current literature?}--|Yes|b1(Fail); d1{Is this idea in the current literature?}--|No|b2[Weeks of work]; b2[Weeks of work]--b1(Fail);  ","authors":null,"summary":null,"date":"2020-10-31T00:00:00Z","references":null},{"type":"cards","title":"Conditional Probability Table","url":"https://datumorphism.leima.is/cards/statistics/conditional-probability-table/","category":["Math"],"tags":["Statistics","Basics"],"description":"The conditional probability table is also called CPT","authors":null,"summary":null,"date":"2020-10-27T00:00:00Z","references":[{"link":"https://en.wikipedia.org/wiki/Bayes%27_theorem#/media/File:Bayes'_Theorem_2D.svg","name":"Tree Diagram of Bayes' Theorem @ Wikipedia"}]},{"type":"til","title":"Pandas Groupby Does Not Guarantee Unique Content in Groupby Columns","url":"https://datumorphism.leima.is/til/machine-learning/pandas-groupby-caveats/","category":["machine-learning"],"tags":["Python","Pandas"],"description":"Pandas Groupby Does Not Guarantee Unique Content in Groupby Columns, it also considers the datatypes. Dealing with mixed types requires additional attention.","authors":["Lei Ma"],"summary":"Pandas Groupby Does Not Guarantee Unique Content in Groupby Columns, it also considers the datatypes. Dealing with mixed types requires additional attention.","date":"2020-04-20T00:00:00Z","references":null},{"type":"til","title":"== and is in Python","url":"https://datumorphism.leima.is/til/programming/python/python-none/","category":["programming","basics"],"tags":["Python"],"description":"== and is are different","authors":null,"summary":"== and is are different","date":"2020-04-01T00:00:00Z","references":null},{"type":"cards","title":"Bonferroni Correction","url":"https://datumorphism.leima.is/cards/statistics/bonferroni-correction/","category":["Statistics"],"tags":["Statistics","Hypothesis Testing"],"description":"Bonferroni correction is very useful in a multiple comparison problem","authors":null,"summary":null,"date":"2020-04-01T00:00:00Z","references":[{"link":"https://en.wikipedia.org/wiki/Bonferroni_correction","name":"Contributors to Wikimedia projects. Bonferroni correction. In: Wikipedia [Internet]. 22 Feb 2022 [cited 18 Apr 2022]. Available: https://en.wikipedia.org/wiki/Bonferroni_correction"},{"link":"https://en.wikipedia.org/wiki/Multiple_comparisons_problem","name":"Contributors to Wikimedia projects. Multiple comparisons problem. In: Wikipedia [Internet]. 11 Apr 2022 [cited 18 Apr 2022]. Available: https://en.wikipedia.org/wiki/Multiple_comparisons_problem"}]},{"type":"cards","title":"Multiple Comparison Problem","url":"https://datumorphism.leima.is/cards/statistics/multiple-comparison-problem/","category":["Statistics"],"tags":["Statistics","Hypothesis Testing"],"description":"In a multiple comparisons problem, we deal with multiple statistical tests simultaneously.\nExamples We see such problems a lot in IT companies. Suppose we have a website and would like to test if a new design of a button can lead to some changes in five different KPIs (e.g., view-to-click rate, click-to-book rate, \u0026hellip;).\nIn multi-horizon time series forecasting, we sometimes choose to forecast multiple future data points in one shot. To properly find the confidence intervals of our predictions, one approach is the so called conformal prediction method. This becomes a multiple comparisons problem because we have to tell if we can reject at least one true null hypothesis.","authors":null,"summary":null,"date":"2020-04-01T00:00:00Z","references":[{"link":"https://en.wikipedia.org/wiki/Multiple_comparisons_problem","name":"Contributors to Wikimedia projects. Multiple comparisons problem. In: Wikipedia [Internet]. 11 Apr 2022 [cited 18 Apr 2022]. Available: https://en.wikipedia.org/wiki/Multiple_comparisons_problem"},{"key":"Zeni2020","link":"http://arxiv.org/abs/2005.07972","name":"Zeni G, Fontana M, Vantini S. Conformal Prediction: a Unified Review of Theory and New Challenges. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2005.07972"}]},{"type":"cards","title":"Arcsine Distribution","url":"https://datumorphism.leima.is/cards/statistics/distributions/arcsine/","category":["Statistics"],"tags":["Statistics","Distributions"],"description":"Arcsine Distribution The PDF is\n$$ \\frac{1}{\\pi\\sqrt{x(1-x)}} $$\nfor $x\\in [0,1]$.\nIt can also be generalized to\n$$ \\frac{1}{\\pi\\sqrt{(x-1)(b-x)}} $$\nfor $x\\in [a,b]$.\nVisualize ","authors":null,"summary":null,"date":"2020-03-14T00:00:00Z","references":[{"link":"https://en.wikipedia.org/wiki/Arcsine_distribution","name":"Arcsine Distribution @ Wikipedia"},{"link":"https://stdlib.io/docs/api/v0.0.90/@stdlib/stats/base/dists/arcsine","name":"stdlib.js documentation"}]},{"type":"cards","title":"Bernoulli Distribution","url":"https://datumorphism.leima.is/cards/statistics/distributions/bernoulli/","category":["Statistics"],"tags":["Statistics","Distributions"],"description":"Two categories with probability $p$ and $1-p$ respectively.\nFor each experiment, the sample space is $\\{A, B\\}$. The probability for state $A$ is given by $p$ and the probability for state $B$ is given by $1-p$. The Bernoulli distribution describes the probability of $K$ results with state $s$ being $s=A$ and $N-K$ results with state $s$ being $B$ after $N$ experiments,\n$$ P\\left(\\sum_i^N s_i = K \\right) = C _ N^K p^K (1 - p)^{N-K}. $$","authors":null,"summary":null,"date":"2020-03-14T00:00:00Z","references":[{"link":"https://en.wikipedia.org/wiki/Bernoulli_distribution","name":"Bernoulli Distribution @ Wikipedia"}]},{"type":"cards","title":"Beta Distribution","url":"https://datumorphism.leima.is/cards/statistics/distributions/beta/","category":["Statistics"],"tags":["Statistics","Distributions"],"description":"Beta Distribution Interact {% include extras/vue.html %}\n((makeGraph))","authors":null,"summary":null,"date":"2020-03-14T00:00:00Z","references":[{"link":"https://en.wikipedia.org/wiki/Beta_distribution","name":"Beta Distribution @ Wikipedia"},{"link":"https://stdlib.io/docs/api/v0.0.90/@stdlib/stats/base/dists/beta","name":"stdlib.js documentation"}]},{"type":"cards","title":"Binomial Distribution","url":"https://datumorphism.leima.is/cards/statistics/distributions/binomial/","category":["Statistics"],"tags":["Statistics","Distributions"],"description":"The number of successes in $n$ independent events where each trial has a success rate of $p$.\nPMF:\n$$ C_n^k p^k (1-p)^{n-k} $$","authors":null,"summary":null,"date":"2020-03-14T00:00:00Z","references":[{"link":"https://en.wikipedia.org/wiki/Binomial_distribution","name":"Binomial Distribution @ Wikipedia"}]},{"type":"cards","title":"Categorical Distribution","url":"https://datumorphism.leima.is/cards/statistics/distributions/categorical/","category":["Statistics"],"tags":["Statistics","Distributions"],"description":"By generalizing the Bernoulli distribution to $k$ states, we get a categorical distribution. The sample space is $\\{s_1, s_2, \\cdots, s_k\\}$. The corresponding probabilities for each state are $\\{p_1, p_2, \\cdots, p_k\\}$ with the constraint $\\sum_{i=1}^k p_i = 1$.","authors":null,"summary":null,"date":"2020-03-14T00:00:00Z","references":[{"link":"https://en.wikipedia.org/wiki/Categorical_distribution","name":"Categorical Distribution @ Wikipedia"}]},{"type":"cards","title":"Cauchy-Lorentz Distribution","url":"https://datumorphism.leima.is/cards/statistics/distributions/cauchy/","category":["Statistics"],"tags":["Statistics","Distributions"],"description":"Cauchy-Lorentz Distribution  .. ratio of two independent normally distributed random variables with mean zero.\nSource: https://en.wikipedia.org/wiki/Cauchy_distribution\n Lorentz distribution is frequently used in physics.\nPDF:\n$$ \\frac{1}{\\pi\\gamma} \\left( \\frac{\\gamma^2}{ (x-x_0)^2 + \\gamma^2} \\right) $$\nThe median and mode of the Cauchy-Lorentz distribution is always $x_0$. $\\gamma$ is the FWHM.\nVisualize ","authors":null,"summary":null,"date":"2020-03-14T00:00:00Z","references":[{"link":"https://en.wikipedia.org/wiki/Cauchy_distribution","name":"Cauchy Distribution @ Wikipedia"},{"link":"https://stdlib.io/docs/api/v0.0.90/@stdlib/stats/base/dists/cauchy","name":"stdlib.js documentation"}]},{"type":"cards","title":"Gamma Distribution","url":"https://datumorphism.leima.is/cards/statistics/distributions/gamma/","category":["Statistics"],"tags":["Statistics","Distributions"],"description":"Gamma Distribution PDF:\n$$ \\frac{\\beta^\\alpha x^{\\alpha-1} e^{-\\beta x}}{\\Gamma(\\alpha)} $$\nVisualize ","authors":null,"summary":null,"date":"2020-03-14T00:00:00Z","references":[{"link":"https://en.wikipedia.org/wiki/Gamma_distribution","name":"Gamma Distribution @ Wikipedia"},{"link":"https://stdlib.io/docs/api/v0.0.90/@stdlib/stats/base/dists/gamma","name":"stdlib.js documentation"}]},{"type":"cards","title":"Diagnolize Matrices","url":"https://datumorphism.leima.is/cards/math/diagonalize-matrix/","category":["Math"],"tags":["Linear Algebra"],"description":"Diagnolizing a matrix is a transformation using its eigen space.","authors":null,"summary":null,"date":"2020-03-11T00:00:00Z","references":null},{"type":"cards","title":"Mahalanobis Distance","url":"https://datumorphism.leima.is/cards/math/mahalanobis-distance/","category":["Math"],"tags":["Distance","Metric"],"description":"Distance between a point and a distribution by measuring the distance between the point and the mean of the distribution using the coordinate system defined by the principal components.","authors":null,"summary":null,"date":"2020-03-11T00:00:00Z","references":[{"link":"https://en.wikipedia.org/wiki/Mahalanobis_distance","name":"Mahalanobis distance @ Wikipedia"}]},{"type":"cards","title":"Covariance Matrix","url":"https://datumorphism.leima.is/cards/statistics/covariance-matrix/","category":["Math"],"tags":["Statistics","Basics"],"description":"Also known as the second central moment is a measurement of the spread.","authors":null,"summary":null,"date":"2020-03-10T00:00:00Z","references":[{"link":"https://en.wikipedia.org/wiki/Covariance_matrix","name":"Covariance Matrix @ Wikipedia"}]},{"type":"cards","title":"Jackknife Resampling","url":"https://datumorphism.leima.is/cards/statistics/jacknife-resampling/","category":["Statistics"],"tags":["Statistics"],"description":"Jackknife resampling method","authors":null,"summary":null,"date":"2020-01-26T00:00:00Z","references":[{"link":"https://en.wikipedia.org/wiki/Jackknife_resampling","name":"Jackknife Resampling"}]},{"type":"cards","title":"CBOW: Continuous Bag of Words","url":"https://datumorphism.leima.is/cards/machine-learning/embedding/continuous-bag-of-words/","category":["Machine Learning::Embedding"],"tags":["Word2vec"],"description":"Use the context to predict the center word","authors":null,"summary":null,"date":"2020-01-16T00:00:00Z","references":[{"key":"mikolov2013","link":"https://arxiv.org/abs/1301.3781","name":"Mikolov, T., Chen, K., Corrado, G., \u0026 Dean, J. (2013). Efficient estimation of word representations in vector space. arXiv:1301.3781"},{"link":"http://jalammar.github.io/illustrated-word2vec/","name":"The Illustrated Word2vec"}]},{"type":"cards","title":"Data Types","url":"https://datumorphism.leima.is/cards/machine-learning/datatypes/data-types/","category":["Machine Learning"],"tags":["Data","Data Types"],"description":"","authors":null,"summary":null,"date":"2020-01-16T00:00:00Z","references":[{"link":"https://en.wikipedia.org/wiki/Level_of_measurement","name":"Level of Measurement"}]},{"type":"cards","title":"Gini Impurity","url":"https://datumorphism.leima.is/cards/machine-learning/measurement/gini-impurity/","category":["Machine Learning","Measurement"],"tags":["Data"],"description":"The Gini impurity is a measurement of the impurity of a set.","authors":null,"summary":null,"date":"2020-01-16T00:00:00Z","references":[{"link":"https://victorzhou.com/blog/gini-impurity/#gini-impurity","name":"A Simple Explanation of Gini Impurity by Victor Zhou"},{"link":"https://doi.org/10.1017/CBO9781107298019","name":"Shalev-Shwartz, S., \u0026 Ben-David, S. (2013). Understanding machine learning: From theory to algorithms. Understanding Machine Learning: From Theory to Algorithms."}]},{"type":"cards","title":"Information Gain","url":"https://datumorphism.leima.is/cards/machine-learning/measurement/information-gain/","category":["Machine Learning","Measurement"],"tags":["Data"],"description":"The information is a measurement of the entropy of the dataset.","authors":null,"summary":null,"date":"2020-01-16T00:00:00Z","references":[{"link":"https://doi.org/10.1017/CBO9781107298019","name":"Shalev-Shwartz, S., \u0026 Ben-David, S. (2013). Understanding machine learning: From theory to algorithms. Understanding Machine Learning: From Theory to Algorithms."}]},{"type":"cards","title":"Negative Sampling","url":"https://datumorphism.leima.is/cards/machine-learning/embedding/negative-sampling/","category":["Machine Learning::Embedding"],"tags":["Word2vec"],"description":"negative sampling makes the calculations faster","authors":null,"summary":null,"date":"2020-01-16T00:00:00Z","references":[{"key":"mikolov2013","link":"https://arxiv.org/abs/1301.3781","name":"Mikolov, T., Chen, K., Corrado, G., \u0026 Dean, J. (2013). Efficient estimation of word representations in vector space. arXiv:1301.3781"},{"key":"Goldberg2014","link":"http://arxiv.org/abs/1402.3722","name":"Goldberg Y, Levy O. word2vec Explained: deriving Mikolov et al.’s negative-sampling word-embedding method. arXiv [cs.CL]. 2014. Available: http://arxiv.org/abs/1402.3722"},{"link":"http://jalammar.github.io/illustrated-word2vec/","name":"The Illustrated Word2vec"}]},{"type":"cards","title":"PAC: Probably Approximately Correct","url":"https://datumorphism.leima.is/cards/machine-learning/learning-theories/pac/","category":["Machine Learning::Theories"],"tags":["Data"],"description":"","authors":null,"summary":null,"date":"2020-01-16T00:00:00Z","references":[{"link":"https://doi.org/10.1017/CBO9781107298019","name":"Shalev-Shwartz, S., \u0026 Ben-David, S. (2013). Understanding machine learning: From theory to algorithms. Understanding Machine Learning: From Theory to Algorithms."}]},{"type":"cards","title":"skipgram: Continuous skip-gram","url":"https://datumorphism.leima.is/cards/machine-learning/embedding/continuous-skip-gram/","category":["Machine Learning::Embedding"],"tags":["Word2vec"],"description":"Use the center word to predict the context","authors":null,"summary":null,"date":"2020-01-16T00:00:00Z","references":[{"key":"mikolov2013","link":"https://arxiv.org/abs/1301.3781","name":"Mikolov, T., Chen, K., Corrado, G., \u0026 Dean, J. (2013). Efficient estimation of word representations in vector space. arXiv:1301.3781"},{"link":"http://jalammar.github.io/illustrated-word2vec/","name":"The Illustrated Word2vec"}]},{"type":"reading","title":"Improving Document Ranking with Dual Word Embeddings","url":"https://datumorphism.leima.is/reading/word2vec-in-out-embedding/","category":["theory"],"tags":["machine learning"],"description":"Word2vec produces two embedding spaces, the in-embedding and out-embedding.","authors":null,"summary":"Word2vec produces two embedding spaces, the in-embedding and out-embedding.","date":"2019-10-05T00:00:00Z","references":[{"link":"https://doi.org/10.1145/2872518.2889361","name":"Nalisnick, Eric, Bhaskar Mitra, Nick Craswell, and Rich Caruana. 2016. Improving Document Ranking with Dual Word Embeddings."}]},{"type":"til","title":"Switch statement in Python","url":"https://datumorphism.leima.is/til/programming/python/python-switch-statement/","category":["programming","basics"],"tags":["Python"],"description":"Love switch statement? We can design a switch statement it in python.","authors":null,"summary":"Love switch statement? We can design a switch statement it in python.","date":"2019-08-20T00:00:00Z","references":[{"link":"https://medium.com/better-programming/dispatch-tables-in-python-d37bcc443b0b","name":"Dispatch Tables in Python"}]},{"type":"til","title":"Python Tilde Operator","url":"https://datumorphism.leima.is/til/programming/python/python-tilde-operator/","category":["programming","basics"],"tags":["Python"],"description":"tilde operator may not work as you expected","authors":null,"summary":"tilde operator may not work as you expected","date":"2019-08-15T00:00:00Z","references":[{"link":"https://stackoverflow.com/questions/8305199/the-tilde-operator-in-python/8305291","name":"The tilde operator in Python @StackOverflow"}]},{"type":"til","title":"Arrays and Dicts in MongoDB","url":"https://datumorphism.leima.is/til/programming/database/mongodb-array-and-dict/","category":["programming"],"tags":["MongoDB"],"description":"Array of dictionaries becomes hard to update in MongoDB.","authors":null,"summary":"Array of dictionaries becomes hard to update in MongoDB.","date":"2019-08-14T00:00:00Z","references":null},{"type":"til","title":"eval in Python is Dangerous","url":"https://datumorphism.leima.is/til/programming/python/python-eval/","category":["programming","basics"],"tags":["Python"],"description":"eval is powerful but really dangerous","authors":null,"summary":"eval is powerful but really dangerous","date":"2019-08-13T00:00:00Z","references":null},{"type":"wiki","title":"Dealing with Missing Data in Machine Learning","url":"https://datumorphism.leima.is/wiki/machine-learning/feature-engineering/missing-data/","category":["Machine Learning"],"tags":["Machine Learning","Feature Engineering","Data"],"description":"During feature engineering, we have to deal with missing values.","authors":["LM"],"summary":null,"date":"2019-08-05T00:00:00Z","references":null},{"type":"cards","title":"Cramér's V","url":"https://datumorphism.leima.is/cards/statistics/cramers-v/","category":["Statistics"],"tags":["Statistics","Correlation"],"description":"","authors":null,"summary":null,"date":"2019-07-20T00:00:00Z","references":[{"link":"https://en.wikipedia.org/wiki/Cram%C3%A9r%27s_V","name":"Cramér's V"}]},{"type":"cards","title":"Kendall Tau Correlation","url":"https://datumorphism.leima.is/cards/statistics/kendall-correlation-coefficient/","category":["Statistics"],"tags":["Statistics","Correlation"],"description":"Definition  two series of data: $X$ and $Y$ cooccurance of them: $(x_i, x_j)$, and we assume that $i\u0026lt;j$ concordant: $x_i \u0026lt; x_j$ and $y_i \u0026lt; y_j$; $x_i \u0026gt; x_j$ and $y_i \u0026gt; y_j$; denoted as $C$ discordant: $x_i \u0026lt; x_j$ and $y_i \u0026gt; y_j$; $x_i \u0026gt; x_j$ and $y_i \u0026lt; y_j$; denoted as $D$ neither concordant nor discordant: whenever equal sign happens  Kendall\u0026rsquo;s tau is defined as\n$$ \\begin{equation} \\tau = \\frac{C- D}{\\text{all possible pairs of comparison}} = \\frac{C- D}{n^2/2 - n/2} \\end{equation} $$","authors":null,"summary":null,"date":"2019-07-20T00:00:00Z","references":[{"link":"https://academic.oup.com/biomet/article-abstract/30/1-2/81/176907?redirectedFrom=fulltext","name":"Kendall, M. G. (1938). A new measure of correlation. Biometrika, 30(1–2), 81–93."},{"link":"https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient","name":"Kendall rank correlation coefficient"},{"link":"https://en.wikipedia.org/wiki/Rank_correlation","name":"Rank correlation"}]},{"type":"cards","title":"Bayes' Theorem","url":"https://datumorphism.leima.is/cards/statistics/bayes-theorem/","category":["Math"],"tags":["Statistics","Bayesian"],"description":"Bayes' Theorem is stated as\n$$ P(A\\mid B) = \\frac{P(B \\mid A) P(A)}{P(B)} $$\n $P(A\\mid B)$: likelihood of A given B $P(A)$: marginal probability of A  There is a nice tree diagram for the Bayes' theorem on Wikipedia.\n  Tree diagram of Bayes' theorem\n  ","authors":null,"summary":null,"date":"2019-06-18T00:00:00Z","references":[{"link":"https://en.wikipedia.org/wiki/Bayes%27_theorem#/media/File:Bayes'_Theorem_2D.svg","name":"Tree Diagram of Bayes Theorem @ Wikipedia"}]},{"type":"cards","title":"Canonical Decomposition","url":"https://datumorphism.leima.is/cards/math/canonical-decomposition/","category":["Math"],"tags":["Tensor","Factorization","Linear Algebra"],"description":"Canonical decomposition","authors":null,"summary":null,"date":"2019-06-18T00:00:00Z","references":[{"link":"http://statmath.wu.ac.at/research/talks/resources/talkfreudenthaler.pdf","name":"Matrix and Tensor Factorization from a Machine Learning Perspective"}]},{"type":"cards","title":"Cholesky Decomposition","url":"https://datumorphism.leima.is/cards/math/cholesky-decomposition/","category":["Math"],"tags":["Linear Algebra"],"description":"Decomposing a matrix into two","authors":null,"summary":null,"date":"2019-06-18T00:00:00Z","references":null},{"type":"cards","title":"Khatri-Rao Product","url":"https://datumorphism.leima.is/cards/math/khatri-rao/","category":["Math"],"tags":["Tensor","Linear Algebra"],"description":"$$ \\mathbf{A} \\ast \\mathbf{B} = \\left(\\mathbf{A}_{ij} \\otimes \\mathbf{B}_{ij}\\right)_{ij} $$","authors":null,"summary":null,"date":"2019-06-18T00:00:00Z","references":[{"link":"https://en.wikipedia.org/wiki/Kronecker_product#Khatri%E2%80%93Rao_product","name":"Kronecker product"}]},{"type":"cards","title":"Modes and Slices of Tensors","url":"https://datumorphism.leima.is/cards/math/modes-and-slices-of-tensor/","category":["Math"],"tags":["Tensor"],"description":"Simple decomposition of tensors","authors":null,"summary":null,"date":"2019-06-18T00:00:00Z","references":[{"link":"https://www.sandia.gov/~tgkolda/pubs/pubfiles/SAND2007-6702.pdf","name":"Tensor Decompositions and Applications by Tamara G. Kolda and Brett W. Bader"}]},{"type":"cards","title":"Poisson Process","url":"https://datumorphism.leima.is/cards/statistics/poisson-process/","category":["Statistics"],"tags":["Statistics","Poisson Process"],"description":"  ","authors":null,"summary":null,"date":"2019-06-18T00:00:00Z","references":[{"link":"https://github.com/axelpale/poisson-process","name":"axelpale/poisson-process"}]},{"type":"cards","title":"SVD: Singular Value Decomposition","url":"https://datumorphism.leima.is/cards/math/svd/","category":["Math"],"tags":["Matrix","Factorization","Linear Algebra"],"description":"Given a matrix $\\mathbf X \\to X_{m}^{\\phantom{m}n}$, we can decompose it into three matrices\n$$ X_{m}^{\\phantom{m}n} = U_{m}^{\\phantom{m}k} D_{k}^{\\phantom{k}l} (V_{n}^{\\phantom{n}l} )^{\\mathrm T}, $$\nwhere $D_{k}^{\\phantom{k}l}$ is diagonal.\nHere we have $\\mathbf U$ being constructed by the eigenvectors of $\\mathbf X \\mathbf X^{\\mathrm T}$, while $\\mathbf V$ is being constructed by the eigenvectors of $\\mathbf X^{\\mathrm T} \\mathbf X$ (which is also the reason we keep the transpose).\nI find this slide from Christoph Freudenthaler very useful. The original slide has been added as a reference to this article.\n  SVD visualized by Christoph Freudenthaler\n  ","authors":null,"summary":null,"date":"2019-06-18T00:00:00Z","references":[{"link":"http://statmath.wu.ac.at/research/talks/resources/talkfreudenthaler.pdf","name":"Matrix and Tensor Factorization from a Machine Learning Perspective"}]},{"type":"cards","title":"Tucker Decomposition","url":"https://datumorphism.leima.is/cards/math/tucker-decomposition/","category":["Math"],"tags":["Tensor","Factorization","Linear Algebra"],"description":"Tucker decomposition of a generalization of SVD to higher ranks","authors":null,"summary":null,"date":"2019-06-18T00:00:00Z","references":[{"link":"http://statmath.wu.ac.at/research/talks/resources/talkfreudenthaler.pdf","name":"Matrix and Tensor Factorization from a Machine Learning Perspective"}]},{"type":"cards","title":"Frobenius distance","url":"https://datumorphism.leima.is/cards/math/frobenius-distance/","category":["Math"],"tags":["Distance"],"description":"Frobenius distance between the matrix $X_{n}^{\\phantom{n}k}$ and $H_n^{\\phantom{n}r} W_r^{\\phantom{r}k}$,\n$$ \\lVert X_{n}^{\\phantom{n}k} - H_n^{\\phantom{n}r} W_r^{\\phantom{r}k} \\rVert^2 \\equiv \\sum_{n,k} (X_{n}^{\\phantom{n}k} - H_n^{\\phantom{n}r} W_r^{\\phantom{r}k})^2. $$","authors":null,"summary":null,"date":"2019-06-17T00:00:00Z","references":[{"link":"https://mathworld.wolfram.com/FrobeniusNorm.html","name":"Weisstein. Frobenius Norm. [cited 8 Nov 2021]. Available: https://mathworld.wolfram.com/FrobeniusNorm.html"}]},{"type":"cards","title":"Levenshtein Distance","url":"https://datumorphism.leima.is/cards/math/levenshtein-distance/","category":["Math"],"tags":["Distance","NLP"],"description":"Levenshtein distance calculates the number of operations needed to change one word to another by applying single-character edits (insertions, deletions or substitutions).\nThe reference explains this concept very well. For consistency, I extracted a paragraph from it which explains the operations in Levenshtein algorithm. The source of the following paragraph is the first reference of this article.\nLevenshtein Matrix\n  Cell (0:1) contains red number 1. It means that we need 1 operation to transform M to an empty string. And it is by deleting M. This is why this number is red. Cell (0:2) contains red number 2. It means that we need 2 operations to transform ME to an empty string.","authors":null,"summary":null,"date":"2019-05-19T00:00:00Z","references":[{"link":"https://github.com/trekhleb/javascript-algorithms/tree/master/src/algorithms/string/levenshtein-distance","name":"levenshtein-distance @ trekhleb/javascript-algorithms"}]},{"type":"cards","title":"n-gram","url":"https://datumorphism.leima.is/cards/math/n-gram/","category":["Math"],"tags":["NLP"],"description":"n-gram is a method to split words into set of substring elements so that those can be used to match words.\nExamples Use the following examples to get your first idea about it. I created two columns so that we could compare the n-grams of two different words side-by-side.\nn in n-gram is   Word One  Clean Word: (( sentenceOneWords ))  n-grams: (( sentenceOneWordsnGram ))   Word Two  Clean Word: (( sentenceTwoWords ))  n-grams: (( sentenceTwoWordsnGram ))       /*************************/ /** The function nGram is a copy of https://github.com/words/n-gram , under MIT License **/ nGram.","authors":null,"summary":null,"date":"2019-05-19T00:00:00Z","references":[{"link":"https://github.com/words/n-gram/blob/master/index.js","name":"words/n-gram"}]},{"type":"til","title":"Add New Kernels to Jupyter Notebook in Conda Environment","url":"https://datumorphism.leima.is/til/programming/jupyter-notebook-add-new-kernels-in-conda-env/","category":["programming"],"tags":["Python","Jupyter"],"description":"Python package or python module autoreloading in jupyter notebook","authors":null,"summary":"Python package or python module autoreloading in jupyter notebook","date":"2019-05-12T00:00:00Z","references":null},{"type":"til","title":"Auto-reload Python Packages or Python Modules in Jupyter Notebook","url":"https://datumorphism.leima.is/til/programming/jupyter-notebook-autoreload-python-modules-or-packages/","category":["programming"],"tags":["Python","Jupyter"],"description":"Python package or python module autoreloading in jupyter notebook","authors":null,"summary":"Python package or python module autoreloading in jupyter notebook","date":"2019-05-12T00:00:00Z","references":null},{"type":"til","title":"BigQuery Meta Tables","url":"https://datumorphism.leima.is/til/data/bigquery-meta-tables/","category":["data"],"tags":["Data Warehouse","Big Query"],"description":"Meta tables are very useful when it comes to get bigquery table information programmatically.","authors":["Lei Ma"],"summary":"Meta tables are very useful when it comes to get bigquery table information programmatically.","date":"2019-05-12T00:00:00Z","references":[{"link":"https://cloud.google.com/bigquery/docs/information-schema-tables","name":"Getting table metadata using INFORMATION_SCHEMA"}]},{"type":"til","title":"Calculate Moving Average Using SQL/BigQquery","url":"https://datumorphism.leima.is/til/data/bigquery-moving-average/","category":["data"],"tags":["Data Warehouse","Big Query","SQL"],"description":"Snippet for calculating moving avg using sql/biguqery","authors":["Lei Ma"],"summary":"Snippet for calculating moving avg using sql/biguqery","date":"2019-05-12T00:00:00Z","references":[{"link":"https://cloud.google.com/bigquery/docs/information-schema-tables","name":"Getting table metadata using INFORMATION_SCHEMA"}]},{"type":"til","title":"Generate a Column of Continuous Dates in BigQuery","url":"https://datumorphism.leima.is/til/data/bigquery-generate-continuous-dates-as-a-column/","category":["data"],"tags":["Data Warehouse","Big Query"],"description":"Generate a table with a column of continuous dates","authors":["Lei Ma"],"summary":"Generate a table with a column of continuous dates","date":"2019-05-12T00:00:00Z","references":null},{"type":"til","title":"Get Current User in BigQuery","url":"https://datumorphism.leima.is/til/data/bigquery-get-current-user/","category":["data"],"tags":["Data Warehouse","Big Query"],"description":"BigQuery Current User","authors":["Lei Ma"],"summary":"BigQuery Current User","date":"2019-05-12T00:00:00Z","references":null},{"type":"til","title":"Materialize the Query Result for Performance","url":"https://datumorphism.leima.is/til/data/bigquery-materialize-query-results-for-performance/","category":["data"],"tags":["Data Warehouse","Big Query"],"description":"Materialize the query result for multistage queries to make your query faster and lower the costs.","authors":["Lei Ma"],"summary":"Materialize the query result for multistage queries to make your query faster and lower the costs.","date":"2019-05-12T00:00:00Z","references":null},{"type":"cards","title":"Cosine Similarity","url":"https://datumorphism.leima.is/cards/math/cosine-similarity/","category":["Math"],"tags":["Set","Distance"],"description":"As simple as the inner product of two vectors\n$$ d_{cos} = \\frac{\\vec A}{\\vert \\vec A \\vert} \\cdot \\frac{\\vec B }{ \\vert \\vec B \\vert} $$\nExamples To use cosine similarity, we have to vectorize the words first. There are many different methods to achieve this. For the purpose of illustrating cosine similarity, we use term frequency.\nTerm frequency is the occurrence of the words. We do not deal with duplications so duplicate words will have some effect on the similarity.\nIn principle, we could also use word set for a sentence to remove the effect of duplicate words. In most cases, if a word is repeating, it would indeed make the sentences different.","authors":null,"summary":null,"date":"2019-05-06T00:00:00Z","references":[{"link":"https://en.wikipedia.org/wiki/Cosine_similarity","name":"Cosine Similarity"}]},{"type":"cards","title":"Eigenvalues and Eigenvectors","url":"https://datumorphism.leima.is/cards/math/eigendecomposition/","category":["Math"],"tags":["Linear Algebra","Basics"],"description":"To find the eigenvectors $\\mathbf x$ of a matrix $\\mathbf A$, we construct the eigen equation\n$$ \\mathbf A \\mathbf x = \\lambda \\mathbf x, $$\nwhere $\\lambda$ is the eigenvalue.\nWe rewrite it in the components form,\n$$ \\begin{equation} A_{ij} x_j = \\lambda x_i. \\label{eqn-eigen-decomp-def} \\end{equation} $$\nMathematically speaking, it is straightforward to find the eigenvectors and eigenvalues.\nEigenvectors are Special Directions Judging from the definition in Eq.($\\ref{eqn-eigen-decomp-def}$), the eigenvectors do not change direction under the operation of the matrix $\\mathbf A$.\nReconstruct $\\mathbf A$ We can reconstruct $\\mathbf A$ using the eigenvalues and eigenvectors.\nFirst of all, we will construct a matrix of eigenvectors,","authors":null,"summary":null,"date":"2019-05-06T00:00:00Z","references":[{"link":"https://setosa.io/ev/eigenvectors-and-eigenvalues/","name":"Eigenvectors and Eigenvalues @ Explained Visually"}]},{"type":"cards","title":"Jaccard Similarity","url":"https://datumorphism.leima.is/cards/math/jaccard-similarity/","category":["Math"],"tags":["Set","Distance"],"description":"Jaccard index is the ratio of the size of the intersect of the set and the size of the union of the set.\n$$ J(A, B) = \\frac{ \\vert A \\cap B \\vert }{ \\vert A \\cup B \\vert } $$\nJaccard distance $d_J(A,B)$ is defined as\n$$ d_J(A,B) = 1 - J(A,B). $$\nProperties If the two sets are the same, $A=B$, we have $J(A,B)=1$ or $d_J(A,B)=0$. We have maximum similarity.\nIf the two sets have nothing in common, we have $J(A,B)=0$ or $d_J(A,B)=1$. We have minimum similarity.\nExamples Sentence One  Word Set: (( sentenceOneWords ))   Sentence Two  Word Set: (( sentenceTwoWords ))    Intersect: (( intersectWords ))   Union: (( unionWords ))   Jaccard Index: (( jaccardIndex ))   Jaccard Distance: (( jaccardDistance ))      Vue.","authors":null,"summary":null,"date":"2019-05-06T00:00:00Z","references":[{"link":"https://en.wikipedia.org/wiki/Jaccard_index","name":"Jaccard index"}]},{"type":"cards","title":"Term Frequency - Inverse Document Frequency","url":"https://datumorphism.leima.is/cards/math/tf-idf/","category":["Math"],"tags":["NLP"],"description":"","authors":null,"summary":null,"date":"2019-05-06T00:00:00Z","references":[{"link":"https://en.wikipedia.org/wiki/Tf%E2%80%93idf","name":"Tf-idf"},{"link":"http://www.tfidf.com/","name":"tf-idf"}]},{"type":"reading","title":"The Art of Data Science","url":"https://datumorphism.leima.is/reading/art-of-data-science/","category":["guides"],"tags":["data science"],"description":"A nice and elegant book on data science","authors":null,"summary":"A nice and elegant book on data science","date":"2019-04-19T00:00:00Z","references":[{"link":null,"name":"The Art of Data Science"},{"link":"https://doi.org/10.1126/science.aaa6146","name":"Leek, J. T., \u0026 Peng, R. D. (2015). What is the question? Science, 347(6228), 1314–1315."}]},{"type":"projects","title":"Awesome Stuff","url":"https://datumorphism.leima.is/projects/awesome/","category":["Awesome"],"tags":["Fun"],"description":"Summarizations, workflows, experiences, fails, etc","authors":null,"summary":"Summarizations, workflows, experiences, fails, etc","date":"2019-04-07T00:00:00Z","references":null},{"type":"projects","title":"Blog Posts","url":"https://datumorphism.leima.is/projects/blog/","category":["Blog"],"tags":["Fun"],"description":"My blog posts for fun.","authors":null,"summary":"My blog posts for fun.","date":"2019-04-07T00:00:00Z","references":null},{"type":"cards","title":"Combinations","url":"https://datumorphism.leima.is/cards/math/combinations/","category":["Math"],"tags":["Combinations","Basics"],"description":"Choose X from N is\n$$ C_N^X = \\frac{N!}{ X! (N-X)! } $$","authors":null,"summary":null,"date":"2019-04-07T00:00:00Z","references":null},{"type":"projects","title":"My Data Wiki","url":"https://datumorphism.leima.is/projects/wiki/","category":["Data Science"],"tags":["Introduction","Machine Learning","Data Science","Statistical Learning"],"description":"A collection of my wiki articles related to data.","authors":null,"summary":"A collection of my wiki articles related to data.","date":"2019-04-07T00:00:00Z","references":null},{"type":"projects","title":"My Knowledge Cards","url":"https://datumorphism.leima.is/projects/cards/","category":["Data Science"],"tags":["Introduction","Machine Learning","Data Science","Statistical Learning"],"description":"A collection of my snippets of knowledge","authors":null,"summary":"A collection of my snippets of knowledge","date":"2019-04-07T00:00:00Z","references":null},{"type":"projects","title":"My Reading Notes","url":"https://datumorphism.leima.is/projects/reading/","category":["Data Science"],"tags":["Machine Learning","Data Science","Statistical Learning"],"description":"A collection of my reading notes","authors":null,"summary":"A collection of my reading notes","date":"2019-04-07T00:00:00Z","references":null},{"type":"projects","title":"TIL","url":"https://datumorphism.leima.is/projects/til/","category":["Data Science"],"tags":["Introduction","Machine Learning","Data Science","Statistical Learning"],"description":"Today I Learned","authors":null,"summary":"Today I Learned","date":"2019-04-07T00:00:00Z","references":null},{"type":"reading","title":"Human Graphical Perception of Quantitative Information in Data Visualization","url":"https://datumorphism.leima.is/reading/graphical-perception/","category":["theory"],"tags":["data visualization"],"description":"Data visualization caveats","authors":null,"summary":"Data visualization caveats","date":"2019-03-17T00:00:00Z","references":[{"link":"http://www.sciencemag.org/cgi/doi/10.1126/science.229.4716.828","name":"Graphical Perception and Graphical Methods for Analyzing Scientific Data"}]},{"type":"til","title":"Add Data Files to Python Package","url":"https://datumorphism.leima.is/til/programming/python/python-package-including-data-file/","category":["programming","basics"],"tags":["Python"],"description":"Add Data Files to Python Package using manifest.in and setup.py","authors":null,"summary":"Add Data Files to Python Package using manifest.in and setup.py","date":"2019-03-13T00:00:00Z","references":[{"link":"https://stackoverflow.com/Questions/1612733/including-non-python-files-with-setup-py","name":"Including non-Python files with setup.py"}]},{"type":"til","title":"Installing requirements.txt in Conda Environments","url":"https://datumorphism.leima.is/til/programming/python/python-anaconda-install-requirements/","category":["programming","basics"],"tags":["Python","Anaconda"],"description":"Why is pip install -r requirements.txt not working in conda?","authors":null,"summary":"Why is pip install -r requirements.txt not working in conda?","date":"2019-03-13T00:00:00Z","references":[{"link":"https://github.com/ContinuumIO/anaconda-issues/issues/1429","name":"Use 'pip install' in the virtual environment created by conda"},{"link":"https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#id2","name":"Managing environments — conda documentation."}]},{"type":"reading","title":"Information Theory and Statistical Mechanics","url":"https://datumorphism.leima.is/reading/statistical-physics-and-information-theory/","category":["theory"],"tags":["intelligence"],"description":"Max entropy principle as a method to infer distributions of statistical systems","authors":null,"summary":"Max entropy principle as a method to infer distributions of statistical systems","date":"2019-03-01T00:00:00Z","references":[{"link":"https://doi.org/10.1103/PhysRev.106.620","name":"Jaynes, E. T. (1957). Information Theory and Statistical Mechanics. Physical Review, 106(4), 620–630."}]},{"type":"til","title":"Flatten 2D List in Python","url":"https://datumorphism.leima.is/til/programming/python/python-flatten-2d-list/","category":["programming","basics"],"tags":["Python"],"description":"Flatten 2D list using sum","authors":null,"summary":"Flatten 2D list using sum","date":"2019-01-23T00:00:00Z","references":[{"link":"https://stackoverflow.com/questions/952914/how-to-make-a-flat-list-out-of-list-of-lists","name":"How to make a flat list out of list of lists"}]},{"type":"til","title":"Python Datetime on Different OS","url":"https://datumorphism.leima.is/til/programming/python/python-datetime-on-different-os/","category":["programming","basics"],"tags":["Python"],"description":"Python datetime on different os behaves inconsistently","authors":null,"summary":"Python datetime on different os behaves inconsistently","date":"2018-12-31T00:00:00Z","references":null},{"type":"til","title":"Python If on Numbers","url":"https://datumorphism.leima.is/til/programming/python/python-if-condition-on-numbers/","category":["programming","basics"],"tags":["Python"],"description":"If on int is dangerous","authors":null,"summary":"If on int is dangerous","date":"2018-12-31T00:00:00Z","references":null},{"type":"til","title":"Python Long String","url":"https://datumorphism.leima.is/til/programming/python/python-long-string/","category":["programming","basics"],"tags":["Python"],"description":"Python long string formatting","authors":null,"summary":"Python long string formatting","date":"2018-12-31T00:00:00Z","references":null},{"type":"til","title":"Python Reliable Path to File","url":"https://datumorphism.leima.is/til/programming/python/python-reliable-path/","category":["programming","basics"],"tags":["Python"],"description":"Find the actual path to file","authors":null,"summary":"Find the actual path to file","date":"2018-12-31T00:00:00Z","references":[{"link":"https://stackoverflow.com/a/4060259","name":"How to reliably open a file in the same directory as a Python script @ StackOverflow"},{"link":"https://docs.python.org/3.7/library/os.html","name":"Miscellaneous operating system interfaces @ The Python Standard Library"},{"link":"https://docs.python.org/3.7/library/os.path.html","name":"os.path — Common pathname manipulations"}]},{"type":"til","title":"VSCode on Mac Long Press Keys Not Repeating","url":"https://datumorphism.leima.is/til/misc/vscode/vscode-on-mac-do-not-repeat/","category":["misc"],"tags":["VSCode","IDE","Programming"],"description":"Enable your key repeat in vscode on mac in the terminal","authors":null,"summary":"Enable your key repeat in vscode on mac in the terminal","date":"2018-12-31T00:00:00Z","references":[{"key":"jjcm","link":"https://stackoverflow.com/questions/39972335/how-do-i-press-and-hold-a-key-and-have-it-repeat-in-vscode","name":"jjcm. How do I press and hold a key and have it repeat in VSCode? In: Stack Overflow [Internet]. [cited 28 Mar 2022]. Available: https://stackoverflow.com/questions/39972335/how-do-i-press-and-hold-a-key-and-have-it-repeat-in-vscode"}]},{"type":"til","title":"Controlled Experiments","url":"https://datumorphism.leima.is/til/statistics/controlled-experiments/","category":["statistics"],"tags":["Statistics","Science"],"description":"The three levels of controlled experiments","authors":null,"summary":"The three levels of controlled experiments","date":"2018-12-04T00:00:00Z","references":null},{"type":"cards","title":"BiPolar Sigmoid","url":"https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-bi-polar-sigmoid/","category":["Neural Networks"],"tags":["Artificial Neuron","Neural Network","Basics","Activation Function"],"description":"BiPolar sigmoid function and its properties","authors":["L Ma"],"summary":null,"date":"2018-11-19T00:00:00Z","references":null},{"type":"cards","title":"Conic Section Function","url":"https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-conic-section-function/","category":["Neural Networks"],"tags":["Artificial Neuron","Neural Network","Basics","Activation Function"],"description":"Conic Section Function and its properties","authors":["L Ma"],"summary":null,"date":"2018-11-19T00:00:00Z","references":[{"key":"Dorffner1994","link":"https://www.tandfonline.com/doi/abs/10.1080/01969729408902340","name":"Dorffner G. UNIFIED FRAMEWORK FOR MLPs AND RBFNs: INTRODUCING CONIC SECTION FUNCTION NETWORKS. Cybern Syst. 1994;25: 511–554. doi:10.1080/01969729408902340"},{"link":"https://link.springer.com/chapter/10.1007%2F11759966_125","name":"Shenouda EAMA. A Quantitative Comparison of Different MLP Activation Functions in Classification. Advances in Neural Networks - ISNN 2006. Springer Berlin Heidelberg; 2006. pp. 849–857. doi:10.1007/11759966_125"}]},{"type":"cards","title":"ELU","url":"https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-elu/","category":["Neural Networks"],"tags":["Artificial Neuron","Neural Network","Basics","Activation Function"],"description":"ELU and its properties","authors":["L Ma"],"summary":null,"date":"2018-11-19T00:00:00Z","references":[{"link":"https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial3/Activation_Functions.html","name":"Lippe P. Tutorial 3: Activation Functions — UvA DL Notebooks v1.1 documentation. In: UvA Deep Learning Tutorials [Internet]. [cited 23 Sep 2021]. Available: https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial3/Activation_Functions.html"}]},{"type":"cards","title":"Hyperbolic Tanh","url":"https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-hyperbolic-tangent/","category":["Neural Networks"],"tags":["Artificial Neuron","Neural Network","Basics","Activation Function"],"description":"Tanh function and its properties","authors":["L Ma"],"summary":null,"date":"2018-11-19T00:00:00Z","references":null},{"type":"cards","title":"Leaky ReLu","url":"https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-leaky-relu/","category":["Neural Networks"],"tags":["Artificial Neuron","Neural Network","Basics","Activation Function"],"description":"Leaky ReLu and its properties","authors":["L Ma"],"summary":null,"date":"2018-11-19T00:00:00Z","references":[{"link":"https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial3/Activation_Functions.html","name":"Lippe P. Tutorial 3: Activation Functions — UvA DL Notebooks v1.1 documentation. In: UvA Deep Learning Tutorials [Internet]. [cited 23 Sep 2021]. Available: https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial3/Activation_Functions.html"}]},{"type":"cards","title":"Radial Basis Function","url":"https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-radial-basis-function/","category":["Neural Networks"],"tags":["Artificial Neuron","Neural Network","Basics","Activation Function"],"description":"Radial Basis Function function and its properties","authors":["L Ma"],"summary":null,"date":"2018-11-19T00:00:00Z","references":null},{"type":"cards","title":"ReLu","url":"https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-relu/","category":["Neural Networks"],"tags":["Artificial Neuron","Neural Network","Basics","Activation Function"],"description":"Rectified Linear Unit, aka ReLu, and its properties","authors":["L Ma"],"summary":null,"date":"2018-11-19T00:00:00Z","references":[{"link":"https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial3/Activation_Functions.html","name":"Lippe P. Tutorial 3: Activation Functions — UvA DL Notebooks v1.1 documentation. In: UvA Deep Learning Tutorials [Internet]. [cited 23 Sep 2021]. Available: https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial3/Activation_Functions.html"}]},{"type":"cards","title":"Swish","url":"https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-swish/","category":["Neural Networks"],"tags":["Artificial Neuron","Neural Network","Basics","Activation Function"],"description":"Swish and its properties","authors":["L Ma"],"summary":null,"date":"2018-11-19T00:00:00Z","references":[{"link":"https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial3/Activation_Functions.html","name":"Lippe P. Tutorial 3: Activation Functions — UvA DL Notebooks v1.1 documentation. In: UvA Deep Learning Tutorials [Internet]. [cited 23 Sep 2021]. Available: https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial3/Activation_Functions.html"}]},{"type":"cards","title":"Uni-Polar Sigmoid","url":"https://datumorphism.leima.is/cards/machine-learning/neural-networks/activation-uni-polar-sigmoid/","category":["Neural Networks"],"tags":["Artificial Neuron","Neural Network","Basics","Activation Function"],"description":"Uni-polar sigmoid function and its properties","authors":["L Ma"],"summary":null,"date":"2018-11-19T00:00:00Z","references":null},{"type":"reading","title":"Schaum's Outline of Theories and Problems of Elements of Statistics I and II","url":"https://datumorphism.leima.is/reading/elements-of-statistics/","category":["theory"],"tags":["statistics","basics"],"description":"The basics and all of modern statistics","authors":null,"summary":"The basics and all of modern statistics","date":"2018-11-01T00:00:00Z","references":[{"link":null,"name":"Schaum's Outline of Theories and Problems of Elements of Statistics I and II, by Ruth Bernstein and Stephen Bernstein"}]},{"type":"til","title":"Pandas with MultiProcessing","url":"https://datumorphism.leima.is/til/programming/pandas/pandas-parallel-multiprocessing/","category":["programming","basics"],"tags":["Python","Pandas"],"description":"Adding new data to dataframe using multiprocessing","authors":null,"summary":null,"date":"2018-09-09T00:00:00Z","references":null},{"type":"blog","title":"Beer and Life Expectancy","url":"https://datumorphism.leima.is/blog/ruthless/beer-and-life-expectancy/","category":["blog"],"tags":["Ruthless Data Scientist"],"description":"This is a post of no analysis at all. Everything in this post is meant for fun.\nI moved to Germany a few weeks ago and one of the most astonishing things I noticed is that everyone is drinking so much. Yet the life expectance of Germany is pretty high. So I performed this \u0026ldquo;analysis\u0026rdquo; for fun.\n  Life expectancy vs beer consumption (L) per capita per year. Data obtained from wikipediaList of countries by life expectancy and List of countries by beer consumption per capita.\n  As you might be curious about the life expectancy vs the life expectancy, I also made one plot about it.","authors":null,"summary":null,"date":"2018-08-08T00:00:00Z","references":null},{"type":"reading","title":"Data Mining: Concepts and Techniques","url":"https://datumorphism.leima.is/reading/data-mining/","category":["theory"],"tags":["data mining"],"description":"How data mining was done in the past","authors":null,"summary":"How data mining was done in the past","date":"2018-08-01T00:00:00Z","references":[{"link":"https://www.amazon.com/Data-Mining-Concepts-Techniques-Management/dp/0123814790","name":"Data Mining: Concepts and Techniques"}]},{"type":"til","title":"Fitt's Law","url":"https://datumorphism.leima.is/til/misc/fitts-law/","category":["misc"],"tags":["Visual"],"description":"How fast can you move your mouse to target","authors":null,"summary":"How fast can you move your mouse to target","date":"2018-07-22T00:00:00Z","references":null},{"type":"til","title":"Copy Scalars and Lists in Python","url":"https://datumorphism.leima.is/til/programming/python/python-copy-value-or-address/","category":["programming","basics"],"tags":["Python"],"description":"Python copy values of scalars but addresses of lists","authors":null,"summary":"Python copy values of scalars but addresses of lists","date":"2018-07-03T00:00:00Z","references":null},{"type":"til","title":"Certificate Errors in urllib","url":"https://datumorphism.leima.is/til/data/python-urllib-ssl/","category":["data"],"tags":["Web","Web Scraping"],"description":"Dealing with errors when scraping data","authors":["Lei Ma"],"summary":"Dealing with errors when scraping data","date":"2018-06-25T00:00:00Z","references":[{"link":"https://www.youtube.com/watch?v=mhaHWiSPxxE","name":"Worked Example: BeautifulSoup (Chapter 12)"}]},{"type":"til","title":"Calculated Columns in Pandas","url":"https://datumorphism.leima.is/til/programming/pandas/pandas-new-column-from-other/","category":["programming","basics"],"tags":["Python","Pandas"],"description":"Create new columns in pandas","authors":null,"summary":"Create new columns in pandas","date":"2018-05-20T00:00:00Z","references":null},{"type":"til","title":"tree in Linux","url":"https://datumorphism.leima.is/til/programming/trees/","category":["programming","basics"],"tags":["Linux"],"description":"Trees in computer science","authors":null,"summary":"Trees in computer science","date":"2018-03-20T00:00:00Z","references":null},{"type":"til","title":"Heap on Mac and Linux","url":"https://datumorphism.leima.is/til/programming/cpp/cpp-heap-mac-linux-diff/","category":["programming"],"tags":["Linux","Mac","C++"],"description":"Some caveats about heap on mac and linux","authors":null,"summary":"Some caveats about heap on mac and linux","date":"2017-09-26T00:00:00Z","references":null},{"type":"til","title":"C++ int Multiplication","url":"https://datumorphism.leima.is/til/programming/cpp/cpp-int-multiply/","category":["programming"],"tags":["C++"],"description":"int multiplication in C++ should be processed with caution.","authors":null,"summary":"int multiplication in C++ should be processed with caution.","date":"2017-09-21T00:00:00Z","references":null},{"type":"til","title":"CMake Usage","url":"https://datumorphism.leima.is/til/programming/cmake-usage/","category":["programming"],"tags":["C++"],"description":"How to use CMake to generate makefiles","authors":null,"summary":"How to use CMake to generate makefiles","date":"2017-09-21T00:00:00Z","references":null},{"type":"til","title":"Allocating Memory for Multidimensional Array in C++","url":"https://datumorphism.leima.is/til/programming/cpp/cpp-allocating-memory-multidimensional-array/","category":["programming"],"tags":["C++"],"description":"Some caveats","authors":null,"summary":"Some caveats","date":"2017-09-14T00:00:00Z","references":null},{"type":"til","title":"C++ range-for-statement","url":"https://datumorphism.leima.is/til/programming/cpp/cpp-range-for-statement/","category":["programming"],"tags":["C++"],"description":"In C++ we can use range-for-statement","authors":null,"summary":"In C++ we can use range-for-statement","date":"2017-09-12T00:00:00Z","references":null},{"type":"til","title":"List All Folders in Linux or Mac","url":"https://datumorphism.leima.is/til/programming/linux-mac-list-all-folders/","category":["programming"],"tags":["Linux"],"description":"Using ls and tree commands to list folders only","authors":null,"summary":"Using ls and tree commands to list folders only","date":"2017-08-01T00:00:00Z","references":null},{"type":"til","title":"Python Default Parameters Tripped Me Up","url":"https://datumorphism.leima.is/til/programming/python/python-default-parameters-mutable/","category":["programming","basics"],"tags":["Python"],"description":"Python default parameters might be changed with each run","authors":null,"summary":"Python default parameters might be changed with each run","date":"2017-06-03T00:00:00Z","references":[{"link":"http://effbot.org/zone/default-values.htm","name":"Default Parameter Values in Python"}]},{"type":"til","title":"Some Tests on Matplotlib Backends","url":"https://datumorphism.leima.is/til/programming/matplotlib-backend/","category":["programming"],"tags":["Python","Matplotlib"],"description":"Matplotlib provides many different backends","authors":null,"summary":"Matplotlib provides many different backends","date":"2017-05-23T00:00:00Z","references":null},{"type":"til","title":"Mathematica Provides Great PlotTheme Options","url":"https://datumorphism.leima.is/til/programming/mathematica/mathematica-plottheme/","category":["programming"],"tags":["Mathematica"],"description":"Amazingly, Mathematica provides an option for plot that automatically generates beautiful plots.","authors":null,"summary":"Amazingly, Mathematica provides an option for plot that automatically generates beautiful plots.","date":"2017-05-19T00:00:00Z","references":null},{"type":"til","title":"Turn a Series Expansion into Function in Mathematica","url":"https://datumorphism.leima.is/til/programming/mathematica/mathematica-turn-series-into-function/","category":["programming"],"tags":["Mathematica"],"description":"Turn a series expansion in Mathematica into a function","authors":null,"summary":"Turn a series expansion in Mathematica into a function","date":"2017-05-15T00:00:00Z","references":[{"link":"https://reference.wolfram.com/language/tutorial/ConvertingPowerSeriesToNormalExpressions.html","name":"Converting Power Series to Normal Expressions"}]},{"type":"reading","title":"Overcoming catastrophic forgetting in neural networks","url":"https://datumorphism.leima.is/reading/overcoming-catastrophic-forgetting-in-neural-networks/","category":["theory"],"tags":["neural network"],"description":"Using a newly defined loss function the authors could implement an idea that achieves the multi-task within one network.","authors":["Lei Ma"],"summary":"Using a newly defined loss function the authors could implement an idea that achieves the multi-task within one network.","date":"2017-05-14T00:00:00Z","references":[{"link":"http://www.pnas.org/content/114/13/3521","name":"Overcoming catastrophic forgetting in neural networks"}]},{"type":"til","title":"Git Asks for Password Whenever I Pull or Push","url":"https://datumorphism.leima.is/til/programming/git/git-ssh-asking-pwd-everytime/","category":["programming"],"tags":["Git"],"description":"My git asks for password every time I pull or push even with ssh configured.","authors":null,"summary":"My git asks for password every time I pull or push even with ssh configured.","date":"2017-05-11T00:00:00Z","references":null},{"type":"til","title":"Command Line Russian Roulette","url":"https://datumorphism.leima.is/til/programming/command-line-russian-roulette/","category":["programming"],"tags":["Linux"],"description":"Play russian roulette in your command line","authors":null,"summary":"Play russian roulette in your command line","date":"2017-05-09T00:00:00Z","references":null},{"type":"til","title":"GNU Screen Key Conflict with Bash","url":"https://datumorphism.leima.is/til/programming/gnu-screen-key-conflict-with-bash/","category":["programming"],"tags":["Linux","Bash"],"description":"GNU screen key conflict with bash can be solved","authors":null,"summary":"GNU screen key conflict with bash can be solved","date":"2017-05-08T00:00:00Z","references":null},{"type":"til","title":"How to Run Mathematica Script in Terminal","url":"https://datumorphism.leima.is/til/programming/run-mathematica-script-in-terminal/","category":["programming","basics"],"tags":["Mathematica"],"description":"Using math -run or wolfram -run we could execute a Mathematica script through ssh in terminal.","authors":null,"summary":"Using math -run or wolfram -run we could execute a Mathematica script through ssh in terminal.","date":"2017-05-08T00:00:00Z","references":null},{"type":"til","title":"GNUPLOT Inline Output in iterm2","url":"https://datumorphism.leima.is/til/programming/gnuplot-iterm2-imgcat/","category":["programming"],"tags":["Visualization","gnuplot"],"description":"Using gnuplot in iterm2 we can output result inside terminal combined with imgcat","authors":null,"summary":"Using gnuplot in iterm2 we can output result inside terminal combined with imgcat","date":"2017-04-07T00:00:00Z","references":null},{"type":"til","title":"Mathematica Exclude Singularities in Plot","url":"https://datumorphism.leima.is/til/programming/mathematica/mathematica-plot-exclude-singularities/","category":["programming"],"tags":["Mathematica"],"description":"Mathematica Plot might include some non-existant lines sometimes, Exclusions is the potion for it.","authors":null,"summary":"Mathematica Plot might include some non-existant lines sometimes, Exclusions is the potion for it.","date":"2017-03-22T00:00:00Z","references":null},{"type":"til","title":"Passing Function Arguments Through Lists in Mathematica","url":"https://datumorphism.leima.is/til/programming/mathematica/mathematica-passing-arguments-through-lists/","category":["programming"],"tags":["Mathematica"],"description":"We can pass a list of arguments using Sequence","authors":null,"summary":"We can pass a list of arguments using Sequence","date":"2017-02-20T00:00:00Z","references":null},{"type":"til","title":"Git Pull with Submodule","url":"https://datumorphism.leima.is/til/programming/git/git-pull-with-submodule/","category":["programming"],"tags":["Git"],"description":"Pull git repo with submodule","authors":null,"summary":"Pull git repo with submodule","date":"2017-02-03T00:00:00Z","references":[{"link":"http://stackoverflow.com/questions/1030169/easy-way-pull-latest-of-all-submodules","name":"Easy way pull latest of all submodules @ stackoverflow.com"}]},{"type":"til","title":"Positioning textblock in LaTeX Beamer","url":"https://datumorphism.leima.is/til/programming/latex-beamer-textblock-position/","category":["programming"],"tags":["LaTeX"],"description":"Positioning textblock in LaTeX Beamer using textpos package and eso pic package","authors":null,"summary":"Positioning textblock in LaTeX Beamer using textpos package and eso pic package","date":"2017-01-17T00:00:00Z","references":null},{"type":"til","title":"Mathematica Different Output Forms","url":"https://datumorphism.leima.is/til/programming/mathematica/mathematica-different-output-forms/","category":["programming"],"tags":["Mathematica"],"description":"Mathematica has many different output forms. Understanding them is extremely helpful when making plots.","authors":null,"summary":"Mathematica has many different output forms. Understanding them is extremely helpful when making plots.","date":"2016-11-28T00:00:00Z","references":null},{"type":"til","title":"Git Branch Options","url":"https://datumorphism.leima.is/til/programming/git/git-branch-details/","category":["programming"],"tags":["Git"],"description":"Some useful options about git branch","authors":null,"summary":"Some useful options about git branch","date":"2016-11-27T00:00:00Z","references":null},{"type":"til","title":"git pull multi remote","url":"https://datumorphism.leima.is/til/programming/git/git-pull-multi-remote/","category":["programming"],"tags":["Git"],"description":"working with multi remote","authors":null,"summary":"working with multi remote","date":"2016-11-22T00:00:00Z","references":null},{"type":"reading","title":"Working Memory and Brain Waves","url":"https://datumorphism.leima.is/reading/working-memory-and-brain-waves/","category":["neuroscience"],"tags":["neuroscience","recurrent network","population model"],"description":"Working memory might be related to the background brain waves from theoretical point of view","authors":["Lei Ma"],"summary":"Working memory might be related to the background brain waves from theoretical point of view","date":"2016-11-20T00:00:00Z","references":[{"link":"http://www.pnas.org/content/110/31/12828.short","name":"Flexible frequency control of cortical oscillations enables computations required for working memory"}]},{"type":"reading","title":"Popularity versus similarity in growing networks","url":"https://datumorphism.leima.is/reading/popularity-vs-similarity/","category":["theory"],"tags":["complex network","graph"],"description":"Introduce geometry into the manifold of complex networks","authors":null,"summary":"Introduce geometry into the manifold of complex networks","date":"2016-11-06T00:00:00Z","references":[{"link":"http://www.nature.com/nature/journal/v489/n7417/abs/nature11459.html","name":"Popularity versus similarity in growing networks"}]},{"type":"til","title":"Formatting Numbers in Python","url":"https://datumorphism.leima.is/til/programming/formating-numbers-python/","category":["programming"],"tags":["Python"],"description":"Formatting numbers in python using format","authors":null,"summary":"Formatting numbers in python using format","date":"2016-10-11T00:00:00Z","references":[{"link":"https://pyformat.info/","name":"PyFormat"}]},{"type":"til","title":"Solving Equations Using Differential Transformation Method","url":"https://datumorphism.leima.is/til/math/differential-transformation-method-solving-equations/","category":["math"],"tags":["Differential Equation"],"description":"Differential transformation method can be used to solve differential equation even integro-differential equations.","authors":null,"summary":"Differential transformation method can be used to solve differential equation even integro-differential equations.","date":"2016-10-11T00:00:00Z","references":[{"link":"http://doi.org/10.1080/0020739X.2013.877609","name":"Munganga, J. M. W., Mwambakana, J. N., Maritz, R., Batubenge, T. a., \u0026 Moremedi, G. M. (2014). Introduction of the differential transform method to solve differential equations at undergraduate level. International Journal of Mathematical Education in Science and Technology, 45(5), 781–794."},{"link":"http://doi.org/10.1016/j.camwa.2008.05.017","name":"Arikoglu, A., \u0026 Ozkol, I. (2008). Solutions of integral and integro-differential equation systems by using differential transform method. Computers and Mathematics with Applications, 56(9), 2411–2417."}]},{"type":"til","title":"The Great Chrome Dev Tool","url":"https://datumorphism.leima.is/til/programming/chrome-dev-tool-usage/","category":["programming"],"tags":["Web","Front-end"],"description":"How to use the chrome dev tool wisely","authors":null,"summary":"How to use the chrome dev tool wisely","date":"2016-09-28T00:00:00Z","references":null},{"type":"til","title":"Start a Simple Server","url":"https://datumorphism.leima.is/til/programming/start-simple-server/","category":["programming","basics"],"tags":["Python","Web"],"description":"With one line of python command","authors":null,"summary":"With one line of python command","date":"2016-09-17T00:00:00Z","references":null},{"type":"til","title":"matplotlib x y limit and aspect ratio","url":"https://datumorphism.leima.is/til/programming/matplotlib-x-y-limit-and-aspect-ratio/","category":["programming","basics"],"tags":["Python","Matplotlib"],"description":"matplotlib x y limit and aspect ratio","authors":null,"summary":"matplotlib x y limit and aspect ratio","date":"2016-07-21T00:00:00Z","references":null},{"type":"til","title":"TOP Command","url":"https://datumorphism.leima.is/til/programming/top/","category":["programming","basics"],"tags":["Linux"],"description":"Some tips about top command","authors":null,"summary":"Some tips about top command","date":"2016-07-21T00:00:00Z","references":null},{"type":"til","title":"Assigning Values to Multiple Variables","url":"https://datumorphism.leima.is/til/programming/python/python-assigning-values-to-multiple-variables/","category":["programming","basics"],"tags":["Python"],"description":"Assigning Values to Multiple Variables","authors":null,"summary":"Assigning Values to Multiple Variables","date":"2015-12-04T00:00:00Z","references":null},{"type":"til","title":"gitignore by file size","url":"https://datumorphism.leima.is/til/programming/git/gitignore-by-file-size/","category":["programming"],"tags":["Git"],"description":"gitignore by file size","authors":null,"summary":"gitignore by file size","date":"2015-12-04T00:00:00Z","references":[{"link":"http://stackoverflow.com/questions/4035779/gitignore-by-file-size","name":"gitignore by file size"}]},{"type":"til","title":"HTML Animations Using CSS: AnimateCSS","url":"https://datumorphism.leima.is/til/programming/html-animate-css/","category":["programming"],"tags":["Web","Front-end"],"description":"HTML Animations Using CSS AnimateCSS","authors":null,"summary":"HTML Animations Using CSS AnimateCSS","date":"2015-12-04T00:00:00Z","references":null},{"type":"til","title":"Import in Python","url":"https://datumorphism.leima.is/til/programming/import-in-python/","category":["programming"],"tags":["Python"],"description":"Import in Python","authors":null,"summary":"Import in Python","date":"2015-12-04T00:00:00Z","references":null},{"type":"til","title":"IPython or Jupyter Notebook Magics","url":"https://datumorphism.leima.is/til/programming/ipython-or-jupyter-notebook-magics/","category":["programming"],"tags":["Python","Jupyter","Bash"],"description":"IPython or Jupyter Notebook Magics","authors":null,"summary":"IPython or Jupyter Notebook Magics","date":"2015-12-04T00:00:00Z","references":null},{"type":"til","title":"LaTeX Automatically Adjust Figure","url":"https://datumorphism.leima.is/til/programming/latex-automatically-adjust-figure/","category":["programming"],"tags":["LaTeX"],"description":"LaTeX Automatically Adjust Figure","authors":null,"summary":"LaTeX Automatically Adjust Figure","date":"2015-12-04T00:00:00Z","references":null},{"type":"til","title":"Mathematica Plot Default Font Style and Ticks Style: BaseStyle","url":"https://datumorphism.leima.is/til/programming/mathematica/mathematica-plot-basestyle-default-font-style-and-ticks-style/","category":["programming"],"tags":["Mathematica"],"description":"Mathematica Plot Default Font Style and Ticks Style BaseStyle","authors":null,"summary":"Mathematica Plot Default Font Style and Ticks Style BaseStyle","date":"2015-12-04T00:00:00Z","references":[{"link":"https://reference.wolfram.com/language/tutorial/FormatsForTextInGraphics.html","name":"Formats for Text in Graphics"}]},{"type":"til","title":"Mathematica Smooth Plot","url":"https://datumorphism.leima.is/til/programming/mathematica/mathematica-smooth-plot/","category":["programming"],"tags":["Mathematica"],"description":"Mathematica Smooth Plot","authors":null,"summary":"Mathematica Smooth Plot","date":"2015-12-04T00:00:00Z","references":null},{"type":"til","title":"Migrating Wordpress to Static","url":"https://datumorphism.leima.is/til/programming/migrating-wordpress-to-static-site/","category":["programming","basics"],"tags":["Web"],"description":"Migrating Wordpress to Static","authors":null,"summary":"Migrating Wordpress to Static","date":"2015-12-04T00:00:00Z","references":null},{"type":"til","title":"Open URL using python using webbrowser module","url":"https://datumorphism.leima.is/til/programming/open-url-using-python-webbrowser-module/","category":["programming","basics"],"tags":["Python","Web"],"description":"Open URL using python using webbrowser module","authors":null,"summary":"Open URL using python using webbrowser module","date":"2015-12-04T00:00:00Z","references":null},{"type":"til","title":"Python Code Style","url":"https://datumorphism.leima.is/til/programming/python/python-code-style/","category":["programming","basics"],"tags":["Python"],"description":"Python Code Style","authors":null,"summary":null,"date":"2015-12-04T00:00:00Z","references":[{"link":"http://docs.python-guide.org/en/latest/writing/style/","name":"Code Style of Python Guide"}]},{"type":"til","title":"Python Creating Lists","url":"https://datumorphism.leima.is/til/programming/python/python-creating-lists/","category":["programming","basics"],"tags":["Python"],"description":"Code Style of Python Guide","authors":null,"summary":"Code Style of Python Guide","date":"2015-12-04T00:00:00Z","references":null},{"type":"til","title":"Python enumertate","url":"https://datumorphism.leima.is/til/programming/python/python-enumerate/","category":["programming","basics"],"tags":["Python"],"description":"Python enumertate function","authors":null,"summary":"Python enumertate function","date":"2015-12-04T00:00:00Z","references":null},{"type":"til","title":"Python List Comprehensions","url":"https://datumorphism.leima.is/til/programming/python/python-list-comprehensions/","category":["programming","basics"],"tags":["Python"],"description":"Python List Comprehensions","authors":null,"summary":"Python List Comprehensions","date":"2015-12-04T00:00:00Z","references":null},{"type":"til","title":"Python Making a List","url":"https://datumorphism.leima.is/til/programming/python/python-making-a-list/","category":["programming","basics"],"tags":["Python"],"description":"Python Making a List","authors":null,"summary":"Python Making a List","date":"2015-12-04T00:00:00Z","references":null},{"type":"til","title":"Python Map vs For in Python","url":"https://datumorphism.leima.is/til/programming/python/python-map-vs-for/","category":["programming","basics"],"tags":["Python"],"description":"Python Map vs For in Python","authors":null,"summary":"Python Map vs For in Python","date":"2015-12-04T00:00:00Z","references":null},{"type":"til","title":"Python Onliner: Filter Prime Numbers","url":"https://datumorphism.leima.is/til/programming/filter-prime-numbers/","category":["programming"],"tags":["Python"],"description":"Python Onliner Filter Prime Numbers","authors":null,"summary":"Python Onliner Filter Prime Numbers","date":"2015-12-04T00:00:00Z","references":null},{"type":"til","title":"Python Stupid numpy.piecewise","url":"https://datumorphism.leima.is/til/programming/python/python-stupid-numpy-piecewise/","category":["programming","basics"],"tags":["Python","numpy"],"description":"Python Stupid numpy.piecewise","authors":null,"summary":"Python Stupid numpy.piecewise","date":"2015-12-04T00:00:00Z","references":null},{"type":"til","title":"Python Various Ways of Writing Loops","url":"https://datumorphism.leima.is/til/programming/python/python-writing-loops/","category":["programming","basics"],"tags":["Python"],"description":"Python Various Ways of Writing Loops","authors":null,"summary":"Python Various Ways of Writing Loops","date":"2015-12-04T00:00:00Z","references":[{"link":"http://forums.udacity.com/questions/1002566/great-python-tricks-for-avoiding-unnecessary-loops-in-your-code","name":"Great Python Tricks for avoiding unnecessary loops in your code"}]},{"type":"til","title":"Run a program in the background on ubuntu","url":"https://datumorphism.leima.is/til/programming/run-program-in-background-ubuntu/","category":["programming","basics"],"tags":["Linux"],"description":"Run a program in the background on ubuntu","authors":null,"summary":"Run a program in the background on ubuntu","date":"2015-12-04T00:00:00Z","references":null},{"type":"til","title":"snakeviz","url":"https://datumorphism.leima.is/til/programming/python/python-profile-snakeviz/","category":["programming","basics"],"tags":["Python"],"description":"Python snakeviz","authors":null,"summary":"Python snakeviz","date":"2015-12-04T00:00:00Z","references":null},{"type":"til","title":"Area Enclosed by a Line","url":"https://datumorphism.leima.is/til/math/area-enclosed-in-a-line/","category":["math"],"tags":["Geometry"],"description":"Calculate the area enclosed by a line","authors":"Lei Ma","summary":"Calculate the area enclosed by a line","date":"2015-02-15T00:00:00Z","references":null},{"type":"til","title":"Eigensystem of A Special Matrix","url":"https://datumorphism.leima.is/til/math/eigensystem-of-a-special-matrix/","category":["math"],"tags":["Linear Algebra"],"description":"Eigenstates of a very special matrix","authors":null,"summary":"Eigenstates of a very special matrix","date":"2015-02-15T00:00:00Z","references":null},{"type":"til","title":"Feynman Trick","url":"https://datumorphism.leima.is/til/math/feynman-tricks/","category":["math"],"tags":["Integration"],"description":"An identity about integral","authors":null,"summary":"An identity about integral","date":"2015-02-15T00:00:00Z","references":null},{"type":"til","title":"Symmetry of second derivatives","url":"https://datumorphism.leima.is/til/math/symmetry-of-second-derivatives/","category":["math"],"tags":["Derivative"],"description":"Symmetry of second derivatives","authors":null,"summary":"Symmetry of second derivatives","date":"2015-02-15T00:00:00Z","references":null},{"type":"wiki","title":null,"url":"https://datumorphism.leima.is/wiki/dynamical-system/integration-of-ode/","category":null,"tags":null,"description":"","authors":null,"summary":null,"date":null,"references":null},{"type":"wiki","title":null,"url":"https://datumorphism.leima.is/wiki/survival-analysis/survival-probability/","category":null,"tags":null,"description":"","authors":null,"summary":null,"date":null,"references":null},{"type":"","title":"About","url":"https://datumorphism.leima.is/about/","category":null,"tags":null,"description":"Datumorphism is my notebook about programming, data scraping, statistics, machine learning, and data visualization.\nJoin our Enki Team Learn, practice, and play together. Programmers are unstoppable.   Intelligence Notebook Notes about neuroscience, machine intelligence, and collective intelligence.   Lei Ma Visit this page if you would like to know more about me.    ","authors":null,"summary":null,"date":null,"references":null},{"type":"awesome","title":"Cheatsheets","url":"https://datumorphism.leima.is/awesome/cheatsheets/","category":["Cheatsheets"],"tags":null,"description":"Easy references","authors":null,"summary":null,"date":null,"references":null},{"type":"til","title":"Gridlines in Matplotlib","url":"https://datumorphism.leima.is/til/programming/matplotlib-gridlines/","category":["programming"],"tags":["Python","Matplotlib"],"description":"Adding gridlines in matplotlib","authors":null,"summary":"Adding gridlines in matplotlib","date":null,"references":null},{"type":"awesome","title":"Researchers","url":"https://datumorphism.leima.is/awesome/researchers/","category":null,"tags":null,"description":"Curated list of researchers","authors":null,"summary":null,"date":null,"references":null},{"type":"awesome","title":"Tools","url":"https://datumorphism.leima.is/awesome/tools/","category":null,"tags":null,"description":"Curated list of tools","authors":null,"summary":null,"date":null,"references":null},{"type":"","title":"Typography of this Website","url":"https://datumorphism.leima.is/typography/","category":null,"tags":null,"description":"Basic Syntax This website uses kramdown as the basic syntax. However, a lot of html/css/js has been applied to generate some certain contents or styles.\nMath also follows the kramdown syntax.\nNotes div {% highlight html %}\nFigure with Caption {% highlight html %}\n![]({{ site.url }}/assets/programming/chrome-dev-tools-inspect.png) where {{ site.url }} is the configured url of the site.\nAlternatively, we can use the set attributes syntax in kramdown.\n{% highlight md %} This is a paragraph with some class. The class is specified in the end of the paragraph. {: .notes\u0026ndash;warning} {% endhighlight %}\nThe results shows as a paragraph with the corresponding class.","authors":null,"summary":null,"date":null,"references":null}]